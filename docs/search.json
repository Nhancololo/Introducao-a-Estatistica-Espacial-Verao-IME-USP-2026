[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introdução à Estatística Espacial",
    "section": "",
    "text": "Prefácio\nO presente texto constitui as notas de aula do curso de verão sobre Introdução à Estatística Espacial, a ser ministrado no Instituto de Matemática e Ciência da Computação da Universidade de São Paulo (IME-USP). O conteúdo programático abrange os três pilares fundamentais da área: geoestatística, dados de área e processos pontuais, tratados aqui sob uma perspectiva introdutória.\nDada a imprescindibilidade da computação para a aplicação prática dos conceitos, precede-se a apresentação da teoria espacial com uma introdução ao ambiente R e RStudio. Ressalta-se, contudo, que este material possui caráter de notas de aula e não pretende exaurir o tema.\nPara aqueles interessados em um aprofundamento teórico e prático, recomenda-se enfaticamente a obra ANÁLISE DE DADOS ESPACIAIS COM APLICAÇÃOES EM R, de autoria do Professor João Domingos Scalon (Universidade Federal de Lavras - UFLA). Trata-se, até o momento (pelo que sei), da única obra em língua portuguesa que contempla as três grandes subáreas da estatística espacial supracitadas. Aos alunos inscritos no curso de verão de 2026, a Livraria UFLA oferecerá um DESCONTO de 20% na aquisição deste livro, cujo custo já é bastante acessível.\nAos estudantes que buscam incorporar esta disciplina em seus programas de graduação ou pós-graduação, a UFLA oferta regularmente a disciplina de Geoestatística. Adicionalmente, para quem seja econometrista e queira a visão econométrica da estatística espacial o professor André Luis Squarize Chagas da Faculdade de Economia, Administração, Contabilidade e Atuária da Universidade de São Paulo, Butantã (FEA-USP), ministra as disciplinas de econometria espacial I e econometria espacial II. Tais cursos possuem enfoque em dados de área e apresentam rigoroso desenvolvimento matemático e computacional (R/Rstudio).\nNo cenário da literatura internacional, recomenda-se a leitura de Model-based Geostatistics dos professores Paulo Justiniano Ribeiro e Peter Diggle; e Spatial Statistics for Data Science: Theory and Practice with R, da professora Paula Moraga. Outras referências essenciais incluem obras de dos professores Benjamin S. Baumer, Daniel T. Kaplan, and Nicholas J. Horton, Adrian Baddeley, Noel Cressie. Para o estudo de Regressão Ponderada Geograficamente (GWR), sugere-se o livro dos professores Brunsdon, Charlton e Fotheringham, complementada pelos artigos do professor Alan Ricardo da Silva da UnB.\nSob a ótica da inferência Bayesiana e não só, destacam-se as contribuições dos professores Dani Gamerman (UFRJ), Alexandra M. Schmidt (McGill University), Marcos Oliveira Prates (UFMG), Guilherme Ludwig (UNICAMP), bem como os livros e artigos do professor Jorge Mateu (Universitat Jaume I). Outro livro que merece menção é a obra dos professores Jorge Kazuo Yamamoto e Paulo M. Barbosa Landim, intitulada Geoestatística: conceitos e aplicações.\nA comunidade científica de estatística espacial é vasta; os nomes aqui citados representam apenas uma fração de excelentes pesquisadores, tanto no Brasil quanto no exterior, cujas obras merecem ser exploradas.\n\n\n\n\n\n\nImportanteNota sobre Direitos Autorais e Citação\n\n\n\nO conteúdo da 5  Processos Pontuais é baseado na dissertação de mestrado do Alex Monito Nhancololo, desenvolvida sob orientação do Prof. Dr. João Domingos Scalon na Universidade Federal de Lavras (UFLA). Caso utilize informações ou ilustrações deste capítulo, por favor, cite a fonte original: Nhancololo (2024).\n\n\n\n\n\n\nNhancololo, A. M. 2024. “Processos pontuais espaciais univariados aplicados à distribuição de espécies arbóreas em florestas naturais”. Dissertação (Mestrado em Estatística e Experimentação Agropecuária), Lavras: Universidade Federal de Lavras.",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introdução ao R/RStudio",
    "section": "",
    "text": "1.1 O Ecossistema R vs RStudio\nA leitura desta seção é fundamental para compreender a base computacional deste material. Utilizamos o ambiente R/RStudio em todas as análises aqui apresentadas.\nAinda que existam alternativas no mercado como Python, SPSS, julia, Jamovi, Excel, etc. A escolha pelo R para o ensino de Estatística (especialmente a Espacial) é deliberada. Além de ser um software gratuito, de código livre (open source) e criado para computação estatística e gráfica, o R destaca-se pela vasta biblioteca de pacotes dedicados à fronteira do conhecimento científico.\nUma distinção fundamental para quem está começando:\nO RStudio divide-se em quatro painéis principais que você deve conhecer (Figura 1.1):",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#o-ecossistema-r-vs-rstudio",
    "href": "intro.html#o-ecossistema-r-vs-rstudio",
    "title": "1  Introdução ao R/RStudio",
    "section": "",
    "text": "R: é uma linguagem de programação e um ambiente de software livre usado para analisar dados, criar gráficos e aplicar métodos estatísticos. Foi criado por Ross Ihaka e Robert Gentleman na Universidade de Auckland, Nova Zelândia, em 1993, inspirado na linguagem S. Hoje, é muito utilizado por estatísticos, cientistas de dados e pesquisadores em várias áreas, contando com uma grande comunidade que desenvolve pacotes e amplia suas funcionalidades.\nRStudio: é um programa que facilita o uso do R. Ele oferece uma interface gráfica amigável, onde o usuário pode escrever e executar códigos, visualizar gráficos, organizar arquivos e acompanhar os objetos criados durante a análise. É um Ambiente de Desenvolvimento Integrado (IDE), ou seja, um espaço que reúne várias ferramentas para programar de forma mais prática e organizada. O RStudio foi desenvolvido pela empresa RStudio, PBC (atualmente Posit Software, PBC), fundada por Joseph J. Allaire, e teve sua primeira versão lançada em 2011.\n\n\n\n\n\n\n\nDicaSaiba mais\n\n\n\nClique aqui para ver o vídeo produzido pela \\(C^2\\) Conexão Ciência. Para mais informações sobre R, Clique aqui.\n\n\n\n\nSource Editor (Editor de Fonte): Onde você escreve seus scripts (.R) e relatórios em Rmarkdown (.Rmd) ou Quarto (.qmd). É aqui que o código é salvo.\nConsole: Onde o código é executado interativamente e os resultados de texto aparecem imediatamente.\nEnvironment (Ambiente): Mostra todos os objetos (dados, variáveis, funções) carregados na memória RAM (Workspace).\nFiles/Plots/Help: Área multifuncional para gerenciamento de arquivos, visualização de gráficos e consulta de documentação.\n\n\n\n\n\n\n\nFigura 1.1: Página de download do R no CRAN.",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#instalação-do-rrstudio",
    "href": "intro.html#instalação-do-rrstudio",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.2 Instalação do R/Rstudio",
    "text": "1.2 Instalação do R/Rstudio\nPara usar o RStudio, você deve primeiro instalar o R.\nPasso 1: Instalar o R\n\nAcesse o site oficial do R: Clique aqui e assista ao vídeo colocado no final destas instruções.\nEscolha o link de download para o seu sistema operacional (Linux, MacOS ou Windows) (veja a Figura 1.2, onde seleciono Windows porque é meu sistema operacional. Se não conhece o seu sistema operacional, veja o vídeo do Ativando o Saber: Clique aqui. Se em algum lugar aparecer Windows, então você usa Windows).\nClique em base (ou na versão mais recente recomendada) para baixar o instalador principal.\nExecute o arquivo baixado e siga as instruções, mantendo as configurações padrão.\n\nAssista ao vídeo 1 da Professora Fernanda Peres: Clique aqui para ver o vídeo.\n\n\n\n\n\n\nFigura 1.2: Página de download do R no CRAN.\n\n\n\nPasso 2: Instalar o RStudio\n\nVisite a página de download do RStudio: Clique aqui.\nA versão gratuita “RStudio Desktop” é a recomendada para iniciantes. Clique no botão de download (a Figura 1.3 mostra isso e o vídeo contém o tutorial).\nO site geralmente detecta seu sistema operacional e sugere o instalador correto. Baixe-o.\nExecute o instalador e siga as instruções, mantendo as opções padrão.\n\n\n\n\n\n\n\nFigura 1.3: Página de download do RStudio.\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nAssim que instalar o R e o RStudio, sempre que quiser fazer análises, use o RStudio clicando nele duas vezes. Se não encontrar o RStudio na área de trabalho, pesquise por Rstu em uma das opções abaixo (1 ou 2) mostradas na Figura 1.4.\n\n\n\n\n\n\n\n\nFigura 1.4: Encontrar aplicativo que sumiu.",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#atalhos-de-teclado-essenciais",
    "href": "intro.html#atalhos-de-teclado-essenciais",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.3 Atalhos de teclado essenciais",
    "text": "1.3 Atalhos de teclado essenciais\n\nCtrl+Enter (no sistema operacional Windows) / Cmd+Enter (no Mac): Executa a linha de código atual.\nAlt+-: Insere o operador de atribuição (&lt;-).\nCtrl+Shift+M: Insere o operador Pipe (%&gt;% ou |&gt;).\n\n\n\n\n\n\n\nNotaPipe (%&gt;% ou |&gt;)\n\n\n\nO Pipe transfere o resultado da expressão à esquerda para ser o primeiro argumento da função à direita, transformando a difícil leitura matemática aninhada \\(f(g(x))\\) em uma sequência linear lógica \\(x \\to g \\to f\\). Ele é fundamental para a higiene do código, permitindo ler o script como uma receita (“pegue os dados, e então filtre, e então calcule”), eliminando o excesso de parênteses e variáveis temporárias.\n\n\n\nSem Pipe (Leitura: de dentro para fora, propenso a erro): localize a função central (subset) para filtrar os dados, depois expanda o olhar para calcular a média (mean) desse resultado, e finalmente olhe para as extremidades para aplicar o arredondamento (round) com o argumento 2 que está longe do início.\n\n\n\nCódigo\nround(mean(subset(dados, valor &gt; 10)$valor, na.rm = TRUE), 2)\n\n\n\nCom Pipe (Leitura Lógica: da esquerda para a direita): Pegue os dados, e então filtre onde valor &gt; 10, e então calcule a média, e então arredonde o resultado final para 2 casas, seguindo a ordem lógica e cronológica da execução.\n\n\n\nCódigo\ndados |&gt; \n  subset(valor &gt; 10) |&gt; \n  mean(na.rm = TRUE) |&gt; \n  round(2)\n\n\n\nEsc: Interrompe um comando que está demorando muito ou travou.\nCtrl+Shift+F10: Reinicia a sessão do R (limpa a memória RAM).\nCtrl + Shift + C: Comentar/ignorar algo no Windows/Linux\nCmd + Shift + C: Comentar/ignorar algo no Mac\n\n\n\n\n\n\n\nDica\n\n\n\nAlém de Ctrl + Shift + C ou Cmd + Shift + C, você também pode usar o caractere # para instruir o R a ignorar completamente tudo o que estiver escrito à direita dele na mesma linha. Isso serve para deixar anotações para você mesmo (ou outros programadores) sem interferir na execução do código.\n\n\n\n\nCódigo\nx &lt;- 10\n# x &lt;- 20  &lt;-- Esta linha não vai rodar, o valor continua 10.\ny &lt;- x + 5\n\n\nA Filosofia do objeto\nNo R, você raramente verá uma tela cheia de resultados imediatos após uma análise complexa. O R opera salvando resultados em objetos. Se você roda uma regressão linear, o resultado não é apenas um texto impresso, mas um objeto complexo que pode ser consultado, manipulado e usado como entrada para outras funções posteriormente.",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#fundamentos-da-linguagem-r",
    "href": "intro.html#fundamentos-da-linguagem-r",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.4 Fundamentos da linguagem R",
    "text": "1.4 Fundamentos da linguagem R\nAntes de algoritmos complexos, você precisa entender como o R processa informações básicas.\n\n1.4.1 Ajuda e Documentação\nO R possui um sistema de documentação robusto integrado. Aprenda a pedir ajuda:\n\nhelp(topic): Abre a documentação completa e técnica sobre o tópico especificado. Exemplo, se você quiser saber o que a função str() faz, faça help(str)\n\n\n\nCódigo\nhelp(str) \n\n\n\n?topic: Um atalho rápido para o comando acima (ex: ?mean).\n\n\n\nCódigo\n?str \n\n\n\nhelp.search(\"termo\") ou ??termo: Pesquisa o termo em todo o sistema de ajuda (títulos e descrições), útil quando você não sabe o nome exato da função (ex: ??regression).\n\n\n\nCódigo\nhelp.search(\"str\")\n\n\n\napropos(\"termo\"): Retorna uma lista com os nomes de todos os objetos e funções disponíveis no ambiente atual que contêm o termo (ex: apropos(\"test\")).\n\n\n\nCódigo\napropos(\"str\") #descomente\n\n\n\nhelp.start(): Inicia a versão HTML completa da ajuda no seu navegador padrão.\n\n\n\nCódigo\nhelp.start() #descomente\n\n\n\n\n1.4.2 Operadores e Atribuição\nAtribuição\n\n\n\n\n\n\nImportanteAtenção: O R é case-sensitive\n\n\n\nO R diferencia letras maiúsculas de minúsculas. Isso vale para nomes de funções, variáveis e também para caminhos de arquivos e pastas no seu computador.\n\n\n\n&lt;-: Atribuição à esquerda. É a convenção padrão e histórica do R. Cria uma variável. Se eu quisesse criar um objeto de nome “Alex”, recebendo número 4, temos:\n\n\n\nCódigo\nAlex &lt;- 4\n\n\n\n=: Atribuição à esquerda. Funciona, mas é preferível usar apenas para definir argumentos dentro de funções.\n\n\n\nCódigo\nAlex = 4\n\n\n\n&lt;&lt;-: Superatribuição. Atribui um valor a uma variável no ambiente pai (global), geralmente usado dentro de funções avançadas para alterar variáveis externas.\n\n\n\nCódigo\ntotal &lt;- 100 # Variável definida no ambiente global\n\n# Função com atribuição normal (&lt;-)\nteste_local &lt;- function() {\n  total &lt;- 50  # Cria uma nova variável LOCAL chamada 'total'. A global continua 100.\n}\n\n# Função com superatribuição (&lt;&lt;-)\nteste_global &lt;- function() {\n  total &lt;&lt;- 50 # Altera a variável 'total' que já existe lá fora.\n}\n\n# Executando:\nteste_local()\nprint(total) # Resultado: 100 (não mudou)\n\n\n[1] 100\n\n\nCódigo\nteste_global()\nprint(total) # Resultado: 50 (foi alterada pela superatribuição)\n\n\n[1] 50\n\n\nOperações Aritméticas * +, -, *, /: Soma, Subtração, Multiplicação e Divisão.\n\n\nCódigo\n2 + 4  # Soma\n\n\n[1] 6\n\n\nCódigo\n2 - 4  # Subtração\n\n\n[1] -2\n\n\nCódigo\n2 * 4  # Multiplicação\n\n\n[1] 8\n\n\nCódigo\n2 / 4  # Divisão\n\n\n[1] 0.5\n\n\n\n^ ou **: Exponenciação (potência).\n\n\n\n\n\n\n\nNota\n\n\n\n^ é o mais comum no R, mas ** também funciona).\n\n\n\n\nCódigo\n2 ^ 3  # 2 elevado a 3 = 2x2x2= 8\n\n\n[1] 8\n\n\n\n%%: Módulo (Resto da divisão). Útil para verificar paridade ou ciclos.\n\n\n\nCódigo\n5 %% 2  # Retorna 1 (pois 5 dividido por 2 dá 2 e sobra 1)\n\n\n[1] 1\n\n\n\n%/%: Divisão inteira (Quociente). Descarta a parte decimal.\n\n\n\nCódigo\n5 %/% 2 # Retorna 2 (a parte inteira da divisão)\n\n\n[1] 2\n\n\n\n\n\n\n\n\nImportanteExecução de uma operação\n\n\n\nSe você executar uma operação sem atribuí-la a um objeto (usando &lt;- ou =), o R apenas imprime o resultado no console e o descarta imediatamente. Ele não fica salvo na memória.\n\n\n\nExemplo: Ao rodar apenas 2 + 4, você vê 6 na tela, mas não consegue usar esse 6 em contas futuras. Solução: use sempre nome_do_objeto_da_sua_preferência &lt;- 2 + 4\nSobrescrita de Variáveis: O R não guarda o histórico de uma variável. Se você atribuir um novo valor a um objeto que já existe, o valor antigo é apagado e substituído pelo novo (a última operação prevalece).\n\n\n\nCódigo\nb &lt;- 10      # 'b' vale 10\nb &lt;- 2 + 4   # Agora 'b' vale 6. O 10 foi perdido.\n\n\nLógica e Comparação\nResultam sempre em TRUE ou FALSE. * ==, !=: Igualdade exata e Diferença.\n\n\nCódigo\n10 == 10  # TRUE (10 é igual a 10)\n\n\n[1] TRUE\n\n\nCódigo\n10 != 10  # FALSE (10 não é diferente de 10)\n\n\n[1] FALSE\n\n\nCódigo\n\"R\" == \"Python\" # FALSE\n\n\n[1] FALSE\n\n\n\n&lt;, &gt;, &lt;=, &gt;=: Menor, Maior, Menor ou igual, Maior ou igual.\n\n\n\nCódigo\n5 &gt; 3    # TRUE\n\n\n[1] TRUE\n\n\nCódigo\n5 &lt;= 2   # FALSE\n\n\n[1] FALSE\n\n\nCódigo\nx &lt;- 10\nx &gt;= 10  # TRUE\n\n\n[1] TRUE\n\n\n\n!: Negação (NÃO). Inverte o valor lógico (!TRUE é FALSE).\n\n\n\nCódigo\n!TRUE         # Retorna FALSE\n\n\n[1] FALSE\n\n\nCódigo\n!(5 &gt; 3)      # 5 &gt; 3 é TRUE, logo a negação retorna FALSE\n\n\n[1] FALSE\n\n\nCódigo\nis.na(4)      # Verifica se é Nulo (FALSE)\n\n\n[1] FALSE\n\n\nCódigo\n!is.na(4)     # \"Não é nulo\" (TRUE)\n\n\n[1] TRUE\n\n\n\n&, |: “E” / “OU” vetorizados. Comparam elemento por elemento de dois vetores.\n\n\n\nCódigo\nv1 &lt;- c(TRUE, TRUE, FALSE)\nv2 &lt;- c(FALSE, TRUE, FALSE)\n\nv1 & v2  # Retorna: FALSE TRUE FALSE (Apenas a 2ª posição tem TRUE nos dois)\n\n\n[1] FALSE  TRUE FALSE\n\n\nCódigo\nv1 | v2  # Retorna: TRUE TRUE FALSE (Basta um ser TRUE)\n\n\n[1]  TRUE  TRUE FALSE\n\n\n\n&&, ||: “E” / “OU” de controle. Avaliam apenas o primeiro elemento.\n\n\n\nCódigo\nx &lt;- 10\ny &lt;- 3\ndivisao_inteira &lt;- x %/% y\nresto &lt;- x %% y\nlogica &lt;- (x &gt; 5) & (y &lt; 5) # TRUE E TRUE = TRUE\n\n\nUsados exclusivamente em estruturas de controle como if.\n\n\nCódigo\ntem_saldo &lt;- TRUE\nconta_ativa &lt;- TRUE\n\nif (tem_saldo && conta_ativa) {\n  print(\"Compra aprovada\")\n}\n\n\n[1] \"Compra aprovada\"\n\n\n\n\n1.4.3 Vetores\nNão existem escalares (números soltos) no R. Um número sozinho é, na verdade, um vetor de comprimento 1. Saiba mais sobre vetores clicando aqui\nCriação\n\nc(...): Função “combine” ou “concatenate”. Junta elementos num mesmo vetor.\n\n\n\nCódigo\nnúmeros &lt;- c(10, 20, 30) # Vetor numérico\n\nnomes &lt;- c(\"Ana\", \"Beto\", \"Carla\") # Vetor de texto\n\nmisturado &lt;- c(10, \"Ana\") # Resultado: \"10\", \"Ana\" (Tudo vira texto)\n\n\n\n\n\n\n\n\nNota\n\n\n\nO c é Minúsculo\n\n\n\nfrom:to: Gera uma sequência de inteiros (ex: 1:10).\n\n\n\nCódigo\n1:5   # Retorna: 1 2 3 4 5\n\n\n[1] 1 2 3 4 5\n\n\nCódigo\n5:1   # Retorna: 5 4 3 2 1\n\n\n[1] 5 4 3 2 1\n\n\n\nseq(from, to, by, length): Gera sequências complexas. Permite definir o passo (by) ou o tamanho final desejado (length).\n\n\n\nCódigo\n# Sequência de 0 a 10, pulando de 2 em 2\nseq(from = 0, to = 10, by = 2) # Resultado: 0 2 4 6 8 10\n\n\n[1]  0  2  4  6  8 10\n\n\nCódigo\n# Quero exatos 5 números entre 0 e 10\nseq(from = 0, to = 10, length.out = 5)# Resultado: 0.0  2.5  5.0  7.5  10.0\n\n\n[1]  0.0  2.5  5.0  7.5 10.0\n\n\n\nrep(x, times, each): Replica elementos. times repete o vetor todo; each repete cada elemento individualmente.\n\n\n\nCódigo\nvetor &lt;- c(1, 2)\n\nrep(vetor, times = 3) # Resulta em: 1, 2, 1, 2, 1, 2\n\n\n[1] 1 2 1 2 1 2\n\n\nCódigo\nrep(vetor, each = 3) # Resulta em:  1, 1, 1, 2, 2, 2\n\n\n[1] 1 1 1 2 2 2\n\n\n\n%in%: Verifica se um valor está dentro de um vetor e retornaTRUEouFALSE`\n\n\n\nCódigo\n1 %in% vetor  # 1 está dentro do objeto vetor?\n\n\n[1] TRUE\n\n\nCódigo\nc()\n\n\nNULL\n\n\nA Regra da reciclagem\nO superpoder do R é a vetorização. Se você tentar operar dois vetores de tamanhos diferentes, o R “recicla” (repete) o vetor menor até que ele tenha o tamanho do maior.\n\n\nCódigo\nvalores &lt;- c(10, 20, 30, 40)\npesos &lt;- c(1, 2) \n\nresultado &lt;- valores * pesos # O que acontece internamente: 10*1, 20*2, 30*1, 40*2\nprint(resultado)\n\n\n[1] 10 40 30 80\n\n\n\n\n1.4.4 Tipos e/ou valores especiais e coerção\nO R possui termos específicos para lidar com exceções matemáticas e dados faltantes. É crucial saber diferenciá-los.\n\nNA: Not Available. Representa um dado ausente ou desconhecido. O NA é “contagioso”. A maioria das operações matemáticas envolvendo um NA resultará em NA (pois não se pode somar algo a um valor desconhecido).\n\n\n\n\n\n\n\nImportante\n\n\n\nNunca use == para testar se algo é NA (ex: x == NA retorna NA, e não TRUE ou FALSE). Use sempre a função is.na().\n\n\n\n\nCódigo\nvetor &lt;- c(1, 2, NA, 4)\nmean(vetor)      # Retorna NA (a média de dados desconhecidos é desconhecida)\n\n\n[1] NA\n\n\nCódigo\nis.na(vetor)     # Retorna: FALSE FALSE TRUE FALSE\n\n\n[1] FALSE FALSE  TRUE FALSE\n\n\n\nNaN: Not a Number. Resultado de uma indeterminação matemática. Ocorre quando o cálculo é impossível de ser definido numericamente. Resultado de indeterminações matemáticas (ex: 0/0).\n\n\n\nCódigo\n0 / 0  # Retorna NaN\n\n\n[1] NaN\n\n\n\nInf: Infinito (ex: 1/0).\n\n\n\nCódigo\n10 / 0   # Retorna Inf\n\n\n[1] Inf\n\n\nCódigo\n-5 / 0   # Retorna -Inf\n\n\n[1] -Inf\n\n\n\nNULL: Vazio. Representa a ausência total de conteúdo. Diferente do NA (que é um “espaço reservado para um dado que falta”), o NULL significa que a estrutura ou vetor nem sequer existe naquele ponto. É muito usado para apagar elementos de listas.\n\n\n\nCódigo\nx &lt;- NULL  # O objeto x existe, mas é vazio (tamanho 0).\nc(1, 2, NULL, 3) # Resultado: 1, 2, 3 (O NULL é ignorado na concatenação)\n\n\n[1] 1 2 3\n\n\n\n\n1.4.5 Funções de conversão e verificação\nNo R, é fundamental saber qual é o tipo do objeto (classe) com o qual você está lidando. Estas famílias de funções permitem testar ou alterar esses tipos.\nFunções as... (Conversão / Coerção Explícita)\nEstas funções tentam forçar a transformação de um objeto de um tipo para outro.\n\nas.numeric(), as.character(), as.logical(), as.factor(): Forçam a conversão de um tipo para outro.\n\n\n\nCódigo\n# Convertendo Número para Texto\nas.character(123) # Resultado: \"123\" (Note as aspas)\n\n\n[1] \"123\"\n\n\nCódigo\n# Convertendo Texto para Número\nas.numeric(\"10.5\") # Resultado: 10.5\n\n\n[1] 10.5\n\n\nCódigo\nas.numeric(\"Bola\") # Resultado: NA (\"NAs introduzidos por coerção\")\n\n\n[1] NA\n\n\n\n\n\n\n\n\nDica\n\n\n\nUma técnica muito comum é converter TRUE e FALSE em números para fazer contagens. TRUE vira 1 e FALSE vira 0.\n\n\n\n\nCódigo\nas.numeric(TRUE)  # Retorna 1\n\n\n[1] 1\n\n\nCódigo\nas.numeric(FALSE) # Retorna 0\n\n\n[1] 0\n\n\nFunções is... (Verificação / Teste lógico)\nEstas funções fazem uma pergunta ao objeto e retornam sempre TRUE ou FALSE. São ideais para usar dentro de condicionais (if).\n\nis.numeric(), is.character(), etc.: Verificam se o objeto é do tipo especificado.\n\n\n\nCódigo\nx &lt;- 10\ny &lt;- \"10\"\n\nis.numeric(x)    # TRUE (É um número)\n\n\n[1] TRUE\n\n\nCódigo\nis.numeric(y)    # FALSE (É um texto, apesar de conter dígitos)\n\n\n[1] FALSE\n\n\nCódigo\nis.character(y)  # TRUE\n\n\n[1] TRUE",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#gerenciamento-de-diretórios-e-objetos",
    "href": "intro.html#gerenciamento-de-diretórios-e-objetos",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.5 Gerenciamento de diretórios e objetos",
    "text": "1.5 Gerenciamento de diretórios e objetos\nSaber onde o R está salvando seus arquivos (diretório de trabalho) e como limpar a memória é essencial para a organização.\nDiretório de trabalho\nÉ a pasta no seu computador onde o R irá ler e salvar arquivos (como .csv ou .xlsx) por padrão.\n\ngetwd() (Get Working Directory): Descobre em qual pasta você está trabalhando agora.\n\n\n\nCódigo\ngetwd() # Exemplo de saída: /home/almonha/curso-de-verao-notas-de-aulas\"\n\n\n\nsetwd() (Set Working Directory): Muda o diretório de trabalho para outra pasta.\n\n\n\nCódigo\nsetwd(\"C:/Users/Alex/Curso_verao\")\n\n\n\n\n\n\n\n\nImportanteUse setwd() com cautela\n\n\n\nSe usarsetwd(\"C:/Users/SeuNome/...\"), seu código não será reprodutível. Se você enviar esse script para outra pessoa, o código quebrará, pois o computador dele não tem a pasta “SeuNome”, isto é, usam diretórios diferentes.\n\n\nA solução profissional é trabalhar com R Projects (.Rproj).\nCriação de projeto\n\nNo RStudio: File &gt; New Project &gt; New Directory.\nIsso cria uma raiz autocontida.\nUtilize o pacote here para referenciar arquivos. Ele encontra automaticamente a raiz do projeto, independente de qual subpasta você esteja. Isto é, se rodar o código:\n\n\n\nCódigo\ndados &lt;- read.csv(here(\"dados\", \"tabela.csv\"))\n\n\n, o computador entende: Na raiz do projeto, entre na pasta dados, leia tabela.csv, ou seja, esta é a forma ideal e universal de ler arquivos.\n\n\n\n\n\n\nAvisoCuidado com barras\n\n\n\nNo Windows, o caminho copiado usa contrabarra (\\), mas no R você deve usar barra normal (/) ou barra dupla (\\\\).\n\n\n\nForma correta (Barra normal ou dupla):\n\n\n\nCódigo\nsetwd(\"C:/Users/SeuNome/Projetos/Analise_Dados\")\n\n\n\nForma incorreta (Vai dar erro no Windows)\n\n\n\nCódigo\nsetwd(\"C:\\Users\\SeuNome\\Projetos\\Analise_Dados\")\n\n\nGerenciando objetos na memória\nConforme você cria variáveis (x, y, dados), elas ocupam a memória RAM (o Environment).\n\nls() (Listar): Mostra os nomes de todos os objetos criados no ambiente atual.\n\n\n\nCódigo\nls()\n\n\n\nrm() (Remover): Apaga objetos da memória para liberar espaço ou evitar confusão.\n\n\n\nCódigo\nrm(nome_do_objeto_a_remover)\n\n\n\nLimpeza total (O comando “Vassoura”): Para apagar tudo o que está na memória e começar do zero (muito comum no início de scripts) use rm(list = ls())\n\n\n\nCódigo\nrm(list = ls())\n\n\n\ngc(): (Garbage Collection) Força o sistema a liberar memória RAM que não está mais sendo usada. Essencial ao trabalhar com Big Data.\n\n\n\nCódigo\ngc()\n\n\n\nLimpeza de de nomes das colunas de objetos: Nomes de colunas com espaços, acentos ou caracteres especiais (Média (kg), Ano/Mês) exigem o uso de crases irritantes no código. Limpe-os imediatamente.\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, janitor,flextable)\n\ndf_sujo &lt;- data.frame(\n  'Nome do Aluno' = c(\"Ana\", \"Beto\"),\n  'NOTA FINAL' = c(9, 8),\n  check.names = FALSE\n)\n\ndf_limpo &lt;- df_sujo %&gt;%\n  janitor::clean_names() # Transforma tudo em: nome_do_aluno, nota_final\nnames(df_limpo)\n\n\n[1] \"nome_do_aluno\" \"nota_final\"",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-install_pacotes",
    "href": "intro.html#sec-install_pacotes",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.6 Pacotes: Leitura e instalação",
    "text": "1.6 Pacotes: Leitura e instalação\nO R base vem apenas com as funcionalidades essenciais. Para fazer análises avançadas, precisamos instalar pacotes (conjuntos de funções extras criadas pela comunidade). Pense no R como um celular novo: ele vem com funções de fábrica (ligar, agenda), mas para usar o Instagram ou WhatsApp, você precisa instalar os Apps (Pacotes).\nInstalação (install.packages)\nVocê faz isso apenas uma vez por computador (como baixar o App da loja).\n\n\nCódigo\ninstall.packages(\"tidyverse\")\ninstall.packages(\"ggplot2\")\n\n\n\n\n\n\n\n\nDica\n\n\n\nNote que o nome do pacote deve estar entre aspas\n\n\nCarregamento (library(.) ou require(.))\nVocê deve fazer isso toda vez que abrir o RStudio ou iniciar uma nova sessão (como abrir o App para usar). Note que aqui não precisa de aspas.\n\n\nCódigo\nlibrary(tidyverse)\n\n\n\n\n\n\n\n\nNota\n\n\n\nColoque todos os library() ou require() necessários nas primeiras linhas do seu script.\n\n\n\n\n\n\n\n\nDicaUse pacman e p_load\n\n\n\nPara evitar deixar seu script cheio de install.packages(\".\") e library(.) , onde . é pacote, use o código abaixo comando que permite instalar e carregar os pacotes simultaneamente.\n\n\n\n\nCódigo\n# O comando que segue diz: se não carregar o pacote pacman instale-o\nif (!require(\"pacman\")) install.packages(\"pacman\")\n#O comando abaixo instala os pacotes se não estão instalados e carrega-os\npacman::p_load(tidyverse, flextable)",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#estruturas-de-dados",
    "href": "intro.html#estruturas-de-dados",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.7 Estruturas de dados",
    "text": "1.7 Estruturas de dados\nAlém de vetores, o R organiza dados em estruturas mais complexas.\nIndexação e Seleção (Subsetting)\nNo R, utilizamos colchetes [ ] para acessar, extrair ou modificar pedaços específicos dos dados.\nVetores:\n\n\nCódigo\nx &lt;- c(10, 20, 30, 40, 50)\nnames(x) &lt;- c(\"Ana\", \"Beto\", \"Carla\", \"Davi\", \"Eva\")\n\n\n\nx[n]: Seleciona o elemento na posição n.\n\n\n\nCódigo\nx[2]  # Retorna 20 (o segundo elemento)\n\n\nBeto \n  20 \n\n\n\nx[-n]: Seleciona tudo exceto o elemento na posição n.\n\n\n\nCódigo\nx[-2] # Retorna: 10, 30, 40, 50 (O 20 foi removido)\n\n\n  Ana Carla  Davi   Eva \n   10    30    40    50 \n\n\n\nx[1:n]: Seleciona uma sequência de posições.\n\n\n\nCódigo\nx[2:4] # Retorna: 20, 30, 40 (Da 2ª à 4ª posição)\n\n\n Beto Carla  Davi \n   20    30    40 \n\n\n\nx[c(1,4)]: Seleciona posições específicas não sequenciais.\n\n\n\nCódigo\nx[c(1, 5)] # Retorna: 10, 50 (O primeiro e o último)\n\n\nAna Eva \n 10  50 \n\n\n\nx[x &gt; 3]: Seleção lógica (filtro). Retorna elementos que satisfazem a condição (TRUE). Isto é, O R testa cada elemento. Onde for TRUE, ele mantém; onde for FALSE, ele descarta.\n\n\n\nCódigo\nx[x &gt; 30] # Resultado: 40, 50\n\n\nDavi  Eva \n  40   50 \n\n\n\nx[\"nome\"]: Seleciona pelo nome, se o vetor tiver o atributo names.\n\n\n\nCódigo\nx[\"Carla\"]      # Retorna 30\n\n\nCarla \n   30 \n\n\nCódigo\nx[c(\"Ana\", \"Eva\")] # Retorna 10 e 50\n\n\nAna Eva \n 10  50 \n\n\nListas: Diferente dos vetores (que só aceitam um tipo de dado), as listas são a estrutura mais flexível do R. Elas podem conter qualquer coisa: números, textos, vetores, data frames e até outras listas.\n\n\nCódigo\naluno &lt;- list(\n  nome = \"Mariana\",\n  notas = c(9.5, 8.0, 7.5),\n  ativo = TRUE\n)\n\n\n\nlista[n]: Retorna uma nova lista contendo apenas o elemento n. (Pense: pega a gaveta inteira).\n\n\n\nCódigo\nx &lt;- aluno[2]\n# O que é 'x'? É uma LISTA contendo as notas.\nclass(x) # Resultado: \"list\"\n\n\n[1] \"list\"\n\n\n\nlista[[n]]: Extrai o conteúdo do objeto na posição n. (Pense: tira o objeto da gaveta).\n\n\n\nCódigo\ny &lt;- aluno[[2]]\n# O que é 'y'? É um VETOR numérico (as notas em si).\nclass(y) # Resultado: \"numeric\"\n\n\n[1] \"numeric\"\n\n\nCódigo\nmean(y)  # Funciona (média das notas).\n\n\n[1] 8.333333\n\n\nCódigo\n# mean(x) daria erro, pois não se calcula média de uma \"lista\".\n\n\n\nlista$nome: É a forma mais comum e legível de usar o [[...]]. Extrai o conteúdo diretamente usando o nome atribuído ao elemento.\n\n\n\nCódigo\naluno$nome   # Retorna: \"Mariana\"\n\n\n[1] \"Mariana\"\n\n\nCódigo\naluno$notas  # Retorna: 9.5 8.0 7.5\n\n\n[1] 9.5 8.0 7.5\n\n\nMatrizes e Data Frames\nDiferente dos vetores, aqui lidamos com duas dimensões. A regra de ouro no R é sempre: [ LINHA , COLUNA ].\n\n\nCódigo\ndados &lt;- data.frame(\n  Nome = c(\"Ana\", \"Beto\", \"Carla\"),\n  Idade = c(22, 30, 25),\n  Nota = c(8, 7, 9)\n)\n\n\n\ndf[i, j]: Elemento na linha i, coluna j.\n\n\n\nCódigo\ndados[1, 2] # Linha 1 (\"Ana\"), Coluna 2 (\"Idade\") -&gt; Retorna 22\n\n\n[1] 22\n\n\n\ndf[i, ]: Seleciona toda a linha i.\n\n\n\nCódigo\ndados[2, ]# Retorna todos os dados do \"Beto\"\n\n\n\n  \n\n\n\n\ndf[, j]: Seleciona toda a coluna j.\n\n\n\nCódigo\ndados[, 2]# Retorna o vetor: 22 30 25\n\n\n[1] 22 30 25\n\n\n\ndf$coluna: Seleciona a coluna pelo nome (específico para Data Frames e Listas). Note que este não funciona em matrizes.\n\n\n\nCódigo\ndados$Nome  #seleciona a coluna \"Nome\"\n\n\nMatrizes e Arrays\n\nMatrizes\n\nMatrizes são vetores com duas dimensões (linhas e colunas). Todos os dados devem ser do mesmo tipo (ex: tudo numérico).\n\nmatrix(data, nrow, ncol): Cria a matriz de nrow linhas e ncol colunas. O preenchimento padrão é por coluna. Use byrow=TRUE para preencher por linha.\n\n\n\nCódigo\nM &lt;- matrix(1:6, nrow = 2, ncol = 3, byrow = TRUE);M\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\n\n\nt(x): Transposta (inverte linhas por colunas).\n\n\n\nCódigo\nM_t &lt;- t(M);M_t\n\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\n\n%*%: O operador para multiplicação matricial real (diferente de * que multiplica elemento por elemento).\n\n\n\nCódigo\nM_mult &lt;- M %*% M_t; M_mult\n\n\n     [,1] [,2]\n[1,]   14   32\n[2,]   32   77\n\n\n\nsolve(A, b): Resolve sistemas lineares \\(Ax = b\\). Se b for omitido, inverte a matriz A.\n\n\n\nCódigo\nA &lt;- matrix(c(4, 2, 7, 6), nrow = 2)\nA_inv &lt;- solve(A) # Inversa de A\n\n\n\n\nCódigo\n#SISTEMA DE EQUAÇÕES: Ax=b\n# 3x + 2y = 5\n# 1x + 4y = 10\nA &lt;- matrix(c(3, 1, 2, 4), nrow = 2)\nb &lt;- c(5, 10)\n\n#RESOLVENDO-O\nresultado_x &lt;- solve(A, b)\nprint(resultado_x) \n\n\n[1] 0.0 2.5\n\n\n\nArrays\n\nEnquanto matrizes são estritamente bidimensionais (linhas e colunas), arrays são estruturas de dados n-dimensionais. Pense em um array 3D como um cubo de dados ou uma pilha de matrizes. Eles são fundamentais em estatística espacial (x, y, tempo) ou em imagens (x, y, canais de cor).\n\narray(data, dim): O argumento dim é um vetor que define o tamanho de cada dimensão c(linhas, colunas, profundidade/camadas, ...).\n\n\n\nCódigo\n# Imagine que são dados de temperatura de 2 cidades, em 3 meses, por 2 anos.\nmeu_array &lt;- array(data = 1:12, dim = c(2, 3, 2))\n\nprint(meu_array)\n\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n\n\nCódigo\ndim(meu_array) # Retorna 2 3 2\n\n\n[1] 2 3 2\n\n\nData Frames\nÉ a estrutura mais importante para Ciência de Dados. Pense nele como uma planilha de Excel: colunas podem ter tipos diferentes (texto, números, datas), mas todas devem ter o mesmo comprimento (número de linhas) ( Tabela 1.1).\n\ndata.frame(...): Cria um data frame manualmente.\n\n\n\nCódigo\npacman::p_load(gt)\n\ndf &lt;- data.frame(\n  id = 1:3,\n  nome = c(\"Ana\", \"Beto\", \"Carla\"),\n  nota = c(8.5, 9.0, 7.5)\n)\n\ndf |&gt;\n  gt()\n\n\n\n\nTabela 1.1: Notas dos alunos\n\n\n\n\n\n\n\n\n\nid\nnome\nnota\n\n\n\n\n1\nAna\n8.5\n\n\n2\nBeto\n9.0\n\n\n3\nCarla\n7.5\n\n\n\n\n\n\n\n\n\n\n\nhead(df), tail(df): Mostra as primeiras/últimas linhas ( Tabela 1.2 ).\n\n\nCódigo\n# Tabela 1 (Esquerda)\nhead(df) |&gt; \n  gt() \n# Tabela 2 (Direita)\ntail(df) |&gt; \n  gt()\n\n\n\n\nTabela 1.2: Inspeção dos Dados: Primeiras e Últimas linhas\n\n\n\n\n\n\n\n\n\n\n(a) Visualização do Topo (Head)\n\n\n\n\n\nid\nnome\nnota\n\n\n\n\n1\nAna\n8.5\n\n\n2\nBeto\n9.0\n\n\n3\nCarla\n7.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Visualização do Final (Tail)\n\n\n\n\n\nid\nnome\nnota\n\n\n\n\n1\nAna\n8.5\n\n\n2\nBeto\n9.0\n\n\n3\nCarla\n7.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnrow(df), ncol(df): Número de linhas e colunas.\n\n\n\nCódigo\nnrow(df) #número de linhas\n\n\n[1] 3\n\n\nCódigo\nncol(df) #número de colunas\n\n\n[1] 3\n\n\n\ndim(df): Retorna dimensões (linhas, colunas).\n\n\n\nCódigo\ndim(df) #dimensão\n\n\n[1] 3 3\n\n\n\nstr(df), glimpse(df): Mostra a estrutura interna (tipos de dados de cada coluna).\n\n\n\nCódigo\nstr(df)\n\n\n'data.frame':   3 obs. of  3 variables:\n $ id  : int  1 2 3\n $ nome: chr  \"Ana\" \"Beto\" \"Carla\"\n $ nota: num  8.5 9 7.5\n\n\nEssencial para depuração.\n\nnames(df): Retorna ou define os nomes das colunas.\n\n\n\nCódigo\nnames(df)\n\n\n[1] \"id\"   \"nome\" \"nota\"\n\n\n\nrbind(df1, df2): É usada quando você tem dados novos com a mesma estrutura (mesmas colunas) e quer adicioná-los aos antigos ( Tabela 1.3).\n\n\nCódigo\npacman::p_load(gt,dplyr)\n\ngrupo_jan &lt;- data.frame(\n  id = 1:2,\n  vendas = c(100, 150)\n)\n\ngrupo_jan %&gt;%\n  gt()\ngrupo_fev &lt;- data.frame(\n  id = 3:4,\n  vendas = c(200, 120)\n) \n\ngrupo_fev %&gt;%\n  gt()\n# Juntando (Bind de Linhas - Rows)\ntodos_dados &lt;- rbind(grupo_jan, grupo_fev)\ntodos_dados %&gt;%\n  gt()\n\n\n\n\nTabela 1.3: Demonstração da função rbind\n\n\n\n\n\n\n\n\n\n\n(a) Grupo Janeiro\n\n\n\n\n\nid\nvendas\n\n\n\n\n1\n100\n\n\n2\n150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Grupo Fevereiro\n\n\n\n\n\nid\nvendas\n\n\n\n\n3\n200\n\n\n4\n120\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Resultado da União\n\n\n\n\n\nid\nvendas\n\n\n\n\n1\n100\n\n\n2\n150\n\n\n3\n200\n\n\n4\n120\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nOs nomes das colunas devem ser exatamente iguais e estar na mesma ordem (embora data frames modernos tentem alinhar por nome, é boa prática garantir a ordem).\n\n\n\ncbind(df1, df2): Esta função cola/junta dois data frames lado a lado. É uma colagem cega baseada na posição. Ela cola a linha 1 do df1 com a linha 1 do df2 ( Tabela 1.4).\n\n\nCódigo\nlibrary(gt)\nlibrary(dplyr) # Necessário se usar o pipe %&gt;%\n\ndf_nomes &lt;- data.frame(nome = c(\"Ana\", \"Beto\", \"Carla\"))\ndf_nomes %&gt;%\n  gt()\ndf_idades &lt;- data.frame(idade = c(25, 30, 22))\ndf_idades %&gt;%\n  gt()\n# Colando lado a lado e exibindo\ndf_completo &lt;- cbind(df_nomes, df_idades)\ndf_completo %&gt;%\n  gt()\n\n\n\n\nTabela 1.4: Demonstração da função cbind\n\n\n\n\n\n\n\n\n\n\n(a) Tabela de nomes\n\n\n\n\n\nnome\n\n\n\n\nAna\n\n\nBeto\n\n\nCarla\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Tabela de idades\n\n\n\n\n\nidade\n\n\n\n\n25\n\n\n30\n\n\n22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Tabela final\n\n\n\n\n\nnome\nidade\n\n\n\n\nAna\n25\n\n\nBeto\n30\n\n\nCarla\n22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nSe a ordem das linhas estiver diferente (ex: o usuário 1 está na primeira linha da tabela A, mas o usuário 5 está na primeira linha da tabela B), seus dados ficarão corrompidos. Use apenas quando tiver certeza absoluta que a ordem das linhas é idêntica.\n\n\n\nmerge(x, y): Diferente do cbind, o merge ( Tabela 1.5) não depende da ordem das linhas. Ele procura uma coluna chave (ID, CPF, Código) comum entre as duas tabelas e alinha as informações corretamente. Os parâmetros importantes são, by = coluna_chave, a coluna usada para fazer o cruzamento; all = TRUE, que mantém todas as linhas (Full Outer Join) e all.x = TRUE que mantém todas as linhas da tabela da esquerda (Left Join).\n\n\nCódigo\nfuncionarios &lt;- data.frame(\n  id = c(3, 1, 2),\n  nome = c(\"Carlos\", \"Ana\", \"Bia\")\n)\n\nfuncionarios %&gt;%\n  gt()\nsalarios &lt;- data.frame(\n  id = c(1, 2, 3),\n  salario = c(5000, 6000, 5500)\n)\nsalarios %&gt;%\n  gt()\n# O merge procura o 'id' igual e alinha as linhas corretamente\nfusao &lt;- merge(x = funcionarios, y = salarios, by = \"id\")\n\nfusao %&gt;%\n  gt()\n\n\n\n\nTabela 1.5: Demonstração da função merge\n\n\n\n\n\n\n\n\n\n\n(a) Tabela Funcionários\n\n\n\n\n\nid\nnome\n\n\n\n\n3\nCarlos\n\n\n1\nAna\n\n\n2\nBia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Tabela Salários\n\n\n\n\n\nid\nsalario\n\n\n\n\n1\n5000\n\n\n2\n6000\n\n\n3\n5500\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Resultado do Merge\n\n\n\n\n\nid\nnome\nsalario\n\n\n\n\n1\nAna\n5000\n\n\n2\nBia\n6000\n\n\n3\nCarlos\n5500\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsummary(df): Resumo estatístico básico.\n\n\n\nCódigo\nsummary(fusao)\n\n\n       id          nome              salario    \n Min.   :1.0   Length:3           Min.   :5000  \n 1st Qu.:1.5   Class :character   1st Qu.:5250  \n Median :2.0   Mode  :character   Median :5500  \n Mean   :2.0                      Mean   :5500  \n 3rd Qu.:2.5                      3rd Qu.:5750  \n Max.   :3.0                      Max.   :6000  \n\n\nFatores\nFatores são usados para variáveis categóricas (qualitativas). O R armazena internamente como números inteiros (1,2,3…), mas exibe rótulos (labels). A ordem dos níveis (levels) é crucial para a ordem em gráficos.\n\nfactor(x, levels, ordered): Cria um fator. Definir levels é crucial para fixar a ordem das categorias (ex: em gráficos ou modelos).\n\n\n\nCódigo\nsexo &lt;- c(\"M\", \"F\", \"F\", \"M\") # Cria um vetor de texto\n\nfator_sexo &lt;- factor(sexo, levels = c(\"F\", \"M\")) # Converte para fator",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-Entr_Man",
    "href": "intro.html#sec-Entr_Man",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.8 Entrada e Manipulação de Dados",
    "text": "1.8 Entrada e Manipulação de Dados\nImportação e Exportação\nPara analisar os dados primeiro deve ler (importar) eles para o R/Rstudio. O pacote readr (do Tidyverse) é preferível ao R base por ser mais rápido e não converter texto em fator automaticamente.\nPacote readr (Tidyverse - Moderno e Rápido):\n\nread_csv(\"arquivo.csv\"): Lê arquivos separados por vírgula.\nread_csv2(\"arquivo.csv\"): Lê arquivos separados por ponto e vírgula (comum no Brasil/Europa onde a vírgula é decimal).\nread_delim(\"arquivo.txt\", delim = \"|\"): Lê arquivos com delimitadores personalizados.\nwrite_csv(x, \"arquivo.csv\"): Salva um data frame em disco.\n\n\n\nCódigo\npacman::p_load(readxl)\ndados &lt;- read_csv2(\"dados_brasil.csv\")\n\nwrite_csv(dados, \"dados_limpos.csv\")\n\n# Para Excel (requer pacote extra)\ndados_excel &lt;- read_excel(\"planilha.xlsx\", sheet = \"Aba1\")\n\n\nBase R (Clássico):\n\nread.table(\"arquivo.txt\"): A função base mais flexível e genérica para importar dados tabulares de arquivos de texto, permitindo controle total sobre todos os parâmetros (separadores, decimais, cabeçalhos). Utilize quando o arquivo de dados não segue padrões comuns (como CSV padrão) ou quando você precisa especificar manualmente como o R deve interpretar o arquivo.\n\n\n\nCódigo\n# Lê arquivo separado por tabulação ('\\t') com cabeçalho\ndados &lt;- read.table(\"dados.txt\", header = TRUE, sep = \"\\t\")\n\n\n\nread.csv(): Um “wrapper” (atalho) do read.table pré-configurado especificamente para arquivos separados por vírgula (padrão internacional). Utilize para a leitura rápida de arquivos .csv padrão sem precisar configurar parâmetros extras.\n\n\n\nCódigo\nvendas &lt;- read.csv(\"vendas_2024.csv\")\ndados &lt;- readr::read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\") #usando link\n\n\n\n\n\n\n\n\nImportanteProblemas no encoding\n\n\n\nNo Português se usa vírgula (,) para decimal e acentos (Latin1/ISO-8859-1). O padrão mundial é ponto (.) para decimal e UTF-8.\n\nuse read_csv(): Espera separador vírgula (padrão US).\nUse read_csv2(): Espera separador ponto-e-vírgula (padrão BR).\n\nSe seus textos (nomes de cidades) aparecerem com símbolos estranhos (SÃ£o Paulo), force o encoding\n\n\n\n\nCódigo\npacman::p_load(readr)\ndados &lt;- read_csv2(\"dados_br.csv\", locale = locale(encoding = \"Latin1\"))\n\n\n\nwrite_csv(): Salva data frames em arquivo CSV de forma mais rápida e moderna que o write.csv base. Utilize sempre que precisar exportar dados processados para CSV, pois ele não escreve nomes de linhas (row names) por padrão e lida melhor com caracteres especiais.\n\n\n\nCódigo\n# Salva o arquivo sem criar aquela coluna de índice numérico (1, 2, 3...)\nreadr::write_csv(iris, \"iris_limpo.csv\")\n\n\n\nscan(): Leitura primitiva e/ou função de baixo nível que lê dados sequencialmente e os converte em vetores ou listas, em vez de data frames. Utilize para ler arquivos com estrutura irregular, para colar dados copiados diretamente no console ou quando read.table falha devido a inconsistências no arquivo.\n\n\n\nCódigo\n# Lê números de um arquivo diretamente para um vetor numérico\nvetor_números &lt;- scan(\"números.txt\", what = numeric())\n\n\n\nsave(obj, file=\"dados.RData\"): Salva um ou mais objetos R específicos (variáveis, modelos, data frames) que estão no Environment em um arquivo binário compactado (.RData), preservando tipos e classes. Utilize para salvar resultados intermediários importantes (ex: ajustou um modelo que levou muito tempo e não quer voltar ajustar novamente) para uso futuro, sem salvar o “lixo” do ambiente de trabalho inteiro.\n\n\n\nCódigo\n# Você ajustou um modelo e chamou-o de fit\n# O codigo abaixo salva apenas o dataframe 'dados' e o modelo 'fit' que estão no Environment\nsave(dados, fit, file = \"resultados_parciais.RData\")\n\n\n\nload(\"dados.RData\"): Carrega objetos salvos em arquivos .RData diretamente para a memória do R, mantendo os nomes originais dos objetos. Utilize para retomar análises carregando dados pré-processados ou modelos salvos, economizando o tempo de rodar scripts novamente.\n\n\n\nCódigo\nload(\"resultados_parciais.RData\")\n\n\n\nsave.image(): Um atalho que salva todos os objetos presentes no seu ambiente de trabalho (Workspace) atual em um único arquivo. Utilize ao encerrar uma sessão de trabalho complexa para garantir que você possa continuar exatamente de onde parou (geralmente salva como .RData).\n\n\n\nCódigo\nsave.image(file = \"backup_projeto_tarde.RData\")\n\n\nExcel e Google Sheets:\n\nreadxl::read_excel(\"arq.xlsx\", sheet = 1): A função mais robusta e eficiente para importar dados de arquivos Microsoft Excel (.xls e .xlsx) sem depender de instalações externas complexas (como Java). Utilize para carregar dados armazenados localmente em arquivos Excel, permitindo especificar qual aba (sheet) deve ser lida pelo nome ou índice.\n\n\n\nCódigo\ntabela &lt;- readxl::read_excel(\"relatorio_anual.xlsx\", sheet = \"Dados_Brutos\")\n\n\n\ngooglesheets4::read_sheet(\"URL\"): Função que conecta diretamente à API do Google para baixar e importar dados de planilhas hospedadas na nuvem (Google Drive), gerenciando a autenticação do usuário. Utilize para acessar dados diretamente pela URL ou ID da planilha, eliminando a necessidade de baixar o arquivo manualmente antes de ler.\n\n\n\nCódigo\ndados_online &lt;- googlesheets4::read_sheet(\"https://docs.google.com/spreadsheets/d/...\")\n\n\nLer dados de alta dimensão (Big Data)\n\nread_parquet(): Lê arquivos no formato Parquet do pacote arrow, um formato de armazenamento colunar altamente comprimido e eficiente, amplamente usado em Big Data. Utilize para importar grandes volumes de dados (milhões de linhas) com extrema rapidez e baixo uso de memória.\n\n\n\nCódigo\ndados_gigantes &lt;- read_parquet(\"dados_grandes.parquet\")\n\n\n\nwrite_parquet(): Salva um data frame ou tibble no formato Parquet. Utilize para armazenar dados processados ocupando muito menos espaço em disco que um CSV e permitindo leituras futuras muito mais rápidas.\n\n\n\nCódigo\nwrite_parquet(iris, \"iris_otimizado.parquet\")\n\n\n\nsaveRDS() salva um único objeto do R em um arquivo binário, sem salvar o nome original da variável. A melhor opção para salvar objetos individuais para ser carregado posteriormente com qualquer nome.\n\n\n\nCódigo\n# Vc ajustou modelo e pode salvar apenas o resultado final em um arquivo .rds\nsaveRDS(modelo_final, \"meu_modelo.rds\")\n\n\n\nreadRDS(): Lê um arquivo .rds e retorna o objeto salvo, exigindo que você o atribua a uma nova variável. Utilize para carregar objetos salvos com saveRDS. Diferente de load(), ele não suja seu ambiente com nomes variáveis desconhecidos; você escolhe o nome.\n\n\n\nCódigo\n# Carrega o modelo salvo atribuindo a uma nova variável\nmodelo_carregado &lt;- readRDS(\"meu_modelo.rds\")",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-dplyr",
    "href": "intro.html#sec-dplyr",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.9 Manipulação com dplyr (Tidyverse)",
    "text": "1.9 Manipulação com dplyr (Tidyverse)\nO padrão ouro atual para manipulação de dados. Utiliza o operador pipe %&gt;% para encadear ações de forma legível.\nAlgumas funções:\n\nselect(df, col1, col2): Seleciona colunas e mantém apenas as colunas selecionadas em um data frame. Utilize para reduzir o conjunto de dados, descartando variáveis irrelevantes para a análise atual ou para reordenar colunas.\n\n\n\nCódigo\n# Seleciona apenas as colunas Sepal.Length e Species\niris %&gt;% select(Sepal.Length, Species)%&gt;%\n  reactable(  searchable = TRUE,\n)\n\n\n\n\n\n\n\nfilter(df, condicao): Filtra linhas baseado em condições lógicas. Utilize para extrair observações específicas, como “apenas vendas acima de 100” ou “apenas dados de 2024”.\n\n\n\nCódigo\n# Filtra apenas linhas da espécie setosa\niris %&gt;% filter(Species == \"setosa\")%&gt;%\n  reactable(  sortable = TRUE,\n  resizable = TRUE,\n  filterable = TRUE,\n  searchable = TRUE)\n\n\n\n\n\n\n\nmutate(df, nova_col = x + y): Cria ou modifica colunas existentes preservando as demais. Utilize para criar cálculos (ex: conversão de unidades), transformar dados ou criar variáveis derivadas.\n\n\n\nCódigo\n# Cria uma nova coluna com a razão (divisão) entre sépala e pétala\niris %&gt;% mutate(razao = Sepal.Length / Petal.Length)%&gt;%\n  reactable(  sortable = TRUE,\n  resizable = TRUE,\n  filterable = TRUE,\n  searchable = TRUE)\n\n\n\n\n\n\n\narrange(df, col): Ordena as linhas. Use desc(col) para ordem decrescente. Utilize para classificar dados, como colocar os maiores valores no topo (desc()) ou ordenar alfabeticamente.\n\n\n\nCódigo\n# Ordena por Sepal.Length de forma decrescente\niris %&gt;% arrange(desc(Sepal.Length))%&gt;%\n  reactable()\n\n\n\n\n\n\n\nsummarise(df, media = mean(x)): Reduz múltiplos valores a um único resumo estatístico (soma, média, contagem). Utilize, geralmente após um group_by, para obter métricas agregadas dos seus dados.\n\n\n\nCódigo\n# Calcula a média do comprimento das sépalas\niris %&gt;% summarise(media_sepala = mean(Sepal.Length))%&gt;%\n  gt()\n\n\n\n\n\n\n\n\nmedia_sepala\n\n\n\n\n5.843333\n\n\n\n\n\n\n\n\ngroup_by(df, categoria): Agrupa os dados. Utilize imediatamente antes de summarise ou mutate para aplicar a cada grupo separadamente.\n\n\n\nCódigo\n# Agrupa por espécie (preparação para cálculo subsequente)\niris %&gt;% group_by(Species)%&gt;%\n  reactable(filterable = TRUE,\n  searchable = TRUE,)\n\n\n\n\n\n\n\nrename(df, novo = velho): Renomeia colunas. Utilize para tornar nomes de variáveis mais legíveis ou compatíveis com padrões de código (novo_nome = velho_nome).\n\n\n\nCódigo\n# Renomeia Sepal.Length para comprimento_sepala\niris %&gt;% rename(comprimento_sepala = Sepal.Length)%&gt;%\n  reactable(filterable = TRUE,\n  searchable = TRUE,)\n\n\n\n\n\n\n\nrelocate(df, col, .before = col2): Reordena a posição das colunas. Isto é, move colunas para novas posições dentro do data frame. Utilize para organizar a visualização, trazendo colunas importantes para o início (.before ou .after).\n\n\n\nCódigo\n# Move a coluna Species para antes de todas as outras\n\niris %&gt;% relocate(Species, .before = everything())%&gt;%\n  reactable()\n\n\n\n\n\n\n\nslice(df, n:m): Seleciona linhas baseando-se em suas posições (índices) inteiras. Utilize quando precisar de linhas específicas pela posição, como as 5 primeiras ou a última linha, independente dos valores ( Tabela 1.6).\n\n\n\nCódigo\n# Seleciona da linha 10 até a linha 15\niris %&gt;% slice(10:15)%&gt;%\n  gt()\n\n\n\n\nTabela 1.6: Linhas 10 a 15 do conjunto de dados flor Iris\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n\n\n\n\n\n\n\n\n\nrecode(): Substitui valores específicos num vetor numérico ou de caracteres ( Tabela 1.7). Utilize dentro de um mutate para corrigir erros de digitação ou traduzir categorias específicas rapidamente (Substituído modernamente por case_match).\n\n\n\nCódigo\n# Renomeia \"setosa\" para \"Setosa_Pura\" na coluna Species\niris %&gt;% mutate(Species = recode(Species, \"setosa\" = \"Setosa_Pura\"))%&gt;%\n  reactable()\n\n\n\n\nTabela 1.7: Coluna Species renomeada de Setosa para Setosa_Pura\n\n\n\n\n\n\n\n\n\n\n\nacross(): Função auxiliar que permite aplicar uma mesma transformação ou função de resumo a múltiplas colunas selecionadas simultaneamente. Utilize dentro de mutate() ou summarise() quando precisar repetir a mesma operação (ex: calcular média, converter tipo) em várias variáveis sem duplicar código ( Tabela 1.8).\n\n\n\nCódigo\n# Exemplo: Calcular a média apenas das colunas que começam com \"Sepal\"\niris %&gt;%\n  summarise(across(starts_with(\"Sepal\"), mean))%&gt;%\n  gt()\n\n\n\n\nTabela 1.8: Média das Sepalas\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\n\n\n\n\n5.843333\n3.057333\n\n\n\n\n\n\n\n\n\n\n\ncase_when(): Uma estrutura condicional vetorizada que permite criar ou modificar valores baseando-se em uma sequência de múltiplas condições lógicas (como vários if-else encadeados). Utilize para categorizar variáveis ou criar novas colunas baseadas em regras complexas, evitando o uso confuso de múltiplos ifelse() aninhados (Table Tabela 1.9).\n\n\n\nCódigo\n# Exemplo: Categorizar o tamanho da pétala em Pequena, Média, Grande\niris %&gt;%\n  mutate(Categoria = case_when(\n    Petal.Length &lt; 2 ~ \"Pequena\",\n    Petal.Length &lt; 5 ~ \"Média\",\n    TRUE ~ \"Grande\" # 'TRUE' age como o 'else' (caso contrário) final\n  )) %&gt;%\n  head()%&gt;%\n  gt()\n\n\n\n\nTabela 1.9: Pétala categorizadas em pequena, média, grande\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\nCategoria\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\nPequena\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\nPequena\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\nPequena\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\nPequena\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\nPequena\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\nPequena\n\n\n\n\n\n\n\n\n\n\n\n\nCódigo\n# \"Pegue o df, ENTÃO filtre notas altas, ENTÃO crie uma coluna de status\"\ndf_novo &lt;- df %&gt;%\n  filter(nota &gt; 8) %&gt;%\n  mutate(status = \"Aprovado com Louvor\") %&gt;%\n  select(nome, status)\n\n# Agrupamento\nresumo &lt;- df %&gt;%\n  group_by(nome) %&gt;%   # Supondo que 'nome' seja uma categoria\n  summarise(\n    media_nota = mean(nota, na.rm = TRUE),\n    total = n() \n  )\n\n\nJoins (Combinação de Tabelas):\n\nleft_join(x, y, by = \"key\"): Mantém todas as linhas de x, traz correspondências de y, baseando-se na coluna key em comum entre x e y. Isto é, combina duas tabelas mantendo todas as linhas da tabela da esquerda (x) e adicionando as colunas da direita (y) onde houver correspondência na chave (key). Utilize para enriquecer uma tabela principal com dados auxiliares sem perder observações originais.\n\n\n\nCódigo\n# Adiciona dados dos produtos à tabela de vendas\nvendas_detalhadas &lt;- left_join(vendas, produtos, by = \"id_produto\")\n\n\n\ninner_join(x, y): Mantém apenas linhas que existem em ambas as tabelas. Isto é, retorna apenas as linhas onde a chave de ligação existe simultaneamente em ambas as tabelas (interseção), descartando o resto. Utilize quando você precisa analisar apenas casos completos que tenham dados em ambas as fontes.\n\n\n\nCódigo\n# Mantém apenas alunos que têm notas registradas\nalunos_com_notas &lt;- inner_join(alunos, notas, by = \"matricula\")\n\n\n\nfull_join(x, y): Mantém todas as linhas de ambas as tabelas. Onde não houver correspondência, o R preenche os valores faltantes com NA. Utilize para garantir que nenhum dado seja perdido de nenhum dos lados, ideal para comparar cadastros discrepantes.\n\n\n\nCódigo\npresenca_total &lt;- full_join(dia_1, dia_2, by = \"nome_aluno\")\n\n\n\nanti_join(x, y): Retorna linhas de x que NÃO têm correspondência em y. Essencial para identificar inconsistências entre duas bases de dados.\n\n\n\nCódigo\n# Encontra produtos cadastrados que nunca foram vendidos\nprodutos_encalhados &lt;- anti_join(produtos, vendas, by = \"id_produto\")\n\n\n\npivot_longer(df, cols, ...): Transforma colunas em linhas (formato longo). Útil quando variáveis estão espalhadas no cabeçalho (ex: anos 2000, 2001, 2002). Ist é, converte tabela “larga” para “longa”, empilhando cabeçalhos de colunas em uma única variável categórica e seus valores em outra ( Tabela 1.10) .\n\n\n\nCódigo\n# Transforma colunas de anos (2000 a 2010) em: coluna 'ano' e coluna 'pib'\ndf_longo &lt;- pivot_longer(pib_paises, cols = `2000`:`2010`, names_to = \"ano\", values_to = \"pib\")\n\n\n\npivot_wider(df, names_from, values_from): Transforma linhas em colunas (formato largo). Isso é, é o inverso do anterior (pivot_longer(df, cols, ...)); expande categorias de uma coluna em múltiplas colunas novas, preenchendo com valores associados. Utilize para criar tabelas de resumo final (pivot tables) legíveis para humanos ou relatórios em Excel ( Tabela 1.10) .\n\n\n\nCódigo\n# Transforma a coluna 'tipo_despesa' em várias colunas (Aluguel, Comida, etc)\ndf_largo &lt;- pivot_wider(financas, names_from = tipo_despesa, values_from = valor)\n\n\n\nCódigo\npacman::p_load(gt,dplyr,tidyr)\n\ndados_largo &lt;- data.frame(pais = \"Brasil\", ano2020 = 10, ano2021 = 12)\n\n# Tabela 1 (Esquerda)\ndados_largo %&gt;%\n  gt()\n# Transformação\ndados_longo &lt;- dados_largo %&gt;%\n  pivot_longer(\n    cols = c(ano2020, ano2021),\n    names_to = \"ano\",\n    values_to = \"pib\"\n  )\n\n# Tabela 2 (Direita)\ndados_longo %&gt;%\n  gt()\n\n\n\n\nTabela 1.10: Transformação de dados com pivot_longer\n\n\n\n\n\n\n\n\n\n\n(a) Dados Largos (Original)\n\n\n\n\n\npais\nano2020\nano2021\n\n\n\n\nBrasil\n10\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Dados Longos (Resultado)\n\n\n\n\n\npais\nano\npib\n\n\n\n\nBrasil\nano2020\n10\n\n\nBrasil\nano2021\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nseparate(df, col, ...): Divide uma coluna de texto em múltiplas novas colunas usando um caractere separador (ponto, traço, barra). Útil para quebrar datas (2024-12-01), nomes completos ou códigos compostos (BR-SP-01) em componentes individuais.\n\n\n\nCódigo\n# Divide \"2024-12-01\" em três colunas: ano, mes, dia\ndf_limpo &lt;- separate(df, data_string, into = c(\"ano\", \"mes\", \"dia\"), sep = \"-\")\n\n\n\nunite(df, ...): Junta várias colunas em uma string única. É operação inversa ao separate; concatena valores de múltiplas colunas em uma única string, inserindo um separador. Utilize para criar chaves únicas combinando ID e Data, ou juntar Nome e Sobrenome.\n\n\n\nCódigo\n# Junta 'ddd' e 'número' para formar 'telefone_completo'\ndf_contato &lt;- unite(df, \"telefone_completo\", ddd, número, sep = \" \")\n\n\n\nany(is.na(.)) e sum(is.na(.)): Funções lógicas que varrem os dados para detectar a existência de algum (any) ou quantificar/somar (sum) valores ausentes. Obrigatório na Análise Exploratória (EDA) para decidir se você deve remover as linhas (drop_na) ou realizar imputação.\n\n\n\nCódigo\n# Verifica se existe algum NA em cada coluna do dataframe\niris %&gt;% summarise(across(everything(), ~any(is.na(.))))\n\n\n\n  \n\n\n\nCódigo\n# Conta quantos NAs existem em cada coluna\niris %&gt;% summarise(across(everything(), ~sum(is.na(.))))\n\n\n\n  \n\n\n\n\ndrop_na(df): Remove linhas inteiras se houver qualquer valor ausente (NA) nas colunas especificadas (ou em todas, se nenhuma for citada). Para limpeza rápida de dados onde observações incompletas não são úteis para a modelagem estatística.\n\n\n\nCódigo\ndf_limpo &lt;- drop_na(cliente_df, idade, renda)\n\n\n\nfill(df): Substitui valores NA propagando o último valor válido observado anterior/posterior.\n\n\n\nCódigo\n# Preenche os NAs da cotação com o valor do dia anterior (down)\ndf_preenchido &lt;- fill(acoes, cotacao, .direction = \"down\")\n\n\n\ntidyr::replace_na(): Substitui valores ausentes por um valor fixo específico (como zero ou “Desconhecido”). Utilize quando o valor ausente tem um significado real (ex: falta de registro de dívida significa dívida zero) ou para variáveis categóricas.\n\n\n\nCódigo\n# Substitui NA na coluna 'Species' por \"Não Identificada\"\ndf_limpo &lt;- df %&gt;% \n  mutate(Species = replace_na(Species, \"Não Identificada\"))\n\n\n\n\nCódigo\n# Substitui NAs da coluna 'Sepal.Length' pela sua média\ndf_imputado &lt;- iris %&gt;% \n  mutate(Sepal.Length = if_else(is.na(Sepal.Length), \n                                mean(Sepal.Length, na.rm = TRUE), \n                                Sepal.Length))\n\n\n\nmice::mice(): Multivariate Imputation by Chained Equations. Cria múltiplos datasets completos estimando os valores faltantes com base nas correlações com outras variáveis (regressão, florestas aleatórias, etc.). Utilize quando a imputação pela média introduziria viés nos seus modelos e a eliminação dos valores ausentes faria-lhe perder muita informação.\n\n\n\nCódigo\n# Cria 5 datasets com dados imputados usando método padrão (pmm)\ndados_imputados &lt;- mice(iris, m = 5, method = 'pmm', printFlag = FALSE)\n\n# Completa o dataset final (pega o primeiro dos 5 gerados)\ndf_final &lt;- complete(dados_imputados, 1)",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#manipulação-de-strings-stringr",
    "href": "intro.html#manipulação-de-strings-stringr",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.10 Manipulação de Strings (stringr)",
    "text": "1.10 Manipulação de Strings (stringr)\n\npaste(..., sep), paste0(): paste concatena/junta vetores de strings usando um separador especificado; paste0 é um atalho que concatena sem separador. Utilize para criar chaves compostas, frases dinâmicas ou combinar colunas (ex: Nome + Sobrenome).\n\n\n\nCódigo\n# paste junta com espaço padrão; paste0 cola tudo junto\nnome_comp &lt;- paste(\"João\", \"Silva\", sep = \"_\") # resultado \"João_Silva\"\ncod_id &lt;- paste0(\"ID\", 123)                    # # resultado \"ID123\"\n\n\n\nstr_detect(string, pattern): Retorna um vetor lógico (TRUE/FALSE) indicando se um padrão (texto fixo ou Regex) existe na string. Essencial dentro de um filter() para selecionar linhas que contenham termos específicos (ex: emails que contêm @gmail).\n\n\n\nCódigo\n# Filtra apenas frutas que terminam com \"a\" (regex $)\nfrutas_a &lt;- frutas %&gt;% filter(str_detect(nome, \"a$\"))\n\n\n\nstr_replace(string, pattern, replacement), str_replace_all(): Substitui ocorrências de um padrão (pattern) por um novo texto. str_replace altera apenas a primeira ocorrência encontrada; str_replace_all altera todas. Utilize para limpeza de dados, como remover símbolos de moeda, corrigir erros de digitação recorrentes ou padronizar nomes.\n\n\n\nCódigo\n# Remove o cifrão e vírgulas para converter em número depois\nvalor_limpo &lt;- str_replace_all(\"R$ 1.200,00\", \"[R$.]\", \"\")\n\n\n\nstr_sub(string, start, end): Extrai ou substitui partes de uma string baseando-se em posições de índices (início e fim). Utilize quando os dados têm posição fixa, como extrair o DDD de um telefone (caracteres 1 e 2) ou o ano de uma data sem separadores.\n\n\n\nCódigo\n# Pega os 3 primeiros caracteres da string\nprefixo &lt;- str_sub(\"São Paulo\", start = 1, end = 3) # resultado\"São\"\n\n\n\nstr_extract() / str_extract_all(): Extrai o texto real que corresponde a um padrão (Regex), ignorando o resto da string. Utilize para “pescar” informações específicas dentro de um texto sujo, como extrair apenas números de um endereço ou apenas o domínio de um email.\n\n\n\nCódigo\n# Extrai apenas a sequência de dígitos da string\nnúmero &lt;- str_extract(\"Pedido número 4502 enviado\", \"\\\\d+\") #resultado \"4502\"\n\n\n\nstr_to_lower() / str_to_upper(): Converte todo o texto para minúsculas (lower) ou maiúsculas (upper). Passo obrigatório antes de fazer junção ou comparações de texto para evitar que “Brasil” seja diferente de “brasil”.\n\n\n\nCódigo\n# Normaliza os nomes para evitar duplicidade de caixa\nnomes_norm &lt;- str_to_lower(c(\"Ana\", \"ANA\", \"ana\"))\n\n\n\nstr_squish(): Remove espaços em branco no início e no fim da string, e também reduz múltiplos espaços internos consecutivos a um único espaço. Muito superior ao str_trim para limpar dados digitados por humanos, removendo acidentes como “Nome Sobrenome”.\n\n\n\nCódigo\n# Transforma \"  Data   Science  \" em \"Data Science\"\ntexto_limpo &lt;- str_squish(\"  Data   Science  \")\n\n\n\nstr_glue(): Uma evolução moderna do paste, permite inserir variáveis diretamente dentro da string usando chaves {}. Utilize para tornar o código mais legível ao criar mensagens de log, títulos de gráficos ou URLs dinâmicas.\n\n\n\nCódigo\nnome &lt;- \"Maria\"; idade &lt;- 30\nmsg &lt;- str_glue(\"A aluna {nome} tem {idade} anos.\") # Retorna \"A aluna Maria tem 30 anos",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#datas-lubridate",
    "href": "intro.html#datas-lubridate",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.11 Datas (lubridate)",
    "text": "1.11 Datas (lubridate)\n\nymd(), dmy(), mdy(): Funções que convertem texto em objetos Date. O nome da função dita a ordem esperada dos componentes (y=ano, m=mês, d=dia). Utilize para transformar colunas de texto (importadas de CSVs/Excel) em datas reais, independentemente do separador (barra, traço, ponto) usado no texto original.\n\n\n\nCódigo\npacman::p_load(lubridate)\n# Converte texto \"2023/12/25\" ou \"2023-12-25\" para data\nnatal &lt;- ymd(\"20231225\") \ndata_br &lt;- dmy(\"31/01/2024\") # Dia, Mês, Ano\n\n\n\nyears(), months(), days(): Criam objetos de “Período” que permitem aritmética intuitiva com datas, lidando automaticamente com nuances como anos bissextos ou meses com 30/31 dias. Utilize para calcular datas de vencimento, projetar cenários futuros ou filtrar dados de um período específico (“últimos 3 meses”).\n\n\n\nCódigo\n# Adiciona 1 ano e 6 meses a uma data\nvencimento &lt;- ymd(\"2024-01-01\") + years(1) + months(6)\n\n\n\nymd_hms() (e suas variantes): Similar ao ymd(), mas para dados que incluem horário (timestamp: Hora, Minuto, Segundo). Cria objetos POSIXct. Essencial para analisar logs de servidor, transações financeiras intraday ou qualquer dado onde a hora exata importa.\n\n\n\nCódigo\n# Lê data e hora com fuso horário UTC\nmomento_exato &lt;- ymd_hms(\"2024-05-10 14:30:59\", tz = \"UTC\")\n\n\n\nyear(), month(), wday(): Funções que retornam apenas uma parte específica da data (o ano, o mês ou o dia da semana). Fundamental para análise de sazonalidade (ex: “vendas por dia da semana” ou “evolução anual”). O argumento label = TRUE em wday retorna o nome (Dom, Seg…) em vez do número.\n\n\n\nCódigo\ndata &lt;- ymd(\"2024-12-25\")\nmes &lt;- month(data)         # Retorna 12\ndia_sem &lt;- wday(data, label = TRUE) # Retorna \"Wed\" (ou \"qua\" dependendo do locale)\n\n\n\nfloor_date(): Arredonda uma data para baixo até a unidade de tempo especificada (semana, mês, hora). A função mais importante para agrupar séries temporais. Use para transformar dados diários em mensais/semanais antes de um group_by.\n\n\n\nCódigo\n# Agrupa todas as datas para o primeiro dia do respectivo mês\nvendas %&gt;% \n  mutate(mes_referencia = floor_date(data_venda, unit = \"month\"))\n\n\n\ntoday() / now(): Retornam, respectivamente, a data atual (Date) e o instante atual exato (POSIXct) do sistema. Utilize para calcular a idade de registros (“dias desde o cadastro até hoje”) ou para carimbar data de execução em relatórios.\n\n\n\nCódigo\n# Calcula a diferença em dias entre hoje e uma data passada\ndias_passados &lt;- today() - ymd(\"2000-01-01\")\n\n\n\ninterval() e %--%: Cria um objeto de intervalo entre duas datas, permitindo cálculos precisos de duração. Use para verificar se uma data cai dentro de um período específico ou para calcular a duração exata em segundos/anos entre dois pontos.\n\n\n\nCódigo\ninicio &lt;- ymd(\"2023-01-01\")\nfim &lt;- today()\nmeu_intervalo &lt;- interval(inicio, fim) # ou inicio %--% fim\n\n# Verifica se a data X está dentro do intervalo\nymd(\"2023-06-01\") %within% meu_intervalo",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#funções-matemáticas-básicas",
    "href": "intro.html#funções-matemáticas-básicas",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.12 Funções Matemáticas Básicas",
    "text": "1.12 Funções Matemáticas Básicas\n\nsum() / prod(): Calculam, respectivamente, o somatório \\((\\sum)\\) e o produtório \\((\\prod)\\) de todos os valores de um vetor numérico. Utilize para agregações básicas, verificações de totais (ex: receita total) ou cálculos de probabilidade conjunta (produtório).\n\n\n\nCódigo\ntotal &lt;- sum(c(10, 20, 30))  # 60\nfatorial_simples &lt;- prod(1:5) # 1 * 2 * 3 * 4 * 5 = 120\n\n\n\nmin() / max(): Retornam o valor mais baixo e o mais alto de um conjunto de dados. Utilize para identificar limites (inferiores e superiores), detectar outliers óbvios ou definir escalas de gráficos.\n\n\n\nCódigo\n# Encontra o valor máximo ignorando NAs\nmaior_nota &lt;- max(c(5, 8, 9, NA), na.rm = TRUE)\n\n\n\nrange(): Retorna um vetor de dois elementos contendo o mínimo e o máximo: c(min, max). Não retorna a amplitude (diferença) diretamente. Utilize para verificar rapidamente a extensão dos dados ou definir os limites (limits) de eixos em gráficos.\n\n\n\nCódigo\nlimites &lt;- range(iris$Sepal.Length) # Retorna c(4.3, 7.9)\namplitude &lt;- diff(limites)          # Calcula a diferença (3.6)\n\n\n\nround(): Arredonda números para um número especificado de casas decimais seguindo o padrão internacional (arredonda para o par mais próximo). Utilize para formatar saídas para relatórios ou simplificar a visualização de números com muitas casas decimais.\n\n\n\nCódigo\n# Arredonda para 2 casas\npi_curto &lt;- round(3.14159, digits = 2) # 3.14\n\n\n\nabs(): Retorna o valor absoluto (módulo) de um número, ignorando o sinal negativo (\\(|x|\\)). Utilize para calcular distâncias, erros absolutos (diferença entre previsto e real) ou magnitudes.\n\n\n\nCódigo\n# Transforma diferenças negativas em positivas\nerro_absoluto &lt;- abs(-50) # 50\n\n\n\nlog() / exp(): log calcula o logaritmo natural (\\(\\ln\\), base \\(e\\)) e exp calcula a exponencial (\\(e^x\\)). Para base 10, use log10(). Utilize para transformar dados assimétricos (normalizar distribuição), calcular retornos financeiros ou reverter transformações logarítmicas.\n\n\n\nCódigo\n# Transformação Log para reduzir assimetria de salários\nlog_salario &lt;- log(c(1000, 10000, 100000)) \n\n\n\ncumsum(): Retorna a soma acumulada dos elementos. O resultado tem o mesmo tamanho do vetor original. Essencial para criar Gráficos de Pareto, calcular saldos bancários dia a dia ou frequências acumuladas.\n\n\n\nCódigo\nvendas_diarias &lt;- c(10, 20, 5)\nacumulado &lt;- cumsum(vendas_diarias) # Retorna 10, 30, 35",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#estatística-descritiva",
    "href": "intro.html#estatística-descritiva",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.13 Estatística Descritiva",
    "text": "1.13 Estatística Descritiva\n\nmean() / median(): Calculam a média aritmética (centro de gravidade) e a mediana (valor central que divide a amostra em 50/50). Use a média para distribuições normais e a mediana quando houver outliers (valores extremos) que distorcem a média.\n\n\n\nCódigo\nsalarios &lt;- c(1000, 1200, 50000) # O 50000 distorce a média\nmedia &lt;- mean(salarios)   # 17400 (Distorcida)\nmediana &lt;- median(salarios) # 1200 (Representativa)\n\n\n\nsd() / var(): Calculam o desvio padrão e a variância amostral (\\(n-1\\)), medidas de dispersão que indicam o quanto os dados variam em torno da média. Utilize para quantificar o risco, volatilidade ou a consistência de um processo.\n\n\n\nCódigo\n# Calcula o desvio padrão ignorando falhas\nvolatilidade &lt;- sd(c(10, 12, 9, NA), na.rm = TRUE)\n\n\n\nquantile(): Divide os dados ordenados em probabilidades específicas. Por padrão retorna mín, 25%, 50%, 75%, máx. Utilize para entender a distribuição detalhada, criar Boxplots manuais ou identificar faixas de valores (ex: “os 10% mais ricos”).\n\n\n\nCódigo\n# Calcula os decis (10%, 20%... 90%)\ndecis &lt;- quantile(iris$Sepal.Length, probs = seq(0, 1, 0.1))\n\n\n\ncor(): Calcula a força e direção da relação linear entre duas variáveis (vai de -1 a 1). Utilize na análise exploratória para checar multicolinearidade ou se uma variável influencia a outra.\n\n\n\nCódigo\ncorrelacao &lt;- cor(iris$Sepal.Length, iris$Petal.Length, use = \"complete.obs\")\n\n\n\nsummary(): Uma função polimórfica que retorna um “raio-x” do objeto. Para vetores numéricos, dá as medidas descritivas; para fatores, a contagem. O primeiro comando a rodar ao receber dados novos para ter uma visão geral rápida da distribuição e identificar NAs.\n\n\n\nCódigo\nsummary(iris) # Resumo de todas as colunas do dataset\n\n\n\ntable() / prop.table(): table cria tabelas de frequência (contagem) para dados categóricos; prop.table converte essas contagens em porcentagens/proporções. Fundamental para analisar variáveis qualitativas (ex: “Quantos clientes são de SP vs RJ?”).\n\n\n\nCódigo\ncontagem &lt;- table(iris$Species)\nporcentagem &lt;- prop.table(contagem) * 100 # Em %\n\n\n\nscale(): Padroniza (normaliza) os dados subtraindo a média e dividindo pelo desvio padrão (Z-score), \\(z=\\frac{x-\\bar{x}}{\\sigma}\\). Obrigatório antes de algoritmos de Machine Learning baseados em distância (como K-means ou KNN) para que variáveis grandes não dominem as pequenas.\n\n\n\nCódigo\n# Coloca os dados na mesma escala (Média 0, SD 1)\ndados_normalizados &lt;- scale(iris[, 1:4])\n\n\n\nunique() / length(): unique retorna os valores únicos (sem repetição); length conta o tamanho total do vetor. Use a combinação length(unique(x)) para saber a cardinalidade (quantos itens distintos existem).\n\n\n\nCódigo\nqtd_especies &lt;- length(unique(iris$Species)) # 3\n\n\nAqui estão as definições aprimoradas para distribuições e modelagem estatística, organizadas para clareza e aplicação prática.",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#distribuições-de-probabilidade-prefixos-d-p-q-r",
    "href": "intro.html#distribuições-de-probabilidade-prefixos-d-p-q-r",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.14 Distribuições de Probabilidade (Prefixos d, p, q, r)",
    "text": "1.14 Distribuições de Probabilidade (Prefixos d, p, q, r)\nO R utiliza um sistema consistente: [prefixo][distribuição]. Ex: norm, binom, pois, exp, unif, t, chisq, f.\nd… (Densidade/Probabilidade Pontual): Calcula a altura da curva da densidade (PDF) para variáveis contínuas, ou a probabilidade exata \\(P(X=x)\\) para variáveis discretas. Utilize para desenhar o gráfico da distribuição ou calcular verossimilhança (Likelihood).\n\n\nCódigo\n# Qual a probabilidade exata de obter 2 caras em 3 lançamentos (Binomial)?\nprob_exata &lt;- dbinom(x = 2, size = 3, prob = 0.5) \n\n\n\np... (Probabilidade Acumulada - CDF): Calcula a área sob a curva à esquerda de um ponto \\(P(X \\le x)\\). Utilize para calcular valores-p ou a probabilidade de uma variável ser menor que um certo valor.\n\n\n\nCódigo\n# Qual a probabilidade de um valor numa Normal(0,1) ser menor que -1.96?\nprob_acumulada &lt;- pnorm(-1.96) # ~0.025 (2.5%)\n\n\n\nq... (Quantil - Inverso da CDF): Dado uma probabilidade (área), retorna o valor de \\(x\\) correspondente. Utilize para encontrar valores críticos para intervalos de confiança (ex: o Z para 95%).\n\n\n\nCódigo\n# Qual valor deixa 2.5% da cauda superior na distribuição T (gl=29)?\nvalor_critico &lt;- qt(p = 0.975, df = 29) \n\n\n\nr... (Random - Geração de números (pseudo) Aleatória): Gera números (pseudo) aleatórios que seguem a distribuição especificada. Utilize para simulações de Monte Carlo, criar dados sintéticos ou bootstrap.\n\n\n\nCódigo\n# Gera 100 observações de uma Poisson com lambda = 5\namostra &lt;- rpois(n = 100, lambda = 5)",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#testes-de-hipótese",
    "href": "intro.html#testes-de-hipótese",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.15 Testes de Hipótese",
    "text": "1.15 Testes de Hipótese\n\nt.test(): Realiza o teste T de Student para comparar médias de uma ou duas amostras (independentes ou pareadas). Utilize para verificar se há diferença significativa entre dois grupos numéricos (ex: tratamento vs controle).\n\n\n\nCódigo\npacman::p_load(mtcars)\n# Compara se há diferença significativa no consumo (mpg) entre carros automáticos e manuais (am) no dataset mtcars.\nresultado &lt;- t.test(mpg ~ am, data = mtcars)\n\n\n\ncor.test(): Testa se a correlação entre duas variáveis é significativamente diferente de zero. Utilize para validar estatisticamente a associação linear observada com cor().\n\n\n\nCódigo\n# Testa a correlação entre o peso do carro (wt) e a eficiência (mpg). Espera-se uma correlação negativa forte.\nteste_cor &lt;- cor.test(mtcars$wt, mtcars$mp, method = \"pearson\")\n\n\n\nchisq.test(): Teste Qui-Quadrado de independência. Utilize para verificar associação entre duas variáveis categóricas (ex: Cor dos Olhos vs Cor do Cabelo).\n\n\n\n\n\n\n\nImportante\n\n\n\nA entrada ideal é uma tabela de contingência (table(.)).\n\n\n\n\nCódigo\n# Verifica se existe associação entre o tipo de motor (vs: em V ou reto) e o tipo de transmissão (am: auto ou manual).\nteste_qui &lt;- chisq.test(table(mtcars$vs, mtcars$am))\n\n\n\nshapiro.test(): Teste de normalidade de Shapiro-Wilk (\\(H_0\\): Os dados seguem uma distribuição Normal). Utilize para verificar os pressupostos de normalidade dos resíduos de um modelo ou de uma variável antes de aplicar testes paramétricos. (Melhor para \\(N &lt; 5000\\)).\n\n\n\nCódigo\n# P-valor &lt; 0.05 indica que os dados NÃO são normais\nteste_norm &lt;- shapiro.test(resid(modelo_linear)) \n#Verifica se a largura das sépalas (Sepal.Width) das flores iris segue uma distribuição Normal.\nShap &lt;- shapiro.test(iris$Sepal.Width) # H0: Os dados são normais (p &gt; 0.05 indica normalidade)\n\n\n\nwilcox.test(): Versão não-paramétrica do Teste T (Teste de Mann-Whitney ou Wilcoxon). Utilize para comparar grupos quando a suposição de normalidade falha. Compara postos (ranks) e não médias.\n\n\n\nCódigo\n# Comparação sem assumir normalidade\nteste_np &lt;- wilcox.test(valor ~ grupo, data = dados)",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#modelagem-estatística",
    "href": "intro.html#modelagem-estatística",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.16 Modelagem Estatística",
    "text": "1.16 Modelagem Estatística\n\nlm(): Ajusta modelos de regressão linear (Mínimos Quadrados Ordinários - OLS). A base para prever uma variável numérica contínua baseada em preditores. Use summary() no objeto para ver os coeficientes e \\(R^2\\).\n\n\n\nCódigo\n# Modela o consumo de combustível (mpg) baseando-se no peso (wt) e na potência (hp) dos carros.\nmodelo_linear &lt;- lm(mpg ~ wt + hp, data = mtcars)\nsummary(modelo_linear) #saída do modelo, embora não elegante\n\n\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\n\n\n\n\n\n\nDica\n\n\n\nUsar summary(modelo_ajustado) é forma mais simples mas não profissional para trabalhos acadêmicos. Existem vários pacotes que permitem uma extração mais profissional, como segue abaixo.\n\n\n\n\nCódigo\npacman::p_load(broom, dplyr)\n# Extraido os resultados do modelo ajustado acima de forma mais profissional\nresultados_tidy &lt;- tidy(modelo_linear, conf.int = TRUE)\ngt(resultados_tidy)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n37.22727012\n1.59878754\n23.284689\n2.565459e-20\n33.95738245\n40.49715778\n\n\nwt\n-3.87783074\n0.63273349\n-6.128695\n1.119647e-06\n-5.17191604\n-2.58374544\n\n\nhp\n-0.03177295\n0.00902971\n-3.518712\n1.451229e-03\n-0.05024078\n-0.01330512\n\n\n\n\n\n\n\nCódigo\n# Agora você pode acessar o p-valor assim: resultados_tidy$p.value[2]\n\n#Resumo do modelo (R^2, AIC, etc)\nqualidade_modelo &lt;- glance(modelo_linear)\ngt(qualidade_modelo)\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.8267855\n0.8148396\n2.593412\n69.21121\n9.109054e-12\n2\n-74.32617\n156.6523\n162.5153\n195.0478\n29\n32\n\n\n\n\n\n\n\nCódigo\n# Diagnóstico (Valores preditos e resíduos linha a linha)\ndiagnostico &lt;- augment(modelo_linear)\nreactable(diagnostico,   filterable = TRUE,\n  searchable = TRUE)\n\n\n\n\n\n\n\nglm(): Modelos Lineares Generalizados. Estende a regressão linear para variáveis resposta não-normais através de funções de ligação (link function). Utilize para Regressão Logística (family=binomial - resposta 0/1) ou Contagem (family=poisson), etc.\n\n\n\nCódigo\npacman::p_load(report) \n# Prevê a probabilidade de um carro ser automático ou manual (am, binário 0/1) baseado no consumo (mpg).\nmodelo_log &lt;- glm(am ~ mpg, family = \"binomial\", data = mtcars)\nreport(modelo_log) #Gera interpretação em inglês.\n\n\nWe fitted a logistic model (estimated using ML) to predict am with mpg\n(formula: am ~ mpg). The model's explanatory power is substantial (Tjur's R2 =\n0.37). The model's intercept, corresponding to mpg = 0, is at -6.60 (95% CI\n[-12.33, -2.77], p = 0.005). Within this model:\n\n  - The effect of mpg is statistically significant and positive (beta = 0.31, 95%\nCI [0.12, 0.59], p = 0.008; Std. beta = 1.85, 95% CI [0.74, 3.54])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation.\n\n\n\n\nCódigo\npacman::p_load(gtsummary) #mais técnico\n\nmodelo_linear %&gt;%\n  tbl_regression(\n    intercept = TRUE,         # Mostrar o intercepto \n    estimate_fun = ~ style_number(.x, digits = 3) # Formatar casas decimais\n  ) %&gt;%\n  add_global_p() %&gt;%          # Adiciona p-valor global se tiver variáveis categóricas\n  bold_p(t = 0.05) %&gt;%        # Negrito nos valores-p significativos\n  add_glance_source_note(     # Adiciona R^2 e estatísticas no rodapé\n    label = list(r.squared ~ \"$R^2$\", AIC ~ \"AIC\")\n  )\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\n(Intercept)\n37.227\n33.957, 40.497\n&lt;0.001\n\n\nwt\n-3.878\n-5.172, -2.584\n&lt;0.001\n\n\nhp\n-0.032\n-0.050, -0.013\n0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\\(R^2\\) = 0.827; Adjusted R² = 0.815; Sigma = 2.59; Statistic = 69.2; p-value = &lt;0.001; df = 2; Log-likelihood = -74.3; AIC = 157; BIC = 163; Deviance = 195; Residual df = 29; No. Obs. = 32\n\n\n\n\n\n\n\n\n\ngam() (do pacote mgcv): Modelos Aditivos Generalizados. Estende os GLMs permitindo modelar relações não-lineares e formas livres (curvas) através de funções de suavização (smooth functions, denotadas por s()). Utilize quando a relação entre X e Y não for uma linha reta simples.\n\n\n\nCódigo\npacman::p_load(mgcv, report,ggplot2, gtsummary) \n\n# Prevê o consumo (mpg) baseado na potência (hp) assumindo uma relação não-linear (curva suave)\n# Note o uso de s() para indicar um termo de suavização (spline)\nmodelo_gam &lt;- gam(mpg ~ s(hp), data = mtcars)\n\nreport(modelo_gam) # Gera interpretação textual descrevendo a não-linearidade.\n\n\nWe fitted a linear model (estimated using GCV and magic optimizer) to predict\nmpg with hp (formula: mpg ~ s(hp)). The model's explanatory power is\nsubstantial (R2 = 0.73). The model's intercept, corresponding to hp = 0, is at\n20.09 (95% CI [18.97, 21.21], p &lt; .001). Within this model:\n\n  - The effect of Smooth term (hp) is statistically significant and NA (beta = ,\np &lt; .001; Std. beta = , )\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset.\n\n\nCódigo\nmodelo_gam %&gt;%\n  tbl_regression(\n    intercept = TRUE,             # Mostrar o intercepto\n    estimate_fun = ~ style_number(.x, digits = 3) # Formatar casas decimais\n  ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\n(Intercept)\n20.091\n19.015, 21.166\n&lt;0.001\n\n\ns(hp)\n\n\n\n\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\nCódigo\n# Criando a previsão para plotar\npred_data &lt;- data.frame(hp = seq(min(mtcars$hp), max(mtcars$hp), length = 100))\npred_data$fit &lt;- predict(modelo_gam, newdata = pred_data)\n\nggplot() +\n  geom_line(data = pred_data, aes(x = hp, y = fit), color = \"blue\", size = 1) +\n  geom_rug(data = mtcars, aes(x = hp), sides = \"b\", alpha = 0.5) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDicaSugestão, use o pacote gratia\n\n\n\nExiste um pacote excelente chamado gratia (criado por Gavin Simpson), feito especificamente para traduzir modelos do mgcv para a linguagem do ggplot2 ( Figura 1.5).\n\n\n\n\nCódigo\npacman::p_load(gratia)\n\nmodelo &lt;- gam(mpg ~ s(hp) + s(wt), data = mtcars)\ndraw(modelo)\n\n\n\n\n\n\n\n\nFigura 1.5: Gráfico do modelo gam usando pacote gratia\n\n\n\n\n\n\naov(): Análise de Variância (ANOVA). Matematicamente equivalente a um modelo linear com preditores categóricos, mas focado na variância entre grupos. Utilize para comparar médias de três ou mais grupos. Use TukeyHSD() depois para ver onde estão as diferenças.\n\n\n\nCódigo\n# ANOVA de uma via\nanova_res &lt;- aov(Petal.Length ~ Species, data = iris)\nsummary(anova_res)\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  437.1  218.55    1180 &lt;2e-16 ***\nResiduals   147   27.2    0.19                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nDicaUse o pacote broom\n\n\n\nAssim como na regressão, o summary(anova_res) retorna um objeto difícil de manipular ou não profissionalmente formatado e o gtsummary foca mais em coeficientes de regressão, para a Tabela de ANOVA (aquela com Soma de Quadrados, GL, F), a melhor combinação é limpar com broom e formatar com flextable ou gt.\n\n\n\n\nCódigo\nanova_res &lt;- aov(mpg ~ factor(cyl) + factor(gear), data = mtcars)\n\n# Transforma a saída feia do console em um data frame organizado\ntabela_limpa &lt;- tidy(anova_res)\n\ngt(tabela_limpa)\n\n\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfactor(cyl)\n2\n824.784590\n412.392295\n38.000627\n1.412658e-08\n\n\nfactor(gear)\n2\n8.251855\n4.125927\n0.380191\n6.873334e-01\n\n\nResiduals\n27\n293.010743\n10.852250\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nDicaPacote effectsize\n\n\n\nAtualmente, apresentar apenas o valor-p da ANOVA é considerado insuficiente. Você deve apresentar o tamanho do efeito (\\(\\eta^2\\) ou Partial Eta Squared). O pacote effectsize (da família easystats) faz isso e já formata para você.\n\n\n\n\nCódigo\npacman::p_load(effectsize)\n\n# Calcula Eta Squared e já formata a tabela\neta_squared(anova_res, partial = FALSE) %&gt;%\n  as.data.frame() %&gt;%\n  gt() # ?effectsize para mais informações\n\n\n\n\n\n\n\n\nParameter\nEta2\nCI\nCI_low\nCI_high\n\n\n\n\nfactor(cyl)\n0.732460060\n0.95\n0.5667547\n1\n\n\nfactor(gear)\n0.007328161\n0.95\n0.0000000\n1\n\n\n\n\n\n\n\n\npredict(): Função genérica para gerar valores previstos usando um modelo ajustado (lm, glm, etc.) em novos dados. Utilize para aplicar seu modelo treinado em um conjunto de teste ou novos dados reais.\n\n\n\nCódigo\n# Prevê probabilidades para novos clientes (Regressão Logística)\nprevisoes &lt;- predict(mod_log, newdata = novos_clientes, type = \"response\")\n\n\n\n\n\n\n\n\nDicaPacote finalfit\n\n\n\nRápido e muito técnico também\n\n\n\n\nCódigo\npacman::p_load(finalfit, dplyr, ggplot2,Hmisc)\n\nexplanatory = c(\"age.factor\", \"sex.factor\", \n  \"obstruct.factor\", \"perfor.factor\")\ndependent = 'mort_5yr'\ncolon_s %&gt;%\n  or_plot(dependent, explanatory)\n\n\n\n\n\n\n\n\n\nExtraindo informações dos modelos:\n\ncoef(modelo): Extrai os coeficientes estimados.\nresiduals(modelo): Extrai os resíduos.\nanova(modelo): Tabela de análise de variância.\nAIC(modelo): Critério de informação de Akaike.\n\n\n\n\n\n\n\nDicaRecomendação Especial: GLM\n\n\n\nAos interessados em dominar Modelos Lineares Generalizados (GLM), deixo minha forte recomendação para a disciplina MAE5763-Modelos Lineares Generalizados. Ela é oferecida no segundo semestre no IME-USP e ministrada pelo Professor Gilberto Alvarenga Paula. Pela profundidade e didática, considero esta uma das disciplinas mais valiosas da minha formação.",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-Visualização",
    "href": "intro.html#sec-Visualização",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.17 Visualização de Dados",
    "text": "1.17 Visualização de Dados\nGráficos doR Base\nÚteis para inspeção rápida de dados.\n\nplot(x, y): Função genérica e polimórfica que adapta o gráfico dependendo do tipo de objeto fornecido. Se receber dois vetores numéricos, cria um gráfico de dispersão ( Figura 1.6); se receber um fator e um numérico, cria boxplots. A ferramenta universal para uma primeira olhada na relação entre duas variáveis \\((X \\times Y)\\) ou para visualizar objetos complexos (como resíduos de um modelo linear plot(modelo)).\n\n\n\nCódigo\nplot(x = mtcars$wt, y = mtcars$mpg, \n     main = \"Peso vs Consumo\", xlab = \"Peso\", ylab = \"MPG\", pch = 19)\n\n\n\n\n\n\n\n\nFigura 1.6: Gráfico de dispersão (Scatterplot) entre Peso (wt) e Consumo (mpg)\n\n\n\n\n\n\nhist(x): Divide uma variável numérica contínua em intervalos (bins) e conta a frequência de observações em cada um ( Figura 1.7). Utilize para entender a distribuição dos dados: verificar normalidade, assimetria (skewness) ou identificar se a distribuição é bimodal.\n\n\n\nCódigo\nhist(iris$Petal.Length, xlab = \"Petal.Length\",\n     col = \"lightblue\", main = \"Distribuição das Pétalas\", breaks = 10)\n\n\n\n\n\n\n\n\nFigura 1.7: Histograma do comprimento das pétalas\n\n\n\n\n\n\nboxplot(x): Representação visual do resumo de cinco números: mínimo, 1º quartil, mediana, 3º quartil e máximo ( Figura 1.8). Pontos fora dos “bigodes” indicam outliers. A melhor ferramenta para comparar distribuições entre diferentes grupos e detectar anomalias (outliers) rapidamente. Aceita sintaxe de fórmula (y ~ grupo).\n\n\n\nCódigo\nboxplot(len ~ supp, data = ToothGrowth, \n        col = c(\"orange\", \"yellow\"), main = \"\")\n\n\n\n\n\n\n\n\nFigura 1.8: Tamanho do dente por tipo de suplemento (ToothGrowth)\n\n\n\n\n\n\nbarplot(height): Cria barras com alturas proporcionais aos valores fornecidos ( Figura 1.9).\n\n\n\n\n\n\n\nImportanteAtenção\n\n\n\nDiferente do histograma, ele exige que você forneça um vetor ou matriz de contagens/valores já sumarizados (ex: resultado de table()). Utilize para comparar quantidades entre categorias discretas.\n\n\n\n\nCódigo\ncontagem &lt;- table(mtcars$cyl)\nbarplot(contagem, xlab=\"Número de carros\",\n        main = \"\", col = \"steelblue\", ylab = \"Número de cilindros\")\n\n\n\n\n\n\n\n\nFigura 1.9: Contagem de carros por número de cilindros\n\n\n\n\n\n\npairs(df): Cria uma matriz de scatterplots, cruzando todas as variáveis numéricas do dataframe umas contra as outras (N x N). Essencial na fase inicial de Análise Exploratória (EDA) multivariada para detectar correlações lineares, clusters naturais ou padrões entre múltiplas variáveis simultaneamente ( Figura 1.10).\n\n\n\nCódigo\npairs(iris[, 1:4], \n      col = iris$Species, pch = 1, main = \"\")\n\n\n\n\n\n\n\n\nFigura 1.10: Matriz de dispersão das 4 variáveis numéricas do iris\n\n\n\n\n\n\nabline(): Adiciona linhas retas (horizontais, verticais ou regressão) a um gráfico existente. Útil para adicionar a linha de tendência.\n\n\n\nCódigo\nplot(mtcars$wt, mtcars$mpg, xlab = \"wt\", ylab=\"mpg\")\nabline(lm(mpg ~ wt, data = mtcars), col = \"red\") # Adiciona linha de regressão\n\n\n\n\n\n\n\n\n\nElementos de Baixo Nível\n\npoints(x, y): Adiciona símbolos (pontos) a um gráfico já aberto. Permite sobrepor novas séries de dados ou destacar pontos específicos com cores/formatos diferentes. Utilize para destacar outliers, adicionar uma segunda variável no mesmo eixo Y ou marcar médias sobre um boxplot ( Figura 1.11).\n\n\n\nCódigo\nplot(mtcars$wt, mtcars$mpg, xlab = \"wt\", ylab = \"mpg\")\npoints(mtcars$wt[mtcars$cyl == 8], mtcars$mpg[mtcars$cyl == 8], col = \"red\", pch = 19)\n\n\n\n\n\n\n\n\nFigura 1.11: Adicionando pontos vermelhos para carros com 8 cilindros\n\n\n\n\n\n\nlines(x, y): Conecta coordenadas (x, y) com segmentos de linha.\n\n\n\n\n\n\n\nImportante\n\n\n\nAtenção: Os dados devem estar ordenados pelo eixo X, senão a linha ficará “rabiscada”. Utilize para desenhar séries temporais, curvas de modelos ajustados (suavizações) ou polígonos de contorno ( Figura 1.12).\n\n\n\n\nCódigo\nplot(pressure$temperature, pressure$pressure, xlab = \"temperature\", ylab=\"pressure\")\nlines(pressure$temperature, pressure$pressure, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\nFigura 1.12: Conectando os pontos com uma linha azul\n\n\n\n\n\n\nabline(a, b): Adiciona linhas retas de referência que atravessam todo o gráfico. Pode ser horizontal (h), vertical (v) ou baseada em intercepto e inclinação (a, b). Ideal para adicionar a linha de regressão (abline(modelo)), linhas de média (horizontal) ou limites de especificação/datas importantes (vertical) ( Figura 1.13).\n\n\n\nCódigo\nplot(mtcars$wt, mtcars$mpg, xlab = \"wt\", ylab = \"mpg\")\nabline(h = mean(mtcars$mpg), col = \"red\"); abline(v = 3, col = \"green\")\n\n\n\n\n\n\n\n\nFigura 1.13: Adiciona linha horizontal na média (vermelha) e vertical em x=3 (verde)\n\n\n\n\n\n\ntext(x, y, labels): Plota strings de texto dentro da área do gráfico nas coordenadas (x, y) especificadas. Utilize para rotular pontos de dados individuais (ex: nome do país no scatterplot) ou anotar equações e valores-p ( Figura 1.14).\n\n\n\nCódigo\nplot(mtcars$wt, mtcars$mpg, xlab = \"wt\", ylab = \"mpg\")\ntext(x = max(mtcars$wt)-2, y = min(mtcars$mpg)+3, labels = \"Carro Mais Pesado\", pos = 2)\n\n\n\n\n\n\n\n\nFigura 1.14: Escreve o nome do carro no ponto mais pesado\n\n\n\n\n\n\nlegend(x, y, legend): Adiciona uma caixa de legenda explicando o significado das cores, símbolos ou tipos de linha ( Figura 1.15). Essencial sempre que você usar cores ou tipos de linha diferentes para representar grupos, pois o R Base não cria legendas automáticas (ao contrário do ggplot2).\n\n\n\nCódigo\n# O dataset iris tem 3 espécies. Vamos criar um vetor de 3 cores.\nminhas_cores &lt;- c(\"red\", \"blue\", \"forestgreen\")\n\n#Criar o gráfico base\nplot(x = iris$Sepal.Length, \n     y = iris$Sepal.Width,\n     col = minhas_cores[iris$Species], \n     pch = 19, # Bolinha sólida\n     main = \"\",\n     xlab = \"Comprimento\",\n     ylab = \"Largura\")\n\n#Adicionar a legenda\nlegend(\"topright\", \n       legend = levels(iris$Species), # Pega os nomes reais: setosa, versicolor...\n       col = minhas_cores,            # Usa AS MESMAS cores definidas acima\n       pch = 19,                      # Usa O MESMO símbolo do plot\n       title = \"Espécies\",\n       bty = \"n\")                     #Remove a caixa em volta da legenda\n\n\n\n\n\n\n\n\nFigura 1.15: Sépala: Comprimento vs Largura\n\n\n\n\n\nParâmetros Gráficos (par): Configuram o dispositivo gráfico globalmente.\n\npar(mar = c(bottom, left, top, right)): Define as margens da figura na ordem: Baixo, Esquerda, Cima, Direita. A unidade é “linhas de texto”. O padrão é c(5, 4, 4, 2) + 0.1. Utilize quando os rótulos dos eixos são cortados ou quando você precisa de espaço extra fora do gráfico para uma legenda externa ( Figura 1.16).\n\n\nCódigo\n# Gráfico 1: Margens padrão\n# O texto longo no eixo X será cortado\nbarplot(1:5, \n        names.arg = c(\"Nome Muito Longo 1\", \"Nome Muito Longo 2\", \"...\", \"...\", \"...\"), \n        las = 2,\n        main = \"Margem Padrão\")\n\n# Gráfico 2: Margens ajustadas\n# par(mar = c(baixo, esquerda, cima, direita))\n# Aumentamos o primeiro valor (baixo) de ~5 para 10\npar(mar = c(10, 4, 4, 2))\n\nbarplot(1:5, \n        names.arg = c(\"Nome Muito Longo 1\", \"Nome Muito Longo 2\", \"...\", \"...\", \"...\"), \n        las = 2,\n        main = \"Margem Ajustada\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sem aumentar as margens (Corta o texto)\n\n\n\n\n\n\n\n\n\n\n\n(b) Aumentando as margens (Texto visível)\n\n\n\n\n\n\n\nFigura 1.16: Ajuste de margens em barplots\n\n\n\n\npar(mfrow = c(rows, cols)): Cria uma grade (matriz) para plotar múltiplos gráficos na mesma janela. c(linhas, colunas). Utilize para comparar visualizações lado a lado (ex: um histograma ao lado de um boxplot da mesma variável).\n\n\n\nCódigo\npar(mfrow = c(1, 2)) # Prepara grade 1 linha x 2 colunas\nhist(mtcars$mpg, xlab=\"\", ylab=\"\", main=\"\")\nboxplot(mtcars$mpg, xlab=\"\", ylab=\"\", main=\"\") # Plota os dois\n\n\n\n\n\n\n\n\n\nCódigo\npar(mfrow = c(1, 1)) # Restaura para 1 gráfico por vez\n\n\n\npar(usr): Retorna (ou define) os limites extremos das coordenadas do gráfico no formato c(x1, x2, y1, y2). Utilize para saber exatamente onde terminam os eixos para posicionar textos nos cantos absolutos da figura, independente dos dados.\n\n\n\nCódigo\nplot(1:10)\n\n\n\n\n\n\n\n\n\nCódigo\nlimites &lt;- par(\"usr\") # Retorna ex: c(0.64, 10.36, 0.64, 10.36)\n# Use o limite superior do eixo Y para posicionar algo no topo\n\n\n\nstrheight(), strwidth(): Calcula quanto espaço (nas coordenadas do usuário) uma string de texto vai ocupar. Utilize para posicionar legendas personalizadas exatamente “ao lado” de um ponto sem sobrepor, ou para desenhar caixas ao redor de textos manualmente.\n\n\n\nCódigo\nplot(1:10)\nlargura &lt;- strwidth(\"Meu Texto\")\n# Desenha uma linha exatamente do tamanho do texto\nsegments(x0=1, y0=5, x1=1+largura, y1=5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportanteggplot2\n\n\n\nUm pacote muito bom e completo para questões de visualização gráfica",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#visualização-profissional-de-dados-com-ggplot2",
    "href": "intro.html#visualização-profissional-de-dados-com-ggplot2",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.18 Visualização Profissional de Dados com ggplot2",
    "text": "1.18 Visualização Profissional de Dados com ggplot2\nO pacote ggplot2, implementação da Gramática dos Gráficos, que constrói visualizações complexas sobrepondo camadas lógicas independentes. Foca no mapeamento de dados em atributos visuais, permitindo flexibilidade total sem precisar memorizar dezenas de funções isoladas. Clique aqui para se informar melhor.\n\n1.18.1 Estrutura fundamental\nTodo gráfico necessita de três componentes mínimos:\n\nDados (Data): O data frame contendo as informações brutas e variáveis que alimentarão o gráfico. É a base fundamental que permanece inalterada, servindo de fonte para todas as camadas visuais subsequentes.\nEstética (Aesthetics - aes): O que representa o eixo X?, o eixo Y? a cor?, etc. Mapeia colunas de dados para propriedades visuais do gráfico (como eixos X/Y, cores ou tamanhos). Define “o quê” será mostrado e como as variáveis controlarão a aparência dinâmica dos elementos.\nGeometria (Geoms): Qual a forma visual (barra, ponto, linha)?. É o objeto visual escolhido para representar os dados na tela, definindo a forma do gráfico (ex: barras, pontos, linhas).\n\n\nTemplate Universal /\"A Anatomia do ggplot\"\n\nO ggplot2 funciona através da sobreposição de camadas (layers) conectadas pelo sinal de +.\n\n\n\n\n\n\nImportante\n\n\n\nA ordem importa, o que vem depois é desenhado por cima do anterior.\n\n\nggplot(\n  data = &lt;DADOS&gt;, \n  mapping = aes(x = &lt;VAR_X&gt;, y = &lt;VAR_Y&gt;, color = &lt;VAR_GRUPO&gt;, fill = &lt;VAR_PREENCHIMENTO&gt;)\n) +\n  \n#1. GEOMETRIAS (A forma visual) ---\n# Podem herdar os aes() globais ou ter os seus próprios\n&lt;GEOM_FUNÇÃO&gt;(\n  mapping = aes(...),           # Mapeamento específico desta camada\n  data = &lt;DADOS_FILTRADOS&gt;,     # Usar dados diferentes nesta camada\n  stat = \"&lt;ESTATÍSTICA&gt;\",       # Transformação estatística (ex: \"identity\", \"count\")\n  position = \"&lt;POSIÇÃO&gt;\",       # Ajuste de sobreposição (ex: \"dodge\", \"jitter\", \"stack\")\n  alpha = 0.5,                  # Transparência fixa (não mapeada)\n  size = 2                      # Tamanho fixo\n) +\n\n#2. ESTATÍSTICAS\n# Útil quando você quer resumir dados (ex: média e erro) sem pré-calcular\nstat_summary(fun = mean, geom = \"point\") +\nstat_smooth(method = \"lm\") +\n\n#FACETAS (Small Multiples)\n# Dividir em sub-gráficos\nfacet_wrap(~ &lt;VAR_CATEGORIA&gt;, scales = \"free_y\") +  # Ou facet_grid(l ~ c)\n\n#ESCALAS (O controle fino dos mapeamentos)\n# Controlam eixos (limites, log), cores, tamanhos e formatos\nscale_x_continuous(limits = c(0, 100), breaks = seq(0, 100, 10)) +\nscale_y_log10() +\nscale_color_manual(values = c(\"red\", \"blue\", \"green\")) + # Cores personalizadas\nscale_fill_viridis_d() +                                 # Paletas prontas\n\n#COORDENADAS (O espaço do gráfico)\n# Inverter eixos, fixar proporção, mapas ou polar\ncoord_flip() +           # Deita o gráfico (x vira y)\n# coord_cartesian(ylim = c(0, 50)) + # Zoom sem cortar dados\n# coord_fixed(ratio = 1) +           # Garante proporção 1:1\n\n#RÓTULOS E ANOTAÇÕES\nlabs(\n  title = \"Título Principal\",\n  subtitle = \"Subtítulo explicativo\",\n  caption = \"Fonte: Base de dados X\",\n  x = \"Rótulo Eixo X\",\n  y = \"Rótulo Eixo Y\",\n  color = \"Título da Legenda de Cor\",\n  tag = \"Fig. A\" # Atribui letra de identificação a figura, \n                 #casos em em vai colocar dois ou mais graficos e quer identificar-os pode letra\n) +\n\n# Adicionar texto/setas manuais soltos no gráfico\nannotate(\"text\", x = 10, y = 50, label = \"Ponto Crítico\", color = \"red\") +\n\n# GUIAS (Customização das Legendas)\n# Ajustes finos na aparência das legendas (remover, mudar linhas, etc)\nguides(\n  color = guide_legend(override.aes = list(size = 5)), # Aumenta bolinha na legenda\n  fill = \"none\" # Remove a legenda de preenchimento\n) +\n\n# TEMA (Aparência Geral)\ntheme_minimal(base_size = 14) +  # Comece com um tema pronto\n\n# AJUSTES DE TEMA (Personalização final)\n# Sobrescreve detalhes do tema escolhido acima\ntheme(\n  legend.position = \"top\",             # Posição da legenda (top, bottom, left, right, none)\n  plot.title = element_text(face = \"bold\", hjust = 0.5), # Título centralizado e negrito\n  axis.text.x = element_text(angle = 45, hjust = 1),     # Rotação do texto do eixo X\n  panel.grid.minor = element_blank()   # Remove grades menores\n)\nMapeamento vs. Configuração\nA maior fonte de erros para iniciantes e intermediários é a distinção entre estar dentro ou fora do aes().\n\n\n\n\n\n\nImportanteaes()\n\n\n\n\nDentro do aes() (Mapeamento): Conecta uma variável dos dados a uma propriedade visual, fazendo a aparência mudar dinamicamente conforme o valor (ex: cores diferentes para espécies diferentes). O ggplot2 cria automaticamente uma legenda para explicar essa relação entre dado e visual.\n\nEx: aes(color = Species) - “A cor muda conforme a Espécie”.\n\nFora do aes() (Configuração): Aplica um estilo fixo e uniforme a todos os elementos da geometria, ignorando os valores dos dados (ex: pintar todos os pontos de azul). Não gera legenda, pois define apenas uma constante estética sem vínculo estatístico.\n\nEx: geom_point(color = \"blue\") - “Todos os pontos são azuis”.\n\n\n\n\n\n\n1.18.2 Geometrias (Geoms)\nDefinem a forma do gráfico.\nCaso Univariado (Uma Variável)\nA. Para Variáveis Contínuas (Números Reais)\nEx: Salário, Idade, Temperatura, Altura.\n\ngeom_histogram() Figura 1.17\n\nDivide os dados em compartimentos (bins) e conta a frequência. Útil para visualização da distribuição dos dados.\nParâmetros Chave: binwidth (largura do intervalo) ou bins (quantidade de barras). Sempre teste larguras de bins diferentes; a escolha padrão (30) raramente é a ideal.\n\n\n\n\nCódigo\npacman::p_load(ggplot2)\nggplot(diamonds, aes(x = price)) +\n  geom_histogram(binwidth = 1000, fill = \"dodgerblue\", color = \"white\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.17: Histograma do Preço de Diamantes\n\n\n\n\n\n\ngeom_density() Figura 1.18\n\nEstima a Função Densidade de Probabilidade (KDE). É um histograma “suavizado”. Útil quando você quer ver a forma da distribuição sem o ruído das barras.\nParâmetros Chave: adjust (suavização: &lt;1 é detalhado, &gt;1 é liso), alpha (transparência).\n\n\n\n\nCódigo\nggplot(diamonds, aes(x = price, fill = cut)) +\n  geom_density(alpha = 0.5) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.18: Densidade suavizada\n\n\n\n\n\n\ngeom_freqpoly() Figura 1.19\n\nCalcula o mesmo que o histograma, mas desenha linhas conectando os topos das barras em vez das barras em si. Ideal para comparar distribuições de vários grupos sobrepostos (onde histogramas ficariam bagunçados).\n\n\n\n\nCódigo\nggplot(diamonds, aes(x = price, color = cut)) +\n  geom_histogram(binwidth = 1000) + #barras histograma\n  geom_freqpoly(binwidth = 1000) + #linhas \n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.19: Polígono de frequência comparando cortes\n\n\n\n\n\n\ngeom_dotplot() Figura 1.20\n\nEmpilha um ponto para cada observação. É ótimo para conjuntos de dados pequenos (\\(N &lt; 100\\)) onde você quer mostrar cada indivíduo.\n\n\n\n\n\n\n\n\nImportanteAtenção\n\n\n\nRequer binaxis = \"x\" e stackdir = \"center\" ou \"up\" para funcionar bem como univariado.\n\n\n\n\nCódigo\nggplot(mtcars, aes(x = mpg)) +\n  geom_dotplot(binaxis = \"x\", stackdir = \"center\", dotsize = 0.7) +\n  theme_minimal() # Remove eixos para focar nos pontos\n\n\n\n\n\n\n\n\nFigura 1.20: Dotplot do consumo de combustivel\n\n\n\n\n\n\nstat_ecdf() (ou geom_step) Figura 1.21\n\nFunção de Distribuição Acumulada Empírica. Útil para pesponder perguntas como “Qual porcentagem dos meus dados está abaixo do valor X?”. É a visualização mais estatisticamente íntegra, pois não depende de bins ou suavização.\n\n\n\n\nCódigo\nggplot(diamonds, aes(x = price)) +\n  stat_ecdf(geom = \"step\", color = \"darkred\") +\n  labs(y = \"Probabilidade Acumulada\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.21: Distribuição Acumulada\n\n\n\n\n\n\ngeom_rug() Figura 1.22\n\nDesenha pequenos riscos (tiques) nas margens dos eixos para cada dado existente (Veja eixo X da Figura 1.22 e Figura 1.5). Usado como complemento em histogramas ou densidades para mostrar onde estão os dados reais, especialmente para identificar outliers ou clusters.\n\n\n\n\nCódigo\nggplot(mtcars, aes(x = wt)) +\n  geom_density(fill = \"gray90\") +\n  geom_rug(sides = \"b\", length = unit(0.2, \"cm\")) + # 'b' = bottom (embaixo)\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.22: Densidade com Rug Plot na base\n\n\n\n\n\nB. Para Variáveis Discretas (Categorias)\nEx: Espécie, País, Mês, Sim/Não.\n\ngeom_bar() Figura 1.23\n\nConta quantas linhas existem para cada categoria. O R faz o trabalho de contar por você. Útil quando você tem a lista bruta de dados (ex: uma planilha com 1000 linhas onde a coluna “Estado” repete SP, MG, RJ várias vezes).\nStat Padrão: count (o R calcula a contagem).\nParâmetros Chave: fill (cor interna das barras), width (largura das barras).\n\n\n\n\nCódigo\nggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(fill = \"steelblue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.23: Contagem automática de carros por cilindro\n\n\n\n\n\n\ngeom_col() Figura 1.24\n\nDesenha barras com alturas baseadas em valores numéricos que já existem na sua tabela. Útil quando você já tem o resumo pronto (ex: uma tabela pequena com apenas 3 linhas: SP=500, MG=300, RJ=200).\nStat Padrão: identity (o R usa o valor exatamente como ele é).\n\n\n\n\nCódigo\n# Criando uma tabela resumo fictícia\nresumo &lt;- data.frame(fruta = c(\"Maçã\", \"Banana\"), valor = c(20, 30))\n\nggplot(resumo, aes(x = fruta, y = valor)) +\n  geom_col(fill = \"orange\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.24: Barras com valores pré-definidos\n\n\n\n\n\n\n\n\n\n\n\nDicaBar vs Col\n\n\n\n\nUse geom_bar() se você tiver dados brutos e quiser que o R conte.\nUse geom_col() se você já tiver uma tabela resumo com os totais.\n\n\n\nC. Gráficos de Resumo e Distribuição (Pseudo-Univariados)\nEmbora geralmente usados para comparar grupos (Bivariados), eles são excelentes para analisar uma única variável numérica se você mapear x = \"\" ou x = 1.\n\ngeom_boxplot() Figura 1.25\n\nO diagrama de caixa. Mostra a Mediana (linha central), o Intervalo Interquartil (a caixa, de 25% a 75%) e os outliers (pontos além dos bigodes). Útil para para detectar valores extremos, distribuição, assimetria, etc.\nParâmetros Chave: outlier.color (destacar os outliers), notch = TRUE (adiciona um entalhe na mediana para comparação visual de significância).\n\n\n\n\nCódigo\nggplot(mtcars, aes(x = factor(cyl), y = hp)) +\n  geom_boxplot(outlier.color = \"red\", outlier.shape = 8) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigura 1.25: Boxplot da potência por cilindro\n\n\n\n\n\n\ngeom_violin() Figura 1.26\n\nUm espelhamento do geom_density (densidade) rotacionado em 90 graus. Mostra a forma completa da distribuição de maneira compacta. É mais rico que o boxplot porque revela se a distribuição é bimodal (tem “duas corcovas”), algo que o boxplot esconde.\nParâmetros Chave: draw_quantiles (desenha linhas nos quartis dentro do violino).\n\n\n\n\nCódigo\nggplot(diamonds[1:500,], aes(x = cut, y = price)) +\n  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75), fill = \"lightblue\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nFigura 1.26: Violin plot comparando distribuições\n\n\n\n\n\n\ngeom_jitter() Figura 1.27\n\nAdiciona um pequeno ruído aleatório aos pontos para evitar sobreposição (overplotting). Frequentemente usado por cima do geom_boxplot() para mostrar os dados brutos reais junto com o resumo estatístico.\nParâmetros Chave: width e height (controlam o quanto os pontos podem “tremer”).\n\n\n\n\nCódigo\nggplot(mtcars, aes(x = factor(cyl), y = hp)) +\n  geom_boxplot(alpha = 0.3) +\n  geom_jitter(width = 0.2, color = \"darkblue\") + # width controla a 'tremida' horizontal\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.27: Boxplot + Jitter (Dados brutos)\n\n\n\n\n\nBivariada: Contínua X Contínua Ex: Peso vs Altura, Preço vs Quilometragem.\n\ngeom_point() Figura 1.28\n\nO clássico gráfico de dispersão (Scatterplot). Mostra a relação exata entre duas variáveis.\n\n\n\n\nCódigo\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(size = 3, color = \"darkgreen\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.28: Scatterplot clássico\n\n\n\n\n\n\ngeom_jitter() Figura 1.29\ngeom_smooth()\n\nAdiciona uma linha de tendência (regressão linear, LOESS, GAM). Ajuda o olho a ver o padrão no meio dos pontos.\n\n\n\n\nCódigo\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") + # se=TRUE mostra intervalo de confiança\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.29: Scatterplot com linha de tendência linear\n\n\n\n\n\n\ngeom_quantile() Figura 1.30\n\nRegressão quantílica. Em vez da média (como o smooth), mostra tendências para os 25%, 50% e 75% mais extremos.\n\n\n\n\nCódigo\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(alpha = 0.3) +\n  \n  # Regressão Linear (Média) em Vermelho para comparação\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  \n  # Regressão Quantílica (25%, 50%, 75%) em Azul\n  geom_quantile(quantiles = c(0.25, 0.5, 0.75), color = \"blue\", size = 1) +\n  \n  theme_minimal() +\n  labs(title = \"Tendências dos Extremos vs Média\",\n       subtitle = \"Azul: Quantis (0.25, 0.50, 0.75) | Vermelho: Média (LM)\")\n\n\n\n\n\n\n\n\nFigura 1.30: Regressão Quantílica (25%, 50%, 75%) vs Regressão Linear (Vermelho)\n\n\n\n\n\n\ngeom_text() e geom_label() Figura 1.31\n\nEscrevem texto no gráfico nas coordenadas X e Y.\nDiferença: geom_label desenha uma caixinha colorida atrás do texto para facilitar a leitura; geom_text desenha apenas as letras.\n\n\n\n\nCódigo\ndf &lt;- data.frame(x = c(1, 3), y = c(1, 1), texto = c(\"Texto Puro\", \"Etiqueta\"))\n\nggplot(df, aes(x = x, y = y, label = texto)) +\n  xlim(0, 4) + ylim(0, 2) +\n  # Lado Esquerdo: Texto solto\n  geom_text(data = subset(df, x == 1), size = 6) +\n  # Lado Direito: Texto dentro da caixinha\n  geom_label(data = subset(df, x == 3), size = 6, fill = \"white\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigura 1.31: Diferença entre geom_text e geom_label\n\n\n\n\n\n\ngeom_bin2d() e geom_hex() Figura 1.32\n\nQuando você tem milhões de pontos, o scatterplot vira uma mancha preta. Estes geoms dividem o plano em quadrados (bin2d) ou hexágonos (hex) e pintam a cor baseada na contagem de pontos naquela região (mapa de calor 2D).\n\n\n\n\nCódigo\npacman::p_load(hexbin)\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_hex(bins = 30) +\n  scale_fill_viridis_c() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.32: Mapa de calor hexagonal (Hexbin)\n\n\n\n\n\nBivariada: Discreta X Contínua\nEx: Grupo (A, B) vs Nota.\nAlém dos já citados geom_boxplot, geom_violin e geom_col:\n\ngeom_dotplot(binaxis = \"y\") Figura 1.33\n\nSemelhante ao violino, mas feito de bolinhas empilhadas.\n\n\n\n\nCódigo\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  # binaxis = \"y\" faz as bolinhas subirem no eixo Y\n  # stackdir = \"center\" empilha elas para os dois lados (parece um violino)\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", dotsize = 0.8) +\n  theme_minimal() +\n  theme(legend.position = \"none\") # Remove a legenda pois a cor já indica os cilindros\n\n\n\n\n\n\n\n\nFigura 1.33: Dotplot Bivariado: Distribuição do consumo por cilindros\n\n\n\n\n\n\n\n1.18.3 Séries Temporais e Funções (Evolução)\nEx: Data no Eixo X.\n\ngeom_line() Figura 1.34\n\nConecta os pontos na ordem da variável X. Padrão para séries temporais.\n\n\n\n\nCódigo\neconomics |&gt; \n  ggplot(aes(x = date, y = unemploy)) +\n  geom_line(color = \"blue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.34: Série temporal simples\n\n\n\n\n\n\ngeom_path() Figura 1.35\n\nConecta os pontos na ordem em que aparecem na tabela (mesmo que o X volte para trás). Usado para desenhar trajetórias em mapas ou formas complexas.\n\n\n\n\nCódigo\n# Note que a linha faz voltas e loops, o que seria impossível com geom_line()\n# geom_path conecta os dados cronologicamente\nggplot(economics, aes(x = uempmed, y = psavert)) +\n  geom_path(color = \"purple\") +\n  labs(\n    title = \"Trajetória Econômica (1967-2015)\",\n    subtitle = \"A linha conecta os pontos na ordem cronológica\",\n    x = \"Duração do Desemprego (Mediana)\",\n    y = \"Taxa de Poupança Pessoal\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.35: Geom Path: Ciclo Econômico (Desemprego vs Poupança)\n\n\n\n\n\n\ngeom_step() Figuar Figura 1.36\n\nGráfico de “escada”. A linha só muda de nível abruptamente. Bom para visualizar taxas de juros ou estoques que mudam em saltos.\n\n\n\n\nCódigo\n# Criando dados fictícios de uma taxa que muda pouco\ndados_step &lt;- data.frame(\n  mes = 1:12,\n  taxa = c(2, 2, 2, 5, 5, 5, 5, 3, 3, 6, 6, 6)\n)\n\nggplot(dados_step, aes(x = mes, y = taxa)) +\n  geom_step(color = \"darkblue\", size = 1) +\n  geom_point(color = \"red\") + # Adicionando pontos para mostrar quando a medição ocorreu\n  labs(title = \"Evolução da Taxa (Gráfico de Escada)\") +\n  scale_x_continuous(breaks = 1:12) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigura 1.36: Geom Step: Mudanças na Taxa de Juros (Simulado)\n\n\n\n\n\n\ngeom_area() Figura 1.37\n\nUm gráfico de linha onde a área abaixo dela (até o 0) é preenchida.\n\n\n\n\nCódigo\neconomics |&gt; \n  ggplot(aes(x = date, y = unemploy)) +\n  geom_area(fill = \"lightblue\", alpha = 0.6) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.37: Gráfico de área\n\n\n\n\n\n\ngeom_ribbon() Figura 1.38\n\nDesenha uma faixa colorida entre um valor mínimo (ymin) e máximo (ymax). Fundamental para desenhar Intervalos de Confiança ao redor de uma linha.\n\n\n\n\nCódigo\n# Criando dados fictícios de previsão\ndf_ribbon &lt;- data.frame(\n  ano = 2000:2010,\n  valor = 10:20,\n  min = (10:20) - 2,\n  max = (10:20) + 2\n)\n\nggplot(df_ribbon, aes(x = ano, y = valor)) +\n  geom_ribbon(aes(ymin = min, ymax = max), fill = \"grey80\") +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.38: Ribbon (faixa) ao redor de uma linha\n\n\n\n\n\nVisualização de Incerteza e Erro Ex: Médias com desvio padrão.\n\ngeom_errorbar() Figura 1.39\n\nA clássica barra de erro (formato “I”).\n\n\n\n\nCódigo\ndf_erro &lt;- data.frame(grp = c(\"A\", \"B\"), media = c(10, 15), sd = c(1, 2))\n\np1 &lt;- ggplot(df_erro, aes(x = grp, y = media)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = media-sd, ymax = media+sd), width = 0.2) +\n  ggtitle(\"Errorbar\")+\n  theme_minimal()\n\np2 &lt;- ggplot(df_erro, aes(x = grp, y = media)) +\n  geom_pointrange(aes(ymin = media-sd, ymax = media+sd)) +\n  ggtitle(\"Pointrange\")+\n  theme_minimal()\n\npacman::p_load(patchwork)\np1 + p2\n\n\n\n\n\n\n\n\nFigura 1.39: Barras de erro e Pointrange\n\n\n\n\n\n\ngeom_linerange() Figura 1.40\n\nUma linha vertical simples indicando o intervalo (sem os traços horizontais nas pontas). Visual mais limpo.\n\n\n\n\nCódigo\ndados_erro &lt;- data.frame(\n  grupo = c(\"Controle\", \"Tratamento A\", \"Tratamento B\"),\n  media = c(20, 25, 18),\n  min = c(18, 23, 15),\n  max = c(22, 27, 21)\n)\n\nggplot(dados_erro, aes(x = grupo, y = media)) +\n  # O linerange desenha apenas a linha vertical\n  geom_linerange(aes(ymin = min, ymax = max), size = 1.2, color = \"gray50\") +\n  # Adicionamos o ponto separadamente para marcar a média\n  geom_point(size = 4, color = \"blue\") +\n  theme_minimal() +\n  labs(title = \"Geom Linerange + Point\")\n\n\n\n\n\n\n\n\nFigura 1.40: Linerange: Intervalo limpo (Min-Max)\n\n\n\n\n\n\ngeom_pointrange() Figura 1.41\n\nCombina um ponto (média) e uma linha vertical (intervalo) em um único geom geométrico. Muito usado em Forest Plots.\n\n\n\n\nCódigo\nggplot(dados_erro, aes(x = grupo, y = media)) +\n  # ymin e ymax são obrigatórios aqui\n  geom_pointrange(aes(ymin = min, ymax = max), color = \"darkred\") +\n  coord_flip() + # Forest plots geralmente são deitados\n  theme_bw() +\n  labs(title = \"Geom Pointrange (Invertido)\")\n\n\n\n\n\n\n\n\nFigura 1.41: Pointrange: O padrão para Forest Plots\n\n\n\n\n\n\ngeom_crossbar() Figura 1.42\n\nUma caixa vazia ou preenchida representando o intervalo, com uma linha na média.\n\n\n\n\nCódigo\nggplot(dados_erro, aes(x = grupo, y = media)) +\n  geom_crossbar(\n    aes(ymin = min, ymax = max), \n    width = 0.5, \n    fill = \"lightblue\", \n    alpha = 0.5\n  ) +\n  theme_minimal() +\n  labs(title = \"Geom Crossbar\")\n\n\n\n\n\n\n\n\nFigura 1.42: Crossbar: Intervalo como caixa sólida\n\n\n\n\n\nTrês Variáveis (Mapas de Calor e Contornos)\nEx: Z = Altitude, variando em X (Lat) e Y (Long).\n\ngeom_tile() e geom_raster() Figura 1.43\n\nCriam superfícies ou mapas de calor (heatmaps). geom_raster é uma versão otimizada de geom_tile para quando todos os quadrados têm o mesmo tamanho (renderiza mais rápido).\n\n\n\n\nCódigo\n# Matriz de correlação transformada em formato longo\ncormat &lt;- as.data.frame(as.table(cor(mtcars)))\n\nggplot(cormat, aes(Var1, Var2, fill = Freq)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.43: Heatmap (Mapa de Calor)\n\n\n\n\n\n\ngeom_contour() e geom_contour_filled() Figura 1.44\n\nDesenha linhas de contorno (como em mapas topográficos) baseadas na variável Z.\n\n\n\n\nCódigo\npacman::p_load(reshape2)\nvolcano_df &lt;- melt(volcano)\nnames(volcano_df) &lt;- c(\"x\", \"y\", \"z\")\n\nggplot(volcano_df, aes(x, y, z = z)) +\n  # Preenchimento das regiões (o visual colorido)\n  geom_contour_filled() +\n  # Linhas pretas finas para demarcar \n  geom_contour(color = \"black\", size = 0.1) +\n  theme_void() + # Remove eixos para parecer um mapa\n  coord_fixed()\n\n\n\n\n\n\n\n\nFigura 1.44: Topografia do Vulcão Maunga Whau (Contour Filled)\n\n\n\n\n\nMapas e Espacial{#sec-espacial_visualizacao}\n\ngeom_sf() Figura 1.45 e Figura 1.46\n\nPlota objetos Simple Features (do pacote sf). Lida automaticamente com projeções geográficas, fronteiras de países e coordenadas.\n\n\n\n\nCódigo\npacman::p_load(sf, ggplot2)\n\n# Lendo um shapefile de exemplo que vem dentro do pacote sf\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package=\"sf\"), quiet = TRUE)\n\nggplot(nc) +\n  geom_sf(aes(fill = AREA), color = \"white\") +\n  scale_fill_viridis_c(name = \"Área\") +\n  theme_minimal() +\n  labs(title = \"\")\n\n\n\n\n\n\n\n\nFigura 1.45: Mapa da Carolina do Norte (Shapefile nativo)\n\n\n\n\n\n\n\nCódigo\npacman::p_load(sf, ggplot2, geodata, terra, viridis)\n\n#Baixar dados do GADM\n# country = \"MOZ\" (Código ISO de Moçambique)\n# level = 2 (Distritos)\n# path = tempdir() salva numa pasta temporária. Para salvar no projeto, use path = \".\"\n\ntryCatch({\n    moz_data_spat &lt;- geodata::gadm(country = \"MOZ\", level = 2, path = tempdir(), version=\"latest\")\n  }, error = function(e) {\n    message(\"Erro ao baixar do GADM. Verifique a conexão.\")\n    # Cria um objeto sf vazio em caso de erro para não quebrar o render\n    moz_data_spat &lt;- NULL\n})\n\n\nif (!is.null(moz_data_spat)) {\n  #Converter de SpatVector (terra) para objeto sf\n  moz_sf &lt;- sf::st_as_sf(moz_data_spat)\n\n  #Plotar\n  ggplot2::ggplot(moz_sf) +\n    geom_sf(aes(fill = NAME_1), color = \"white\", size = 0.1) +\n    scale_fill_viridis_d(name = \"Província\", option = \"D\") + # Paleta discreta bonita\n    theme_minimal() +\n    theme(legend.position = \"right\") + # Legenda à direita\n    labs(title = \"\",\n         caption = \"Fonte: Dados GADM via pacote {geodata}\")\n\n} else {\n  # Mensagem alternativa no plot caso o download falhe\n  ggplot2::ggplot() +\n    annotate(\"text\", x=0, y=0, label=\"Não foi possível baixar os dados do mapa.\") +\n    theme_void()\n}\n\n\n\nO GADM as vezes tem problemas, se não funcionou o codigo acima baixa pelo link abaixo ou vá ao site do GADM\n\n\n\nCódigo\npacman::p_load(sf, ggplot2, ggrepel) \n\n#Se geodata não funcionar baixe os dados no link: https://drive.google.com/drive/folders/1pL_MLujCv_6h5VJqK20B8pUHSNlZ5Olt?usp=sharing\n\n# Abra o arquivo que terá baixado no link acima caso gadm não tenha funcionado\n#troque meu \"/home/almonha/Downloads/Curso de Verão/moz_adm/moz_admbnda_adm1_ine_20190607.shp\" pelo seu.\n\ntryCatch({  \n  moz_data_spat &lt;- read_sf(\"/home/almonha/Downloads/Curso de Verão/moz_adm/moz_admbnda_adm1_ine_20190607.shp\") \n  }, error = function(e) {\n    message(\"O link não funcionou vou baixar pelo GADM.\")\nmoz_data_spat &lt;- geodata::gadm(country = \"MOZ\", level = 2, path = tempdir(), version=\"latest\")\n}) \n\n\n\nif (!is.null(moz_data_spat)) {\n  \n  moz_sf &lt;- sf::st_as_sf(moz_data_spat)\n  \n  ggplot2::ggplot(data = moz_sf) +\n    geom_sf(aes(fill = ADM1_PT), color = \"white\", size = 0.1) +\n  scale_fill_viridis_d(name = \"Província\", option = \"D\") + \n  theme_minimal()+\n    theme(legend.position = \"right\",\n    axis.title = element_blank()\n    )+ # Remove X e Y de forma mais limpa) + \n    labs(\n      title = \"Mapa de Moçambique\",\n      caption = \"Fonte: Dados GADM via pacote {geodata}\",\n    )\n\n} else {\n  ggplot2::ggplot() +\n    annotate(\"text\", x=0, y=0, label=\"Não foi possível baixar os dados do mapa.\") +\n    theme_void()\n}\n\n\n\n\n\n\n\n\nFigura 1.46: Mapa de Moçambique - Nível Administrativo 2 (Distritos)\n\n\n\n\n\n\ngeom_map()\n\nVersão antiga para desenhar polígonos de mapas. Prefira geom_sf.\n\n\nPrimitivas e Referências: Usados para anotações manuais.\n\ngeom_abline() , geom_hline(), geom_vline(), segue o descrito na seção Seção 1.17 .\ngeom_segment() e geom_curve() Figura 1.47\n\nDesenha linhas ou curvas de um ponto A (x, y) até um ponto B (xend, yend). Útil para setas e anotações.\n\n\n\n\nCódigo\ndf_pontos &lt;- data.frame(x = c(2, 8), y = c(2, 8), label = c(\"Início\", \"Fim\"))\n\nggplot(df_pontos, aes(x, y)) +\n  geom_point(size = 3) +\n  xlim(0, 10) + ylim(0, 10) +\n  # Segmento Reto com Seta\n  geom_segment(aes(x = 2, y = 2, xend = 5, yend = 5), \n               arrow = arrow(length = unit(0.3, \"cm\")), color = \"blue\") +\n  # Curva com Seta\n  geom_curve(aes(x = 5, y = 5, xend = 8, yend = 8), \n             curvature = -0.3, arrow = arrow(), color = \"red\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigura 1.47: Anotações com Segmentos e Curvas\n\n\n\n\n\n\ngeom_rect() e geom_polygon() Figura 1.48\n\nDesenha retângulos ou formas arbitrárias baseadas em coordenadas.\n\n\n\n\nCódigo\nggplot(mtcars, aes(wt, mpg)) +\n  # O retângulo deve vir ANTES dos pontos para ficar no fundo\n  geom_rect(aes(xmin = 3, xmax = 4, ymin = 10, ymax = 25), \n            fill = \"yellow\", alpha = 0.05, color = NA) +\n  geom_point() +\n  annotate(\"text\", x = 3.5, y = 26, label = \"Zona de Atenção\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.48: Destacando uma região de interesse com geom_rect\n\n\n\n\n\n\ngeom_spoke() Figura 1.49\n\nDesenha segmentos definidos por ângulo e raio (útil para campos vetoriais, como direção do vento).\n\n\n\n\nCódigo\ndf_vento &lt;- expand.grid(x = 1:10, y = 1:10)\ndf_vento$angle &lt;- runif(100, 0, 2*pi) # Ângulo em radianos\ndf_vento$speed &lt;- runif(100, 0.4, 0.8) # Comprimento do vetor\n\nggplot(df_vento, aes(x, y)) +\n  geom_spoke(aes(angle = angle, radius = speed), arrow = arrow(length = unit(0.1, \"cm\"))) +\n  theme_void() +\n  labs(title = \"\")\n\n\n\n\n\n\n\n\nFigura 1.49: Campo Vetorial (Direção e Intensidade Aleatórias)\n\n\n\n\n\n\ngeom_qq() e geom_qq_line() Figura 1.50\n\nGráfico Quantil-Quantil (QQ Plot) para checar normalidade dos resíduos.\n\n\n\n\nCódigo\nggplot(mtcars, aes(sample = mpg)) +\n  geom_qq() +        # Os pontos\n  geom_qq_line(color = \"red\") + # A linha de referência normal\n  labs(title = \"Os dados de MPG são normais?\", \n       subtitle = \"Pontos fora da linha indicam desvio da normalidade\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 1.50: QQ Plot: Verificação de Normalidade\n\n\n\n\n\nFaceting (Small Multiples)\nA melhor técnica para evitar gráficos “espaguete” (muitas linhas misturadas).\n\nfacet_wrap(~variavel): Fluxo contínuo (quebra linha quando acaba espaço), Figura 1.51 .\n\n\n\nCódigo\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(alpha = 0.5) +\n  # Divide o gráfico pelo tipo de carro ('class')\n  # ncol = 3 força 3 colunas\n  facet_wrap(~class, ncol = 3) + \n  theme_bw() +\n  labs(title = \"Consumo vs Motor\", subtitle = \"Separado por categoria (class)\")\n\n\n\n\n\n\n\n\nFigura 1.51: Facet Wrap: Consumo por Tipo de Carro\n\n\n\n\n\n\nfacet_grid(lin ~ col): Matriz estrita. Use . se quiser apenas uma dimensão (ex: . ~ col), Figura 1.52 .\n\n\n\nCódigo\nmtcars_facets &lt;- mtcars\nmtcars_facets$am &lt;- factor(mtcars$am, labels = c(\"Automático\", \"Manual\"))\nmtcars_facets$cyl &lt;- factor(mtcars$cyl, labels = c(\"4 Cilindros\", \"6 Cilindros\", \"8 Cilindros\"))\n\nggplot(mtcars_facets, aes(x = wt, y = mpg)) +\n  geom_point(color = \"darkred\") +\n  # Linhas definidas pelo Câmbio (am) ~ Colunas definidas pelos Cilindros (cyl)\n  facet_grid(am ~ cyl) + \n  theme_bw() +\n  labs(title = \"Comparação Matricial\")\n\n\n\n\n\n\n\n\nFigura 1.52: Facet Grid: Cruzamento de Câmbio (Linhas) vs Cilindros (Colunas)\n\n\n\n\n\n\n\n\n\n\n\nDicaEscalas Livres\n\n\n\nPor padrão, o ggplot2 força todos os painéis a terem a mesma escala de eixo X e Y para facilitar a comparação. Se os seus grupos têm magnitudes muito diferentes (ex: um vai até 10, outro até 1000), use o argumento scales:\n\nscales = “free”: Eixos X e Y independentes.\nscales = “free_y”: Só o Y varia.\nscales = “free_x”: Só o X varia.\n\n\n\nEscalas e Temas\n\nScales (scale_*)\n\nControlam como os dados são mapeados visualmente.\n\nEixos: scale_x_log10(), scale_x_continuous(limits = c(0, 100), breaks = seq(0,100,10)).\nCores: scale_color_manual() (controle total), scale_color_viridis_d() (acessibilidade).\n\n\nThemes (theme)\n\nControlam elementos que não são dados.\n\nTemas prontos: theme_bw(), theme_minimal(), theme_classic(), theme_avoid(), etc.\nAjuste fino: theme(legend.position = \"bottom\", axis.text = element_text(...)).\n\n\n\n\n\n\n\nNotaNota sobre Extensões\n\n\n\nO poder do ggplot2 é infinito graças aos pacotes da comunidade. Se não achou o geom aqui, provavelmente ele existe em um pacote extra:\n\nggforce: Geoms avançados (círculos, elipses, zoom).\nggridges: Gráficos de “Joyplot” (distribuições empilhadas).\nggraph: Para redes e grafos.",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#extensões-essenciais-do-ggplot2",
    "href": "intro.html#extensões-essenciais-do-ggplot2",
    "title": "1  Introdução ao R/RStudio",
    "section": "1.19 Extensões Essenciais do ggplot2",
    "text": "1.19 Extensões Essenciais do ggplot2\nO poder do ggplot2 reside em suas extensões.\nRótulos Inteligentes (ggrepel)\nEvita que rótulos de texto se sobreponham uns aos outros ou aos pontos. Essencial para scatterplots com nomes.\n\n\nCódigo\npacman::p_load(ggrepel)\n\n# Seleciona alguns carros para rotular\nsubset_cars &lt;- mtcars[1:10, ]\nsubset_cars$car_name &lt;- rownames(subset_cars)\n\nggplot(subset_cars, aes(wt, mpg, label = car_name)) +\n  geom_point(color = \"red\") + #adiciona os pontos\n  geom_text_repel() + # Afasta os textos automaticamente\n  theme_classic()\n\n\n\n\n\n\n\n\n\nComposição (patchwork)\nSubstitui par(mfrow) e gridExtra. Permite somar gráficos matematicamente.\n\n\nCódigo\npacman::p_load(patchwork)\ng1 &lt;- ggplot(mtcars, aes(mpg)) + geom_histogram()+theme_classic()\ng2 &lt;- ggplot(mtcars, aes(wt, mpg)) + geom_point()+theme_classic()\n\n\n(g1 | g2) / g2 +plot_annotation(title = \"\",tag_levels = \"A\") #tag_levels adiciona letras em cada grafico\n\n\n\n\n\n\n\n\n\nInteratividade (plotly)\nTransforma ggplots estáticos em HTML interativo com zoom e tooltips.\n\n\nCódigo\npacman::p_load(plotly)\ng &lt;- ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point()\nplotly::ggplotly(g)\n\n\n\n\n\n\nGuardar/salvar o gráfico (ggsave)\nEvite usar o botão Export da janela de plots para artigos. Ele não garante resolução (DPI) nem tamanho fixo.\n\n\nCódigo\n# Salva o último gráfico plotado\nggsave(\n  filename = \"figura_final.pdf\", # PDF é vetorial (melhor para artigos)\n  width = 10, \n  height = 6, \n  units = \"in\", \n  dpi = 300 # 300dpi é o padrão para impressão\n)",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#funções-personalizadas",
    "href": "intro.html#funções-personalizadas",
    "title": "1  Introdução ao R/RStudio",
    "section": "2.1 Funções personalizadas",
    "text": "2.1 Funções personalizadas\nCrie suas Próprias Ferramentas\nO princípio DRY (Don’t Repeat Yourself — Não se Repita) defende que se você precisou copiar e colar o mesmo bloco de código mais de duas vezes, é hora de transformá-lo em uma função.\n\nUma função funciona como uma “caixa preta” ou uma pequena máquina: você insere ingredientes (argumentos), a máquina processa (corpo) e entrega um produto final (retorno).\n\nnome &lt;- function(...): Você “salva” a lógica dentro de um objeto. A partir de agora, o R reconhece esse nome como um comando executável.\n( arg1, arg2 = padrão ) Argumentos: São as entradas. Funcionam como variáveis temporárias que só existem dentro da função. Você pode definir valores padrão (=), que serão usados caso o usuário não informe nada naquele argumento.\n\n{ } Corpo: O bloco de processamento. É um “ambiente isolado” (escopo local). Variáveis criadas aqui dentro nascem e morrem aqui; elas não poluem o seu ambiente de trabalho global.\nreturn(…) Retorno: A saída. Define explicitamente o que a função devolve para o usuário. Se omitido, o R retorna o resultado da última linha executada, mas usar return() torna o código mais legível.\n\n\n\nCódigo\nnome_da_funcao &lt;- function(arg1, arg2 = valor_padrao) {\n  soma &lt;- arg1 + arg2\n  return(soma)          # Retorno: O que sai da função\n}\n\n# Uso\nresultado &lt;- nome_da_funcao(arg1 = 10, arg2 = 5)\n\n\n\n\nCódigo\ncalcular_imc &lt;- function(peso, altura) {\n  imc_valor &lt;- peso / (altura ^ 2)              # O R recebe os valores, calcula e guarda em 'imc_valor'\n  return(imc_valor)                             # Retorno: O que é enviado de volta para quem chamou\n}\n\nmeu_indice &lt;- calcular_imc(peso = 80, altura = 1.80)\nprint(meu_indice) # Resultado: ~24.69\n\n\n[1] 24.69136\n\n\nCódigo\n# Nota: A variável 'imc_valor' não existe aqui fora. Ela morreu ao fim da função.\n\n\n\n\n\n\n\n\nImportanteEscopo Local vs. Global\n\n\n\nTudo o que acontece dentro das chaves {} da função fica isolado. Se você criar uma variável x &lt;- 10 dentro da função, ela não altera uma variável x que já exista no seu ambiente global. Isso garante segurança e evita bugs.",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#programação-funcional-a-alternativa-aos-loops",
    "href": "intro.html#programação-funcional-a-alternativa-aos-loops",
    "title": "1  Introdução ao R/RStudio",
    "section": "2.2 Programação Funcional (A alternativa aos Loops)",
    "text": "2.2 Programação Funcional (A alternativa aos Loops)\nO R é, em sua essência, uma linguagem funcional. Enquanto o loop for é imperativo (você dá ordens passo-a-passo: pegue o item 1, faça isso, salve ali), a programação funcional é declarativa (você diz o que quer: aplique esta função a todos os itens desta lista). Essa abordagem abstrai a complexidade da iteração, resultando em códigos mais limpos, legíveis e menos propensos a erros de indexação.\nA. Família apply (R Base)\nEste é o conjunto de funções nativas do R para iteração. São robustas e onipresentes em códigos legados e pacotes fundamentais.\n\napply(X, MARGIN, FUN): Projetada especificamente para estruturas bidimensionais (Matrizes e Data Frames). O argumento MARGIN define a direção: se for 1-aplica a função nas linhas, se for 2-aplica nas colunas.\n\n\n\nCódigo\nmatriz &lt;- matrix(1:9, nrow = 3)  # Criando uma matriz simples 3x3\n\n# Aplicando apply nas LINHAS (Margin = 1) -&gt; Soma\napply(matriz, 1, sum)  # 1+4+7, 2+5+8...\n\n\n[1] 12 15 18\n\n\nCódigo\n# Aplicando apply nas COLUNAS (Margin = 2) -&gt; Média\napply(matriz, 2, mean)\n\n\n[1] 2 5 8\n\n\n\nlapply(X, FUN) (List Apply): A função mais importante da família. Recebe um vetor ou lista, aplica a função a cada elemento e retorna sempre uma Lista. É a mais segura programaticamente, pois o formato de saída é previsível.\n\n\n\nCódigo\nnúmeros &lt;- list(a = 4, b = 16, c = 25)\n\n# Aplica a raiz quadrada em cada item\nresultado_lista &lt;- lapply(números, sqrt);resultado_lista\n\n\n$a\n[1] 2\n\n$b\n[1] 4\n\n$c\n[1] 5\n\n\nCódigo\nclass(resultado_lista) # Confirma que é uma lista\n\n\n[1] \"list\"\n\n\n\nsapply(X, FUN) (Simplify Apply): Uma versão de conveniência do lapply. Ela tenta “adivinhar” o formato de saída mais simples (um vetor ou matriz) para o resultado.\n\n\n\nCódigo\n# A mesma operação acima, mas retornando um VETOR limpo\nresultado_vetor &lt;- sapply(números, sqrt);resultado_vetor\n\n\na b c \n2 4 5 \n\n\nCódigo\nclass(resultado_vetor) # Confirma que simplificou para numérico\n\n\n[1] \"numeric\"\n\n\n\n\n\n\n\n\nImportanteCuidado\n\n\n\nSe a função falhar ou retornar tipos diferentes, o sapply pode devolver uma lista inesperadamente, quebrando scripts automatizados.\n\n\nB. Pacote purrr (Tidyverse)\nA evolução moderna da família apply. O purrr resolve o problema da imprevisibilidade do sapply introduzindo a Segurança de Tipos (Type Safety). O sufixo da função determina obrigatoriamente o tipo de dado que será retornado.\n\n\nCódigo\npacman::p_load(purrr)\n\nnotas &lt;- list(\n  joao = c(10, 9, 8),\n  maria = c(5, 6, 7),\n  pedro = c(2, 4, 3)\n)\n\n\n\nmap(x, fun): Equivalente ao lapply. Retorna sempre uma Lista.\n\n\n\nCódigo\n# Retorna uma lista com a média de cada aluno\nmap(notas, mean)\n\n\n$joao\n[1] 9\n\n$maria\n[1] 6\n\n$pedro\n[1] 3\n\n\n\nmap_dbl(x, fun): Aplica a função e retorna um vetor Numérico (double).\n\n\n\nCódigo\n# Retorna um vetor numérico limpo com as médias\nmap_dbl(notas, mean)\n\n\n joao maria pedro \n    9     6     3 \n\n\n\n\n\n\n\n\nDica\n\n\n\nSe a função tentar retornar um texto ou booleano, o map_dbl gera um erro imediato, alertando sobre a inconsistência nos dados.\n\n\n\nmap_dfr(x, fun): Aplica a função e tenta empilhar os resultados, retornando um Data Frame unido pelas linhas (Data Frame Row-bind).\n\n\n\nCódigo\n# A função anônima (~) cria uma tabela para cada aluno\nmap_dfr(notas, ~data.frame(\n  media = mean(.x), \n  status = if(mean(.x) &gt;= 7) \"Aprovado\" else \"Reprovado\"\n), .id = \"aluno\")%&gt;% # .id cria a coluna com o nome da lista original\ngt()\n\n\n\n\n\n\n\n\naluno\nmedia\nstatus\n\n\n\n\njoao\n9\nAprovado\n\n\nmaria\n6\nReprovado\n\n\npedro\n3\nReprovado\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nSe tentássemos usar map_dbl em uma coluna de texto, o R daria erro, protegendo nossa análise de cálculos inválidos.",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#relatórios-dinâmicos-r-markdown-e-quarto",
    "href": "intro.html#relatórios-dinâmicos-r-markdown-e-quarto",
    "title": "1  Introdução ao R/RStudio",
    "section": "2.3 Relatórios Dinâmicos (R Markdown e Quarto)",
    "text": "2.3 Relatórios Dinâmicos (R Markdown e Quarto)\nA reprodutibilidade é o pilar da ciência moderna. Ferramentas como R Markdown (.Rmd) e seu sucessor Quarto (.qmd) permitem integrar código, narrativa e resultados em um único documento. Leia os materiais colocados nas referências bibliográficas.",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#tabelas-profissionais",
    "href": "intro.html#tabelas-profissionais",
    "title": "1  Introdução ao R/RStudio",
    "section": "2.4 Tabelas Profissionais",
    "text": "2.4 Tabelas Profissionais\nA apresentação de tabelas deve ser adequada ao formato final do relatório (HTML, Word ou PDF).\nA. Para Word e PowerPoint (flextable)\nSe o seu destino é um documento Office editável, esta é a melhor opção. Converte data frames em objetos nativos do Word.\n\n\nCódigo\npacman::p_load(gdtools,flextable)\nTab &lt;- head(mtcars) |&gt; \n  flextable() |&gt; \n  autofit() |&gt; # Ajusta largura das colunas\n  color(i = 1, color = \"black\", part = \"header\") |&gt; # Cabeçalho vermelho\n  save_as_docx(path = \"tabela.docx\") #salva em word\n#\nhead(mtcars) |&gt; \n  flextable() |&gt; \n  autofit() |&gt; # Ajusta largura das colunas\n  color(i = 1, color = \"black\", part = \"header\")\n\n\nB. Para Publicação Acadêmica - HTML/PDF (gt)\nConhecido como o “ggplot das tabelas”. Permite estruturações complexas com notas de rodapé, títulos e agrupamentos.\n\n\nCódigo\npacman::p_load(gt)\nhead(mtcars) |&gt; \n  gt() |&gt; \n  tab_header(title = \"\") |&gt; \n  fmt_number(columns = c(mpg, wt), decimals = 2) |&gt; # Formatação precisa\n  tab_source_note(source_note = \"Fonte: Motor Trend\")\n\n\n\n\n\n\nAnálise de Veículos\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n21.00\n6\n160\n110\n3.90\n2.62\n16.46\n0\n1\n4\n4\n\n\n21.00\n6\n160\n110\n3.90\n2.88\n17.02\n0\n1\n4\n4\n\n\n22.80\n4\n108\n93\n3.85\n2.32\n18.61\n1\n1\n4\n1\n\n\n21.40\n6\n258\n110\n3.08\n3.21\n19.44\n1\n0\n3\n1\n\n\n18.70\n8\n360\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\n18.10\n6\n225\n105\n2.76\n3.46\n20.22\n1\n0\n3\n1\n\n\n\nFonte: Motor Trend\n\n\n\n\n\n\n\n\nC. Para Resumo Estatístico (gtsummary)\nAutomatiza a criação da Tabela de artigos científicos, calculando descritivas e testes estatísticos automaticamente.\n\n\nCódigo\npacman::p_load(gtsummary)\nmtcars |&gt; \n  select(mpg, cyl, hp) |&gt; \n  tbl_summary(\n    by = cyl, # Agrupa por cilindros\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\")\n  ) |&gt; \n  add_p() # Adiciona p-valor (ANOVA/Kruskal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n4\nN = 111\n6\nN = 71\n8\nN = 141\np-value2\n\n\n\n\nmpg\n26.7 (4.5)\n19.7 (1.5)\n15.1 (2.6)\n&lt;0.001\n\n\nhp\n83 (21)\n122 (24)\n209 (51)\n&lt;0.001\n\n\n\n1 Mean (SD)\n\n\n2 Kruskal-Wallis rank sum test\n\n\n\n\n\n\n\n\nD. Para Dashboards Interativos (reactable)\nIdeal para relatórios HTML onde o usuário precisa filtrar, buscar ou ordenar os dados.\n\n\nCódigo\npacman::p_load(reactable)\nreactable(\n  mtcars,\n  searchable = TRUE,  # Adiciona barra de busca\n  striped = TRUE,     # Linhas zebradas\n  highlight = TRUE    # Destaque ao passar o mouse\n)",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#equações-matemáticas-latex",
    "href": "intro.html#equações-matemáticas-latex",
    "title": "1  Introdução ao R/RStudio",
    "section": "2.5 Equações Matemáticas (LaTeX)",
    "text": "2.5 Equações Matemáticas (LaTeX)\nA escrita matemática em relatórios dinâmicos utiliza a sintaxe LaTeX, o padrão global para tipografia científica. O R Markdown/Quarto renderiza esses códigos em fórmulas visuais de alta qualidade tanto em HTML quanto em PDF.\nA. Modos de Exibição (Inline vs. Display)\nO modo inline ($) insere a fórmula na fluidez do texto, enquanto o modo display ($$) quebra a linha e centraliza a equação para destaque.\nO modelo de regressão linear simples é dado por $y = \\alpha + \\beta x + \\epsilon$.\n\nA estimativa dos parâmetros minimiza a Soma dos Quadrados dos Resíduos:\n$$S(\\alpha, \\beta) = \\sum_{i=1}^{n} (y_i - \\alpha - \\beta x_i)^2$$\nO modelo de regressão linear simples é dado por \\(y = \\alpha + \\beta x + \\epsilon\\).\nA estimativa dos parâmetros minimiza a Soma dos Quadrados dos Resíduos: \\[S(\\alpha, \\beta) = \\sum_{i=1}^{n} (y_i - \\alpha - \\beta x_i)^2\\]\nB. Sintaxe Essencial (Símbolos e Estruturas)\nComandos especiais iniciam com barra invertida (\\). Use underscore (_) para subscritos, acento circunflexo (^) para expoentes e chaves {} para agrupar termos.\n$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$$\n\n$$\\sigma = \\sqrt{\\frac{\\sum(x - \\mu)^2}{N}}$$\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]\n\\[\\sigma = \\sqrt{\\frac{\\sum(x - \\mu)^2}{N}}\\]\nC. Extração Automática de Modelos (equatiomatic)\nAssim como o gtsummary automatiza tabelas, o pacote equatiomatic extrai a equação matemática formatada diretamente de um modelo estatístico ajustado no R.\n\n\nCódigo\npacman::p_load(equatiomatic)\n\nmodelo &lt;- lm(mpg ~ cyl + hp, data = mtcars)\n\nextract_eq(modelo, use_coefs = TRUE, wrap = TRUE)\n\n\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{mpg}} &= 36.91 - 2.26(\\operatorname{cyl}) - 0.02(\\operatorname{hp})\n\\end{aligned}\n\\]\n\n\nD. Equações com Valores Dinâmicos (Inline R)\nCombine a tipografia LaTeX com o poder do R inserindo códigos executáveis diretamente na fórmula via r código. Garante que os números na equação sejam atualizados se os dados mudarem.\n\n\nCódigo\nmedia &lt;- mean(mtcars$mpg)\ndesvio &lt;- sd(mtcars$mpg)\n\n\nA distribuição segue uma Normal: $X \\sim N(\\mu = 20.1, \\sigma^2 = 6)$.\nA distribuição segue uma Normal: \\(X \\sim N(\\mu = 20.1, \\sigma^2 = 6)\\).\n\n\n\n\n\n\nDicaCheatsheets (Guias de Consulta Rápida)\n\n\n\nDecorar todos os comandos e funcionalidades dos pacotes no ecossistema R/RStudio é uma tarefa desafiadora. Por isso, a comunidade desenvolve os chamados cheatsheets (ou folhas de consulta): resumos concisos que reúnem informações essenciais, fórmulas e comandos sobre temas específicos. Eles são projetados para facilitar a consulta rápida, economizar tempo e auxiliar na memorização durante o fluxo de trabalho.\nUm dos repositórios mais completos, que inclui guias para pacotes de análise espacial (como o sf), pode ser acessado neste link.\nVeja também a formatação de tabelas no link\n\n\n\n\n\n\nParadis, Emmanuel. 2005. R for Beginners. https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf.\n\n\nVenables, W. N., D. M. Smith, e R Core Team. 2015. An Introduction to R. Vienna, Austria: R Foundation for Statistical Computing. https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf.\n\n\nWickham, Hadley. 2016. ggplot2: Elegant Graphics for Data Analysis. 2º ed. Springer-Verlag New York.\n\n\n———. 2019. Advanced R. 2º ed. CRC Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, e Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2º ed. O’Reilly Media.",
    "crumbs": [
      "Introdução ao R/Rstudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao R/RStudio</span>"
    ]
  },
  {
    "objectID": "fundEstspatial.html",
    "href": "fundEstspatial.html",
    "title": "2  Fundamentos da Estatística Espacial",
    "section": "",
    "text": "2.1 Estatística Clássica vs. Estatística Espacial\nA estatística é definida pela Associação Americana de Estatística (ASA) como a ciência de aprender com dados, e de medir, controlar e comunicar a incerteza (Wild, Utts, e Horton 2017). Ela atua como uma metadisciplina focada em como pensar sobre a transformação de dados em conhecimento do mundo real, sendo também descrita como o estudo da variabilidade e da tomada de decisão diante da incerteza (Bartholomew 1995; Fienberg 2014).\nA incerteza mencionada está intrinsecamente associada a uma característica da população, a qual é representada por um parâmetro, denotado por \\(\\theta\\in \\Theta\\). Para quantificar a incerteza associada a \\(\\theta\\), destacam-se duas grandes abordagens:\nIndependentemente da abordagem (Frequentista ou Bayesiana), se o processo de modelagem não considera explicitamente a estrutura espacial ou as coordenadas geográficas dos eventos em estudo, estamos diante da estatística clássica. Por outro lado, quando a localização geográfica é incorporada ao processo de modelagem, entramos no domínio espacial: trata-se de estatística espacial quando há quantificação da incerteza, e de análise espacial na ausência dessa quantificação.\nA estatística espacial representa uma mudança paradigmática. Enquanto a estatística clássica investiga o quê?, quanto?, quando? e como?, a estatística espacial impõe a pergunta fundamental: onde? e, crucialmente, a localização geográfica condiciona o valor observado?\nNesta seção, rompemos com a suposição de independência entre observações que é estabelecida pela estatística clássica e estabelecemos a estrutura teórica para modelar fenômenos onde o espaço geográfico influencia nos valores que observamos.\nPara compreender a distinção entre a estatística clássica e a espacial, devemos primeiro definir formalmente o que é um modelo estatístico e, em seguida, observar como a estrutura de dependência altera esse modelo.\nUm modelo estatístico é definido como um par ordenado \\((\\Omega, \\mathcal{P})\\), em que \\(\\Omega\\) é o espaço amostral, representando o conjunto de todos os possíveis valores observáveis dos dados \\(\\{Y_i\\}_{i=1}^{n}\\), e \\(\\mathcal{P} = \\{P_\\theta : \\theta \\in \\Theta\\}\\) é uma família de distribuições de probabilidade indexada por um parâmetro \\(\\theta\\), sendo \\(\\Theta\\) o espaço paramétrico que contém todos os valores possíveis para o parâmetro desconhecido \\(\\theta\\). Seja \\(\\mathbf{Y} = (Y_1, Y_2, \\dots, Y_n)^\\top\\) um vetor aleatório de dimensão \\(n \\times 1\\) representando nossos dados observados; a distinção central entre a abordagem clássica e a espacial reside na estrutura da matriz de covariância deste vetor.\nNa estatística clássica, assume-se frequentemente que as observações \\(\\{Y_i\\}_{i=1}^{n}\\) são independentes e identicamente distribuídas (i.i.d.) Getis (1999). Isso implica que a informação contida em uma observação \\(Y_i\\) não altera a probabilidade de ocorrência de outra observação \\(Y_j\\) (para \\(i \\neq j\\)). Assim, considerando inicialmente a equação para uma única observação \\(i\\) (onde \\(i = 1, \\dots, n\\)):\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\varepsilon_i, \\quad \\varepsilon_i \\overset{iid}{\\sim} N(0, \\sigma^2)\n\\]\nPela propriedade de independência, temos que \\(Cov(\\varepsilon_i, \\varepsilon_j) = E[\\varepsilon_i \\varepsilon_j] = 0\\) para todo \\(i \\neq j\\), o que implica que a distribuição condicional de \\(Y_i\\) é dada por \\(Y_i | \\mathbf{x}_i \\sim N(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}, \\sigma^2)\\). Ou seja, a informação contida em \\(Y_i\\) não altera a probabilidade de ocorrência de \\(Y_j\\). Matricialmente, temos que:\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\]\nOnde \\(\\mathbf{Y}\\) é o vetor de variáveis resposta (observações) de dimensão \\(n \\times 1\\), \\(\\mathbf{X}\\) é a matriz de planejamento (ou matriz de design) de dimensão \\(n \\times p\\) contendo as variáveis explicativas, \\(\\boldsymbol{\\beta}\\) é o vetor de parâmetros desconhecidos (coeficientes de regressão) de dimensão \\(p \\times 1\\), e \\(\\boldsymbol{\\varepsilon}\\) é o vetor de erros aleatórios de dimensão \\(n \\times 1\\). Assume-se que os erros seguem uma distribuição Normal multivariada, denotada por \\(\\boldsymbol{\\varepsilon} \\sim N_n(\\mathbf{0}, \\Sigma)\\), o que implica que a distribuição de \\(\\mathbf{Y}\\) condicionada a \\(\\mathbf{X}\\) é \\(\\mathbf{Y}| \\mathbf{X} \\sim N_n(\\mathbf{X}\\boldsymbol{\\beta}, \\Sigma)\\).\nNeste contexto clássico, a matriz de variância-covariância \\(\\Sigma\\) assume uma forma específica:\n\\[\nCov(\\mathbf{Y}) = \\Sigma = \\sigma^2 \\mathbf{I}_n =\n\\begin{bmatrix}\n\\sigma^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix}\n\\tag{2.1}\\]\nEm que \\(\\sigma^2\\) representa a variância constante do erro (homocedasticidade) e \\(\\mathbf{I}_n\\) é a matriz identidade de ordem \\(n\\), com valores 1 na diagonal principal e 0 fora dela. Consequentemente, temos que a covariância entre quaisquer duas observações distintas é nula, ou seja, \\(Cov(Y_i, Y_j) = 0\\) para todo \\(i \\neq j\\) (Figura 2.1). Isso é análogo ao processo de retirar bolas de uma urna com reposição: a probabilidade de retirar uma bola vermelha na segunda tentativa independe completamente do resultado da primeira.\nNa estatística espacial, a independência é considerada a exceção, não a regra. Assume-se que a covariância entre observações em locais distintos é não nula, ou seja, \\(Cov(Y_i, Y_j) \\neq 0\\), e que essa covariância é uma função explícita da estrutura espacial \\(S\\) (Getis 1999). Para formalizar essa relação, denotamos o processo estocástico por \\(\\{Y(\\mathbf{s}) : \\mathbf{s} \\in D\\}\\) conforme definido por Cressie e Moores (2022), onde \\(\\mathbf{s} \\in D \\subset \\mathbb{R}^d\\) representa o vetor de coordenadas de localização no domínio espacial de interesse. Consequentemente, a matriz de covariância \\(\\Sigma\\) defnida na Eq. 2.1 deixa de ser diagonal e torna-se uma matriz densa, capturando as interações entre todas as localidades:\n\\[Cov(\\mathbf{Y}) = \\Sigma =\n\\begin{bmatrix}\nC(\\mathbf{s}_1, \\mathbf{s}_1) & C(\\mathbf{s}_1, \\mathbf{s}_2) & \\dots & C(\\mathbf{s}_1, \\mathbf{s}_n) \\\\\nC(\\mathbf{s}_2, \\mathbf{s}_1) & C(\\mathbf{s}_2, \\mathbf{s}_2) & \\dots & C(\\mathbf{s}_2, \\mathbf{s}_n) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nC(\\mathbf{s}_n, \\mathbf{s}_1) & C(\\mathbf{s}_n, \\mathbf{s}_2) & \\dots & C(\\mathbf{s}_n, \\mathbf{s}_n)\n\\end{bmatrix} \\tag{2.2}\\]\nA função \\(C(\\mathbf{s}_i, \\mathbf{s}_j)\\) na Eq. 2.2 define a estrutura de dependência e pode ser modelada de diferentes formas, dependendo da natureza dos dados. No contexto da geoestatística (Capítulo 3), assume-se frequentemente a estacionariedade (Seção 2.3), onde a covariância depende apenas da distância euclidiana \\(h = ||\\mathbf{s}_i - \\mathbf{s}_j||\\) entre os pontos, tal que \\(Cov(Y(\\mathbf{s}_i), Y(\\mathbf{s}_j)) = C(h)\\). Aqui, \\(C(h)\\) é uma função de distância que decai com a distância, indicando que a correlação entre observações diminui à medida que a distância \\(h\\) entre elas aumenta Figura 2.1. Alternativamente, para dados de área/lattice (Capítulo 4), a dependência é modelada através da estrutura de vizinhança ou contiguidade. Introduz-se uma matriz de pesos espaciais \\(\\mathbf{W}\\), onde o elemento \\(w_{ij}\\) é definido binariamente (ou por distâncias inversas) para indicar a conexão entre unidades espaciais: \\(w_{ij} = 1\\) se as áreas \\(\\mathbf{s}_i\\) e \\(\\mathbf{s}_j\\) compartilham fronteira (são vizinhos) e \\(w_{ij} = 0\\) caso contrário. Neste cenário, modelos autorregressivos (como o CAR - Conditional Autoregressive) assumem que o valor esperado ou a variância de \\(Y_i\\) é condicionado aos seus vizinhos, expresso formalmente como \\(Y_i | \\mathbf{Y}_{-i} \\sim N(\\sum_{j \\in \\text{vizinhos}} w_{ij} Y_j, \\tau^2)\\), onde \\(\\mathbf{Y}_{-i}\\) representa todas as observações exceto a \\(i\\)-ésima e \\(\\tau^2\\) é a variância condicional.\nA Primeira Lei da Geografia\nEsta lei justifica a existência de métodos de interpolação (como Krigagem) e modelos de regressão espacial. Se o mundo fosse puramente aleatório (como na Figura 2.1 lado esquerdo), a geografia seria irrelevante e a melhor previsão para um local não amostrado seria a média global, e não a média dos vizinhos.",
    "crumbs": [
      "Fundamentos da Estatística Espacial",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos da Estatística Espacial</span>"
    ]
  },
  {
    "objectID": "fundEstspatial.html#sec-class_spac",
    "href": "fundEstspatial.html#sec-class_spac",
    "title": "2  Fundamentos da Estatística Espacial",
    "section": "",
    "text": "A Abordagem Clássica\n\n\n\n\n\n\n\n\n\n\nAbordagem Espacial\n\n\n\n\n\n\n\n\n\n\nImportanteNem tudo é estatística espacial\n\n\n\nÉ fundamental distinguir dois termos frequentemente confundidos, conforme elucidado por Cressie e Moores (2022):\n\nAnálise Espacial: Refere-se, de modo amplo, ao estudo da informação de localização associada a atributos. É um termo comum na Geografia e em Sistemas de Informação Geográfica (SIG). Caracteriza-se pelo uso de algoritmos determinísticos para manipulação e consulta de dados espaciais, sem a quantificação explícita da incerteza via modelos probabilísticos.\n\nExemplos: Mapas de áreas, zonas de influência, caminho mínimo, Interpolação IDW, sobreposição de camadas.\n\nEstatística Espacial: Ocorre quando a análise incorpora formalmente a quantificação da incerteza. Baseia-se na premissa de que valores próximos são estatisticamente mais dependentes do que os distantes. Ela utiliza as localizações espaciais (\\(s\\)) para modelar essa dependência através de efeitos fixos (tendências) e aleatórios (covariância) dentro de um modelo de probabilidade estocástico.\n\nExemplos: Krigagem (geoestatística), Testes de Autocorrelação (Moran/Geary), Regressão Espacial (SAR/CAR), Modelagem de Processos Pontuais (Ripley’s K).\n\n\n\n\n\n\n\n“Everything is related to everything else, but near things are more related than distant things.” (Todas as coisas estão relacionadas entre si, mas coisas próximas estão mais relacionadas do que coisas distantes.) — Waldo Tobler (1970)\n\n\n\n\n\n\n\n\n\nDicaDecaimento da Distância\n\n\n\nA influência de A sobre B tende a diminuir conforme a distância entre eles aumenta. A forma como essa influência cai (linearmente, exponencialmente, etc.) é o que modelamos estatísticamente.",
    "crumbs": [
      "Fundamentos da Estatística Espacial",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos da Estatística Espacial</span>"
    ]
  },
  {
    "objectID": "fundEstspatial.html#sec-dependencia",
    "href": "fundEstspatial.html#sec-dependencia",
    "title": "2  Fundamentos da Estatística Espacial",
    "section": "2.2 Autocorrelação, dependência e vizinhança espacial",
    "text": "2.2 Autocorrelação, dependência e vizinhança espacial\nDependência espacial\nA Dependência Espacial é a propriedade estatística fundamental que descreve a tendência de que os valores de uma variável em uma determinada localização geográfica estejam funcionalmente relacionados aos valores dessa mesma variável em localizações vizinhas (Figura 2.1 direita) (Crawford 2009) . Ela representa a manifestação estatística da Primeira Lei da Geografia de Tobler.\nA existência de dependência espacial constitui uma violação direta da suposição de independência estatística (i.i.d.) (Figura 2.1 esquerda), uma premissa basilar de muitos métodos convencionais (como a regressão linear clássica/OLS). Segundo Chun e Griffith (2017), a dependência refere-se à existência de uma covariância não nula entre valores de uma única variável quando inspecionados em conjunto com suas localizações espaciais, indicando que o evento observado em um ponto \\(s_i\\) é condicionado pelo seu entorno/vizinhos mais próximos \\(s_j\\).\nPara ilustrar a ruptura com a estatística clássica, Unwin e Hepple (1974) oferecem uma analogia: enquanto a estatística tradicional trata as observações como “bolas em uma urna” (onde a posição física da bola na urna é irrelevante para sua probabilidade de ser sorteada), a estatística espacial trata os dados como “cachos de uvas”. Em um cacho, a posição de uma uva fornece informações cruciais sobre as uvas adjacentes (maturação, tamanho, exposição ao sol). Portanto, na análise espacial, a ordem e a proximidade carregam informação estatística que não pode ser descartada.\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(ggplot2, dplyr, viridis, gstat, sf, tibble)\n\n#\ngrid_df &lt;- expand.grid(x = 1:50, y = 1:50)\nset.seed(42)\ngrid_df$valor_classico &lt;- rnorm(2500)\ngrid_sf &lt;- st_as_sf(grid_df, coords = c(\"x\", \"y\"))\nmodelo_vgm &lt;- gstat::vgm(psill = 1, model = \"Sph\", range = 15, nugget = 0.1)\ng_dummy &lt;- gstat::gstat(formula = z~1, locations = grid_sf, dummy = T, beta = 0, model = modelo_vgm, nmax = 20)\n\n# Predição\ninvisible(capture.output(yy &lt;- predict(g_dummy, newdata = grid_sf, nsim = 1)))\ngrid_df$valor_espacial &lt;- yy$sim1\n\n#\nminha_legenda &lt;- guide_colorbar(\n  title = NULL,\n  barwidth = unit(.5, \"npc\"),\n  barheight = unit(0.5, \"cm\"),\n  label.position = \"bottom\"\n)\n\n# Clássico\nggplot(grid_df, aes(x, y, fill = valor_classico)) +\n  geom_tile() +\n  scale_fill_viridis_c(option = \"B\") +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position = \"bottom\",\n        legend.margin = margin(t = 5, r = 0, b = 0, l = 0)) +\n  guides(fill = minha_legenda)\n\n# Espacial\nggplot(grid_df, aes(x, y, fill = valor_espacial)) +\n  geom_tile() +\n  scale_fill_viridis_c(option = \"B\") +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position = \"bottom\",\n        legend.margin = margin(t = 5, r = 0, b = 0, l = 0)) +\n  guides(fill = minha_legenda)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Estatística Clássica: Sem dependência espacial (i.i.d.)\n\n\n\n\n\n\n\n\n\n\n\n(b) Estatística Espacial: Existe dependência espacial\n\n\n\n\n\n\n\nFigura 2.1: Comparação Visual entre Processos Estocásticos\n\n\n\nAutocorrelação espacial\nEnquanto a dependência espacial constitui a propriedade teórica intrínseca ao processo gerador dos dados, a autocorrelação espacial é a medida estatística utilizada para quantificá-la. O termo distingue-se fundamentalmente da correlação convencional, como a de Pearson, que avalia a associação linear entre duas variáveis distintas (\\(X\\) e \\(Y\\)). A autocorrelação espacial, por sua vez, examina a correlação de uma única variável consigo mesma, porém deslocada no espaço geográfico, confrontando o valor da variável no local \\(s_i\\) com os valores observados na sua vizinhança \\(s_j\\). Segundo Chun e Griffith (2017), essa métrica quantifica simultaneamente a força e a direção da relação espacial, servindo como um diagnóstico crucial para a validade das análises subsequentes. A sua identificação não é apenas descritiva, mas um pré-requisito metodológico, uma vez que Getis (1999) alerta que ignorar essa estrutura e aplicar métodos clássicos, como os Mínimos Quadrados Ordinários (MQO), viola o pressuposto de independência dos erros, resultando em estimativas de variância enviesadas e testes de hipótese inválidos.\nA autocorrelação espacial manifesta-se em três padrões estruturais distintos, visualizados na Figura 2.2. A configuração mais frequente em fenômenos naturais e sociais é a autocorrelação espacial positiva, que ocorre quando valores similares tendem a se agrupar no espaço, formando clusters. Neste cenário, observa-se que locais com valores altos são vizinhos de outros valores altos, e locais com valores baixos são vizinhos de outros valores baixos, indicando processos de continuidade ou contágio, comuns em variáveis como temperatura, altitude ou preços imobiliários. Em contraste, a autocorrelação espacial negativa caracteriza-se pela adjacência de valores dissimilares, onde um local com valor alto tende a ser cercado por vizinhos com valores baixos, e vice-versa. Este padrão, visualmente semelhante a um tabuleiro de xadrez, é menos frequente na natureza e geralmente sinaliza processos de competição ou inibição espacial, como a localização de estabelecimentos comerciais concorrentes. Por fim, a ausência de autocorrelação denota uma distribuição puramente estocástica (completa aleatoriedade espacial) dos valores no espaço, onde o valor observado em um ponto não fornece informação estatística sobre seus vizinhos, representando a independência espacial e constituindo a hipótese nula (\\(H_0\\)) na maioria dos testes estatísticos espaciais.\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(ggplot2, patchwork, viridis)\n\ndf_grid &lt;- expand.grid(x = 1:8, y = 1:8)\n\n#Simular Padrão Positivo\ndf_grid$z_pos &lt;- (df_grid$x + df_grid$y) + rnorm(64, 0, 0.5)\n\n# Simular Padrão Negativo \ndf_grid$z_neg &lt;- ifelse((df_grid$x + df_grid$y) %% 2 == 0, 1, 0)\n\n#Simular Padrão Aleatório\nset.seed(123)\ndf_grid$z_rand &lt;- rnorm(64)\n\nplot_pattern &lt;- function(data, var, title) {\n  ggplot(data, aes(x, y, fill = {{var}})) +\n    geom_tile(color = \"white\", lwd = 0.5) +\n    scale_fill_viridis_c(option = \"mako\", guide = \"none\") +\n    theme_void() +\n    coord_fixed() +\n    labs(title = title) +\n    theme(plot.title = element_text(hjust = 0.5, size = 10))\n}\n\np1 &lt;- plot_pattern(df_grid, z_pos, \"Positiva (Agrupamento)\")\np2 &lt;- plot_pattern(df_grid, z_rand, \"Aleatória (Independência)\")\np3 &lt;- plot_pattern(df_grid, z_neg, \"Negativa (Competição)\")\n\n# Combinar com patchwork\np1 + p2 + p3\n\n\n\n\n\n\n\n\nFigura 2.2: Tipologia da Autocorrelação Espacial.\n\n\n\n\n\nVizinhança Espacial\nPara operacionalizar a mensuração da dependência e o cálculo da autocorrelação, torna-se imperativo definir formalmente a estrutura de conectividade entre as unidades espaciais, estabelecendo o conceito de vizinhança. Conforme definem Chun e Griffith (2017), a quantificação da dependência exige a identificação precisa de um conjunto de valores vizinhos que covariam com a observação de interesse, sendo essa relação estruturada algebricamente através de uma Matriz de Pesos Espaciais (\\(\\mathbf{W}\\)). Tradicionalmente, essa matriz é construída sob critérios de contiguidade física, utilizando analogias do xadrez para dados de área: o critério Torre (Rook) estipula que as unidades são vizinhas apenas se compartilharem uma fronteira física (arestas), enquanto o critério Rainha (Queen) considera vizinhas as unidades que compartilham qualquer ponto (arestas ou vértices). Alternativamente, especialmente em geoestatística, a vizinhança é definida por funções de distância, onde todas as unidades dentro de um raio \\(d\\) são consideradas conectadas, ou onde a magnitude da influência decai inversamente à distância euclidiana entre os centróides Figura 2.3.\nEntretanto, a definição de “espaço” na modelagem contemporânea expandiu-se para além da geografia física. Econometristas e teóricos regionais, como Kelejian e Piras (2017), argumentam que a distância não deve se limitar à métrica euclidiana, mas sim representar o enfraquecimento das conexões entre unidades observacionais em múltiplas dimensões. Nesta perspectiva, a matriz \\(\\mathbf{W}\\) deve ser capaz de capturar a “distância econômica” ou institucional, permitindo que a vizinhança seja definida por semelhanças em estruturas de mercado, alinhamento político ou hierarquia urbana. Sob essa ótica, metrópoles fisicamente distantes (como São Paulo e Nova York) podem ser consideradas vizinhas devido aos fluxos financeiros e competição econômica direta, enquanto municípios contíguos de menor porte podem apresentar uma interação estatística negligenciável.\nA crítica à primazia exclusiva da proximidade física é aprofundada pela geografia econômica evolucionária. Boschma (2005) defende que a proximidade geográfica não é condição necessária nem suficiente para a interação e o aprendizado, atuando, no máximo, como uma facilitadora para outras formas de conexão. Para que ocorra o efetivo transbordamento de conhecimento (spillovers), a “vizinhança” real deve ser compreendida através de outras quatro dimensões de proximidade: a cognitiva, que envolve uma base de conhecimento compartilhada; a organizacional, referente à capacidade de controle e coordenação; a social, baseada em relações de confiança e parentesco; e a institucional, que diz respeito a normas e legislações comuns. Portanto, uma definição robusta de vizinhança em modelos espaciais modernos deve reconhecer que a interação entre agentes é frequentemente moldada pela afinidade socioeconômica e institucional, transcendendo a simples adjacência física.\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(ggplot2, patchwork)\n\nplot_neighbors &lt;- function(type) {\n  # Grid 3x3\n  df &lt;- expand.grid(x = 1:3, y = 1:3)\n  # Definir o centro\n  center &lt;- df$x == 2 & df$y == 2\n  # Definir vizinhos\n  if (type == \"Torre (Rook)\") {\n    neighbors &lt;- (abs(df$x - 2) + abs(df$y - 2)) == 1\n  } else if (type == \"Rainha (Queen)\") {\n    neighbors &lt;- (abs(df$x - 2) &lt;= 1 & abs(df$y - 2) &lt;= 1) & !center\n  }\n  \n  df$legenda &lt;- \"Outros\"\n  df$legenda[neighbors] &lt;- \"Vizinho\"\n  \n  df$legenda[center] &lt;- \"Local de interesse\"\n  df$legenda &lt;- factor(df$legenda, levels = c(\"Local de interesse\", \"Vizinho\", \"Outros\"))\n  \n  # Plot\n  ggplot(df, aes(x, y, fill = legenda)) + \n    geom_tile(color = \"black\", lwd = 1) +\n    scale_fill_manual(values = c(\"Local de interesse\" = \"darkred\", \n                                 \"Vizinho\" = \"steelblue\", \n                                 \"Outros\" = \"white\")) +\n    coord_fixed() +\n    theme_void() +\n    labs(title = type, fill = \"\") +\n    theme(legend.position = \"bottom\", \n          plot.title = element_text(hjust = 0.5))\n}\n\n#\np1 &lt;- plot_neighbors(\"Torre (Rook)\")\np2 &lt;- plot_neighbors(\"Rainha (Queen)\")\n\n#\np1 + p2\n\n\n\n\n\n\n\n\nFigura 2.3: Critérios de Vizinhança por Contiguidade.\n\n\n\n\n\nHeterogeneidade espacial\nA Heterogeneidade Espacial é definida fundamentalmente como a complexidade e a variabilidade de uma propriedade de um sistema no espaço e no tempo Li e Reynolds (1994). No âmbito da estatística espacial, ela transcende a simples variação dos dados brutos e representa uma característica intrínseca do Processo Gerador de Dados (Data Generation Process), o qual se mostra inconsistente ou instável ao longo do domínio geográfico Peng e Inoue (2024). É crucial distinguir este conceito da dependência espacial: enquanto a dependência foca na força da conexão/interação ou autocorrelação entre vizinhos (o quanto se influenciam), a heterogeneidade foca na variação da estrutura do fenômeno (o como se relacionam), implicando que os parâmetros estatísticos (como médias, variância, e coeficientes dos modelos, etc.) não são constantes em toda a área de estudo Figura 2.4.\nSob uma ótica estatística, a presença de heterogeneidade implica frequentemente na violação da suposição de estacionariedade. Segundo Wagner e Fortin (2005), ela deve ser compreendida como a variabilidade espacialmente estruturada de uma propriedade de interesse, significando que parâmetros estatísticos fundamentais como a média, a variância ou a estrutura de covariância não são constantes em toda a área de estudo. Isso distingue a heterogeneidade, frequentemente associada a fatores exógenos e não-estacionários, da autocorrelação espacial pura, que é comumente associada a processos endógenos estacionários.\nEssa variabilidade estrutural manifesta-se simultaneamente de duas formas: como heterogeneidade contínua, onde as relações mudam gradualmente através do espaço (em gradientes globais ou locais), ou como heterogeneidade discreta, caracterizada por mudanças abruptas em fronteiras administrativas ou zonas geográficas específicas Peng e Inoue (2024). Para dados categóricos, Li e Reynolds (1994) operacionalizam essa heterogeneidade como a complexidade na composição (número e proporção de tipos de manchas) e na configuração (arranjo espacial e forma). Na ecologia de paisagens, o conceito expande-se para descrever variações vitais para a dinâmica populacional, influenciando diretamente a persistência, extinção e coexistência de espécies, uma vez que a localização espacial determina a abundância e distribuição dos organismos.\nA consequência prática da heterogeneidade é que o contexto local altera as “regras do jogo”. Na estatística clássica, assume-se um modelo global \\(y = \\alpha + \\beta x+\\varepsilon, \\: y|x \\sim FE(.)\\), onde \\(FE(.)\\) é família exponencial, o coeficiente \\(\\beta\\) (o efeito de \\(x\\) em \\(y\\)) é fixo e universal para todo o banco de dados. Na presença de heterogeneidade espacial, reconhecemos que este efeito é local. Considere um modelo hedônico (sugestão de leitura: (Fávero 2003)) prevendo o preço de imóveis (\\(y\\)) com base na área construída (\\(x\\)): em um bairro nobre, um metro quadrado adicional pode valorizar o imóvel em R$ 10.000 (um \\(\\beta\\) alto), enquanto em uma área periférica sem infraestrutura, o mesmo metro quadrado adicional pode agregar apenas R$ 1.000 (um \\(\\beta\\) baixo). Devido a essa inconsistência (não-estacionariedade do parâmetro), modelos globais tendem a produzir resultados enviesados (Peng e Inoue 2024).\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(ggplot2, viridis, sf)\n\n#\ndf_het &lt;- expand.grid(x = 1:20, y = 1:20)\n\n# Simular Heterogeneidade Contínua\ndf_het$beta_real &lt;- (df_het$y / 20) * 2 + (df_het$x / 20) \n\n# Simular Heterogeneidade Discreta (uma \"zona\" diferente no centro)\ncentro &lt;- (df_het$x - 10)^2 + (df_het$y - 10)^2 &lt; 16\ndf_het$beta_real[centro] &lt;- df_het$beta_real[centro] + 3\n\nggplot(df_het, aes(x, y, fill = beta_real)) +\n  geom_tile() +\n  scale_fill_viridis_c(option = \"turbo\", name = expression(beta(s))) +\n  coord_fixed() +\n  theme_void() +\n  labs(title = \"\",\n       subtitle = \"\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigura 2.4: Heterogeneidade Espacial: O efeito de X em Y não é constante (\\(\\beta(s)\\) muda gradualmente e abruptamente no centro)",
    "crumbs": [
      "Fundamentos da Estatística Espacial",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos da Estatística Espacial</span>"
    ]
  },
  {
    "objectID": "fundEstspatial.html#sec-estacionaridade",
    "href": "fundEstspatial.html#sec-estacionaridade",
    "title": "2  Fundamentos da Estatística Espacial",
    "section": "2.3 Estacionariedade e Não Estacionariedade Espacial",
    "text": "2.3 Estacionariedade e Não Estacionariedade Espacial\nEstacionariedade\nA Estacionariedade é um conceito fundamental que sustenta a inferência estatística em processos estocásticos espaciais. Na maioria das investigações geocientíficas, deparamo-nos com um desafio intrínseco: possuímos apenas uma única realização (uma única “foto”) do processo estocástico sob investigação. Não podemos replicar o processo gerador e observar como o padrão de chuva ou a distribuição de minérios se formaria novamente sob as mesmas condições probabilísticas. Portanto, para calcularmos estatísticas vitais como a média e a variância, e realizarmos previsões (inferência), precisamos assumir algum grau de estabilidade ou repetição nas propriedades do fenômeno através do espaço.\nIntuitivamente, a estacionariedade espacial sugere que as propriedades estatísticas (momentos da distribuição) do fenômeno são uniformes em toda a região de estudo, permanecendo inalteradas sob translação da origem do sistema de coordenadas (Schmidt e O’Hagan 2003). Em termos práticos, isso significa que as “leis” que governam a variabilidade dos dados não mudam de um local para outro, permitindo que utilizemos dados de uma parte da região para estimar parâmetros válidos para outra parte.\nPara formalizar este conceito, conforme detalhado por Sahu (2022), devemos decompor a estacionariedade em níveis hierárquicos baseados nos momentos da distribuição (média, variância e covariância):\n\nEstacionariedade de Primeira Ordem\n\nUm processo estocástico espacial \\(Y(\\mathbf{s})\\) é classificado como estacionário de primeira ordem se o seu valor esperado (média) for constante em todo o domínio de estudo \\(D\\):\n\\[E[Y(\\mathbf{s})] = \\mu, \\quad \\forall \\mathbf{s} \\in D\\] Isso implica que não existe uma tendência (trend) global ou deriva sistemática nos dados. Um mapa da média teórica desse processo seria “monocromático” ou plano, sem gradientes direcionais. É crucial distinguir a média do processo (o parâmetro populacional \\(\\mu\\), que é constante) da realização observada (os valores \\(y_i\\), que variam). A estacionariedade de primeira ordem garante que as flutuações observadas ocorrem ao redor de um patamar fixo. Se a média do processo altera-se em função da localização (por exemplo, a temperatura média diminuindo sistematicamente conforme a latitude aumenta), dizemos que o processo é não estacionário de primeira ordem. Matematicamente, isso é expresso como \\(E[Y(\\mathbf{s})] = \\mu(\\mathbf{s})\\), indicando que a média é uma função determinística da posição \\(\\mathbf{s}\\).\n\nEstacionariedade de Segunda Ordem (Fraca)\n\nPara a maioria das aplicações em geoestatística, como a Krigagem, a estabilidade apenas da média é insuficiente; necessitamos também que a estrutura de variabilidade e correlação seja estável. Um processo é dito estacionário de segunda ordem (ou fracamente estacionário) se satisfaz simultaneamente duas condições:\n\nPossui média constante (atende à primeira ordem): \\(E[Y(\\mathbf{s})] = \\mu\\).\nA covariância entre dois pontos (\\(\\{s_i,\\: s_j\\}_{j\\neq i}\\)) quaisquer depende exclusivamente do vetor de separação ou distância entre eles (\\(\\mathbf{h} = \\mathbf{s_i} - \\mathbf{s_j}\\)), e não de suas localizações absolutas geográficas:\n\n\\[Cov[Y(\\mathbf{s}), Y(\\mathbf{s}+\\mathbf{h})] = C(\\mathbf{h})\\]\nEsta propriedade é crítica pois permite estimar uma função de covariância global \\(C(\\mathbf{h})\\) ou um variograma utilizando todos os pares de pontos disponíveis na amostra, simplificando drasticamente a modelagem ao reduzir o número de parâmetros necessários (Bandyopadhyay e Rao 2017). Além disso, ela implica na estacionariedade da variância (\\(C(0) = \\sigma^2\\)), ou seja, a dispersão dos dados é constante em todo o domínio (homocedasticidade espacial).\n\n\n\n\n\n\nNotaOutros Graus de Estacionariedade\n\n\n\n\nEstacionariedade Estrita (Forte): Uma condição mais restritiva onde toda a distribuição conjunta de probabilidade permanece inalterada sob qualquer deslocamento espacial. Formalmente, para qualquer conjunto finito de \\(n\\) localizações \\(\\{s_1, s_2, \\dots, s_n\\}\\) e qualquer vetor de deslocamento \\(h\\), a distribuição conjunta de probabilidade deve satisfazer:\n\n\\[P(Y(s_1) \\le y_1, \\dots, Y(s_n) \\le y_n) = P(Y(s_1+h) \\le y_1, \\dots, Y(s_n+h) \\le y_n)\\]\nOu, de forma simplificada, a igualdade em distribuição: \\[(Y(s_1), \\dots, Y(s_n)) \\stackrel{d}{=} (Y(s_1+h), \\dots, Y(s_n+h))\\] Em Processos Gaussianos (GP), como a distribuição é totalmente caracterizada pela média e covariância, a estacionariedade de segunda ordem implica automaticamente a estrita (Schmidt e O’Hagan 2003). isto é, como a distribuição Normal Multivariada (\\(\\mathcal{N}\\)) depende exclusivamente dos dois primeiros momentos (\\(\\mu\\) e \\(\\Sigma\\)), se esses momentos forem invariantes por translação (estacionariedade de segunda ordem), a distribuição inteira também será (estacionariedade estrita). Não há parâmetros de ordem superior (como assimetria ou curtose) que possam variar.\n\nEstacionariedade Intrínseca: Frequentemente usada no cálculo do Variograma, é menos restritiva que a de segunda ordem. Exige apenas que a média das diferenças seja zero e que a variância das diferenças entre observações dependa apenas da distância: \\(Var(Y(s +h) - Y(\\mathbf{s})) = 2\\gamma(\\mathbf{h})\\).\n\n\n\nNão Estacionariedade Espacial\nEm contrapartida, a não estacionariedade espacial descreve a condição onde um modelo global único é incapaz de capturar a complexidade das relações, pois a própria natureza do processo se altera sobre o espaço Brunsdon, Fotheringham, e Charlton (1996). Isso implica a violação das suposições de média ou covariância constantes descritas acima.\nA não estacionariedade pode manifestar-se como uma tendência espacial na média (heterogeneidade de primeira ordem) ou como uma mudança na estrutura de dependência (heterogeneidade de segunda ordem), onde, por exemplo, o alcance da correlação espacial é curto em áreas urbanas, mas longo em áreas rurais Dreesman e Tutz (2001). Ignorar essas heterogeneidades e forçar um modelo estacionário em dados não estacionários pode resultar em erros de estimativa sistemáticos, enviesamento de previsões e, em aplicações práticas, levar a decisões equivocadas (Bandyopadhyay e Rao 2017).\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\npacman::p_load(ggplot2, gstat, sf, viridis, gridExtra, patchwork)\n\n#\ngrid_df &lt;- expand.grid(x = 1:40, y = 1:40)\ngrid_sf &lt;- st_as_sf(grid_df, coords = c(\"x\", \"y\"))\n\nmodelo_vgm &lt;- gstat::vgm(psill = 10, model = \"Sph\", range = 15, nugget = 1)\n\nset.seed(123)\nobjeto_gstat &lt;- gstat::gstat(formula = z~1, locations = grid_sf, dummy = TRUE, beta = 0, model = modelo_vgm, nmax = 20)\ninvisible(capture.output(sim_estacionaria &lt;- predict(objeto_gstat, newdata = grid_sf, nsim = 1)))\n\ndf_sim &lt;- data.frame(grid_df, z_estacionario = sim_estacionaria$sim1)\n\n# z(s) = mu(s) + erro(s)\ndf_sim$tendencia &lt;- 0.5 * df_sim$x + 0.5 * df_sim$y\ndf_sim$z_nao_estacionario &lt;- df_sim$z_estacionario + df_sim$tendencia\n\nminha_legenda &lt;- guide_colorbar(\n  title = \"\",\n  barwidth = unit(0.4, \"npc\"),\n  barheight = unit(0.3, \"cm\"),\n  label.position = \"bottom\"\n)\n\n#\np1 &lt;- ggplot(df_sim, aes(x, y, fill = z_estacionario)) +\n  geom_tile() + scale_fill_viridis_c(option = \"B\") +\n  coord_fixed() + theme_void() +\n  labs(title = \"Estacionário (1ª Ordem)\", fill=\"\") +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"bottom\") +\n  guides(fill = minha_legenda)\n\np2 &lt;- ggplot(df_sim, aes(x, y, fill = z_nao_estacionario)) +\n  geom_tile() + scale_fill_viridis_c(option = \"B\") +\n  coord_fixed() + theme_void() +\n  labs(title = \"Não Estacionário (Tendência)\", fill=\"\") +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"bottom\") +\n  guides(fill = minha_legenda)\n\np1 + p2\n\n\n\n\n\n\n\n\nFigura 2.5: Processo Estacionário vs. Processo com Tendência (Não Estacionário na Média).\n\n\n\n\n\n\n\n\n\n\n\nImportanteConsequência Prática na Modelagem\n\n\n\nA maioria dos métodos clássicos de geoestatística, como a Krigagem Simples ou Ordinária, assume implicitamente a estacionariedade dos dados. Se a análise exploratória revelar um padrão semelhante a Figura 2.5 da direita (Não Estacionário), a aplicação direta desses métodos será inválida. Nestes casos, o analista deve optar por:\n\nRemover a tendência (detrending) modelando-a com uma superfície polinomial e analisando apenas os resíduos; ou\nUtilizar métodos que incorporem a tendência explicitamente, como a Krigagem Universal ou modelos de regressão geograficamente ponderada.\n\n\n\nO mapa como ferramenta analítica (e não decorativa)\nA transição do mapa de um artefato meramente ilustrativo para um instrumento analítico robusto demanda uma reorientação metodológica fundamental na ciência de dados espaciais. Frequentemente, o mapa é tratado como o estágio final da pesquisa — uma imagem estática destinada apenas a validar resultados já obtidos ou indicar a localização de um evento. No entanto, uma abordagem estatística rigorosa reposiciona o mapa não como um fim, mas como um meio dinâmico de inquirição. Segundo Waller (2024), mapas servem para localizar números que, por sua vez, necessitam de mapas para transformar a simples incidência de dados em ideias sobre causalidade. Essa perspectiva insere a cartografia no que o autor descreve como o “vórtice rodopiante da análise” (whirling vortex of analysis, vortex), um ciclo contínuo onde a visualização espacial motiva as perguntas iniciais, define a coleta de dados necessária e expõe as limitações dos métodos estatísticos disponíveis.\nPara exercer essa função diagnóstica, o analista deve interrogar o mapa em busca de padrões estruturais específicos, nomeadamente: tendências, verificando a existência de gradientes direcionais (ex: o Leste é sistematicamente mais rico que o Oeste?); clusters/agrupamentos, identificando a presença de ilhas de valores altos ou baixos (pontos quentes/frios); e outliers/valores atípicos espaciais, detectando observações que desafiam a lógica do seu entorno, como uma ilha de riqueza cercada por um mar de pobreza. Contudo, para que essa leitura seja válida, a construção do mapa deve obedecer a regras estritas de semiologia gráfica. Loonis e Bellefon (2018) alerta que a escolha incorreta da variável visual pode enviesar completamente a interpretação. Um erro clássico e grave é a representação de dados absolutos (volumes, como população total ou PIB) através de mapas coropléticos (áreas coloridas). Isso gera uma distorção perceptiva onde unidades geográficas fisicamente extensas, mas pouco povoadas, dominam visualmente o mapa, sugerindo uma importância que não possuem. A prática analítica correta exige o uso de símbolos proporcionais (círculos ou quadrados) para volumes, reservando o uso de cores (mapas coropléticos) exclusivamente para variáveis normalizadas, como taxas, densidades ou proporções.\nAlém da escolha do tipo de mapa, a eficácia analítica reside na capacidade do pesquisador em manipular conscientemente a distorção da realidade. Monmonier (2005) argumenta que a generalização cartográfica não é uma falha, mas uma necessidade; um mapa que tentasse contar “toda a verdade” na escala 1:1 resultaria em uma exibição confusa e inútil. O ponto crítico dessa manipulação ocorre na discretização dos dados (definição dos intervalos de classe). A aceitação ingênua das classificações automáticas de software (default settings) pode mascarar tendências vitais ou criar padrões espúrios. A escolha entre métodos como quantis (que enfatizam a ordem relativa), intervalos iguais (que facilitam a leitura da legenda mas falham em dados assimétricos) ou quebras naturais de Jenks (que buscam minimizar a variância interna dos grupos) deve ser precedida por uma análise da distribuição dos dados (ex: usando histograma). Como demonstrado por Monmonier (2005), alterar o método de classificação é uma forma de análise exploratória que pode modificar drasticamente a percepção de correlações espaciais.\nA utilidade do mapa como ferramenta científica depende de sua coerência interna e da confiança que ele inspira. Mocnik (2023) propõe que a legibilidade de um mapa deriva da coerência entre as afirmações que ele faz sobre o espaço, permitindo que diferentes observadores convirjam para uma interpretação comum. Em um ambiente saturado de informações, Prestby (2025) destaca que mapas são frequentemente usados como dispositivos retóricos de autoridade. Portanto, para transcender a simples “credibilidade” superficial e fomentar uma confiança duradoura, o mapa analítico deve ser transparente: ele não deve apresentar o território como uma ilha flutuando no vazio, mas incluir contexto vizinho, metadados detalhados e, crucialmente, a visualização explícita das incertezas inerentes ao processo de modelagem.\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(sf, ggplot2, dplyr, viridis, patchwork)\n\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package=\"sf\"), quiet = TRUE)\n\nminha_legenda &lt;- guide_colorbar(\n  title = \"\", \n  barwidth = unit(0.4, \"npc\"), \n  barheight = unit(0.3, \"cm\"),\n  label.position = \"bottom\"\n)\n#\np_erro &lt;- ggplot(nc) +\n  geom_sf(aes(fill = SID74)) +\n  scale_fill_viridis_c(option = \"B\", name = \"\") +\n  theme_void() +\n  labs(title = \"Total de mortalidade súbita infantil (Incorreto) \\n áreas com maior nr de nascimentos dominam\",\n       subtitle = \"\", fill=\"\") +\n  theme(legend.position = \"bottom\", \n        plot.title = element_text(size = 14,hjust = 0.5))+\n  guides(fill = minha_legenda)\n\n#.\nnc$taxa &lt;- (nc$SID74 / nc$BIR74) * 1000\n\n#\nnc$taxa_cat &lt;- cut(nc$taxa, \n                   breaks = quantile(nc$taxa, probs = seq(0, 1, 0.25)), \n                   include.lowest = TRUE, \n                   labels = c(\"Q1 (Baixa)\", \"Q2\", \"Q3\", \"Q4 (Alta)\"))\n\np_taxa &lt;- ggplot(nc) +\n  geom_sf(aes(fill = taxa_cat)) +\n  scale_fill_viridis_d(option = \"B\", name = \"\") +\n  theme_void() +\n  labs(title = \"Taxa de mortalidade súbita infantil\\n por 1.000 nascimentos (Correto)\",\n       subtitle = \"\", fill=\"\") +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(size = 14, hjust = 0.5))\n\n#\np_intervals &lt;- ggplot(nc) +\n  geom_sf(aes(fill = taxa)) +\n  scale_fill_stepsn(colors = viridis::viridis(5), \n                    n.breaks = 4, # Tenta forçar intervalos iguais numéricos\n                    name = \"\") +\n  theme_void() +\n  labs(title = \"Taxa intervalar de mortalidade\\n súbita infantil por 1.000 nascimentos.\",\n       subtitle = \"\", fill=\"\") +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(size = 14, hjust = 0.5))+\n  guides(fill = minha_legenda)\n\np_erro\n\np_taxa\n\np_intervals\n\n\n\n\n\n\n\n\n\n\nFigura 2.6: O Impacto da escolha visual do síndrome da morte súbita infantil (SIDS) na Carolina do Norte: 1974-78\n\n\n\n\n\n\n\n\n\n\n\nFigura 2.7: O Impacto da escolha visual do síndrome da morte súbita infantil (SIDS) na Carolina do Norte: 1974-78\n\n\n\n\n\n\n\n\n\n\n\nFigura 2.8: O Impacto da escolha visual do síndrome da morte súbita infantil (SIDS) na Carolina do Norte: 1974-78\n\n\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nA diferença entre um mapa decorativo e um mapa analítico reside na intenção e na transparência do processo de construção:\n\nDecorativo: Busca a estética e a persuasão imediata, frequentemente ocultando a incerteza e aceitando classificações automáticas de software sem crítica.\nAnalítico: Busca a descoberta e a compreensão de processos. Ele utiliza a generalização como recurso metodológico consciente e estabelece a confiança através da explicitação das fontes, dos métodos de classificação escolhidos e das limitações dos dados.",
    "crumbs": [
      "Fundamentos da Estatística Espacial",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos da Estatística Espacial</span>"
    ]
  },
  {
    "objectID": "fundEstspatial.html#o-espaço-geográfico",
    "href": "fundEstspatial.html#o-espaço-geográfico",
    "title": "2  Fundamentos da Estatística Espacial",
    "section": "2.4 O Espaço Geográfico",
    "text": "2.4 O Espaço Geográfico\nNa estatística espacial, a representação rigorosa do espaço é a base fundamental sobre a qual todas as análises subsequentes são construídas. Diferente da estatística clássica, onde as observações existem em um espaço abstrato, na estatística espacial a localização física \\((x,y,z)\\) e as relações métricas entre as observações são determinantes. Saber onde algo acontece é tão importante quanto saber o que aconteceu. Para determinar a localização de eventos, calcular distâncias entre vizinhos ou mensurar áreas com validade física e estatística, é imprescindível estabelecer superfícies de referência. A seguir, detalhamos os fundamentos geodésicos essenciais para evitar os erros mais comuns em análise espacial.\n\n\n\n\n\n\nNotaDica de Estudo\n\n\n\nEsta seção aborda conceitos fundamentais, porém densos. Se sentir dificuldade na primeira leitura, não desanime: a percepção espacial exige tempo e abstração. Recomendamos que, em caso de dúvida, você complemente o estudo com as referências sugeridas e explore vídeos explicativos no YouTube ou artigos no Google Scholar para ver esses conceitos aplicados na prática.\n\n\nA forma da Terra: esfera, elipsoide e geoide\n\nComo representamos a Terra matematicamente?\n\nA primeira etapa para localizar um ponto no espaço é definir a superfície sobre a qual estamos trabalhando. Historicamente e computacionalmente, trabalhamos com três aproximações da forma da terra, cada uma com um propósito distinto na modelagem espacial.\n\nSuperfície topográfica (A terra real): É o chão onde pisamos e onde realizamos as medições. É irregular e rugosa. Matematicamente, é uma superfície complexa demais para realizar cálculos geométricos globais diretos, servindo apenas como objeto de medição, não de referência matemática Figura 2.9.\n\n\n\n\n\n\n\nFigura 2.9: Superfície topográfica, Fonte: Prof. Adenilson Giovanini\n\n\n\n\nGeoide (A Terra da Física): Imagine que a Terra fosse coberta inteiramente por água, sem ventos ou marés, influenciada apenas pela gravidade. A forma que essa água tomaria é o Geoide. O Geoide é a superfície equipotencial do campo gravitacional que coincide com o Nível Médio do Mar em repouso e se estende continuamente sob os continentes (Iliffe 2000). Se os oceanos pudessem fluir livremente sob a terra através de canais, a superfície que a água formaria seria o Geoide. Fisicamente, ele define a “vertical” (direção da gravidade). Porém, como é ondulado (devido à distribuição desigual de massa da Terra), não serve como superfície de cálculo de coordenadas (latitude/longitude), mas é fundamental para definir a altitude ortométrica (onde a água flui) Figura 2.10.\n\n\n\n\n\n\n\nFigura 2.10: Geoide, Fonte: Prof. Adenilson Giovanini\n\n\n\n\nElipsoide de Revolução ((A Terra da Matemática): Devido à complexidade do Geoide, utiliza-se o Elipsoide de revolução, que é uma figura matemática suave gerada pela rotação de uma elipse ao redor do seu eixo menor. Ele não tem realidade física (você não sente o elipsoide), mas é a superfície de referência onde projetamos as coordenadas para fazer contas. A relação fundamental é \\(h \\approx H + N\\) (Altitude Geométrica = Ortométrica + Ondulação Geoidal). O elipsoide é definido por dois parâmetros principais: o semi-eixo maior \\(a\\) e o semi-eixo menor \\(b\\) Figura 2.11. A partir destes, define-se o achatamento \\(f\\), que descreve o quanto a Terra se desvia de uma esfera perfeita, calculado pela fórmula \\(f = (a - b)/a\\). Ignorar este achatamento em escalas locais ou regionais pode introduzir erros significativos de posição (Iliffe 2000).\n\n\n\n\n\n\n\nFigura 2.11: Elipsoide de Revolução, Fonte: Janssen (2009)\n\n\n\n\n\n\n\n\n\nImportanteSaiba mais\n\n\n\nRecomenda-se aos interessados aprofundar o estudo deste tema através da leitura do livro Datums and Map Projections for remote sensing, GIS, and surveying [iliffe2000datums], bem como consultando os materiais práticos do blog do Professor Adenilson Giovanini.",
    "crumbs": [
      "Fundamentos da Estatística Espacial",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos da Estatística Espacial</span>"
    ]
  },
  {
    "objectID": "fundEstspatial.html#sec-grid",
    "href": "fundEstspatial.html#sec-grid",
    "title": "2  Fundamentos da Estatística Espacial",
    "section": "2.5 Fundamentos de geodesia",
    "text": "2.5 Fundamentos de geodesia\nA Geodesia é a ciência responsável por medir e representar a forma da Terra, sua orientação no espaço e seu campo gravitacional. Sua tarefa primordial é criar o arcabouço matemático que permite transformar a superfície física irregular da Terra em dados de coordenadas \\(X, Y\\) e \\(Z\\) processáveis por computadores Vanicek e Krakiwsky (2015). Enquanto a topografia clássica operava separando o posicionamento horizontal do vertical devido às limitações dos instrumentos óticos, a geodesia moderna, impulsionada pela era espacial, opera num modelo tridimensional integrado. Isso significa que a posição de um ponto é definida por um vetor tridimensional com origem no centro de massa da Terra, unificando a geometria e a física do planeta para fornecer localizações precisas em qualquer lugar do globo.\nSistemas geodésicos e datum (horizontal e vertical)\nUm elipsoide (Figura 2.11) é apenas uma forma geométrica. Para que ele sirva como sistema de coordenadas, precisamos “ancorá-lo” à Terra, e essa ancoragem é definida pelo Datum. O Datum define a posição do centro do elipsoide, sua orientação e escala em relação ao planeta. Existem dois tipos principais de Datum Horizontal Janssen (2009).\n\nDatum Topocêntrico (Local): O elipsoide é encaixado para servir bem a uma região (ex: SAD69 para o Brasil). Seu centro não coincide com o centro de massa da Terra, sendo posicionado e orientado para se ajustar perfeitamente a uma região específica, como um país ou continente.\nDatum Geocêntrico (Global): O centro do elipsoide coincide com o centro de massa da Terra (ex: SIRGAS2000, WGS84). Este é o padrão obrigatório para uso com GNSS/GPS.\n\nA distinção entre datums é crucial porque uma coordenada composta por latitude e longitude não é um local único e absoluto. Sem a especificação do Datum, esses valores numéricos são ambíguos. A mudança de um datum para outro, conhecida como Datum Shift, implica que as coordenadas numéricas de um mesmo ponto físico no chão se alteram. No Brasil (Link), a transição do SAD69 para o SIRGAS2000 implicou um deslocamento de aproximadamente 65 metros para as mesmas coordenadas geográficas. Além do horizontal, existe o Datum Vertical, que define a superfície de referência para a altitude zero, geralmente associada a um marégrafo específico que monitora o nível médio do mar localmente.\nSistema Geodésico Mundial (WGS84)\nO Sistema Geodésico Mundial de 1984, conhecido como WGS84, é o sistema de referência padrão para o sistema GPS. Ele fornece um referencial globalmente consistente que permite que receptores em qualquer lugar do planeta calculem suas posições de forma compatível Leick, Rapoport, e Tatarnikov (2015). O WGS84 não é apenas um elipsoide, mas um sistema geodésico completo que inclui um modelo gravitacional da Terra e parâmetros angulares de rotação. Seus parâmetros definem o semi-eixo maior da Terra como exatamente \\(6.378.137,0\\) metros e um achatamento de aproximadamente \\(1/298.257\\) Janssen (2009). Para a maioria das aplicações práticas em estatística espacial e geoprocessamento no Brasil, o WGS84 é considerado praticamente idêntico ao SIRGAS2000, o padrão oficial brasileiro, diferindo apenas na ordem de milímetros devido a atualizações temporais nas placas tectônicas.\nPosicionamento global (GNSS: GPS, GLONASS, Galileo)\nO termo GNSS (Global Navigation Satellite Systems) refere-se ao conjunto de constelações de satélites (um grupo de satélites similares que orbitam a Terra de forma sincronizada e otimizada) que permitem o posicionamento geoespacial autônomo, englobando o GPS americano, o GLONASS russo, o Galileo europeu e o BeiDou chinês. O funcionamento desses sistemas baseia-se no princípio da trilateração espacial. Um receptor GNSS mede o tempo \\(t\\) que um sinal de rádio leva para “viajar” do satélite até ele. Conhecendo a velocidade da luz \\(c\\), calcula-se a distância \\(d\\) através da equação \\(d = c \\cdot t\\). Como o relógio do receptor não é perfeitamente sincronizado com os relógios atômicos dos satélites, o sistema precisa resolver um sistema de equações lineares com quatro incógnitas: as três coordenadas de posição \\(X, Y, Z\\) e o erro do relógio do receptor \\(\\delta t\\). Portanto, são necessários sinais de no mínimo quatro satélites simultaneamente para que um receptor possa calcular uma posição tridimensional válida (Kaplan e Hegarty 2017).\nPrecisão, acurácia e fontes de erro\nNa coleta de dados espaciais, é vital distinguir precisão de acurácia. A acurácia ou exatidão refere-se a quão próximo o valor medido está do valor verdadeiro no terreno, enquanto a precisão refere-se ao grau de repetibilidade da medida, ou seja, quão próximos os valores medidos estão uns dos outros em repetidas observações. Um conjunto de dados pode ser preciso (todos os pontos agrupados) mas não acurado (todos deslocados do local real e/ou que se deseja).\nVárias fontes de erro afetam o posicionamento GNSS. Os efeitos atmosféricos são significativos, pois a ionosfera e a troposfera refratam o sinal de rádio, alterando sua velocidade e causando atrasos que o receptor interpreta erroneamente como distâncias maiores. O erro de multicaminho (multipath) ocorre quando o sinal reflete em superfícies como prédios, árvores ou o próprio solo antes de chegar à antena, aumentando o tempo de viagem e gerando “fantasmas” de localização, o que é crítico em ambientes urbanos. Além disso, a geometria dos satélites, medida pelo índice DOP (Dilution of Precision), influencia a qualidade da posição; se os satélites visíveis estiverem agrupados em uma pequena região do céu, a incerteza na trilateração aumenta drasticamente Langley et al. (1999).\nCoordenadas geográficas: latitude e longitude\nPara descrever a posição de um ponto sobre a superfície curva do elipsoide, utilizamos coordenadas curvilíneas. A Latitude (\\(\\phi\\)) é o ângulo medido no centro do elipsoide entre o plano equatorial e a linha normal (perpendicular) à superfície do elipsoide que passa pelo ponto de interesse, variando de \\(-90^\\circ\\) no Polo Sul a \\(+90^\\circ\\) no Polo Norte Figura 2.12 e Figura 2.13. A Longitude (\\(\\lambda\\)) é o ângulo medido no plano equatorial entre o Meridiano de Greenwich, que serve como origem, e o meridiano que passa pelo ponto, variando de \\(-180^\\circ\\) a Oeste a \\(+180^\\circ\\) a Leste (Iliffe 2000).\nA relação entre estas coordenadas angulares e a posição cartesiana tridimensional \\((X, Y, Z)\\) é dada por: \\[\n    \\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix} =\n    \\begin{bmatrix}\n    (N + h) \\cos \\phi \\cos \\lambda \\\\\n    (N + h) \\cos \\phi \\sin \\lambda \\\\\n    [N(1-e^2) + h] \\sin \\phi\n    \\end{bmatrix}\n\\] Onde \\(N\\) é o raio de curvatura da primeira vertical (Grande Normal) e \\(e\\) é a excentricidade do elipsoide (Iliffe 2000).\nÉ fundamental notar a implicação prática disto nas distâncias: enquanto um grau de latitude tem comprimento linear quase constante (\\(\\approx 111\\) km), a distância linear correspondente a um grau de longitude (\\(d_{long}\\)) varia com o cosseno da latitude: \\(d_{long} \\approx 111 \\cdot \\cos(\\phi) \\text{ km}\\) Isso explica o estreitamento das distâncias, que vão de \\(\\approx 111\\) km no Equador até zero nos polos.\n\n\n\n\n\n\n\n\n\nFigura 2.12: Coordenadas geográficas, Fonte: Suporte Geográfico\n\n\n\n\n\n\n\n\n\n\n\nFigura 2.13: Sinal de cada polo nas coordenadas geográficas, Fonte: Santa Isabel Tripod\n\n\n\n\n\n\nGraus, minutos e segundos vs. coordenadas decimais\nTradicionalmente, as coordenadas geográficas são expressas no sistema sexagesimal de Graus, Minutos e Segundos (DMS). No entanto, para análise computacional e estatística em ambientes como R ou Python, é imperativo converter esses valores para Graus Decimais (DD). A fórmula de conversão é dada por\n\\[\nDD = (\\text{sinal}) \\times \\left(D_{graus} + M_{minutos}/60 + S_{segundos}/3600\\right).\n\\tag{2.3}\\]\nÉ crucial observar o sinal: se a direção for Sul (S) ou Oeste (W), o resultado final deve ser negativo, isto é, o sinal descrito na Eq. 2.3 deve ser negativo Figura 2.13.\nTomando como exemplo a latitude \\(23^\\circ 33' 15'' S\\), termina por \\(S\\), logo a direção é Sul . Portanto, sabemos de antemão que o resultado numérico final deve ser negativo Figura 2.13. Assim, realizamos a soma das partes e multiplicamos o total por \\(-1\\) para posicionar corretamente a coordenada no globo:\n\\[\n\\begin{aligned}\nDD &= (-1) \\times \\left( D_{graus} + \\frac{M_{minutos}}{60} + \\frac{S_{segundos}}{3600} \\right) \\\\\nDD &= (-1) \\times \\left( 23 + \\frac{33}{60} + \\frac{15}{3600} \\right) \\\\\nDD &= (-1) \\times \\left( 23 + 0,55 + 0,00416\\dots \\right) \\\\\nDD &= -23,554167^\\circ\n\\end{aligned}\n\\tag{2.4}\\]\nA não realização dessa conversão ou o tratamento incorreto dos sinais é uma fonte comum de erros grosseiros em análises espaciais.\nA conversão inversa, de Graus Decimais (\\(DD\\)) de volta para o sistema Sexagesimal (\\(DMS\\)), é igualmente importante para a comunicação de resultados. O procedimento matemático isola a parte inteira e fracionária sucessivamente. Primeiramente, o valor absoluto da coordenada decimal fornece os Graus inteiros (\\(D = \\lfloor |DD| \\rfloor\\)). A parte fracionária restante é multiplicada por 60 para obter os minutos decimais; a parte inteira deste resultado torna-se os Minutos (\\(M\\)). Por fim, a nova parte fracionária restante (dos minutos) é multiplicada novamente por 60 para obter os Segundos (\\(S\\)). O sinal original do valor decimal (\\(+\\) ou \\(-\\)) determina o hemisfério (Norte/Sul para latitude, Leste/Oeste para longitude).\nRevertendo o valor \\(-23,554167^\\circ\\) calculado anteriormente, obtemos:\n\\[\n\\begin{aligned}\n\\text{Graus} &= \\lfloor | -23,554167 | \\rfloor = 23^\\circ \\\\\n\\text{Resto}_1 &= 0,554167 \\\\\n\\text{Minutos} &= \\lfloor 0,554167 \\times 60 \\rfloor = \\lfloor 33,25002 \\rfloor = 33' \\\\\n\\text{Resto}_2 &= 0,25002 \\\\\n\\text{Segundos} &= 0,25002 \\times 60 \\approx 15,00''\n\\end{aligned}\n\\]\nComo o valor original (\\(-23,554167^\\circ\\)) era negativo, combinamos o resultado calculado com a direção identificada no início, resultando em: \\(23^\\circ 33' 15'' S\\).\n\n\n\n\n\n\nAvisoAtenção na Manipulação de Coordenadas\n\n\n\n\nIdentificação dos Eixos: É fundamental identificar corretamente quais valores correspondem à Latitude (Y) e quais à Longitude (X) consultando a documentação dos dados. A inversão acidental dessas coordenadas altera drasticamente a localização geográfica no mapa, podendo posicionar o objeto em outro hemisfério ou invalidar a geometria (ex: Latitude \\(&gt; 90^\\circ\\)).\nPrecisão Numérica: Ao converter de Graus Decimais de volta para DMS, é comum encontrar resíduos numéricos (ex: \\(14.9999''\\) em vez de \\(15''\\)). Isso ocorre devido à aritmética de ponto flutuante (floating point arithmetic) dos computadores. Para visualização em mapas, recomenda-se arredondar os segundos para duas casas decimais, exceto em casos de geodésia de alta precisão.\n\n\n\nGrid geográfico\nO cruzamento de paralelos (linhas de latitude constante) e meridianos (linhas de longitude constante) forma o Grid Geográfico. Este sistema fornece uma localização absoluta e única para cada ponto na superfície terrestre, permitindo a referência universal. Contudo, para a estatística espacial, o grid geográfico curvo apresenta desafios significativos. Métodos analíticos como a estimativa de densidade de Kernel ou a função K de Ripley assumem distâncias euclidianas em um plano cartesiano isotrópico. Calcular essas distâncias diretamente sobre coordenadas angulares (graus) introduz distorções, pois a geometria do grid não é quadrada. Portanto, frequentemente é necessário projetar esse grid geográfico em um plano cartesiano através de um Sistema de Coordenadas Projetadas, processo que introduz distorções de área, forma ou distância, mas permite a aplicação correta da geometria euclidiana localmente.\nSistemas de Referência de Coordenadas (CRS)\nPara manipular dados espaciais em um ambiente computacional e realizar cálculos de distância euclidiana, precisamos traduzir a Terra curva para uma superfície plana. O mecanismo que gerencia essa tradução e assegura a integridade espacial dos dados é o Sistema de Referência de Coordenadas, ou CRS (Coordinate Reference System).\nUm CRS define matematicamente como as coordenadas bidimensionais projetadas (\\(x, y\\)) de um mapa se relacionam com localizações reais na superfície da Terra. Ele contém todas as informações necessárias para entender os números que compõem a geometria dos dados: o datum, o elipsoide de referência e, se aplicável, a projeção cartográfica utilizada (Snyder 1987).\n\n\n\n\n\n\nImportante\n\n\n\nA definição de um Sistema de Referência de Coordenadas (CRS) é indispensável para a análise espacial. Todo objeto espacial, seja um ponto, linha ou polígono, deve ter um CRS associado. Sem isso, essas formas são tratadas apenas como figuras geométricas em um plano abstrato, sem correspondência real com a superfície terrestre, o que impede a correta sobreposição de camadas (layers) e a integração de dados de fontes distintas.\nFrequentemente, bases de dados apresentam unidades diferentes, como graus (sistemas geográficos) e metros (sistemas projetados, ex: UTM). Nessa situação, é imprescindível padronizar todas as camadas para um único sistema. Cabe ao analista escolher o CRS de referência e realizar a conversão (reprojeção) dos demais dados, decisão que deve considerar o objetivo da análise (ex: cálculos de distância ou área requerem sistemas projetados).\n\n\n\n\n\n\n\n\nNotaA Relação entre Coordenadas Geográficas e Eixos Cartesianos\n\n\n\nA associação correta entre coordenadas geográficas e os eixos do plano cartesiano frequentemente gera confusão devido a uma distinção sutil entre a representação visual das linhas e a direção de sua variação numérica. É fundamental compreender que, embora os meridianos (longitude) sejam desenhados como linhas verticais que conectam os polos, o valor da longitude varia deslocando-se no sentido Leste-Oeste; portanto, ela corresponde ao eixo horizontal (\\(X\\)). Inversamente, embora os paralelos (latitude) sejam visualizados como anéis horizontais, o valor da latitude altera-se ao mover-se no sentido Norte-Sul, definindo o posicionamento no eixo vertical (\\(Y\\)).\nEssa lógica impõe uma atenção redobrada na estruturação dos dados, pois existe um conflito direto entre a convenção de fala e a formalização algorítmica. Enquanto na linguagem coloquial e na navegação convencionou-se dizer “Latitude e Longitude”, a matemática e a computação operam rigorosamente com pares ordenados na forma \\((X, Y)\\). Consequentemente, ao introduzir coordenadas em softwares estatísticos ou linguagens de programação, é imperativo inverter a ordem falada e adotar a ordem matemática: \\((\\text{Longitude}, \\text{Latitude})\\).\n\n\nSistemas geográficos vs. sistemas projetados\nExistem dois tipos fundamentais de CRS. Os Sistemas de Coordenadas Geográficas (GCS) utilizam uma superfície tridimensional esférica ou elipsoidal para definir localizações. As unidades são angulares, geralmente graus decimais, e os exemplos mais comuns incluem o WGS84 e o SIRGAS2000. Estes sistemas são ideais para o armazenamento global de dados, mas não são adequados para cálculos diretos de distâncias ou áreas em duas dimensões. Já os Sistemas de Coordenadas Projetadas (PCS) utilizam uma superfície plana bidimensional. Eles são baseados em um GCS, mas aplicam uma transformação matemática (projeção) para “aplanar” a Terra. As unidades nestes sistemas são lineares, como metros ou pés, tornando-os a escolha correta para cálculos de área, distância e para a maioria das análises estatísticas espaciais. Exemplos incluem o sistema UTM e a projeção de Albers (Iliffe 2000).\n\n\n\n\n\n\nDicaSaiba mais\n\n\n\nPara um aprofundamento teórico, recomenda-se a leitura das seções 1 a 6 do livro Spatial Data Science With Applications in R, dos professores Edzer Pebesma e Roger Bivand. O material completo está disponível neste link.\n\n\nPor que projetar a superfície da Terra\nEmbora vivamos em um globo, as telas de computador, os mapas impressos e a matemática da geometria euclidiana são planos. A projeção é necessária não apenas para a visualização, mas fundamentalmente para a análise. Muitos algoritmos de estatística espacial assumem um espaço isotrópico onde o Teorema de Pitágoras (\\(h=\\sqrt{\\Delta x^2 + \\Delta y^2}\\)) é válido. Aplicar essa fórmula diretamente a coordenadas de latitude e longitude gera erros grosseiros e variáveis, uma vez que a distância representada por um grau de longitude diminui à medida que nos afastamos do Equador. Projetar os dados transforma a superfície curva em um plano métrico (\\(X, Y\\)) onde a geometria euclidiana funciona corretamente dentro de limites locais específicos (Iliffe 2000).\nTipos de distorção: área, forma, distância e direção\nÉ matematicamente impossível aplanar uma superfície esférica sem distorcer alguma de suas propriedades geométricas. As projeções cartográficas são classificadas com base na propriedade que elas preservam, aceitando a distorção nas outras.\n\nProjeções conformes preservam formas locais e ângulos, sendo úteis para navegação e topografia, mas distorcem drasticamente as áreas, fazendo regiões polares parecerem maiores do que são.\nProjeções equivalentes ou Equal-Area preservam as proporções das áreas relativas, sendo essenciais para mapas coropléticos estatísticos e análises de densidade, embora distorçam as formas.\nProjeções equidistantes preservam as distâncias a partir de um ou dois pontos específicos, mas distorcem formas e áreas em outros locais.\n\n\n\n\n\n\n\nNotaProjeção\n\n\n\nO analista deve escolher a projeção que minimiza a distorção na propriedade mais importante para sua análise específica.\n\n\nProjeções cartográficas mais usadas em estatística espacial\nDentre as inúmeras projeções existentes, algumas assumem protagonismo na prática da análise espacial devido às suas propriedades geométricas específicas:\n\nSistema UTM (Universal Transversa de Mercator): Segmenta a Terra em 60 zonas (fusos) de 6 graus de longitude. Dentro de cada zona, a projeção é conforme (preserva ângulos) e as distorções lineares são mínimas, tornando-o o padrão para mapeamentos e análises em escalas local e regional.\n\n\n\n\n\n\n\nFigura 2.14: Sistema UTM, Fonte: Wikipedia\n\n\n\n\nProjeção Cônica de Albers (Albers Equal-Area): Frequentemente adotada para análises que cobrem vastas extensões territoriais, como países de dimensões continentais (ex: Brasil ou EUA). Por ser uma projeção equivalente, ela preserva a área real das feições, assegurando que cálculos de densidade e comparações visuais sejam rigorosamente justos.\n\n\n\n\n\n\n\nFigura 2.15: Projeção Cônica de Albers, Fonte: Wikipedia\n\n\n\n\nMercator e Web Mercator: Embora onipresentes em mapas digitais (Google Maps, OpenStreetMap) devido à preservação de ângulos (útil para navegação), estas projeções distorcem severamente as áreas em direção aos polos. Portanto, são inadequadas para análises estatísticas que envolvam comparações de área, densidade global ou mapas coropléticos de grandes extensões.\n\n\n\n\n\n\n\nFigura 2.16: Projeção Mercator, Fonte: Wikipedia\n\n\n\nCódigos EPSG, PROJ e WKT\nPara gerenciar a complexidade de centenas de datums e projeções, padronizou-se internacionalmente o uso dos códigos do registro EPSG (European Petroleum Survey Group). Estes códigos numéricos curtos funcionam como identificadores para um CRS. Por exemplo:\n\nEPSG:4326: Refere-se ao WGS84 geográfico (Lat/Lon), padrão do GPS.\nEPSG:31983: Refere-se ao SIRGAS 2000 projetado na zona UTM 23S.\n\nNos bastidores, softwares de SIG e pacotes de estatística espacial utilizam pacotes como o PROJ para realizar as transformações matemáticas entre esses sistemas. Já as definições completas descrevendo detalhadamente o datum, o elipsoide, a projeção e os parâmetros de transformação são armazenadas em cadeias de texto padronizadas chamadas WKT (Well-Known Text).\n\n\n\n\n\n\nDicaComo descobrir o EPSG\n\n\n\nPara identificar o código correto de uma região, recomenda-se o uso do repositório epsg.io.\nAo acessar o site, você pode inserir a latitude e longitude ou utilizar a opção “Get position on a map”. Ao clicar na área de interesse no mapa, o site lista os zonas UTM e os códigos EPSG vigentes para aquela localização.\n\nUma distinção técnica crítica que frequentemente causa erros é a diferença entre atribuir e transformar um CRS. Atribuir um CRS significa apenas dizer ao software qual é o sistema de coordenadas dos dados, alterando o rótulo sem modificar os valores numéricos das coordenadas. Isso deve ser feito apenas quando os dados não possuem uma definição de CRS associada mas se sabe qual deveria ser. Transformar ou reprojetar um CRS envolve a aplicação de fórmulas matemáticas para converter as coordenadas de um sistema para outro, alterando os valores numéricos de (\\(x, y\\)) para corresponder à nova referência. Atribuir um CRS incorreto e depois tentar transformar resulta em dados posicionados erroneamente no espaço.\n\n\n\n\n\n\n\n\n\nImportanteTodos mapas estão errados\n\n\n\nAssista ao vídeo do canal Ciência Todo Dia, clicando neste Link. Recomendo também o material produzido pela Vox, que é mais detalhado e possui visualização dinâmica, acessível por este Link\nConfira a aula detalhada sobre projeções cartográficas do Prof. Ricardo Marcílio, clicando neste Link",
    "crumbs": [
      "Fundamentos da Estatística Espacial",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos da Estatística Espacial</span>"
    ]
  },
  {
    "objectID": "fundEstspatial.html#formas-de-representação-espacial-dos-dados",
    "href": "fundEstspatial.html#formas-de-representação-espacial-dos-dados",
    "title": "2  Fundamentos da Estatística Espacial",
    "section": "2.6 Formas de representação espacial dos dados",
    "text": "2.6 Formas de representação espacial dos dados\nPara transformar a infinita complexidade do mundo real em um ambiente computacional finito e passível de análise estatística, precisamos de modelos de abstração. Na Ciência da Informação Geográfica, existem duas visões fundamentais e dicotômicas sobre como modelar a realidade: a visão baseada em Objetos (Modelo Vetorial) e a visão baseada em Campos (Modelo Matricial/Raster) (Pebesma 2018).\nA escolha entre um e outro não é meramente técnica, mas ontológica: ela depende de como percebemos o fenômeno estudado. Uma casa é um objeto discreto (tem borda definida), enquanto a temperatura do ar é um campo contínuo (existe em toda parte e varia suavemente).\nGeometria e atributos\nNa geoinformática moderna, a unidade fundamental de análise vetorial é a Feição Simples Feição Simples (Simple Feature). Este conceito, padronizado internacionalmente pela ISO 19125, e descrito em Pebesma (2018) define que um objeto espacial é composto pela indissociabilidade entre sua forma e seus dados:\n\nGeometria (Onde): É a descrição matemática da forma espacial e da localização absoluta do objeto no sistema de coordenadas.\nAtributos (O quê/Quanto): São as variáveis estatísticas (como população, temperatura ou nome) associadas a essa geometria, organizadas em estruturas tabulares onde cada linha corresponde a uma geometria.\n\nA arquitetura definida pela Open Geospatial Consortium (OGC) estipula que uma geometria é simples quando sua representação é bidimensional e a interpolação entre seus vértices é estritamente linear (linhas retas), excluindo curvas matemáticas complexas como splines para maximizar a eficiência computacional (Cox 2011).\nModelo vetorial\nO modelo vetorial representa a geografia através de coordenadas explícitas que definem vértices e arestas. A Cox (2011) estabelece uma hierarquia de classes geométricas que derivam de uma classe raiz abstrata Geometry Figura 2.17.\n\nPontos (POINT): é a entidade geométrica elementar de dimensão zero (0-D). Definidos por uma única coordenada (x,y) ou, em sistemas geográficos, (Longitude, Latitude). Representam eventos onde a localização exata importa, mas a extensão física é irrelevante na escala do mapa (ex: localização de um crime, uma árvore, um poço).\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(sf, ggplot2, patchwork)\n\n#Ponto\nponto &lt;- st_point(c(2, 2))\ndf_ponto &lt;- st_sf(geometry = st_sfc(ponto), id = 1, tipo = \"Ponto\")\nprint(df_ponto)\n\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2 ymin: 2 xmax: 2 ymax: 2\nCRS:           NA\n  id  tipo    geometry\n1  1 Ponto POINT (2 2)\n\n\n\nLinhas (LINESTRING): é uma geometria unidimensional (1-D) definida por uma sequência ordenada de dois ou mais pontos conectados por segmentos retos. Representam fluxos ou redes (ex: rios, estradas, trajetórias). Uma linha é considerada simples se ela não cruza a si mesma (não possui auto-interseção), exceto se o ponto final coincidir com o inicial, formando um anel fechado (LinearRing).\n\n\n\nCódigo\n#Linha\nlinha &lt;- st_linestring(rbind(c(1, 1), c(3, 3), c(4, 1)))\ndf_linha &lt;- st_sf(geometry = st_sfc(linha), id = 2, tipo = \"Linha\")\nprint(df_linha)\n\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 1 ymin: 1 xmax: 4 ymax: 3\nCRS:           NA\n  id  tipo                   geometry\n1  2 Linha LINESTRING (1 1, 3 3, 4 1)\n\n\n\nPolígonos (POLYGON)\n\nO Polígono é uma superfície plana bidimensional (2-D) definida por um anel externo fechado (e opcionalmente anéis internos representando “buracos”). Representam áreas com limites definidos (ex: limites municipais, lagos, edifícios).\n\n\nCódigo\n#Polígono\npoly_coords &lt;- rbind(c(1, 1), c(1, 4), c(4, 4), c(4, 1), c(1, 1))\npoligono &lt;- st_polygon(list(poly_coords))\ndf_poly &lt;- st_sf(geometry = st_sfc(poligono), id = 3, tipo = \"Polígono\")\nprint(df_poly)\n\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1 ymin: 1 xmax: 4 ymax: 4\nCRS:           NA\n  id     tipo                       geometry\n1  3 Polígono POLYGON ((1 1, 1 4, 4 4, 4 ...\n\n\n\n\n\n\n\n\nNotaRegra da Mão Direita\n\n\n\nPara garantir que cálculos de área em superfícies esféricas sejam inequívocos, normas modernas como o RFC 7946 (GeoJSON) impõem uma regra de orientação: o anel exterior deve ser desenhado no sentido anti-horário, enquanto os anéis interiores (buracos) devem seguir o sentido horário (Cox 2011).\n\n\n\n\nCódigo\n#\ng1 &lt;- ggplot(df_ponto) + geom_sf(size = 4, color = \"red\") + \n  ggtitle(\"Ponto (POINT)\") + theme_void() + \n  theme(plot.title = element_text(hjust = 0.5))\n\ng2 &lt;- ggplot(df_linha) + geom_sf(size = 1.5, color = \"blue\") + \n  ggtitle(\"Linha (LINESTRING)\") + theme_void() + \n  theme(plot.title = element_text(hjust = 0.5))\n\ng3 &lt;- ggplot(df_poly) + geom_sf(fill = \"lightgreen\", alpha = 0.5) + \n  ggtitle(\"Polígono (POLYGON)\") + theme_void() + \n  theme(plot.title = element_text(hjust = 0.5))\n\ng1 + g2 + g3\n\n\n\n\n\n\n\n\nFigura 2.17: Primitivas Geométricas Vetoriais\n\n\n\n\n\n\nMulti-geometrias (MULTIPOINT, MULTILINESTRING, MULTIPOLYGON)\n\nNem todo fenômeno geográfico é contínuo ou contíguo. Pense no Japão, na Indonésia ou, em menor escala, em um município que possui ilhas. Embora existam múltiplos polígonos desconexos fisicamente (as ilhas), eles constituem um único objeto lógico no banco de dados. Isso significa que, na tabela de atributos, haverá apenas uma linha (um registro) representando o “Japão”, mas a coluna de geometria conterá um MULTIPOLYGON com centenas de partes. Isso é fundamental para manter a consistência estatística (ex: o PIB é do país inteiro, não de cada ilha separadamente).\n\nGeometrias com buracos\n\nA topologia correta exige rigor na definição de áreas vazias. Um lago dentro de uma ilha, por exemplo, não deve ser modelado como um polígono de água desenhado sobre o polígono de terra. Topologicamente, o lago é uma ausência de área (um buraco) dentro da ilha.\nMatematicamente, um Polígono é definido por:\n\nAnel Exterior (Exterior Ring): Define a fronteira externa.\n0 ou mais Anéis Interiores (Interior Rings): Definem os buracos. O cálculo da área geométrica é feito automaticamente subtraindo-se o interior do exterior.\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(sf, ggplot2, patchwork)\n\n#\np1 &lt;- rbind(c(0,0), c(2,0), c(2,2), c(0,2), c(0,0))\np2 &lt;- rbind(c(3,3), c(4,3), c(4,4), c(3,4), c(3,3))\n\nmulti_poly &lt;- st_multipolygon(list(list(p1), list(p2)))\n\ndf_multi &lt;- st_sf(geometry = st_sfc(multi_poly), \n                  id = 1, \n                  nome = \"País Arquipélago\")\n\nprint(df_multi)\n\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 4 ymax: 4\nCRS:           NA\n  id             nome                       geometry\n1  1 País Arquipélago MULTIPOLYGON (((0 0, 2 0, 2...\n\n\nCódigo\n#\nouter &lt;- rbind(c(0,0), c(5,0), c(5,5), c(0,5), c(0,0))\n# \nhole  &lt;- rbind(c(1,1), c(1,4), c(4,4), c(4,1), c(1,1))\n\npoly_hole &lt;- st_polygon(list(outer, hole))\ndf_hole &lt;- st_sf(geometry = st_sfc(poly_hole), \n                 id = 1, \n                 nome = \"Ilha com Lago\")\n\n\n#\ng1 &lt;- ggplot(df_multi) + \n  geom_sf(fill = \"orange\", color = \"black\") +\n  geom_sf_text(aes(label = id), nudge_y = 0.5, color=\"white\") +\n  ggtitle(\"MULTIPOLYGON\\n(1 Linha de dados, 2 Formas)\") + \n  theme_void() + \n  theme(plot.title = element_text(hjust = 0.5, size=11, face=\"bold\"))\n\ng1\n\n\n\n\n\n\n\n\nFigura 2.18: MULTIPOLYGON\n\n\n\n\n\n\n\nCódigo\ng2 &lt;- ggplot(df_hole) + \n  geom_sf(fill = \"skyblue\", color = \"blue\") +\n  ggtitle(\"POLYGON com Buraco\\n(O branco é 'vazio')\") + \n  theme_void() + \n  theme(plot.title = element_text(hjust = 0.5, size=11, face=\"bold\"))\n\nprint(df_hole);\n\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 5 ymax: 5\nCRS:           NA\n  id          nome                       geometry\n1  1 Ilha com Lago POLYGON ((0 0, 5 0, 5 5, 0 ...\n\n\nCódigo\ng2\n\n\n\n\n\n\n\n\nFigura 2.19: POLYGON\n\n\n\n\n\nModelo raster\nO modelo raster abandona a noção de objetos discretos em favor de uma representação baseada em campo (field-based). O espaço é particionado em uma grade regular (matriz) de células, conhecidas como pixels Figura 2.20.\n\nPixels e Resolução\n\n\nPixels: Cada célula da grade armazena um valor numérico único. Ao contrário do vetor, onde o espaço vazio não consome memória, no raster o vazio deve ser preenchido (com valores NoData ou zero), cobrindo toda a extensão.\nResolução: É determinada pelo tamanho da célula no terreno (ex: 30m x 30m). Há um trade-off constante: resoluções mais finas capturam mais detalhes, mas aumentam quadraticamente o tamanho do arquivo e o custo de processamento.\nMatrizes Espaciais: A geolocalização é implícita; baseia-se na posição da célula (linha/coluna) em relação a uma origem de coordenadas e ao tamanho do pixel, dispensando o armazenamento de coordenadas para cada ponto individualmente.\n\nSuperfícies contínuas e categóricas\nConte (2023) classifica a aplicação dos rasters em dois domínios:\n\nDados Contínuos: Representam variáveis que variam suavemente no espaço, como elevação (DEM), temperatura ou densidade populacional estimada. Os valores dos pixels são números reais (ponto flutuante).\nDados Categóricos: Representam classes discretas. Os pixels contêm números inteiros que funcionam como rótulos para uma tabela de atributos (ex: 1 = Floresta, 2 = Água, 3 = Urbano).\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(terra, ggplot2, tidyterra, patchwork)\n\n#\nr_base &lt;- rast(nrows = 10, ncols = 10, \n               xmin = 0, xmax = 10, ymin = 0, ymax = 10)\n\n#\nvalues(r_base) &lt;- 1:100 \nr_continuo &lt;- r_base\nnames(r_continuo) &lt;- \"Elevacao\"\n\n#\nset.seed(123)\nvalues(r_base) &lt;- sample(c(1, 2, 3), 100, replace = TRUE)\nr_categorico &lt;- as.factor(r_base) \nnames(r_categorico) &lt;- \"Uso_Solo\"\n\n#\ng1 &lt;- ggplot() +\n  geom_spatraster(data = r_continuo) +\n  scale_fill_viridis_c(option = \"B\", name = \"Altitude (m)\") +\n  ggtitle(\"Contínuo\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  coord_sf(expand = FALSE) \n\n#\ng2 &lt;- ggplot() +\n  geom_spatraster(data = r_categorico) +\n  scale_fill_manual(values = c(\"1\" = \"blue\", \"2\" = \"forestgreen\", \"3\" = \"sandybrown\"),\n                    labels = c(\"Água\", \"Floresta\", \"Solo\"),\n                    name = \"Classe\", \n                    na.value = \"transparent\") +\n  ggtitle(\"Categórico\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  coord_sf(expand = FALSE)\n\ng1 + g2\n\n\n\n\n\n\n\n\n\n\n\n(a) Raster Contínuo (Gradiente de Elevação)\n\n\n\n\n\n\nFigura 2.20: Tipos de Dados Raster\n\n\n\n\nFormatos de dados espaciais\nPara garantir a troca de informações entre diferentes softwares e usuários, utilizam-se padrões de arquivo específicos para cada modelo de dados.\n\nFormatos vetoriais\n\n\nShapefile (.shp): Desenvolvido pela ESRI, é historicamente o formato mais ubíquo em SIG. No entanto, é tecnicamente obsoleto e possui limitações severas (tamanho máximo de 2GB, nomes de colunas limitados a 10 caracteres).\n\n\n\n\n\n\n\nImportanteAlerta sobre Shapefiles\n\n\n\nUm erro extremamente comum é tratar o Shapefile como um arquivo único. Ele não é um arquivo único. O Shapefile é, na verdade, um pacote de arquivos que funcionam obrigatoriamente em conjunto. Para que o dado espacial funcione, você precisa ter, no mínimo, três arquivos na mesma pasta e com o mesmo nome:\n\n.shp: Contém a geometria (o desenho do mapa).\n.shx: Contém o índice posicional (para o software ler o desenho rápido).\n.dbf: Contém a tabela de atributos (os dados estatísticos).\n\nAo enviar um shapefile por e-mail ou mover de pasta, você deve mover todos esses arquivos juntos (geralmente zipando-os). Se faltar um deles (especialmente o .shx ou .dbf), o arquivo corrompe e não abre.\n\n\n\nGeoJSON (.json): Um formato baseado em texto (JSON) leve e legível por humanos. É o padrão da web moderna. A norma RFC 7946 impõe restrições estritas para garantir interoperabilidade: utiliza sempre o datum WGS 84 (coordenadas geográficas) e codificação de caracteres UTF-8.\nGeoPackage (.gpkg): A alternativa moderna e aberta ao Shapefile. É um arquivo único (baseado em banco de dados SQLite) que não sofre das limitações de tamanho ou truncamento de nomes de colunas, suportando tanto vetores quanto rasters.\nKML/KMZ: Formatos baseados em XML focados em visualização tridimensional no Google Earth, contendo informações de estilo e simbologia além dos dados.\n\nFormatos raster\n\nGeoTIFF (.tiff): O padrão da indústria para imagens de satélite e modelos de elevação. É um arquivo de imagem TIFF convencional que possui metadados geográficos (tags) embutidos no cabeçalho, permitindo que o software saiba exatamente onde a imagem se encaixa na Terra.\nNetCDF (.nc): O formato Network Common Data Form é o padrão em oceanografia e climatologia. Sua estrutura multidimensional permite armazenar “cubos de dados” (latitude, longitude, tempo, altitude), sendo ideal para séries temporais de dados climáticos.",
    "crumbs": [
      "Fundamentos da Estatística Espacial",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos da Estatística Espacial</span>"
    ]
  },
  {
    "objectID": "fundEstspatial.html#tipos-de-dados-espaciais",
    "href": "fundEstspatial.html#tipos-de-dados-espaciais",
    "title": "2  Fundamentos da Estatística Espacial",
    "section": "2.7 Tipos de dados espaciais",
    "text": "2.7 Tipos de dados espaciais\nConforme discutido anteriormente na seção Seção 2.1, a estatística espacial ocupa-se da análise de dados indexados espacialmente. Esta disciplina diverge da estatística clássica ao incorporar explicitamente a dependência espacial e distancia-se da análise espacial sensu stricto pelo tratamento formal da incerteza (Cressie e Moores 2022). Enquanto a análise espacial pode restringir-se a operações geométricas ou algorítmicas sobre informações geográficas, a estatística espacial fundamenta-se em um formalismo probabilístico, assumindo que a proximidade espacial implica maior dependência estatística entre observações (Tobler 1970).\nA modelagem dessa dependência é sistematizada pela natureza do domínio espacial \\(D\\) onde o processo estocástico {\\(Y(s) : s \\in D \\subset \\mathbb{R}^d\\)} ocorre. Cressie e Moores (2022) formalizam essa estrutura através de um modelo hierárquico de probabilidade conjunta, utilizando a notação de colchetes \\([\\cdot]\\) para densidades:\n\\[[Y, D] = [Y | D] [D] \\tag{2.5}\\]\nO componente \\([D]\\) modela a incerteza sobre onde as observações ocorrem (se as localizações são fixas ou aleatórias), enquanto \\([Y∣D]\\) descreve a variabilidade do atributo condicionada a essas posições. É a natureza desse conjunto \\(D\\), especificamente se ele é contínuo, discreto ou reticulado, que fundamenta a divisão clássica da estatística espacial em três categorias:\n\nGeoestatística (\\(Y(\\mathbf{s}) : \\mathbf {s} \\in D^G \\subset D\\)): O domínio \\(D^G\\) é fixo e contínuo, permitindo que o atributo seja, teoricamente, observado em qualquer ponto. O objetivo principal é a predição em locais não amostrados (Chen, Genton, e Sun 2021; Nhancololo et al. 2024).\nDados de Área (\\(Y(\\mathbf{s}) : \\mathbf{s} \\in D^L \\subset D\\)): O domínio \\(D^L\\) é fixo, mas discreto e contável, consistindo em unidades geográficas agregadas (como municípios ou pixels) onde a dependência é definida por estruturas de vizinhança (Cressie e Moores 2022).\nProcessos Pontuais (\\(Y(\\mathbf{s}): \\mathbf{s} \\in D^P \\subset D\\)): O domínio \\(D^P\\) é aleatório, sendo a própria localização dos eventos a variável de interesse (Nhancololo 2024; Møller e Waagepetersen 2007).\n\nEstes temas são aprofundados detalhadamente nos capítulos Capítulo 3 (Geoestatística), Capítulo 4 (Dados de Área) e Capítulo 5 (Processos Pontuais), respectivamente.\nQuando estas estruturas incorporam a dimensão temporal, o processo é expandido para \\(\\{Y(\\mathbf{s}, t) : \\mathbf{s} \\in D, t \\in \\mathcal{T}\\}\\), configurando os dados espaço-temporais (Chen, Genton, e Sun 2021). Ademais, em uma fronteira metodológica mais recente, os atributos podem ser tratados não como escalares, mas como funções completas análogo a séries temporais indexadas pelo espaço. Estes são os dados funcionais espaciais; caso incluam uma evolução dinâmica no tempo, denominam-se dados funcionais espaço-temporais. Embora estas extensões transcendam o escopo destas notas de aula, leitores interessados podem consultar contribuições fundamentais em Delicado et al. (2010), Moreno et al. (2023), Burbano-Moreno e Mayrink (2024), Mateu e Giraldo (2022), etc. Para os fundamentos teóricos da análise funcional per se, as obras de Ramsay e Silverman (2005) e Wang, Chiou, e Müller (2016) permanecem como referências.\n\n\n\n\n\n\nAvisoO mesmo conjunto de dados\n\n\n\nÉ fundamental compreender que a distinção entre Geoestatística, Dados de Área e Processos Pontuais refere-se à abordagem de modelagem escolhida para responder a uma pergunta científica, e não estritamente ao formato do arquivo de dados.\nUm mesmo conjunto de dados original, como locais de ocorrência de crimes (pontos com coordenadas \\(x,y\\)), pode transitar entre as três categorias:\n\nComo Processo Pontual se o objetivo é entender se a localização dos crimes é aleatória ou se agrupam (o foco está na coordenada \\(\\mathbf{s}\\)).\nComo Dados de Área se contarmos quantos crimes ocorreram dentro de cada bairro e relacionarmos isso com a renda média do bairro (o foco está na agregação em \\(D^L\\)).\nComo Geoestatística se tratarmos a densidade de crimes como uma superfície contínua de risco e tentarmos interpolar esse risco para locais onde não houve medição (o foco está na predição em \\(D^G\\)).\n\nPortanto, olhe para o seu problema e pergunte qual estrutura estocástica melhor representa o fenômeno que você deseja investigar.\n\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(sf, ggplot2, patchwork, dplyr, viridis, gstat, stars)\n\n#\nset.seed(42)\ncidade &lt;- st_polygon(list(rbind(c(0,0), c(10,0), c(10,10), c(0,10), c(0,0)))) %&gt;% \n  st_sfc() %&gt;% st_sf()\n\nn_pontos &lt;- 60 \npontos &lt;- st_sample(cidade, size = n_pontos, type = \"random\") \n\n#\npontos_sf &lt;- st_sf(geometry = pontos) %&gt;% \n  mutate(x = st_coordinates(.)[,1], y = st_coordinates(.)[,2]) %&gt;% \n  filter(x &lt; 6 | y &gt; 6) %&gt;% \n  mutate(z_valor = (x * 2 + y * 3) + rnorm(n(), 0, 5))\n\n#\nlegenda_continua &lt;- guides(fill = guide_colorbar(\n  title = NULL,\n  barwidth = unit(0.29, \"npc\"), \n  barheight = unit(0.3, \"cm\"),\n  label.position = \"bottom\"\n))\n\n#\nlegenda_discreta &lt;- guides(fill = guide_legend(\n  title = NULL,\n  label.position = \"bottom\",\n  direction = \"horizontal\",\n  keywidth = unit(.63, \"cm\"),\n  keyheight = unit(.3, \"cm\"),\n  nrow = 1\n))\n\n#\ntema_base &lt;- theme_void() + \n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(size=9, hjust = 0.5),\n    legend.margin = margin(t = 0, r = 0, b = 0, l = 0)\n  )\n\n#\ng_pp &lt;- ggplot(pontos_sf) +\n  geom_sf(col = \"red\", size = 2, alpha=0.7) +\n  geom_sf(data = cidade, fill = NA, col = \"black\") +\n  tema_base +\n  ggtitle(\"Processo Pontual\\n(O foco é a localização exata do evento)\")\n\n# Dados de Área (Discreto)\ngrid_area &lt;- st_make_grid(cidade, n = c(4,4)) %&gt;% st_sf()\ninterseccao &lt;- st_intersects(grid_area, pontos_sf)\ngrid_area$contagem &lt;- lengths(interseccao)\n\ng_area &lt;- ggplot(grid_area) +\n  geom_sf(aes(fill = factor(contagem)), col = \"white\") +\n  scale_fill_viridis_d(option = \"mako\") + \n  geom_sf(data=pontos_sf, size=0.5, col=\"red\", alpha=0.3) + \n  tema_base +\n  legenda_discreta + \n  ggtitle(\"Dados de Área\\n(O foco é o nr de eventos por pixel)\")\n\n# C. Geoestatística (Contínuo)\ngrid_pred &lt;- st_as_stars(st_bbox(cidade), dx = 0.1, dy = 0.1)\ninvisible(capture.output(interpola &lt;- gstat::idw(z_valor ~ 1, locations = pontos_sf, newdata = grid_pred, idp = 2.0)))\n\ng_geo &lt;- ggplot() +\n  geom_stars(data = interpola, aes(fill = var1.pred)) +\n  scale_fill_viridis_c(option = \"inferno\", na.value = \"white\") + \n  geom_sf(data = cidade, fill = NA, col = \"black\") + \n  tema_base +\n  legenda_continua +\n  ggtitle(\"Geoestatística\\n(O foco é a predição dos\\n eventos onde não existem)\")\n\ng_pp + g_area + g_geo\n\n\n\n\n\n\n\n\nFigura 2.21: O mesmo dado (pontos), três abordagens de modelagem.",
    "crumbs": [
      "Fundamentos da Estatística Espacial",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos da Estatística Espacial</span>"
    ]
  },
  {
    "objectID": "fundEstspatial.html#manuseamento-de-dados-espaciais",
    "href": "fundEstspatial.html#manuseamento-de-dados-espaciais",
    "title": "2  Fundamentos da Estatística Espacial",
    "section": "2.8 Manuseamento de dados espaciais:",
    "text": "2.8 Manuseamento de dados espaciais:\n\n2.8.1 O pacote sf\nO pacote sf (Simple Features for R) Pebesma (2018) representa o padrão moderno para a manipulação de dados espaciais no R. Desenvolvido pelo professor Edzer Pebesma, ele substituiu os antigos pacotes sp, rgdal e rgeos, unificando as funcionalidades de leitura, projeção e operações geométricas numa estrutura compatível com o Tidyverse.\nO sf depende de bibliotecas externas de sistema (C++) poderosas: GDAL (leitura/escrita de arquivos), GEOS (geometria plana) e PROJ (projeções cartográficas). Ao instalar no R, ele compila ou baixa estas dependências.\nA grande revolução do sf é tratar os dados espaciais como data frames (tabelas) comuns. Enquanto no antigo sp os dados eram objetos complexos e opacos, no sf a geometria é apenas uma coluna extra (geralmente chamada geometry ou geom) numa tabela de dados. Isso permite que utilizemos funções do pacote dplyr (select, filter, mutate) diretamente no mapa.\n\nInstalação e Carregamento, ver Seção 1.6\n\n\n\nCódigo\nif (!require (\"pacman\")) install.packages(\"pacman\")\np_load(sf, tidyverse)\n\n\nA estrutura é composta por três níveis hierárquicos:\n\nsfg (Simple Feature Geometry): É a geometria de uma única feição (ex: um único polígono representando um lago).\nsfc (Simple Feature Column): É uma lista que contém todas as geometrias (sfg) de todas as linhas da tabela, além de metadados cruciais como o sistema de coordenadas (CRS) e a caixa delimitadora (Bbox).\nsf (Simple Feature): É a tabela completa, combinando a coluna sfc com os atributos (dados estatísticos como nome, população, etc.).\n\nCriação de Geometrias (st_)\nPodemos criar geometrias do zero usando coordenadas numéricas.\n\nCriar pontos (st_point)\n\n\n\nCódigo\nponto &lt;- st_point(c(2, 2)) # X=2, Y=2\nclass(ponto) # Retorna \"XY\", \"POINT\", \"sfg\"\n\n\n[1] \"XY\"    \"POINT\" \"sfg\"  \n\n\n\nCriar Linhas (st_linestring)\n\n\n\nCódigo\n# Matriz de coordenadas\nmatriz_linha &lt;- rbind(c(1, 1), c(3, 3), c(4, 1))\nlinha &lt;- st_linestring(matriz_linha)\nclass(linha)\n\n\n[1] \"XY\"         \"LINESTRING\" \"sfg\"       \n\n\n\nCriar polígonos (st_polygon)\n\n\n\nCódigo\n# Anel externo (Triângulo)\nmatriz_poly &lt;- rbind(c(1, 1), c(4, 1), c(4, 4), c(1, 1))\npoligono &lt;- st_polygon(list(matriz_poly))\nclass(poligono)\n\n\n[1] \"XY\"      \"POLYGON\" \"sfg\"    \n\n\n\nCriar o objeto final (st_sfc e st_sf)\n\nPara transformar essas geometrias soltas num objeto espacial analisável, agrupamo-las.\n\n\n\n\n\n\nDicaQuando usar st_sfc vs. st_sf?\n\n\n\nA distinção entre estas duas funções é fundamental para entender a estrutura do pacote:\n\nUse st_sfc() (Simple Feature Column): Quando você tem a lista de geometrias e precisa definir o Sistema de Coordenadas (CRS). Pense nela como a função que cria a coluna geométrica. Ela não aceita dados estatísticos (nomes, valores), apenas formas e coordenadas.\nUse st_sf() (Simple Feature): Quando você quer criar o objeto final para análise. Ela age como um data.frame ou tibble, colando a coluna geométrica (sfc) aos seus dados de atributos (IDs, Variáveis, etc.). É com este objeto que você fará gráficos e estatísticas.\n\n\n\n\n\nCódigo\n# Cria a COLUNA de geometria (sfc) \n# É aqui que definimos o CRS (ex: 4326 para GPS/WGS84)\ncoluna_geo &lt;- st_sfc(ponto, linha, poligono, crs = 4326)\n\n#Cria o DATA FRAME espacial (sf) \n# Aqui unimos os dados (atributos) com a coluna criada acima\nmeu_mapa &lt;- st_sf(\n  id = 1:3,\n  nome = c(\"Ponto A\", \"Estrada B\", \"Terreno C\"),\n  geometry = coluna_geo\n)\n\nprint(meu_mapa)\n\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 1 ymin: 1 xmax: 4 ymax: 4\nGeodetic CRS:  WGS 84\n  id      nome                       geometry\n1  1   Ponto A                    POINT (2 2)\n2  2 Estrada B     LINESTRING (1 1, 3 3, 4 1)\n3  3 Terreno C POLYGON ((1 1, 4 1, 4 4, 1 1))\n\n\nAo imprimir o objeto meu_mapa, o cabeçalho (header) fornece metadados vitais:\n\nGeometry type: O tipo geométrico predominante.\nDimension: XY (2D).\nBbox: A caixa delimitadora (limites geográficos mínimos e máximos).\nCRS: O Sistema de Referência de Coordenadas (EPSG: 4326).\n\nEntrada e Saída (I/O)\nO sf utiliza a biblioteca GDAL, permitindo ler e escrever dezenas de formatos de arquivos (Shapefile, GeoJSON, KML, GPKG, PostGIS, etc.).\n\nLeitura (st_read e read_sf)\n\n\nst_read(dsn, layer, ...): Função base R. Imprime um relatório detalhado sobre o ficheiro ao carregar.\nread_sf(dsn, ...): Versão “tidy”. É silenciosa e retorna um objeto do tipo tibble (tabela moderna), facilitando a visualização no console.\n\nVamos carregar o mapa da Carolina do Norte (nc), incluído no pacote:\n\n\nCódigo\np_load(tidyverse)\n# system.file encontra o caminho do ficheiro no seu computador\narquivo &lt;- system.file(\"shape/nc.shp\", package=\"sf\")\n\npaste(\"O caminho do meu arquivo é:\", arquivo)\n\n\n[1] \"O caminho do meu arquivo é: /home/almonha/R/x86_64-pc-linux-gnu-library/4.5/sf/shape/nc.shp\"\n\n\nCódigo\nnc &lt;- read_sf(arquivo) # Aqui vc poderia usar o caminho onde está seu arquivo, exemplo:\n                       # nc &lt;- read_sf(\"/home/almonha/R/x86_64-pc-linux-gnu-library/4.5/sf/shape/nc.shp\")\n                       # onde nc.shp é o ficheiro de interesse\nglimpse(nc) # Visualiza a estrutura\n\n\nRows: 100\nColumns: 15\n$ AREA      &lt;dbl&gt; 0.114, 0.061, 0.143, 0.070, 0.153, 0.097, 0.062, 0.091, 0.11…\n$ PERIMETER &lt;dbl&gt; 1.442, 1.231, 1.630, 2.968, 2.206, 1.670, 1.547, 1.284, 1.42…\n$ CNTY_     &lt;dbl&gt; 1825, 1827, 1828, 1831, 1832, 1833, 1834, 1835, 1836, 1837, …\n$ CNTY_ID   &lt;dbl&gt; 1825, 1827, 1828, 1831, 1832, 1833, 1834, 1835, 1836, 1837, …\n$ NAME      &lt;chr&gt; \"Ashe\", \"Alleghany\", \"Surry\", \"Currituck\", \"Northampton\", \"H…\n$ FIPS      &lt;chr&gt; \"37009\", \"37005\", \"37171\", \"37053\", \"37131\", \"37091\", \"37029…\n$ FIPSNO    &lt;dbl&gt; 37009, 37005, 37171, 37053, 37131, 37091, 37029, 37073, 3718…\n$ CRESS_ID  &lt;int&gt; 5, 3, 86, 27, 66, 46, 15, 37, 93, 85, 17, 79, 39, 73, 91, 42…\n$ BIR74     &lt;dbl&gt; 1091, 487, 3188, 508, 1421, 1452, 286, 420, 968, 1612, 1035,…\n$ SID74     &lt;dbl&gt; 1, 0, 5, 1, 9, 7, 0, 0, 4, 1, 2, 16, 4, 4, 4, 18, 3, 4, 1, 1…\n$ NWBIR74   &lt;dbl&gt; 10, 10, 208, 123, 1066, 954, 115, 254, 748, 160, 550, 1243, …\n$ BIR79     &lt;dbl&gt; 1364, 542, 3616, 830, 1606, 1838, 350, 594, 1190, 2038, 1253…\n$ SID79     &lt;dbl&gt; 0, 3, 6, 2, 3, 5, 2, 2, 2, 5, 2, 5, 4, 4, 6, 17, 4, 7, 1, 0,…\n$ NWBIR79   &lt;dbl&gt; 19, 12, 260, 145, 1197, 1237, 139, 371, 844, 176, 597, 1369,…\n$ geometry  &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-81.47276 3..., MULTIPOLYGON ((…\n\n\nSalvar arquivo sf (st_write e write_sf)\nPara exportar o resultado das suas análises:\n\nst_write(obj, \"arquivo.ext\", delete_layer = TRUE): Exporta o objeto. O argumento delete_layer permite sobrescrever arquivos existentes.\nO formato é determinado automaticamente pela extensão do arquivo (.shp, .json, .gpkg).\n\n\n\nCódigo\n# Guardar como GeoPackage\nwrite_sf(nc, \"mapa_final.gpkg\")\n\n# Guardar como Shapefile\nwrite_sf(nc, \"mapa_final.shp\")\n\n\n\n\n\n\n\n\nNotaOutas formas de salvar\n\n\n\nConsulte Seção 1.8 para ver as outras formas/extensões de salvar\n\n\nSistemas de Coordenadas (CRS)\nA gestão do CRS é a causa número um de erros em análise espacial. O sf oferece duas funções principais para lidar com isto.\n\nIdentificação (st_crs)\n\nConsulta o sistema de coordenadas associado ao objeto. Retorna o código EPSG e a definição WKT.\n\n\nCódigo\nst_crs(nc) # Retorna \"NAD27\" (EPSG 4267)\n\n\nCoordinate Reference System:\n  User input: NAD27 \n  wkt:\nGEOGCRS[\"NAD27\",\n    DATUM[\"North American Datum 1927\",\n        ELLIPSOID[\"Clarke 1866\",6378206.4,294.978698213898,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4267]]\n\n\n\nTransformação (st_transform)\n\nReprojeta (converte matematicamente) as coordenadas de um sistema para outro caso necessário.\nExemplo: O mapa nc está em NAD27 (unidade: graus/pés). Queremos calcular distâncias em metros. Devemos transformar para um sistema projetado, como UTM (Zona 17N - EPSG 32617).\n\n\nCódigo\n# Transformar para UTM (Metros)\nnc_utm &lt;- st_transform(nc, crs = 32617)\nst_crs(nc_utm)\n\n\nCoordinate Reference System:\n  User input: EPSG:32617 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 17N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 17N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-81,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 84°W and 78°W, northern hemisphere between equator and 84°N, onshore and offshore. Bahamas. Ecuador - north of equator. Canada - Nunavut; Ontario; Quebec. Cayman Islands. Colombia. Costa Rica. Cuba. Jamaica. Nicaragua. Panama. United States (USA).\"],\n        BBOX[0,-84,84,-78]],\n    ID[\"EPSG\",32617]]\n\n\n\n\nCódigo\n# Transformar para WGS84 (GPS - Lat/Long)\nnc_gps &lt;- st_transform(nc, crs = 4326)\nst_crs(nc_gps)\n\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\n\n\n\nImportantest_crs vs st_transform\n\n\n\n\nUse st_crs(x) &lt;- valor apenas se os dados não tiverem CRS definido ou estiverem errados (isso apenas coloca um rótulo).\nUse st_transform(x, valor) se os dados tiverem CRS e preterar-o (isso recalcula as coordenadas X e Y).\n\n\n\nConfirmação Geométrica\nEstas funções respondem a perguntas de Verdadeiro/Falso sobre a relação espacial entre duas geometrias (X e Y). Elas retornam, por padrão, uma matriz esparsa (lista de índices) indicando quais elementos de X se relacionam com quais de Y.\n\nst_intersects(x, y): O mais comum. Retorna TRUE se X e Y partilham qualquer ponto no espaço (se tocam, cruzam ou sobrepõem).\nst_disjoint(x, y): O oposto de intersects. Retorna TRUE se X e Y estão completamente separados.\nst_contains(x, y): Retorna TRUE se nenhum ponto de Y está fora de X e pelo menos um ponto do interior de Y está dentro de X (Y está dentro de X).\nst_within(x, y): O inverso de contains. X está dentro de Y.\nst_touches(x, y): Retorna TRUE se as geometrias se tocam apenas na borda/fronteira, mas os seus interiores não se sobrepõem (ex: países vizinhos).\nst_crosses(x, y): Retorna TRUE se as geometrias se cruzam (comum entre linhas ou linha/polígono), resultando numa geometria de dimensão inferior à máxima das duas.\nst_overlaps(x, y): Retorna TRUE se duas geometrias da mesma dimensão partilham espaço, mas uma não contém a outra completamente.\nst_equals(x, y): Retorna TRUE se as geometrias são topologicamente idênticas (mesma forma e localização), independente da ordem dos vértices.\n\n\n\nCódigo\nponto_teste &lt;- st_point(c(-81.5, 36.4)) |&gt; st_sfc(crs = 4267)\n\n#\nresultado &lt;- st_intersects(nc, ponto_teste, sparse = FALSE)\n\n# Qual o nome do condado onde o ponto caiu?\nnc$NAME[resultado[,1]]\n\n\n[1] \"Ashe\"\n\n\nOperações Geométricas\nEstas funções criam novas geometrias a partir de transformações nas existentes.\n\nUnárias (Transformam uma geometria nela mesma)\n\n\nst_buffer(x, dist): Cria um polígono que representa a zona de influência de raio dist ao redor da geometria. Fundamental para análises de proximidade.\nst_centroid(x): Reduz a geometria ao seu centro de massa geométrico (ponto).\nst_boundary(x): Extrai apenas a linha de fronteira (contorno) de um polígono.\nst_convex_hull(x): Cria o menor polígono convexo que envolve todos os pontos da geometria (como um elástico esticado ao redor de pregos).\nst_simplify(x, dTolerance): Reduz o número de vértices de linhas/polígonos complexos, mantendo a forma geral (algoritmo Douglas-Peucker). Útil para tornar mapas leves para web.\nst_segmentize(x, dfMaxLength): Adiciona pontos a linhas longas para garantir que elas sigam a curvatura da terra ao serem reprojetadas.\n\n\n\nCódigo\nnc_utm &lt;- st_transform(nc, 32617) #Transformar para metros (UTM)\ncentroides &lt;- st_centroid(nc_utm) #Calcular Centroide\nbuffers &lt;- st_buffer(centroides, dist = 10000) # Criar Buffer 10000 metros\n\nggplot() +\n  geom_sf(data = nc_utm, fill = \"white\") +\n    geom_sf(data = centroides, fill = \"black\", alpha = 0.3) + #adiciona o centro de cada poligono\n  geom_sf(data = buffers, fill = \"red\", alpha = 0.3) + # Faz um Buffer 10000 metros ao redor de cada centro\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nBinárias (Combinam duas geometrias)\n\nEstas são operações de conjuntos:\n\nst_intersection(x, y): Retorna a área comum entre X e Y. Preserva os atributos de ambos.\nst_difference(x, y): Retorna a parte de X que não está em Y.\nst_union(x, y): Quando fornecidos dois objetos, a função funde as duas camadas numa única, mantendo todas as geometrias (a soma de x e y).\nst_union(x): Quando fornecido apenas um objeto, a função dissolve as fronteiras internas entre as polígonos adjacentes, agregando-os numa única geometria (Multi-Polígono).\nst_sym_difference(x, y): (XOR) Retorna as áreas que estão em X ou em Y, mas não em ambos (o oposto da intersecção).\n\n\n\nCódigo\n# Criar dois círculos sobrepostos\nc1 &lt;- st_point(c(0, 0)) |&gt; st_buffer(1) |&gt; st_sfc() |&gt; st_sf(id = \"A\")\nc2 &lt;- st_point(c(1, 0)) |&gt; st_buffer(1) |&gt; st_sfc() |&gt; st_sf(id = \"B\")\n\nbase_plot &lt;- ggplot() +\n  geom_sf(data = c1, fill = \"red\", alpha = 0.5) +\n  geom_sf(data = c2, fill = \"blue\", alpha = 0.5) +\n  geom_sf_text(data = c1, aes(label = \"A\"), nudge_x = -0.3) +\n  geom_sf_text(data = c2, aes(label = \"B\"), nudge_x = 0.3) +\n  theme_void() +\n  ggtitle(\"Estado Inicial (A e B)\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# INTERSECTION \ninter &lt;- st_intersection(c1, c2)\np1 &lt;- ggplot(inter) + geom_sf(fill = \"purple\", alpha=0.8) + \n      ggtitle(\"st_intersection(A, B)\") + theme_void()+\n  theme(plot.title = element_text(hjust = 0.5))\n\n#DIFFERENCE\ndif &lt;- st_difference(c1, c2)\np2 &lt;- ggplot(dif) + geom_sf(fill = \"red\", alpha=0.8) + \n      ggtitle(\"st_difference(A, B)\") + theme_void()+\n  theme(plot.title = element_text(hjust = 0.5))\n\n#UNION \nuni &lt;- st_union(c1, c2)\np3 &lt;- ggplot(uni) + geom_sf(fill = \"darkgreen\", alpha=0.8) + \n      ggtitle(\"st_union(A, B)\") + theme_void()+\n  theme(plot.title = element_text(hjust = 0.5))\n\n#SYM_DIFFERENCE\nsym &lt;- st_sym_difference(c1, c2)\np4 &lt;- ggplot(sym) + geom_sf(fill = \"orange\", alpha=0.8) + \n      ggtitle(\"st_sym_difference(A, B)\") + theme_void()+\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n(base_plot+p1 + p3) / (p2 + p4)\n\n\n\n\n\n\n\n\nFigura 2.22: Operações Geométricas Binárias\n\n\n\n\n\nMedidas Geométricas\nCalculam propriedades métricas das feições. O sf integra-se com o pacote units, retornando valores com unidades físicas explícitas (m, km, ha).\n\nst_area(x): Calcula a área de polígonos.\nst_length(x): Calcula o comprimento de linhas ou perímetros de polígonos.\nst_distance(x, y): Calcula a distância euclidiana mais curta entre dois conjuntos de geometrias (retorna uma matriz de distâncias).\n\n\n\nCódigo\n# Área\narea_sqm &lt;- st_area(nc_utm[1:5, ])\nprint(area_sqm) # Em metros quadrados\n\n\nUnits: [m^2]\n[1] 1136550961  610599191 1422398043  697400554 1523431642\n\n\nCódigo\n# Converter para km^2 usando pacote units\nlibrary(units)\narea_km2 &lt;- set_units(area_sqm, km^2)\nprint(area_km2)\n\n\nUnits: [km^2]\n[1] 1136.5510  610.5992 1422.3980  697.4006 1523.4316\n\n\nManipulação de Dados\nO sf permite usar a gramática do pacote dplyr para manipular a tabela de atributos sem perder a geometria (ver Seção 1.9).\n\nfilter(): Seleciona linhas (regiões) baseadas em critérios lógicos.\nselect(): Seleciona colunas. Nota: A coluna de geometria é “pegajosa” e permanece mesmo que não seja selecionada explicitamente.\nmutate(): Cria novas colunas (ex: calcular densidade demográfica).\nst_drop_geometry(): Remove a parte espacial e retorna um data.frame puro. Essencial para análises estatísticas convencionais ou exportação para Excel.\n\nJunções Espaciais (st_join)\nEsta é uma das funções mais poderosas do geoprocessamento no R. Enquanto o left_join (Seção 1.9) une duas tabelas baseando-se numa chave comum (como uma coluna de ID ou CPF), o st_join une duas tabelas baseando-se na localização espacial (geometria).\n\nSintaxe e Argumentos Principais\n\nst_join(x, y, join = st_intersects, ..., left = TRUE, largest = FALSE)\n\nx (Alvo): O objeto sf que receberá os dados. A geometria resultante será sempre a geometria de x.\ny (Fonte): O objeto sf de onde vêm os novos atributos.\njoin: A regra geométrica a usar.\n\n\nst_intersects (Padrão): Une se tocarem de qualquer forma.\nst_within: Une apenas se x estiver totalmente dentro de y.\nst_contains: Une apenas se x contiver y.\nst_is_within_distance: Une se estiverem a uma certa distância (requer argumento dist).\nleft (Tipo de Junção):\nTRUE (Padrão): Realiza um Left Join. Mantém todas as linhas de x. Se não houver correspondência espacial em y, as colunas novas vêm preenchidas com NA.\nFALSE: Realiza um Inner Join```. Mantém apenas as linhas dexque realmente intersectam comy`, descartando o resto.\n\n*suffix: Se houver colunas com nomes iguais em ambas as tabelas (ex: nome), define o sufixo para desambiguar (ex: nome.x, nome.y).\n\n\n\n\n\n\nImportanteAtenção à duplicação de registos\n\n\n\nDiferente de um left_join tradicional onde esperamos uma relação 1:1, no espaço é comum haver sobreposições. Se um ponto x cair exatamente na fronteira entre dois polígonos y (ou numa área onde dois polígonos se sobrepõem), o st_join duplicará o ponto x. O resultado terá duas linhas para aquele ponto: uma com os dados do polígono A e outra com os dados do polígono B.\nSempre verifique o número de linhas (nrow) antes e depois do st_join. Se aumentou, houve duplicatas espaciais.\n\n\n\nJunção por Distância (st_is_within_distance)\n\nÀs vezes, os objetos não se tocam, mas queremos unir pela proximidade (ex: atribuir a uma casa os dados da estação de metro num raio de 500m). Para isso, alteramos o argumento join e fornecemos a distância.\n\n\nCódigo\np_load(sf, dplyr)\n\np1 &lt;- st_polygon(list(rbind(c(0,0), c(2,0), c(2,2), c(0,2), c(0,0))))\np2 &lt;- st_polygon(list(rbind(c(2,0), c(4,0), c(4,2), c(2,2), c(2,0))))\nparques &lt;- st_sf(nome_parque = c(\"Parque A\", \"Parque B\"), \n                 gestor = c(\"Prefeitura\", \"Estado\"),\n                 geometry = st_sfc(p1, p2))\n\n#\nset.seed(123)\narvores &lt;- st_as_sf(data.frame(\n  id_arvore = 1:5,\n  x = runif(5, 0, 5), \n  y = runif(5, 0, 3)\n), coords = c(\"x\", \"y\"))\n\n#\nresultado &lt;- st_join(arvores, parques) # usou por padrão `st_intersects`\n\nprint(resultado)\n\n\nSimple feature collection with 5 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1.437888 ymin: 0.1366695 xmax: 4.702336 ymax: 2.677257\nCRS:           NA\n  id_arvore nome_parque     gestor                   geometry\n1         1    Parque A Prefeitura POINT (1.437888 0.1366695)\n2         2    Parque B     Estado  POINT (3.941526 1.584316)\n3         3        &lt;NA&gt;       &lt;NA&gt;  POINT (2.044885 2.677257)\n4         4        &lt;NA&gt;       &lt;NA&gt;  POINT (4.415087 1.654305)\n5         5        &lt;NA&gt;       &lt;NA&gt;  POINT (4.702336 1.369844)\n\n\nJunção pelo Vizinho Mais Próximo\nO st_join padrão não resolve a pergunta “Qual é a farmácia mais próxima desta casa?”, ele apenas resolve “Esta casa toca numa farmácia?”. Para encontrar o vizinho mais próximo (Nearest Neighbor) e trazer seus dados, usamos uma combinação especial:\n# Une x ao y baseando-se em quem está mais perto (mesmo que longe)\nst_join(x, y, join = st_nearest_feature)\nVisualização (ggplot2)\nO sf fornece a geometria geom_sf(), que simplifica a cartografia no R.\n\n\nCódigo\nggplot() +\n  # Camada 1: Mapa base\n  geom_sf(data = nc, aes(fill = BIR74), color = \"white\", size=0.2) +\n  # Escala de cores\n  scale_fill_viridis_c(name = \"Nascimentos (1974)\") +\n  # Camada 2: Centroides\n  geom_sf(data = st_centroid(nc), size = 1, color = \"black\") +\n  # Controlo de Coordenadas e Zoom\n  coord_sf(datum = st_crs(4326)) + # Força grid em Lat/Long\n  theme_minimal() +\n  labs(title = \"Nascimentos na Carolina do Norte\")\n\n\n\n\n\n\n\n\n\nOutras Funções Úteis\n\nst_bbox(x): Retorna a caixa delimitadora (xmin, ymin, xmax, ymax) do seu shapfile. Útil para definir limites de gráficos (xlim, ylim).\nst_make_grid(x, cellsize): Cria uma grade retangular ou hexagonal cobrindo a área de X. Base para análises raster ou amostragem sistemática.\nst_cast(x, to): Converte tipos de geometria.\n\n\nEx: st_cast(poligono, \"LINESTRING\") converte a borda do polígono em linha.\nEx: st_cast(multipoligono, \"POLYGON\") converte multiplos polígonos em polígonos individuais (aumenta o número de linhas).\n\n\nst_make_valid(x): Corrige erros topológicos (ex: polígonos com laços, auto-intersecções) que frequentemente causam erros em operações como st_intersection.\n\n\n\nCódigo\n# Grid Hexagonal sobre a Carolina do Norte\ngrid_hex &lt;- st_make_grid(nc, n = c(20, 20), square = FALSE) # square=FALSE faz hexágonos\n\nggplot() +\n  geom_sf(data = nc, fill = NA, color = \"blue\") +\n  geom_sf(data = grid_hex, fill = NA, color = \"gray\") +\n  theme_void()",
    "crumbs": [
      "Fundamentos da Estatística Espacial",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos da Estatística Espacial</span>"
    ]
  },
  {
    "objectID": "fundEstspatial.html#pacote-geobr",
    "href": "fundEstspatial.html#pacote-geobr",
    "title": "2  Fundamentos da Estatística Espacial",
    "section": "2.9 Pacote geobr",
    "text": "2.9 Pacote geobr\nEnquanto o pacote sf fornece as ferramentas para manipular geometrias, o pacote geobr Pereira e Goncalves (2024) fornece os dados. Desenvolvido por uma equipe liderada por pesquisadores do Ipea (Rafael H. M. Pereira), o geobr é a biblioteca mais robusta para baixar bases de dados espaciais oficiais do Brasil.\nO seu grande diferencial é resolver os maiores problemas de quem trabalha com dados do IBGE:\n\nTodos os dados são baixados já projetados no CRS oficial (SIRGAS 2000 - EPSG 4674).\nOs dados vêm diretamente como objetos sf (Simple Features), prontos para análise no R.\nResolve automaticamente problemas de geometrias inválidas que frequentemente aparecem em shapefiles brutos baixados manualmente.\nPermite baixar malhas territoriais de anos anteriores (ex: municípios como existiam em 1872, 1991, 2010, etc.), essencial para análises temporais consistentes.\n\n\nInstalação e Carregamento\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(geobr, sf, ggplot2, dplyr)\n\n\n\nArgumentos Comuns\n\n\nyear: Define o ano de referência da malha. Se omitido, geralmente baixa o último disponível. Ex: year = 2010 (Censo), year = 2020.\nsimplified: Um argumento lógico (TRUE ou FALSE):\nTRUE (Padrão): Retorna uma geometria simplificada (menos vértices). É muito mais leve e rápido para carregar e plotar, ideal para visualização em mapas nacionais ou regionais. A simplificação usa o st_simplify preservando a topologia.\nFALSE: Retorna a geometria original com resolução máxima. Obrigatório se você for fazer cálculos de precisão (áreas, perímetros) ou análises de fronteira muito detalhadas.\nshowProgress: Mostra a barra de progresso do download (TRUE/FALSE).\ncache: Se TRUE (padrão), salva o arquivo numa pasta temporária do seu computador. Se você rodar o comando novamente na mesma sessão, ele lê do disco em vez de baixar da internet novamente, economizando tempo.\n\n\nDivisões Administrativas\n\n\nEstados (read_state)\n\nBaixa a geometria das Unidades da Federação.\n\ncode_state: Pode ser o código numérico (ex: 33 para RJ), a sigla (\"RJ\") ou \"all\" para baixar o Brasil inteiro.\n\n\n\nCódigo\np_load(geobr)\n# Ler todos os estados em 2010 (Ano de Censo)\nestados &lt;- read_state(code_state = \"all\", year = 2010, showProgress = FALSE)\n\n# Ler apenas São Paulo (código \"SP\")\nSP &lt;- read_state(code_state = \"SP\", year = 2010, showProgress = FALSE)\n\nggplot() +\n  geom_sf(data = estados, fill = \"white\", color = \"gray50\") +\n  geom_sf(data = SP, fill = \"orange\", color = \"black\") +\n  theme_minimal() + # vc pode trocar aqui pelo que deseja\n  labs(title = \"Mapa do Brasil com destaque para o São Paulo\")\n\n\n\n\n\n\n\n\n\nRecortes Regionais Específicos\nO IBGE e outros órgãos definem regiões que não seguem necessariamente limites estaduais clássicos. O geobr facilita o acesso a esses recortes oficiais.\n\nSemiárido Brasileiro (read_semiarid)\n\nEsta função baixa a delimitação oficial do Semiárido Brasileiro. Este conjunto de dados é fundamental para estudos de seca, desenvolvimento regional e políticas públicas.\n\n\nCódigo\nsemiarido &lt;- read_semiarid(year = 2017, showProgress = FALSE)\n\nggplot() +\n  geom_sf(data = estados, fill = \"gray95\", color = \"gray\") +\n  geom_sf(data = semiarido, fill = \"red\", alpha = 0.4, color = NA) +\n  theme_void() +\n  labs(title = \"Delimitação Oficial do Semiárido (2017)\")\n\n\n\n\n\n\n\n\n\nDinâmica Urbana\nPara estudos de urbanismo e morfologia das cidades, o pacote oferece dados que vão além do limite municipal administrativo.\n\nMancha Urbanizada (read_urban_area)\n\nTraz as áreas efetivamente urbanizadas, baseada em imagens de satélite e classificação do IBGE.\n\n\nCódigo\nurbano_rj &lt;- read_urban_area(year = 2015, code_state = \"RJ\", simplified = FALSE, showProgress = FALSE)\nestado_rj &lt;- read_state(code_state = \"RJ\", showProgress = FALSE)\n\nggplot() +\n  geom_sf(data = estado_rj, fill = \"white\") +\n  geom_sf(data = urbano_rj, fill = \"darkblue\", color = NA) +\n  theme_void() +\n  labs(title = \"Manchas Urbanizadas no RJ (2015)\")\n\n\n\n\n\n\n\n\n\n\nConcentrações Urbanas (read_urban_concentrations)\n\nFoca em Arranjos Populacionais. Identifica agrupamentos de municípios com forte integração (deslocamento para trabalho/estudo), definindo grandes cidades que ultrapassam fronteiras municipais isoladas. É essencial para entender metropolização.\n\n\nCódigo\nconcentracoes &lt;- read_urban_concentrations(year = 2015)\n\n\nDados Censitários e Estatísticos\nPara quem trabalha com dados do Censo Demográfico em escalas inframunicipais (dentro da cidade).\n\nÁreas de Ponderação (read_weighting_area)\n\nAs Áreas de Ponderação são a menor unidade geográfica para a qual o IBGE divulga os dados da amostra do Censo (o questionário completo). São agregados de setores censitários.\n\ncode_weighting: Você pode passar o código do município (para baixar todas as áreas daquela cidade) ou do estado.\n\n\n\nCódigo\nap_sobral &lt;- read_weighting_area(code_weighting = 2312908, year = 2010, showProgress = FALSE)\n\nggplot() +\n  geom_sf(data = ap_sobral, fill = \"white\", color = \"blue\") +\n  theme_void() +\n  labs(title = \"Áreas de Ponderação: Sobral/CE (2010)\")\n\n\n\n\n\n\n\n\n\n\n*Grade Estatística (read_statistical_grid)**\n\nA Grade Estatística é uma solução para comparar dados ao longo do tempo sem sofrer com a mudança das fronteiras dos setores censitários. O IBGE divide o Brasil em células de grade (ex: 1km x 1km ou 200m x 200m em áreas urbanas).\n\ncode_grid: Pode ser a abreviação do estado (ex: “DF”). Se code_grid=\"all\", baixa o Brasil todo (Cuidado: arquivo muito pesado).\nNota: A grade geralmente requer poder computacional maior devido ao número de polígonos.\n\n\n\nCódigo\ngrid_df &lt;- read_statistical_grid(code_grid = 53, year = 2010, showProgress = FALSE)\n\nplot(st_geometry(grid_df))\n\n\n\n\n\n\n\n\nImportanteDependência de Conexão\n\n\n\nComo o pacote geobr baixa dados diretamente dos servidores do Ipea/IBGE, você precisa de uma conexão ativa com a internet.\nSe você estiver rodando um script pesado que baixa muitas coisas, ocasionalmente o servidor pode falhar (timeout). O argumento cache = TRUE é seu melhor amigo aqui: uma vez baixado com sucesso, o dado fica salvo na sua máquina temporariamente, evitando downloads repetidos e falhas de conexão.\n\n\n\n2.9.1 Pacote geodata\nO pacote geodata Hijmans (2025) oferece acesso direto a repositórios científicos globais, como o WorldClim (clima), SoilGrids (solos), GADM (fronteiras) e GBIF (biodiversidade).\nA principal vantagem deste pacote é a padronização: ele baixa, descompacta e carrega os dados diretamente no R.\nInstalação e Carregamento\nO pacote está no CRAN. Como ele trabalha com dados raster e vetoriais otimizados, recomenda-se carregar também o pacote [terra](https://rspatial.org/pkg/terraPackage.pdf).\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(geodata, terra, ggplot2, tidyterra)\n\n\n\n\n\n\n\n\nNotaCaminho dos Arquivos (Path)\n\n\n\nQuase todas as funções do geodata exigem o argumento path. Este é o local no seu computador onde os arquivos (muitas vezes pesados) serão salvos.\n\nPara testes rápidos, pode usar path = tempdir() (pasta temporária que apaga ao fechar o R).\nPara projetos reais, defina uma pasta fixa (ex: path = \"dados/\") para evitar baixar a mesma coisa várias vezes.\n\n\n\n\nFronteiras Administrativas (gadm)\n\nEnquanto a função world() baixa um mapa mundi simplificado, a função gadm() acessa o banco de dados de alta resolução das Áreas Administrativas Globais (GADM). Ela permite baixar os limites de um país específico e suas subdivisões internas.\n\ncountry: Código ISO-3 do país (ex: “MOZ” para Moçambique, “BRA” para Brasil, “AGO” para Angola, etc.).\nlevel: Nível de detalhe administrativo:\n0: Fronteira do país.\n1: Províncias/Estados.\n2: Distritos/Municípios.\n3: Postos administrativos (quando disponível).\n\n\n\nCódigo\n# Baixar limites de Moçambique (Nível 1 - Províncias)\nmoz_prov &lt;- geodata::gadm(country = \"MOZ\", level = 1, path = tempdir())\n\n# O objeto é um SpatVector. Vamos converter para sf para usar no ggplot\nmoz_sf &lt;- st_as_sf(moz_prov)\n\nggplot() +\n  geom_sf(data = moz_sf, aes(fill = NAME_1), color = \"white\", show.legend = FALSE) +\n  geom_sf_text(data = moz_sf, aes(label = NAME_1), size = 2.5, color = \"black\") +\n  scale_fill_viridis_d(option = \"mako\", alpha = 0.8) +\n  theme_void() +\n  labs(title = \"Províncias de Moçambique\", caption = \"Fonte: GADM via pacote geodata\")\n\n\n\n\nCódigo\ntryCatch({  \n  moz_data_spat &lt;- read_sf(\"/home/almonha/Downloads/Curso de Verão/moz_adm/moz_admbnda_adm1_ine_20190607.shp\") \n  }, error = function(e) {\n    message(\"O link não funcionou vou baixar pelo GADM.\")\nmoz_data_spat &lt;- geodata::gadm(country = \"MOZ\", level = 2, path = tempdir(), version=\"latest\")\n}) \n\n\nmoz_sf &lt;- sf::st_as_sf(moz_data_spat)\n\n\n\nTopografia e Elevação (elevation_30s)\n\nA elevação é uma covariável fundamental em modelos espaciais. A função elevation_30s baixa o Modelo Digital de Elevação (DEM) da missão SRTM da NASA para um país específico, com resolução de ~1km (30 segundos de arco).\n\n\nCódigo\nelevacao &lt;- elevation_30s(long=32.583, lat = -25.967, path = tempdir())\n\nggplot() +\n  geom_spatraster(data = elevacao) +\n  scale_fill_hypso_tint_c(palette = \"dem_poster\", name = \"Altitude (m)\") +\n  geom_sf(data = moz_sf, fill = NA, color = \"black\", size = 0.2) +\n  theme_minimal() +\n  labs(title = \"Topografia de Moçambique\")\n\n\n\nDados Climáticos (worldclim_country)\n\nO WorldClim é o padrão para dados climáticos em ecologia e agricultura. A função worldclim_country baixa dados históricos (médias de 1970-2000) recortados para o país de interesse. Isso é muito mais leve do que baixar o raster global.\n\nvar: Variável desejada.\ntmin, tavg, tmax: Temperatura (°C).\nprec: Precipitação (mm).\nbio: Variáveis bioclimáticas (derivadas estatísticas biologicamente significativas).\nres: Resolução. 10, 5, 2.5 ou 0.5 minutos. (10 é grosseiro/leve, 0.5 é detalhado/pesado).\n\n\n\nCódigo\n#\nprec_moz &lt;- worldclim_country(country = \"MOZ\", var = \"prec\", res = 10, path = tempdir())\n\n# Selecionar apenas Janeiro (camada 1) e Julho (camada 7)\nprec_sazonal &lt;- prec_moz[[c(1, 7)]]\nnames(prec_sazonal) &lt;- c(\"Janeiro (Chuvoso)\", \"Julho (Seco)\")\n\nggplot() +\n  geom_spatraster(data = prec_sazonal) +\n  facet_wrap(~lyr) + \n  scale_fill_whitebox_c(palette = \"deep\", direction = 1, name = \"Chuva (mm)\") +\n  geom_sf(data = moz_sf, fill = NA, color = \"gray30\", size = 0.1) +\n  theme_void() +\n  labs(title = \"Precipitação Média Mensal (WorldClim v2.1)\")\n\n\n\n\n\n\n\n\nDicaVariáveis Bioclimáticas (Bio)\n\n\n\nPara modelagem de distribuição de espécies (SDM), utilize var = \"bio\". Isso retornará 19 camadas contendo índices como “Temperatura do trimestre mais seco” ou “Precipitação do trimestre mais quente”, que são preditores ecológicos mais fortes do que médias mensais simples.\n\n\n\nDados de Solo (soil_af e soil_world)\n\nOs dados de solo provêm do projeto SoilGrids (ISRIC). São dados complexos que incluem propriedades físicas (areia, argila) e químicas (pH, carbono) em várias profundidades padrão (0-5cm, 5-15cm, etc.).\nPara países da África, existe a função otimizada soil_af (baseada no projeto iSDAsoil). Para o resto do mundo, usa-se soil_world.\nO banco de dados global de solos é imenso (Gigabytes). Se você precisa apenas de dados para alguns pontos específicos (ex: estações de coleta), não baixe o raster inteiro. Use a conexão virtual (_vsi).\n\n\nCódigo\n#pontos de interesse (Longitude, Latitude)\npontos_amostra &lt;- data.frame(\n  local = c(\"Maputo\", \"Beira\", \"Nampula\"),\n  lon = c(32.58, 34.83, 39.26),\n  lat = c(-25.96, -19.83, -15.11)\n)\npontos_vect &lt;- vect(pontos_amostra, geom = c(\"lon\", \"lat\"), crs = \"EPSG:4326\")\n\n# var = \"clay\", depth = 15 (significa intervalo 5-15cm)\nraster_virtual &lt;- soil_world_vsi(var = \"clay\", depth = 15)\n\n#Extrair os valores para os pontos\nvalores_solo &lt;- terra::extract(raster_virtual, pontos_vect)\n\npontos_amostra$argila_percent &lt;- valores_solo[,2] / 10 # SoilGrids geralmente vem escalado x10\n\nprint(pontos_amostra)\n\n\n\nBiodiversidade (sp_occurrence)\n\nEsta função conecta-se ao GBIF (Global Biodiversity Information Facility) para baixar coordenadas de observação de espécies. É vital para estudos de ecologia.\nArgumentos Principais:\n\ngenus: Gênero da espécie.\nspecies: Epíteto específico. Se vazio \"\", baixa todo o gênero.\ngeo: Se TRUE, baixa apenas registros com coordenadas geográficas.\nremoveZeros: Remove registros com coordenadas (0,0) que geralmente são erros.\n\n\n\nCódigo\nmoz_sf &lt;- st_transform(moz_sf, crs = 4326)\nelefantes &lt;- sp_occurrence(genus = \"Loxodonta\", \n                           species = \"africana\", \n                           ext = moz_sf, # Usa o mapa baixado antes como limite\n                           geo = TRUE,     # Apenas com coordenadas\n                           removeZeros = TRUE) # Remove erros de (0,0)\n\n# Converter para sf\nelefantes_sf &lt;- st_as_sf(elefantes, coords = c(\"lon\", \"lat\"), crs = 4326)\n\nggplot() +\n  geom_sf(data = moz_sf, fill = \"gray95\", color = \"gray50\") +\n  # Adiciona os pontos dos elefantes\n  geom_sf(data = elefantes_sf, color = \"darkred\", size = 1, alpha = 0.6) +\n  theme_minimal() +\n  labs(title = \"Ocorrência de Elefantes\")\n\n\n\nUso e Cobertura do Solo (landcover)\n\nA função landcover() baixa mapas de cobertura do solo (floresta, agricultura, urbano, água) baseados no satélite ESA WorldCover. É um dado categórico de alta resolução.\n\npath: Local para salvar. O download é feito por “tiles” (retângulos de área).\n\n\n\nCódigo\n# Baixa o uso do solo para a região central do país\nuso_solo &lt;- landcover(var = \"trees\", path = tempdir(), country = \"MOZ\")\nplot(uso_solo, main = \"Cobertura Arbórea\")\n\n\n\nAcessibilidade (travel_time)\n\nFornece mapas raster de acessibilidade global, representando o tempo de viagem até centros urbanos ou portos, considerando infraestrutura de transporte e topografia.\nArgumentos Principais:\n\nto: Destino (\"city\" ou \"port\").\nsize: O tamanho do destino.\nPara cidades: 1 (muito grande) a 9 (pequena).\nEx: size=1 calcula o tempo até cidades com &gt; 5 milhões de habitantes.\n\n\n\nCódigo\nacesso &lt;- travel_time(to = \"city\", size = 4, path = tempdir())\n\nplot(acesso, main = \"Tempo de Viagem para Cidades Médias\", col = map.pal(\"inferno\"))\n\n\n\n\n\n\n\n\nDicaIntegre tudo\n\n\n\nA verdadeira força da análise espacial surge ao cruzar essas camadas. Com os dados baixados acima, você poderia facilmente responder a perguntas complexas, como: Qual é a precipitação média (WorldClim) nos locais onde foram avistados elefantes (GBIF) na província de Maputo (GADM)?.\n\n\n\n\n2.9.2 O pacote mapsf\nO pacote mapsf Giraud (2025), desenvolvido por Timothée Giraud, é o sucessor do pacote cartography. O seu objetivo é permitir a criação de mapas temáticos de alta qualidade visual com um fluxo de trabalho simplificado, baseado em objetos sf.\nDiferente do ggplot2, que trata o mapa como um gráfico estatístico num plano cartesiano, o mapsf, facilita a inclusão de elementos cartográficos clássicos (escala, norte, sombras, inset) e a gestão de layouts complexos.\n\nInstalação e Carregamento\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(mapsf, sf)\n\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(mapsf, sf, geobr, dplyr)\n\npr_mun &lt;- read_municipality(code_muni = \"PR\", year = 2020, showProgress = FALSE)\n\n#Transformar projeção para SIRGAS 2000 / UTM zone 22S (EPSG: 31982)\npr_mun &lt;- st_transform(pr_mun, crs = 31982)\n\n#Simular dados\nset.seed(123)\npr_mun$populacao &lt;- sample(5000:150000, size = nrow(pr_mun), replace = TRUE)\n# Inserir um outlier realista para Curitiba e Londrina\npr_mun$populacao[pr_mun$name_muni == \"Curitiba\"] &lt;- 1963726\npr_mun$populacao[pr_mun$name_muni == \"Londrina\"] &lt;- 575377\n\n# Calcular Densidade (Hab/km^2)\npr_mun$area_km2 &lt;- as.numeric(st_area(pr_mun)) / 1e6\npr_mun$densidade &lt;- pr_mun$populacao / pr_mun$area_km2\n\n\n\nTemas e Estética (mf_theme)\n\nAntes de plotar qualquer dado, definimos o “humor” do mapa. A função mf_theme() controla margens, cores de fundo, fontes e estilos de borda. O pacote vem com vários temas pré-definidos (ex: “default”, “dark”, “ink”, “agolalight”, “iceberg”).\n\n\nCódigo\n# Define margens, cor de fundo e família de fonte\nmf_theme(\"iceberg\") \n\nmf_map(pr_mun, col = \"grey80\", border = \"black\") \n\n# Adicionamos apenas o contorno do estado\npr_estado &lt;- st_union(pr_mun)\nmf_map(pr_estado, col = NA, border = \"grey40\", lwd = 1.5, add = TRUE)\n\nmf_title(\"Estado do Paraná\", pos = \"center\", tab = FALSE)\n\n\n\n\n\n\n\n\nFigura 2.23: Mapa base com tema Iceberg e sem bordas internas.\n\n\n\n\n\n\nSombras e Profundidade (mf_shadow)\n\nUm recurso estético simples, mas que adiciona grande valor visual (“efeito pop-up”), é a adição de sombras sob os polígonos. A função mf_shadow() desenha uma silhueta deslocada da geometria.\n\n\nCódigo\nmf_shadow(pr_mun, col = \"gray50\", cex = 1.2)\n#Adicionar o mapa por cima\nmf_map(pr_mun, add = TRUE)\n\n\n\nO Mapa Temático (mf_map)\n\nEsta é a função central. O argumento type define como os dados serão representados cartograficamente.\n\nx: Objeto sf.\nvar: Nome da coluna de atributos a ser mapeada.\ntype:\n\"choro\": Mapa coroplético (áreas coloridas por faixas de valores).\n\"prop\": Símbolos proporcionais (círculos).\n\"typo\": Tipologia (mapa qualitativo/categórico).\n\n\n\nCódigo\nmf_theme(\"iceberg\") #vc pode trocar aqui para tirar fundo preto\n\n#Sombra para destacar o estado do fundo (efeito 3D)\nmf_shadow(pr_mun, col = \"grey50\", cex = 1.5)\n\n# 2. Mapa Temático\nmf_map(\n  x = pr_mun, \n  var = \"densidade\", \n  type = \"choro\",\n  breaks = \"quantile\",  \n  nbreaks = 6,          \n  pal = \"YlOrRd\",       \n  border = NA,         \n  leg_title = \"Densidade\\n(hab/km²)\", \n  leg_val_rnd = 0,      # Arredondar valores da legenda\n  leg_pos = \"topright\",\n  add = TRUE\n)\n\n#Contorno do Estado (acabamento)\nmf_map(pr_estado, col = NA, border = \"white\", lwd = 0.5, add = TRUE)\n\nmf_title(\"Densidade Demográfica no Paraná\", pos = \"left\")\nmf_credits(\"Fonte: IBGE/geobr | Elaboração Própria\", cex = 0.6)\nmf_scale(size = 100, pos = \"bottomright\") # Escala discreta\nmf_arrow(pos = \"bottomright\",adj = c(1, 1))\n\n\n\n\n\n\n\n\nFigura 2.24: Densidade Demográfica com paleta contínua e layout limpo.\n\n\n\n\n\n\nElementos de Layout (mf_title, mf_scale, mf_arrow)\n\nUm mapa técnico exige elementos de referência para leitura correta.\n\nmf_title(): Título do mapa.\nmf_scale(): Barra de escala.\nmf_arrow(): Rosa dos ventos ou seta norte.\nmf_credits(): Fonte dos dados e autoria.\n\n\n\nCódigo\np_load(viridis)  #para cores\nmf_theme(NULL)\n\n\n#\nmf_map(\n  x = pr_mun, \n  var = \"area_km2\", \n  type = \"choro\",\n  nbreaks = 5,\n  border = \"white\",  \n  lwd = 0.5,\n  leg_pos = \"bottomright\",\n  leg_title = expression(\"Área Territorial\"~km^2)\n)\n\n#\nmf_map(\n  x = pr_mun,\n  var = \"populacao\",\n  type = \"prop\",\n  inches = 0.25,        # Tamanho do maior círculo\n  col = \"gray50\",   \n  leg_pos = \"topright\",\n  leg_title = \"População Total\",\n  val_max = max(pr_mun$populacao), \n  add = TRUE\n)\n\n#penas para cidades &gt; 300k hab para não poluir\nbig_cities &lt;- pr_mun[pr_mun$populacao &gt; 300000, ]\n\nmf_label(\n  x = big_cities, \n  var = \"name_muni\", \n  col = \"black\", \n  cex = 0.7, \n  overlap = FALSE, \n  lines = FALSE\n)\nmf_scale(size = 100, pos = \"bottomleft\") # Escala discreta\nmf_arrow(pos = \"bottomleft\",adj = c(1, 1))\n\n\n\n\n\n\n\n\nFigura 2.25: Mapa de dados simulados: Área (Cor) e População (Círculos).\n\n\n\n\n\n\nMapas de Localização (mf_inset_on)\n\nEm vez de usar um mapa mundi (mf_worldmap), para dados estaduais é mais comum mostrar onde o estado fica dentro do país (Brasil). Usamos a função mf_inset_on() para criar um “mini-mapa” num canto da figura.\nPrecisaremos do contorno do Brasil para isso.\n\n\nCódigo\n# 1. Carregar bases do Brasil\nbr &lt;- read_country(year = 2020, showProgress = FALSE)\nestados &lt;- read_state(year = 2020, showProgress = FALSE)\n\npr_destaque &lt;- estados[estados$abbrev_state == \"PR\", ]\n\n\np_load(viridis)  #para cores\nmf_theme(NULL)\n\n\n#\nmf_map(\n  x = pr_mun, \n  var = \"area_km2\", \n  type = \"choro\",\n  nbreaks = 5,\n  border = \"white\",  \n  lwd = 0.5,\n  leg_pos = \"bottomright\",\n  leg_title = expression(\"Área Territorial\"~km^2)\n)\n\n#\nmf_map(\n  x = pr_mun,\n  var = \"populacao\",\n  type = \"prop\",\n  inches = 0.25,        # Tamanho do maior círculo\n  col = \"gray50\",   \n  leg_pos = \"topleft\",\n  leg_title = \"População Total\",\n  val_max = max(pr_mun$populacao), \n  add = TRUE\n)\n\n#penas para cidades &gt; 300k hab para não poluir\nbig_cities &lt;- pr_mun[pr_mun$populacao &gt; 300000, ]\n\nmf_label(\n  x = big_cities, \n  var = \"name_muni\", \n  col = \"yellow\", \n  cex = 0.7, \n  overlap = FALSE, \n  lines = FALSE\n)\nmf_scale(size = 100, pos = \"bottomleft\") # Escala discreta\nmf_arrow(pos = \"bottomleft\",adj = c(1, 1))\n\n\n#INÍCIO DO INSET\nmf_inset_on(x = br, pos = \"topright\", cex = 0.25)\n\n#Desenhar todos os estados (Fundo)\nmf_map(estados, col = \"grey90\", border = \"white\", lwd = 0.3)\n\n#Desenhar o destaque\nmf_map(pr_destaque, col = \"black\", border = NA, add = TRUE)\n\n#Caixa\nbox(col = \"grey50\")\n\nmf_inset_off()\n\n\n\n\n\n\n\n\nFigura 2.26\n\n\n\n\n\n\nExportação Vetorial (mf_svg)\n\nPara finalização profissional em softwares de design (como Adobe Illustrator ou Inkscape), é preferível exportar o mapa em formato vetorial (SVG).\n\n\nCódigo\n#\nmf_svg(x = pr_mun, filename = \"mapa_parana.svg\", width = 10)\nmf_theme(\"default\")\nmf_map(pr_mun, var = \"densidade\", type = \"choro\")\nmf_title(\"Mapa Vetorial do Paraná\")\n\ndev.off()\n\n\n\n\n\n\n\n\nDicaDica de Layout\n\n\n\nO mapsf funciona desenhando em camadas, como se estivesse pintando uma tela. A ordem dos comandos importa: primeiro o fundo, depois o mapa, depois as anotações. Se chamar mf_theme() no meio do código, ele não alterará o que já foi desenhado, apenas o que virá a seguir.\n\n\n\n\n2.9.3 pacote ggmapinset: Insets e Zoom\nO pacote ggmapinset Suster (2024) estende as funcionalidades do ggplot2 para permitir a criação de painéis de zoom (insets) dentro da mesma área de plotagem.\nDiferente de abordagens como o patchwork (que cola dois gráficos distintos lado a lado), o ggmapinset transforma a geometria dos dados. Ele recorta a área de interesse, amplia-a (zoom) e move-a para um espaço vazio no mapa (geralmente o oceano), mantendo a coerência dos dados e estilos numa única camada.\n\nInstalação e Carregamento\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\n# Nota: O ggmapinset é recente, garanta que tem uma versão atualizada do R\npacman::p_load(ggmapinset, ggplot2, sf, dplyr, geodata)\n\n\nPara este exemplo, utilizaremos o mapa de Moçambique ao nível das províncias. Vamos focar na província de Maputo Cidade, que é geograficamente muito pequena e difícil de visualizar num mapa nacional sem zoom.\n\n\nCódigo\n#moz &lt;- geodata::gadm(country = \"MOZ\", level = 1, path = tempdir())\n\nmoz_sf &lt;- st_as_sf(moz_sf)\n\n#Identificar o alvo do zoom: Maputo Cidade\nmaputo_alvo &lt;- moz_sf |&gt; \n  filter(ADM1_PT == \"Maputo City\") |&gt;  #se usar gadm troque ADM1_PT por NAME_!\n  st_centroid()\n\n\n\nConfiguração do Inset (configure_inset)\n\nAntes de plotar, precisamos definir o que queremos ampliar e para onde queremos mover essa ampliação.\nA função configure_inset() cria um objeto de configuração que será usado pelo sistema de coordenadas.\n\nshape: Define a forma e o local do recorte (shape_circle ou shape_rectangle).\nscale: O fator de zoom (ex: 4 aumenta 4x).\ntranslation: O deslocamento para onde mover o zoom (X, Y).\nunits: Unidade de medida (km, mi).\n\n\n\nCódigo\nconfig_maputo &lt;- ggmapinset::configure_inset(\n  shape = ggmapinset::shape_circle(centre = maputo_alvo, radius = 50),\n  scale = 4,              # Zoom forte (4x)\n  translation = c(150, -100), # Move 150km para Leste e 100km para Sul\n  units = \"km\"\n)\n\n\n\nO Sistema de Coordenadas (coord_sf_inset)\n\nPara que o ggplot2 entenda que deve aplicar essa transformação, substituímos o tradicional coord_sf() por coord_sf_inset().\n\ninset: O objeto de configuração criado no passo anterior.\n\n\n\nCódigo\n# A estrutura básica do plot será:\nggplot(moz_sf) +\n  ggmapinset::coord_sf_inset(inset = config_maputo)\n\n\n\nGeometrias Adaptadas (geom_sf_inset)\n\nSe usarmos o geom_sf() padrão, ele desenhará o mapa normal. Para que as geometrias respeitem o zoom e sejam desenhadas tanto na base quanto no inset, usamos geom_sf_inset().\n\n\nCódigo\np_load(ggmapinset)\n\nconfig_maputo_direita &lt;- configure_inset(\n  shape = shape_circle(centre = maputo_alvo, radius = 50),\n  scale = 4,\n  translation = c(350, -100), # pode alterar para mover \n  units = \"km\"\n)\n\nggplot(moz_sf) +\n  ggmapinset::geom_sf_inset(aes(fill = ADM1_PT), color = \"white\", show.legend = FALSE) +\n  scale_fill_viridis_d(option = \"mako\") +\n  \n  ggmapinset::coord_sf_inset(inset = config_maputo_direita) + \n  theme_void()\n\n\n\n\n\n\n\n\nFigura 2.27: Mapa de Moçambique com zoom em Maputo (Básico)\n\n\n\n\n\n\n\nCódigo\nforma_personalizada &lt;- filter(moz_sf, moz_data_spat$ADM1_PT==\"Maputo\")%&gt;%\n  select(geometry)\n\nggplot(moz_sf) +\n  geom_sf_inset() + \n  geom_inset_frame() + \n  coord_sf_inset(\n    inset = configure_inset(\n      centre = st_centroid(forma_personalizada), # Centraliza no condado\n      scale = 2,                                 \n      shape = shape_sf(forma_personalizada),\n      translation = c(-370, -140)\n    )\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\nFigura 2.28: Mapa de Moçambique com zoom da Província de Maputo\n\n\n\n\n\n\nMoldura e Linhas de Conexão (geom_inset_frame)\n\nPara que o mapa seja legível, precisamos desenhar a borda do inset e as linhas que o conectam ao local original (“burst lines”). A função geom_inset_frame() faz isso automaticamente.\n\nsource.aes: Estética do círculo original no mapa base (local de origem).\ntarget.aes: Estética da moldura do zoom (local de destino).\nlines.aes: Estética das linhas de conexão.\n\n\n\nCódigo\np_load(ggmapinset)\n\ncoluna_nome &lt;- if(\"ADM1_PT\" %in% names(moz_sf)) \"ADM1_PT\" else \"NAME_1\"\n\nggplot(moz_sf) +\n  ggmapinset::geom_sf_inset(aes(fill = .data[[coluna_nome]]), color = \"white\", show.legend = FALSE) +\n  ggmapinset::geom_inset_frame(\n    data = NULL,\n    inherit.aes = FALSE \n  ) +\n  ggmapinset::coord_sf_inset(inset = config_maputo_direita) +\n  \n  scale_fill_viridis_d(option = \"B\", alpha = 0.8) +\n  theme_minimal() +\n  labs(title = \"Destaque: Província de Maputo Cidade\")\n\n\n\n\n\n\n\n\nFigura 2.29: Mapa com moldura e linhas de conexão\n\n\n\n\n\n\nRótulos e Texto (geom_sf_text_inset)\n\nAdicionar texto em mapas com inset é um desafio: o texto deve aparecer no local original ou no zoom?\nO pacote fornece geom_sf_text_inset() para lidar com isso.\n\nwhere: Define onde o texto aparece.\n\"inset\": O texto aparece dentro da área ampliada (no novo local).\n\"base\": O texto aparece no local original.\n\n\n\nCódigo\nggplot(moz_sf) +\n  ggmapinset::geom_sf_inset(aes(fill = .data[[coluna_nome]]), color = \"white\", show.legend = FALSE) +\n  ggmapinset::geom_inset_frame(\n    data = NULL,\n    inherit.aes = FALSE \n  ) +\n  ggmapinset::coord_sf_inset(inset = config_maputo_direita) +\n  \n  scale_fill_viridis_d(option = \"B\", alpha = 0.8) +\n  labs(title = \"\")+\n  geom_sf_text_inset(\n    aes(label = ADM1_PT), \n    size = 3, \n    color = \"black\",\n    check_overlap = TRUE\n  ) +\n    theme_minimal()+\n  theme(\n    axis.title = element_blank(), # Remove títulos legenda eixo x e y\n    #axis.text = element_blank(),  # Remove os números das coordenadas\n    axis.ticks = element_blank(), # Remove os tracinhos dos eixos (grade)\n    #panel.grid = element_blank()  # Remove as grades de fundo\n  )\n\n\n\n\n\n\n\n\nFigura 2.30: Rótulos aplicados no mapa\n\n\n\n\n\n\nFormas Alternativas (shape_rectangle)\n\nAlém de círculos, podemos usar retângulos. Vamos tentar um exemplo diferente: dar um zoom na região central (Sofala/Beira).\n\n\nCódigo\n#Definir centro na Beira (aproximado)\ncentro_beira &lt;- sf::st_sfc(sf::st_point(c(34.8, -19.8)), crs = 4326)\n\n#Configurar Retângulo\nconfig_retangulo &lt;- configure_inset(\n  shape = shape_rectangle(centre = centro_beira, hwidth = 150, hheight = 100), \n  scale = 2.5,\n  translation = c(700, -140), # Move 400km para o oceano\n  units = \"km\"\n)\n\nggplot(moz_sf) +\n  geom_sf_inset(fill = \"wheat\", color = \"tan\") +\n  # Estilizar a moldura do inset\n  ggmapinset::geom_inset_frame(\n    data = NULL,\n    inherit.aes = FALSE \n  ) +\n  coord_sf_inset(inset = config_retangulo) +\n  theme_void() +\n  labs(title = \"Zoom na Região Centro (Sofala)\")\n\n\n\n\n\n\n\n\nFigura 2.31: Inset retangular na região da Beira\n\n\n\n\n\n\nTransformação Manual (transform_to_inset)\n\nEm casos avançados, pode querer transformar os dados manualmente para realizar cálculos na geometria transformada ou usar com outros pacotes de plotagem.\n\ntransform_to_inset(x, inset): Recebe um objeto sf e a configuração, e retorna um novo objeto sf com as geometrias modificadas (deslocadas e ampliadas).\n\n\n\nCódigo\n# Cria um novo objeto sf com a geometria transformada\nmoz_transformado &lt;- transform_to_inset(moz_sf, config_maputo)\n\n# Note que as coordenadas mudaram para acomodar o inset\n# Isso é útil para debugging ou análises personalizadas\nglimpse(moz_transformado)\n\n\nRows: 11\nColumns: 14\n$ Shape_Leng &lt;dbl&gt; 19.276248, 14.641734, 17.581768, 18.685852, 9.614905, 1.821…\n$ Shape_Area &lt;dbl&gt; 6.47773653, 6.65593496, 6.05059198, 5.34907555, 2.09040114,…\n$ ADM1_PT    &lt;chr&gt; \"Cabo Delgado\", \"Gaza\", \"Inhambane\", \"Manica\", \"Maputo\", \"M…\n$ ADM1_PCODE &lt;chr&gt; \"MZ01\", \"MZ02\", \"MZ03\", \"MZ04\", \"MZ05\", \"MZ06\", \"MZ07\", \"MZ…\n$ ADM1_REF   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA\n$ ADM1ALT1PT &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA\n$ ADM1ALT2PT &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA\n$ ADM0_EN    &lt;chr&gt; \"Mozambique\", \"Mozambique\", \"Mozambique\", \"Mozambique\", \"Mo…\n$ ADM0_PT    &lt;chr&gt; \"Moçambique\", \"Moçambique\", \"Moçambique\", \"Moçambique\", \"Mo…\n$ ADM0_PCODE &lt;chr&gt; \"MZ\", \"MZ\", \"MZ\", \"MZ\", \"MZ\", \"MZ\", \"MZ\", \"MZ\", \"MZ\", \"MZ\",…\n$ date       &lt;date&gt; 2019-04-02, 2019-04-02, 2019-04-02, 2019-04-02, 2019-04-02,…\n$ validOn    &lt;date&gt; 2019-06-07, 2019-06-07, 2019-06-07, 2019-06-07, 2019-06-07,…\n$ validTo    &lt;date&gt; -1-11-30, -1-11-30, -1-11-30, -1-11-30, -1-11-30, -1-11-30,…\n$ geometry   &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((65.58717 24..., MULTIPOLYGON (((33.48387 -…\n\n\n\n\n\n\n\n\nDicaControle Fino de Camadas\n\n\n\nO geom_sf_inset possui dois argumentos poderosos: map_base e map_inset.\n\nmap_base = \"none\": Desenha apenas o inset (zoom), escondendo o mapa original.\nmap_inset = \"none\": Desenha apenas o mapa original, escondendo o zoom.\n\nIsso permite aplicar estilos diferentes (ex: cores diferentes) para a base e para o zoom, chamando a função duas vezes com configurações diferentes.\n\n\n\n\n2.9.4 Pacote ggrepel\nUma das maiores frustrações ao criar gráficos no ggplot2 é a sobreposição de rótulos de texto (labels), tornando a visualização ilegível. O pacote ggrepel Slowikowski (2024), desenvolvido por Kamil Slowikowski, resolve este problema.\nInstalação e Carregamento\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(ggrepel, ggplot2, dplyr)\n\n\nAntes de usar o ggrepel, vamos ver o comportamento padrão do ggplot2 (geom_text) quando tentamos rotular muitos pontos. Usaremos o dataset mtcars para rotular os modelos de carros.\n\n\nCódigo\nggplot(mtcars, aes(wt, mpg, label = rownames(mtcars))) +\n  geom_point(color = \"red\") +\n  # geom_text padrão não evita sobreposição\n  geom_text(hjust = 0, vjust = 0) +\n  theme_classic() +\n  labs(title = \"Sem ggrepel (Ilegível)\")\n\n\n\n\n\n\n\n\nFigura 2.32: Texto sobreposto e ilegível com geom_text padrão\n\n\n\n\n\n\nRótulos de Texto (geom_text_repel)\n\nPara corrigir o gráfico acima, substituímos geom_text() por geom_text_repel(). O pacote calculará automaticamente a melhor posição para cada rótulo e desenhará linhas de conexão (segments) se necessário.\n\n\nCódigo\nggplot(mtcars, aes(wt, mpg, label = rownames(mtcars))) +\n  geom_point(color = \"red\") +\n  #aqui\n  geom_text_repel() +\n  theme_classic() +\n  labs(title = \"Com ggrepel (Legível)\")\n\n\n\n\n\n\n\n\nFigura 2.33: Texto organizado com geom_text_repel\n\n\n\n\n\n\nRótulos com Caixa (geom_label_repel)\n\nSe preferir que o texto fique dentro de uma caixa (para melhor contraste com o fundo), use geom_label_repel(). Ele funciona de forma idêntica ao geom_text_repel, mas desenha um retângulo atrás do texto.\n\n\nCódigo\ncarros_eficientes &lt;- mtcars |&gt; \n  mutate(nome = rownames(mtcars)) |&gt; \n  mutate(label_plot = ifelse(mpg &gt; 25, nome, \"\"))\n\nggplot(carros_eficientes, aes(wt, mpg, label = label_plot)) +\n  geom_point(color = \"grey50\") +\n  # Destacar os pontos eficientes\n  geom_point(data = filter(carros_eficientes, mpg &gt; 25), color = \"blue\") +\n  \n  geom_label_repel(\n    box.padding = 0.5, # Espaço ao redor da caixa\n    point.padding = 0.5, # Espaço entre a ponta da linha e o ponto\n    segment.color = \"grey50\" # Cor da linha de conexão\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 2.34: Uso de geom_label_repel para destaque\n\n\n\n\n\n\nControle Fino (force, max.overlaps)\n\n\nforce: Controla a força de repulsão. Valores maiores afastam mais os rótulos.\nmax.overlaps: Por padrão, o ggrepel esconde rótulos se houver muita sobreposição. Aumente este valor (padrão é 10) se quiser forçar a exibição de todos os rótulos, mesmo que fique um pouco bagunçado.\n\n\n\nCódigo\ngeom_text_repel(\n  force = 2,           # Mais força de repulsão\n  max.overlaps = Inf,  # Nunca esconder rótulos (Infinito)\n  min.segment.length = 0 # Desenhar linha para todos os rótulos, mesmo os próximos\n)\n\n\n\nUso em Mapas\n\nO ggrepel integra-se perfeitamente com mapas (geom_sf). Para isso, usamos a geometria geom_sf_text() ou geom_text_repel combinada com stat_sf_coordinates(), que extrai automaticamente as coordenadas X e Y dos centroides dos polígonos.\n\n\nCódigo\n#\npr_destaque &lt;- pr_mun |&gt; filter(populacao &gt; 200000)\n\nggplot(pr_mun) +\n  geom_sf(fill = \"white\", color = \"grey80\") +\n  geom_sf(data = pr_destaque, fill = \"orange\") +\n  \n  # Usar geom_text_repel \n  geom_text_repel(\n    data = pr_destaque,\n    aes(label = name_muni, geometry = geom),\n    stat = \"sf_coordinates\", # Calcula o X/Y do centroide automaticamente\n    min.segment.length = 0,\n    box.padding = 1,\n    size = 3\n  ) +\n  theme_void() +\n  labs(title = \"Municípios em Destaque no Paraná\")\n\n\n\n\n\n\n\n\nFigura 2.35: Rótulos em mapas com ggrepel\n\n\n\n\n\n\n\n\n\n\n\nDicaSegmentos e Setas\n\n\n\nVocê pode estilizar as linhas que conectam o texto ao ponto usando argumentos como arrow, segment.size, segment.color e curvature (para linhas curvas). Isso é útil para criar anotações elegantes em gráficos de publicação.\n\n\n\n\n2.9.5 Pacote ggspatial\nO ggplot2 é excelente para plotar geometrias, mas falta-lhe “vocabulário cartográfico” nativo. O pacote ggspatial Dunnington (2025), desenvolvido por Dewey Dunnington, preenche essa lacuna fornecendo geometrias e anotações específicas para mapas.\nEle permite adicionar facilmente barras de escala, setas de norte e, crucialmente, mapas de fundo (basemaps) provenientes de serviços como OpenStreetMap, sem sair da sintaxe do ggplot.\nInstalação e Carregamento\nPara este exemplo, usaremos os dados do estado do Rio de Janeiro provenientes do pacote geobr.\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(ggspatial, ggplot2, sf, terra, geobr, prettymapr, rosm)\n\n# Carregar dados do Rio de Janeiro\nrj_mun &lt;- read_municipality(code_muni = \"RJ\", year = 2020, showProgress = FALSE)\n\n\n\nMapas Base (Basemaps) (annotation_map_tile)\n\nUma das funcionalidades mais desejadas é adicionar um mapa de ruas ou satélite como fundo. A função annotation_map_tile() faz isso automaticamente, baixando as “telhas” (tiles) necessárias para a área do seu gráfico.\n\ntype: O tipo de mapa. Padrão é \"osm\" (OpenStreetMap). Outras opções comuns incluem \"cartolight\", \"cartodark\", \"hotstyle\".\nzoom: Nível de detalhe. Se omitido, o pacote calcula automaticamente. Zooms altos baixam muitos arquivos.\nprogress: Defina como \"none\" para evitar poluição visual no console durante o download.\n\n\n\nCódigo\nggplot(rj_mun) +\n  #Adicionar o mapa base (deve vir primeiro para ficar no fundo)\n  annotation_map_tile(type = \"osm\", progress = \"none\") +\n  \n  #Adicionar os dados vetoriais por cima (com transparência)\n  geom_sf(fill = \"orange\", alpha = 0.4, color = \"black\", size = 0.1) +\n  \n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 2.36: Municípios do RJ sobre base do OpenStreetMap\n\n\n\n\n\n\nBarra de Escala (annotation_scale)\n\nDiferente do mapsf, o ggspatial adiciona a escala como uma camada (layer) do ggplot.\n\nlocation: Posição (“tl” = top-left, “br” = bottom-right, etc.).\nwidth_hint: Proporção da largura do mapa que a escala deve ocupar (ex: 0.2 para 20%).\nstyle: Estilo visual (“bar” ou “ticks”).\n\n\n\nCódigo\nggplot(rj_mun) +\n    annotation_map_tile(type = \"osm\", progress = \"none\") +\n  geom_sf(fill = \"orange\", color = \"white\") +\n  \n  annotation_scale(\n    location = \"br\",    # Bottom right (Canto inferior direito)\n    width_hint = 0.3,   # Tamanho relativo\n    style = \"bar\"       # Estilo barra sólida\n  ) +\n  \n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 2.37: Adicionando barra de escala ao mapa do RJ\n\n\n\n\n\n\nSeta Norte (annotation_north_arrow)\n\nAdiciona a rosa dos ventos ou seta norte.\n\nlocation: Posição.\nwhich_north: \"true\" (Norte geográfico) ou \"grid\" (Norte da grade).\nstyle: Estilo visual. O pacote oferece várias funções de estilo:\nnorth_arrow_orienteering (Padrão)\nnorth_arrow_fancy_orienteering (Mais elaborado)\nnorth_arrow_minimal (Simples)\nnorth_arrow_nautical (Estilo náutico)\n\n\n\nCódigo\nggplot(rj_mun) +\n    annotation_map_tile(type = \"osm\", progress = \"none\") +\n  geom_sf(fill = \"orange\", color = \"white\") +\n  \n  annotation_scale(\n    location = \"br\",    \n    width_hint = 0.3,   \n    style = \"bar\"       \n  ) +\n  # Norte Simples no topo esquerdo\n  annotation_north_arrow(\n    location = \"tl\", \n    which_north = \"true\",\n    height = unit(1, \"cm\"),\n    width = unit(1, \"cm\"),\n    style = north_arrow_minimal()\n  ) +\n  \n  # Norte Náutico no fundo direito (apenas demonstração)\n  annotation_north_arrow(\n    location = \"br\", \n    style = north_arrow_nautical(),\n    pad_y=unit(0.55, \"cm\")\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 2.38: Estilos de Seta Norte\n\n\n\n\n\n\nTrabalhando com Rasters (layer_spatial)\n\nO ggplot2 nativo (geom_raster) exige que convertamos imagens para data.frames, o que é lento e consome memória. O ggspatial fornece a função layer_spatial(), que aceita objetos SpatRaster (do pacote terra) ou stars e os plota corretamente, lidando com projeções automaticamente.\n\n\nCódigo\np_load(terra, sf, ggplot2,ggspatial,geobr)\n\nrj_mun &lt;- read_municipality(code_muni = \"RJ\", year = 2020, showProgress = FALSE)\nrj_estado &lt;- st_union(rj_mun)\n\nv_rj &lt;- vect(rj_estado) \n\nr &lt;- rast(ext(v_rj), resolution = 0.01, crs = st_crs(rj_mun)$wkt)\nvalues(r) &lt;- runif(ncell(r), 0, 1000)\n\n#Recorte\nr_rj &lt;- crop(r, v_rj)\nr_rj &lt;- mask(r_rj, v_rj)\n\nggplot() +\n  annotation_map_tile(type = \"osm\", zoom = 8, progress = \"none\") +\n  layer_spatial(r_rj, alpha = 0.8) +\n  scale_fill_viridis_c(name = \"Altitude (m)\", option = \"B\", na.value = NA) +\n  geom_sf(data = rj_estado, fill = NA, color = \"black\", size = 0.8) +\n  annotation_scale(\n    location = \"br\",    \n    width_hint = 0.3,   \n    style = \"bar\"       \n  ) +\n    annotation_north_arrow(\n    location = \"br\", \n    style = north_arrow_nautical(),\n    pad_y=unit(0.55, \"cm\")\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 2.39: Plotando raster diretamente com layer_spatial\n\n\n\n\n\n\n\n\n\n\n\nDica\n\n\n\nAo usar annotation_map_tile, o R baixa as imagens da internet a cada plotagem se elas não estiverem em cache. Se o servidor de tiles (ex: OSM) estiver lento, tente outro type como \"cartolight\" ou \"hotstyle\". Certifique-se também de que seus dados sf tenham um CRS definido (st_crs()).\n\n\n\n\n2.9.6 Mapas Interativos na Web: O pacote leaflet\nO pacote leaflet Cheng et al. (2025) é a ponte entre a linguagem R e a biblioteca JavaScript homônima, que é o padrão da indústria para cartografia web. Diferente do mapsf ou ggplot2, que geram uma imagem estática (como uma foto), o leaflet gera um widget HTML.\n\nTiles (Base): O fundo do mapa (ruas, satélite). São imagens estáticas carregadas da internet.\nMarkers/Polygons (Vetores): Seus dados desenhados sobre a base.\nPopups/Labels: Interatividade que surge ao clicar ou passar o mouse.\nControls: Elementos de interface (zoom, seletor de camadas, legenda).\n\nInstalação e Dados (SP)\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(leaflet, sf, dplyr, geobr)\n\n#Base Cartográfica: Região Metropolitana de SP\nif (!exists(\"rm_sp\")) {\n  sp_mun &lt;- read_municipality(code_muni = \"SP\", year = 2020, showProgress = FALSE)\n  capital &lt;- sp_mun[sp_mun$name_muni == \"São Paulo\", ]\n  vizinhos &lt;- st_filter(sp_mun, capital, .predicate = st_touches)\n  rm_sp &lt;- rbind(capital, vizinhos)\n}\n\n#Escolas (INEP)\nescolas_br &lt;- read_schools(year = 2020, showProgress = FALSE)\n\nescolas_rm &lt;- st_filter(escolas_br, rm_sp) |&gt; \n  select(\n    nome = name_school,\n    endereco = address,\n    nivel = education_level,\n    admin = admin_category\n  ) |&gt; \n  filter(!is.na(nome), !is.na(geom))\n\nleaflet(escolas_rm) |&gt; \n  # Mapas Base\n  addTiles(group = \"OSM (Rua)\") |&gt; \n  addProviderTiles(providers$CartoDB.Positron, group = \"Clean (Claro)\") |&gt; \n  \n  # Camada de Contexto (Municípios)\n  addPolygons(\n    data = rm_sp,\n    color = \"#444\", weight = 1, fillOpacity = 0.1,\n    group = \"Limites Municipais\",\n    label = ~name_muni\n  ) |&gt; \n  \n  # Camada de Escolas (Clusters)\n  addCircleMarkers(\n    radius = 5,\n    color = \"#2E86C1\", # Azul\n    stroke = FALSE, fillOpacity = 0.8,\n    \n    # Popup Rico com dados reais da escola\n    popup = ~paste0(\n      \"&lt;b&gt;Escola:&lt;/b&gt; \", nome, \"&lt;br&gt;\",\n      \"&lt;b&gt;Nível:&lt;/b&gt; \", nivel, \"&lt;br&gt;\",\n      \"&lt;b&gt;Administração:&lt;/b&gt; \", admin, \"&lt;br&gt;\",\n      \"&lt;b&gt;Endereço:&lt;/b&gt; \", endereco\n    ),\n    \n    # Clusterização Automática (Essencial para muitos pontos)\n    clusterOptions = markerClusterOptions(),\n    group = \"Escolas\"\n  ) |&gt; \n  \n  # Controle de Camadas\n  addLayersControl(\n    baseGroups = c(\"Clean (Claro)\", \"OSM (Rua)\"),\n    overlayGroups = c(\"Escolas\", \"Limites Municipais\"),\n    options = layersControlOptions(collapsed = FALSE)\n  )\n\n\n\n\n\n\n\n\nFigura 2.40: Distribuição Real de Escolas na RM de São Paulo (Dados INEP/geobr)\n\n\n\n\n\nO Mapa Base e a Sintaxe Pipe (|&gt;)\n\n\nleaflet(): Instancia o objeto do mapa.\naddTiles(): Adiciona o mapa base padrão (OpenStreetMap).\nsetView(): (Opcional) Define onde a câmera do mapa começa posicionada.\n\n\n\nCódigo\nleaflet() |&gt; \n  addTiles() |&gt;  # Padrão: OpenStreetMap (estilo 'ruas')\n  setView(lng = -46.63, lat = -23.55, zoom = 9)\n\n\n\n\n\n\n\n\nFigura 2.41: Mapa base inicial (OpenStreetMap)\n\n\n\n\n\nPolígonos e Mapas Coropléticos\n\nPara colorir os municípios baseados em dados (ex: Área), precisamos mapear a variável numérica para uma paleta de cores.\n\n\n\n\n\n\nImportanteOperador Til (~)\n\n\n\nNo leaflet, diferentemente do ggplot2 (que usa aes()), usamos o símbolo ~ (til) para indicar que um argumento deve ser lido de dentro dos dados.\n\ncolor = \"red\": Todos os polígonos ficam vermelhos.\ncolor = ~paleta(variavel): A cor depende da variável de cada polígono.\n\n\n\n\n\nCódigo\n# Definir a paleta de cores (Quantis)\nrm_sp$area_km2 &lt;- as.numeric(st_area(rm_sp)) / 1e6\npal_area &lt;- colorQuantile(\"YlOrRd\", domain = rm_sp$area_km2, n = 5)\n\nlabels_html &lt;- paste0(\n  \"&lt;b&gt;\", rm_sp$name_muni, \"&lt;/b&gt;: \", \n  round(rm_sp$area_km2), \" km&lt;sup&gt;2&lt;/sup&gt;\"\n) |&gt; \n  lapply(htmltools::HTML) # Importante: Converte texto para objeto HTML\n\n\nleaflet(rm_sp) |&gt; \n  #ESCOLHA DO MAPA BASE\n  addProviderTiles(providers$CartoDB.Positron) |&gt;  #tem outras além de CartoDB.Positron\n  #CAMADA DE POLÍGONOS\n  addPolygons(\n    fillColor = ~pal_area(area_km2), # O til (~) mapeia a coluna aos dados\n    weight = 1,         \n    color = \"white\",     # Cor da linha de contorno\n    fillOpacity = 0.7,   # Transparência do preenchimento\n    \n    # Comportamento ao passar o mouse (Realce)\n    highlightOptions = highlightOptions(\n      color = \"#666\", \n      weight = 3,\n      bringToFront = TRUE\n    ),\n    \n    # Rótulo (Label/Tooltip)\n    label = ~ labels_html\n  ) |&gt; \n  \n  # Legenda\n  addLegend(\n    position = \"bottomright\", \n    pal = pal_area, \n    values = ~area_km2, \n    title = \"Área (Quantis)\"\n  )\n\n\n\n\n\n\n\n\nFigura 2.42: Mapa Coroplético Interativo: Área dos Municípios\n\n\n\n\n\nMarcadores, Popups HTML e Clusters\n\nUma das maiores vantagens do leaflet é o suporte a HTML dentro dos popups e a capacidade de agrupar pontos automaticamente (Cluster) para evitar poluição visual quando há milhares de registros.\n\n\nCódigo\nif (!exists(\"rm_sp\")) {\n  library(geobr)\n  sp_mun &lt;- read_municipality(code_muni = \"SP\", year = 2020, showProgress = FALSE)\n  capital &lt;- sp_mun[sp_mun$name_muni == \"São Paulo\", ]\n  rm_sp &lt;- capital \n}\n\n#\nset.seed(123) \npontos_poi &lt;- st_sample(rm_sp, size = 100) |&gt; \n  st_as_sf() |&gt; \n  mutate(\n    id = 1:100,\n    tipo = sample(c(\"Escola\", \"Hospital\", \"UBS\"), 100, replace = TRUE),\n    atendimentos = round(runif(100, 50, 500))\n  )\n\npal_tipo &lt;- colorFactor(c(\"#1f77b4\", \"#d62728\", \"#2ca02c\"), domain = pontos_poi$tipo)\n\nleaflet(pontos_poi) |&gt; \n  addProviderTiles(providers$CartoDB.Positron) |&gt; \n  \n  addCircleMarkers(\n    radius = 6,\n    color = ~pal_tipo(tipo),\n    stroke = FALSE, fillOpacity = 0.8,\n    label = ~tipo,\n    \n    # HTML dentro do Popup\n    popup = ~paste0(\n      \"&lt;strong&gt;ID:&lt;/strong&gt; \", id, \"&lt;br&gt;\",\n      \"&lt;strong&gt;Tipo:&lt;/strong&gt; \", tipo, \"&lt;br&gt;\",\n      \"&lt;strong&gt;Média Atend.:&lt;/strong&gt; \", atendimentos\n    ),\n    \n    clusterOptions = markerClusterOptions() \n  )\n\n\n\n\n\n\n\nControle de Camadas (addLayersControl)\nBaseGroups: Camadas de fundo mutuamente exclusivas (ex: Dia ou Noite).\nOverlayGroups: Camadas de dados que podem ser ligadas/desligadas independentemente.\n\n\n\nCódigo\nleaflet() |&gt; \n  addTiles(group = \"OSM (Ruas)\") |&gt; \n  addProviderTiles(providers$Esri.WorldImagery, group = \"Satélite\") |&gt; \n  \n  addPolygons(data = rm_sp, color = \"white\", weight = 2, fillColor = \"transparent\",\n              group = \"Limites Municipais\") |&gt; \n  \n  addCircleMarkers(data = pontos_poi, radius = 5, color = \"orange\", \n                   fillOpacity = 0.8, stroke = FALSE,\n                   group = \"Equipamentos Públicos\") |&gt; \n  \n  addLayersControl(\n    baseGroups = c(\"OSM (Ruas)\", \"Satélite\"),\n    overlayGroups = c(\"Limites Municipais\", \"Equipamentos Públicos\"),\n    options = layersControlOptions(collapsed = FALSE) # Menu sempre aberto\n  )\n\n\n\n\n\n\n\n\nFigura 2.43: Controle de Camadas Completo\n\n\n\n\n\n\nCódigo\n# Fonte: Este exemplo foi retirado no link: https://r-spatial.github.io/mapview/articles/mapview_06-add.html\np_load(leafem, mapview)\nm &lt;- mapview(breweries)\n\nleafem::addLogo(m, \"https://jeroenooms.github.io/images/banana.gif\", #pode adicionar imagem aqui\n                position = \"bottomleft\",\n                offset.x = 5,\n                offset.y = 40,\n                width = 100,\n                height = 100)\n\n\n\n\n\n\n\n\n\n\n\n\nDicaExportação e Salvamento\n\n\n\nO leaflet gera HTML. Para salvar seu mapa:\nUse o pacote mapview: mapview::mapshot(mapa, file = \"mapa.png\").\n\n\n\n\n\n\n\n\nDicaPara ir além\n\n\n\nO ecossistema de análise espacial no R é vasto e dinâmico, com novos pacotes surgindo constantemente. A seleção apresentada neste capítulo focou nas ferramentas essenciais para resolver os problemas mais frequentes do dia a dia.\nVale mencionar também o excelente pacote tmap, que oferece uma sintaxe flexível para mapas temáticos (similar ao ggplot2). Para se manter atualizado e buscar inspiração, recomendo duas fontes indispensáveis:\nR-Spatial.org: O blog oficial com as últimas novidades sobre a evolução espacial do R.\nThe R Graph Gallery: Uma coleção com os melhores códigos e exemplos visuais, cobrindo desde gráficos básicos até mapas complexos.\n\n\n\n\n\n\nBandyopadhyay, Soutir, e Suhasini Subba Rao. 2017. “A test for stationarity for irregularly spaced spatial data”. Journal of the Royal Statistical Society Series B: Statistical Methodology 79 (1): 95–123.\n\n\nBartholomew, David J. 1995. “What is statistics?” Journal of the Royal Statistical Society Series A: Statistics in Society 158 (1): 1–20.\n\n\nBoschma, Ron. 2005. “Proximity and innovation: a critical assessment”. Regional studies 39 (1): 61–74.\n\n\nBrunsdon, Chris, A Stewart Fotheringham, e Martin E Charlton. 1996. “Geographically weighted regression: a method for exploring spatial nonstationarity”. Geographical analysis 28 (4): 281–98.\n\n\nBurbano-Moreno, Alvaro Alexander, e Vinı́cius Diniz Mayrink. 2024. “Spatial functional data analysis: Irregular spacing and bernstein polynomials”. Spatial Statistics 60: 100832.\n\n\nChen, Wanfang, Marc G Genton, e Ying Sun. 2021. “Space-time covariance structures and models”. Annual Review of Statistics and Its Application 8 (1): 191–215.\n\n\nCheng, Joe, Barret Schloerke, Bhaskar Karambelkar, Yihui Xie, e Garrick Aden-Buie. 2025. leaflet: Create Interactive Web Maps with the JavaScript ’Leaflet’ Library. https://doi.org/10.32614/CRAN.package.leaflet.\n\n\nChun, Yongwan, e Daniel A. Griffith. 2017. “Measuring Spatial Dependence”. International Encyclopedia of Geography, março, 1–14. https://doi.org/10.1002/9781118786352.wbieg0850.\n\n\nConte, Bruno. 2023. “Lecture 02: Spatial Data Theory and Tools (a.k.a. GIS Tools Lab.)”. Alma Mater Studiorum Università di Bologna; Lecture slides.\n\n\nCox, Simon Jonathan David. 2011. ISO 19156:2011 - Geographic Information – Observations and Measurements. International Organization for Standardization. https://doi.org/10.13140/2.1.1142.3042.\n\n\nCrawford, T. W. 2009. “Scale Analytical”. Em, 29–36. Elsevier. https://doi.org/10.1016/b978-008044910-4.00399-0.\n\n\nCressie, Noel, e Matthew T Moores. 2022. “Spatial statistics”. Em Encyclopedia of mathematical geosciences, 1–11. Springer.\n\n\nDelicado, Pedro, Ramón Giraldo, Carlos Comas, e Jorge Mateu. 2010. “Statistics for spatial functional data: some recent contributions”. Environmetrics: The official journal of the International Environmetrics Society 21 (3-4): 224–39.\n\n\nDreesman, Johannes M, e Gerhard Tutz. 2001. “Non-Stationary Conditional Models for Spatial Data Based on Varying Coefficients”. Journal of the Royal Statistical Society: Series D (The Statistician) 50 (1): 1–15.\n\n\nDunnington, Dewey. 2025. ggspatial: Spatial Data Framework for ggplot2. https://doi.org/10.32614/CRAN.package.ggspatial.\n\n\nFávero, Luiz Paulo Lopes. 2003. “Modelos de preços hedônicos aplicados a imóveis residenciais em lançamento no municı́pio de São Paulo.” Tese de doutorado, Universidade de São Paulo.\n\n\nFienberg, Stephen E. 2014. “What is statistics?” Annual review of statistics and its application 1 (1): 1–9.\n\n\nGetis, Arthur. 1999. “Spatial statistics”. Geographical information systems 1: 239–51.\n\n\nGiraud, Timothée. 2025. “mapsf: Thematic Cartography”.\n\n\nHijmans, Robert J. 2025. geodata: Access Geographic Data. https://doi.org/10.32614/CRAN.package.geodata.\n\n\nIliffe, Jonathan. 2000. Datums and map projections for remote sensing, GIS, and surveying. CRC Press.\n\n\nJanssen, Volker. 2009. “Understanding coordinate reference systems, datums and transformations”.\n\n\nKaplan, Elliott D, e Christopher Hegarty. 2017. Understanding GPS/GNSS: principles and applications. Artech house.\n\n\nKelejian, Harry, e Gianfranco Piras. 2017. Spatial econometrics. Academic Press.\n\n\nLangley, Richard B et al. 1999. “Dilution of precision”. GPS world 10 (5): 52–59.\n\n\nLeick, Alfred, Lev Rapoport, e Dmitry Tatarnikov. 2015. GPS satellite surveying. John Wiley & Sons.\n\n\nLi, Habin, e James F Reynolds. 1994. “A simulation experiment to quantify spatial heterogeneity in categorical maps”. Ecology 75 (8): 2446–55.\n\n\nLoonis, Vincent, e Marie-Pierre de Bellefon. 2018. “Handbook of spatial analysis: theory and application with R”. Paris: Eurostat, INSEE, 394.\n\n\nMateu, Jorge, e Ramón Giraldo. 2022. “Introduction to Geostatistical Functional Data Analysis”. Geostatistical Functional Data Analysis, 1–25.\n\n\nMocnik, Franz-Benjamin. 2023. “Why we can read maps”. Cartography and Geographic Information Science 50 (1): 1–19.\n\n\nMøller, Jesper, e Rasmus P Waagepetersen. 2007. “Modern statistics for spatial point processes”. Scandinavian Journal of Statistics 34 (4): 643–84.\n\n\nMonmonier, Mark. 2005. “Lying with maps”. Statistical science, 215–22.\n\n\nMoreno, Alvaro Alexander Burbano et al. 2023. “Functional data analysis: spatial association of curves and irregular spacing”.\n\n\nNhancololo, A. M. 2024. “Processos pontuais espaciais univariados aplicados à distribuição de espécies arbóreas em florestas naturais”. Dissertação (Mestrado em Estatística e Experimentação Agropecuária), Lavras: Universidade Federal de Lavras.\n\n\nNhancololo, A. M., Wélson A. Oliveira, Fernandes A. C. Pereira, Bruno Montoani Silva, e João Domingos Scalon. 2024. “Comparison between the laboratory method and the Stolf penetrometer in soil density analysis: a study using geostatistical approaches”. Sigmae 13 (1): 63–78. https://doi.org/10.29327/2520355.13.1-7.\n\n\nPebesma, Edzer. 2018. “Simple features for R: standardized support for spatial vector data”.\n\n\nPeng, Zhan, e Ryo Inoue. 2024. “Multiscale Continuous and Discrete Spatial Heterogeneity Analysis: The Development of a Local Model Combining Eigenvector Spatial Filters and Generalized Lasso Penalties”. Geographical Analysis 56 (2): 303–27.\n\n\nPereira, Rafael HM, e Caio Nogueira Goncalves. 2024. “geobr: download official spatial data sets of Brazil”. R package version 1 (0): 18.\n\n\nPrestby, Timothy J. 2025. “Trust in maps: What we know and what we need to know”. Cartography and Geographic Information Science 52 (1): 1–18.\n\n\nRamsay, James O, e Bernard W Silverman. 2005. Functional data analysis. Springer.\n\n\nSahu, Sujit. 2022. Bayesian modeling of spatio-temporal data with R. Chapman; Hall/CRC.\n\n\nSchmidt, Alexandra M, e Anthony O’Hagan. 2003. “Bayesian inference for non-stationary spatial covariance structure via spatial deformations”. Journal of the Royal Statistical Society Series B: Statistical Methodology 65 (3): 743–58.\n\n\nSlowikowski, Kamil. 2024. ggrepel: Automatically Position Non-Overlapping Text Labels with ’ggplot2’. https://doi.org/10.32614/CRAN.package.ggrepel.\n\n\nSnyder, John Parr. 1987. Map projections–A working manual. Vol. 1395. US Government Printing Office.\n\n\nSuster, Carl. 2024. ggmapinset: Add Inset Panels to Maps. https://doi.org/10.32614/CRAN.package.ggmapinset.\n\n\nTobler, Waldo R. 1970. “A computer movie simulating urban growth in the Detroit region”. Economic geography 46 (sup1): 234–40.\n\n\nUnwin, David J, e Leslie W Hepple. 1974. “The statistical analysis of spatial series”. Journal of the Royal Statistical Society: Series D (The Statistician) 23 (3-4): 211–27.\n\n\nVanicek, Petr, e Edward J Krakiwsky. 2015. Geodesy: the concepts. Elsevier.\n\n\nWagner, Helene H, e Marie-Josée Fortin. 2005. “Spatial analysis of landscapes: concepts and statistics”. Ecology 86 (8): 1975–87.\n\n\nWaller, Lance A. 2024. “Maps: A Statistical View”. Annual Review of Statistics and Its Application 11.\n\n\nWang, Jane-Ling, Jeng-Min Chiou, e Hans-Georg Müller. 2016. “Functional data analysis”. Annual Review of Statistics and its application 3 (1): 257–95.\n\n\nWild, Christopher J, Jessica M Utts, e Nicholas J Horton. 2017. “What is statistics?” Em International handbook of research in statistics education, 5–36. Springer.",
    "crumbs": [
      "Fundamentos da Estatística Espacial",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos da Estatística Espacial</span>"
    ]
  },
  {
    "objectID": "geostat.html",
    "href": "geostat.html",
    "title": "3  Geoestatística",
    "section": "",
    "text": "3.1 Variável Regionalizada e o Processo Estocástico\nImagine o desafio enfrentado por um engenheiro de minas na África do Sul, na década de 1950. A sua tarefa consistia em estimar o teor de ouro num bloco de rocha ainda não escavado, baseando-se apenas em algumas amostras recolhidas através de furos de sondagem Ecker (2003). Ao aplicar os métodos estatísticos convencionais da época, baseados em médias aritméticas, os engenheiros deparavam-se consistentemente com um erro sistemático: as previsões tendiam a superestimar as áreas ricas e a subestimar as áreas pobres. Quando a exploração real avançava, o ouro recuperado não correspondia à estimativa inicial, gerando prejuízos avultados Krige e Kleingeld (2005).\nA raiz deste problema não estava na precisão dos instrumentos, mas na premissa estatística utilizada. A estatística clássica assume, frequentemente, que as observações são independentes e identicamente distribuídas (i.i.d.) Cressie e Moores (2022). No entanto, na natureza, esta independência raramente existe. O engenheiro sul-africano Danie Gerhardus Krige percebeu empiricamente que uma amostra geológica não é um valor isolado; ela carrega consigo uma influência da sua vizinhança Scalon (2024). Uma amostra com elevado teor de minério sugere que a rocha imediatamente adjacente tem também uma alta probabilidade de ser rica Oliver e Webster (2014) . Ao ignorar a localização espacial das amostras e tratá-las como eventos aleatórios independentes, perdia-se a informação mais valiosa para a predição: a continuidade espacial Nhancololo et al. (2024).\nFoi com base nesta intuição que o matemático francês Georges Matheron, na década de 1960, formalizou a disciplina que hoje conhecemos como Geoestatística (Andre G. Journel e Huijbregts 1976; Scalon 2024). Matheron sistematizou as observações de Krige através da Teoria das Variáveis Regionalizadas (G. Matheron 1963; George Matheron 1971), estabelecendo que fenômenos naturais não são nem puramente aleatórios, como o lançamento de um dado, nem puramente determinísticos, descritíveis por uma equação geométrica simples. Eles exibem uma estrutura mista: possuem continuidade estruturada (dependência espacial), mas também uma componente localmente imprevisível (aleatoriedade local) (Cressie 1990; Wackernagel 2003).\nA Geoestatística define-se, portanto, como o ramo da estatística espacial dedicado à modelagem e predição destes fenômenos contínuos Myers (1994). Assume-se que a variável de interesse, denotada por \\(Y(\\mathbf{s})\\), existe em qualquer ponto de um domínio contínuo fixo \\(D^{G} \\subseteq \\mathbb{R}^d\\), mas é observada apenas num conjunto finito de locais \\(\\{\\mathbf{s}_1, \\dots, \\mathbf{s}_n\\}\\) (Scalon 2024; Cressie e Moores 2022).\nNeste contexto de informação incompleta, o objetivo central passa a ser a utilização da estrutura de dependência espacial identificada nas amostras para inferir, com o menor erro possível, o comportamento da variável nos locais onde não foi realizada qualquer medição. Foi precisamente esta mudança de paradigma e robustez preditiva que permitiu resolver o problema original das minas de ouro, impulsionando a subsequente expansão da disciplina para a hidrologia, ciências do solo, meteorologia e epidemiologia, onde se consolidou como a ferramenta padrão para lidar com variáveis contínuas no espaço (Yamamoto e Landim 2013; Nhancololo et al. 2024).\nPara operar matematicamente sobre um fenômeno natural único, utilizamos o conceito de Variável Regionalizada. É crucial estabelecer uma distinção notacional usada aqui: denotamos por \\(y(\\mathbf{s})\\) o valor numérico observado do fenômeno no local \\(\\mathbf{s}\\) (a realização concreta, com letra minúscula), e por \\(Y(\\mathbf{s})\\) a variável aleatória no local \\(\\mathbf{s}\\) (o processo probabilístico antes da observação, com letra maiúscula). Embora na realidade \\(y(\\mathbf{s})\\) seja único e fixo, a geoestatística modela-o como uma realização de um Processo Estocástico (ou Função Aleatória/Campo Aleatório) \\(Y(\\mathbf{s})\\).\nVariável Regionalizada\nUma variável regionalizada é uma função numérica \\(f(\\mathbf{s})\\) que descreve a distribuição espacial de uma grandeza física (ex: teor de ouro, pH do solo) num domínio \\(D^{G}\\). Esta função possui propriedades duais: um aspeto estruturado (refletindo tendências geológicas ou climáticas de larga escala) e um aspeto aleatório (refletindo irregularidades locais imprevisíveis) (George Matheron 1971). Matematicamente, tratamos estas variáveis regionalizadas como realizações de um processo estocástico (ou campo aleatório) \\(\\{Y(s) : s \\in D^{G}\\}\\), onde \\(s\\) denota um vetor de coordenadas em um domínio espacial \\(D^G \\subseteq \\mathbb{R}^d\\) (geralmente \\(d=2\\) ou \\(3\\)) (Cressie 1991)\nProcesso Estocástico\nUm processo estocástico espacial é definido como uma coleção de variáveis aleatórias \\(\\{Y(\\mathbf{s}) : \\mathbf{s} \\in D^{G}\\}\\), onde \\(D^{G} \\subseteq D \\subseteq \\mathbb{R}^d\\) é um conjunto de índices contínuo (uma área ou volume) com medida de Lebesgue positiva (área \\(&gt;0\\)) (Cressie 1993). O objetivo da inferência geoestatística é reconstruir a lei de probabilidade do processo \\(Y(\\mathbf{s})\\) a partir de um conjunto finito de observações \\(\\{y(\\mathbf{s}_1), \\dots, y(\\mathbf{s}_n)\\}\\).\nAo contrário da análise de séries temporais, onde o índice \\(t\\) possui uma ordenação natural (passado \\(\\to\\) futuro), o índice espacial \\(s\\) não possui uma ordenação única em \\(\\mathbb{R}^d\\) para \\(d \\ge 2\\), o que introduz complexidades adicionais na modelação da dependência multidirecional (Cressie e Moores 2022).\nPara tornar o processo estocástico tratável, decompomos a variável aleatória \\(Y(\\mathbf{s})\\) em componentes que explicam diferentes escalas de variação. Existem duas formulações principais: a decomposição simples e a decomposição estrutural completa.\nNa decomposição simples assumimos que o processo estocástico é constituído por uma média determinística e um erro correlacionado:\n\\[Y(\\mathbf{s}) = \\mu(\\mathbf{s}) + \\delta(\\mathbf{s}), \\tag{3.1}\\]\nOnde \\(\\mu(\\mathbf{s}) \\equiv E[Y(\\mathbf{s})]\\) representa a tendência de larga escala (Trend ou Drift), isto é, variação sistemática do fenômeno sobre o domínio \\(DĜ\\). Assume-se frequentemente que \\(\\mu(\\mathbf{s})\\) é uma função suave das coordenadas ou uma combinação linear de covariáveis externas \\(X(\\mathbf{s})\\), tal que \\(\\mu(\\mathbf{s}) = \\mathbf{x}(\\mathbf{s})^\\top \\boldsymbol{\\beta}\\) (Cressie e Moores 2022). \\(\\delta(\\mathbf{s})\\): Representa a variação de pequena escala ou o erro estocástico. Assume-se que este termo tem média zero, \\(E[\\delta(\\mathbf{s})] = 0\\), e captura a dependência espacial estatística (correlação) entre locais vizinhos.\nA formulação usada na Eq. 3.1 é insuficiente pois não distingue entre a variabilidade natural do fenômeno e o erro humano. Cressie (1991) e Diggle, Tawn, e Moyeed (1998), sugerem expandir o termo de erro (\\(\\delta(\\mathbf{s})\\):\n\\[Y(\\mathbf{s}) = \\mu(\\mathbf{s}) + W(\\mathbf{s}) + \\eta(\\mathbf{s}) + \\epsilon(\\mathbf{s}) \\tag{3.2}\\]\nOnde:\nConforme descrito por Diggle, Tawn, e Moyeed (1998) referem-se a componentes similares num contexto de modelos lineares generalizados mistos, onde a heterogeneidade latente de pequena escala deve ser contabilizada.\nA soma das variâncias dos dois últimos componentes (\\(\\eta (s)\\) e \\(\\epsilon (s)\\)) constitui o que chamamos de Efeito Pepita (\\(c_0\\)), visível no variograma experimental (assunto da próxima seção) como uma descontinuidade na origem (\\(\\gamma(h) \\to c_0\\) quando \\(h \\to 0\\)):\nCódigo\npacman::p_load(tidyr)\n\ngrid_df$tendencia &lt;- (grid_df$x + grid_df$y) / 20 \n\ngrid_df$residuo &lt;- grid_df$geo\n\ngrid_df$Y &lt;- grid_df$tendencia + grid_df$residuo\n\ndf_long &lt;- pivot_longer(grid_df, cols = c(Y, tendencia, residuo), names_to = \"Componente\", values_to = \"Valor\")\n\ndf_long$Componente &lt;- factor(df_long$Componente, \n                             levels = c(\"Y\", \"tendencia\", \"residuo\"),\n                             labels = c(\"'Fenômeno Observado' ~ Y(s)\", \n                                        \"'Tendência' ~ mu(s)\", \n                                        \"'Resíduo estocástico' ~ delta(s)\"))\n\nggplot(df_long, aes(x, y, fill = Valor)) +\n  geom_tile() +\n  facet_wrap(~Componente, labeller = label_parsed) +\n  scale_fill_viridis_c(option = \"C\") +\n  coord_fixed() + \n  theme_void() +\n  theme(strip.text = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\nFigura 3.2: Decomposição de uma variável regionalizada: Tendência global + resíduo estocástico",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#variável-regionalizada-e-o-processo-estocástico",
    "href": "geostat.html#variável-regionalizada-e-o-processo-estocástico",
    "title": "3  Geoestatística",
    "section": "",
    "text": "\\(\\mu(\\mathbf{s})\\) continua sendo componente determinística. Podendo ser modelada como uma constante, \\(\\mu(\\mathbf{s})=\\mu\\) (Krigagem simples e ordinária), um polinómio das coordenadas, \\(Y(s) =\\beta_0 + \\beta_1 s_x + \\beta_2 s_y\\), (Krigagem Universal) ou função de covariáveis externas, como elevação (Krigagem com Deriva Externa) (Wackernagel 2003). Captura forçantes globais (ex: o gradiente de temperatura causado pela latitude).\n\\(W(\\mathbf{s})\\)) é o componente estocástico de interesse principal. É um processo estocástico com média zero e continuidade em média quadrática (\\(L_2\\)-contínuo). \\(E[(W(\\mathbf{s+h}) - W(\\mathbf{s}))^2] \\to 0\\) quando \\(\\|\\mathbf{h}\\| \\to 0\\). Este componente captura a estrutura de dependência espacial observável na escala da amostragem. Em abordagens modernas de baixo posto este termo é frequentemente modelado por uma expansão de funções de base, \\(W(\\mathbf{s}) \\approx \\sum \\alpha_j \\phi_j(\\mathbf{s})\\) (sugestão de leitura Cressie, Sainsbury-Dale, e Zammit-Mangion (2022) ).\n\\(\\eta(\\mathbf{s})\\)) representa a variabilidade do fenômeno que ocorre a distâncias menores do que a menor distância de separação entre as observações disponíveis (\\(\\min \\|\\mathbf{s}_i - \\mathbf{s}_j\\|\\)). É uma variação intrínseca e física do fenômeno, não um erro. No entanto, dado que não temos dados suficientes para resolver esta continuidade, modelamo-la estatisticamente como um ruído branco espacialmente não correlacionado na escala de observação. Exemplo: Se medirmos o teor de ouro a cada 10 metros, a variação extrema que ocorre dentro de uma pepita de 1 cm é classificada como variação de microescala (\\(\\eta\\)).\n\n\n\n\\(\\epsilon(\\mathbf{s})\\) é ruído branco puro (\\(\\epsilon \\overset{iid}{\\sim} N[0, \\text{Var}(\\epsilon(\\mathbf{s}))]\\)), introduzido pelo processo de observação (precisão do instrumento, erro de laboratório, erro de localização). É puramente aleatório e não tem realidade no fenômeno natural \\(S(\\mathbf{s})\\) que estamos a tentar estudar.",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#estacionariedade-e-inferência-estatística",
    "href": "geostat.html#estacionariedade-e-inferência-estatística",
    "title": "3  Geoestatística",
    "section": "3.2 Estacionariedade e Inferência Estatística",
    "text": "3.2 Estacionariedade e Inferência Estatística\nO obstáculo central na inferência espacial é a unicidade da realização: na prática, possuímos apenas um único conjunto de dados observados \\(\\{y(\\mathbf{s}) : \\mathbf{s} \\in D\\}\\), que constitui apenas uma das infinitas realizações possíveis do processo estocástico gerador \\(\\{Y(\\mathbf{s})\\}\\). Diferentemente de experiências laboratoriais controladas, não é possível replicar o processo geológico ou climático sob as mesmas condições para gerar múltiplas realizações e, assim, estimar a função de distribuição conjunta para qualquer conjunto finito de \\(k\\) localizações: \\[F(y_1, \\dots, y_k; \\mathbf{s}_1, \\dots, \\mathbf{s}_k) = P\\left(Y(\\mathbf{s}_1) \\le y_1, \\dots, Y(\\mathbf{s}_k) \\le y_k\\right),\\]\nNem tão-pouco é possível calcular empiricamente os seus momentos de primeira \\(\\mu(\\mathbf{s})\\), \\[\\mu(\\mathbf{s}) = E[Y(\\mathbf{s})] = \\int_{-\\infty}^{+\\infty} y \\cdot f(y; \\mathbf{s}) \\, dy\\:, \\] e segunda ordem \\(\\sigma^2(\\mathbf{s})\\), \\[\\sigma^2(\\mathbf{s}) = \\text{Var}(Y(\\mathbf{s})) = E\\left[(Y(\\mathbf{s}) - \\mu(\\mathbf{s}))^2\\right], \\] em cada local \\(\\mathbf{s}\\) (Cressie 1993).\nPara viabilizar a inferência estatística (i.e., estimar os parâmetros do processo a partir de uma única realização), é imperativo invocar a hipótese de estacionariedade. Este conceito assume a invariância das propriedades estatísticas (momentos da distribuição) do processo sob translação no domínio espacial \\(D^G\\) Seção 2.3. Sob esta premissa, assumimos que a estrutura de dependência é homogénea em todo o domínio, o que nos permite utilizar repetições espaciais, diferentes pares de pontos separados pelo mesmo vetor \\(\\mathbf{h}\\) em locais distintos, como substitutos válidos para as inexistentes repetições estocásticas. Este procedimento fundamenta-se na hipótese de ergodicidade, que estabelece a condição necessária para estimar parâmetros probabilísticos a partir de uma única realização observada. Sob condições específicas de mistura (onde a correlação espacial decai suficientemente rápido com a distância), a ergodicidade garante que as médias espaciais calculadas sobre o domínio \\(D^G\\) convirjam para a esperança matemática (média populacional) à medida que o volume ou área do domínio de observação cresce indefinidamente (\\(|D^G| \\to \\infty\\)) (Cressie 1993). Esta convergência, é em média quadrática (ou convergência em \\(L^2\\)). Seja \\(\\bar{Y}_D\\) a média espacial (se existir) do processo no domínio \\(D\\), definida como \\(\\bar{Y}_{D^G} = \\frac{1}{|D|} \\int_{D^G} Y(\\mathbf{s}) d\\mathbf{s}\\). A propriedade ergódica assegura que o erro quadrático médio entre a média espacial e a média teórica \\(\\mu\\) tende para zero:\n\\[\\lim_{|D^G| \\to \\infty} E\\left[ \\left( \\bar{Y}_{D^G} - \\mu \\right)^2 \\right] = 0 , \\Longleftrightarrow \\bar{Y}_{D^G} \\xrightarrow{L^2} \\mu\\] Esta convergência implica que a variância da média espacial diminui assintoticamente, permitindo que as estatísticas calculadas sobre uma única realização extensa sejam estimadores consistentes dos momentos do processo estocástico gerador (George Matheron 1971; Cressie 1989).\nConforme descrito na Seção 2.3 existem dois níveis principais de estacionariedade utilizados na modelação geoestatística:\nA estacionaridade mais comum em análise de séries temporais e estatística espacial é a estacionariedade de segunda ordem (ou fraca). Um processo estocástico \\(\\{Y(\\mathbf{s})\\}\\) diz-se estacionário de segunda ordem se satisfizer duas condições simultâneas:\n\nA esperança matemática deve ser constante e finita em todo o domínio \\(D^G\\), independentemente da localização \\(\\mathbf{s}\\). O parâmetro \\(\\mu\\) representa o nível global em torno do qual o processo flutua:\n\n\\[E[Y(\\mathbf{s})] = \\mu, \\forall \\mathbf{s} \\in D^G\\].\n\nA covariância entre dois pontos não depende das suas localizações absolutas \\(\\mathbf{s}\\) e \\(\\mathbf{s}+\\mathbf{h}\\), mas apenas do vetor de separação \\(\\mathbf{h}\\) (que define a distância e a direção entre eles):\n\n\\[Cov(Y(\\mathbf{s}), Y(\\mathbf{s}+\\mathbf{h})) = E[(Y(\\mathbf{s}) - \\mu)(Y(\\mathbf{s}+\\mathbf{h}) - \\mu)] = C(\\mathbf{h}) \\tag{3.3}\\]\nonde \\(C(\\mathbf{h})\\) denota a função de covariância. Esta definição implica necessariamente que a variância do processo é finita e constante, dada por \\(Var(Y(\\mathbf{s})) = C(\\mathbf{0}) &lt; \\infty\\).\nUma consequência analítica imediata da definição acima (Eq. 3.3) é a estabilidade da variância. Se considerarmos o caso particular onde o vetor de separação é nulo (\\(\\mathbf{h} = \\mathbf{0}\\)), a covariância de um ponto com ele próprio torna-se, por definição, a sua variância. Como \\(C(\\mathbf{h})\\) não depende da localização \\(\\mathbf{s}\\), segue-se que \\(C(\\mathbf{0})\\) também não depende.\nPortanto, a variância do processo é finita e constante em todo o domínio (propriedade de homocedasticidade espacial), sendo definida por:\n\\[Cov(Y(\\mathbf{s}), Y(\\mathbf{s+0}))=Var(Y(\\mathbf{s})) = C(\\mathbf{0}) = \\sigma^2\\]\nEsta relação (\\(C(\\mathbf{0}) = \\sigma^2\\)) estabelece que o “patamar” máximo de variabilidade do processo é fixo e conhecido a priori. Se a variabilidade do fenômeno crescer indefinidamente com a área (como na topografia de uma cadeia montanhosa), a condição \\(C(\\mathbf{0}) &lt; \\infty\\) é violada, e a estacionariedade de segunda ordem não pode ser assumida, exigindo a adoção da hipótese intrínseca.\nMuitos fenômenos naturais, como a dispersão de poluentes ou a topografia, apresentam uma variabilidade que cresce sem limites à medida que a área de estudo aumenta, violando a condição de variância a priori finita. Para acomodar estes processos, G. Matheron (1963) introduziu uma condição mais fraca e generalista: a estacionariedade intrínseca. Esta hipótese exige apenas a estacionariedade dos incrementos do processo \\(Y(\\mathbf{s}+\\mathbf{h}) - Y(\\mathbf{s})\\), definindo-se pelas seguintes propriedades:\n\\[\\begin{aligned}\nE[Y(\\mathbf{s}+\\mathbf{h}) - Y(\\mathbf{s})] &= 0 \\\\\nVar(Y(\\mathbf{s}+\\mathbf{h}) - Y(\\mathbf{s})) &= 2\\gamma(\\mathbf{h})\n\\end{aligned}\\]\nA formulação intrínseca é vantajosa pois abrange uma classe mais ampla de processos estocásticos, incluindo aqueles com variância infinita (como o movimento Browniano), onde a função de covariância \\(C(\\mathbf{h})\\) não estaria definida, mas o variograma está perfeitamente caracterizado.\nA função \\(2\\gamma(\\mathbf{h})\\) é definida formalmente como o variograma, e \\(\\gamma(\\mathbf{h})\\) como o semivariograma.\nPartindo da definição do variograma \\(\\gamma(\\mathbf{h})\\) como a variância do incremento, o desenvolvimento segue:\n\\[\\begin{aligned}\n2\\gamma(\\mathbf{h}) &= Var(Y(\\mathbf{s}+\\mathbf{h}) - Y(\\mathbf{s})) \\\\\n&= E\\left[ \\left( \\{Y(\\mathbf{s}+\\mathbf{h}) - Y(\\mathbf{s})\\} - E[Y(\\mathbf{s}+\\mathbf{h}) - Y(\\mathbf{s})] \\right)^2 \\right] && \\text{(Definição de Variância)} \\\\\n&= E\\left[ \\left( Y(\\mathbf{s}+\\mathbf{h}) - Y(\\mathbf{s}) \\right)^2 \\right] && \\text{(Pois } E[Y(\\mathbf{s}+\\mathbf{h}) - Y(\\mathbf{s})] = 0 \\text{)} \\\\\n&= E\\left[ \\left( \\{Y(\\mathbf{s}+\\mathbf{h}) - \\mu\\} - \\{Y(\\mathbf{s}) - \\mu\\} \\right)^2 \\right] && \\text{(Centrando na média } \\mu \\text{)} \\\\\n&= E\\left[ (Y(\\mathbf{s}+\\mathbf{h}) - \\mu)^2 - 2(Y(\\mathbf{s}+\\mathbf{h}) - \\mu)(Y(\\mathbf{s}) - \\mu) + (Y(\\mathbf{s}) - \\mu)^2 \\right] && \\text{(Expandindo o quadrado)} \\\\\n&= E[(Y(\\mathbf{s}+\\mathbf{h}) - \\mu)^2] - 2E[(Y(\\mathbf{s}+\\mathbf{h}) - \\mu)(Y(\\mathbf{s}) - \\mu)] + E[(Y(\\mathbf{s}) - \\mu)^2] && \\text{(Linearidade da Esperança)} \\\\\n&= Var(Y(\\mathbf{s}+\\mathbf{h})) - 2Cov(Y(\\mathbf{s}+\\mathbf{h}), Y(\\mathbf{s})) + Var(Y(\\mathbf{s})) && \\text{(Definições de Var e Cov)} \\\\\n&= C(0) - 2C(\\mathbf{h}) + C(0) && \\text{(Estacionariedade de 2ª ordem)} \\\\\n&= 2(C(0) - C(\\mathbf{h}))\n\\end{aligned} \\tag{3.4}\\]\nPortanto, simplificando por 2, obtemos a relação fundamental para processos estacionários:\n\\[\\gamma(\\mathbf{h})= C(0) - C(\\mathbf{h}) \\tag{3.5}\\]\nA relação entre estas duas abordagens é hierárquica. Se um processo for estacionário de segunda ordem, ele é necessariamente intrínseco, existindo uma relação analítica direta que conecta o semivariograma \\(\\gamma(\\mathbf{h})\\) à função de covariância Eq. 3.5. Esta equação revela que o semivariograma é a imagem espelhada da covariância: enquanto a covariância \\(C(\\mathbf{h})\\) decresce com a distância (de \\(\\sigma^2\\) para assintoticamente 0), o semivariograma \\(\\gamma(\\mathbf{h})\\) cresce com a distância (de 0 para um patamar \\(\\sigma^2\\)). No entanto, a formulação intrínseca é mais robusta, pois o variograma \\(2\\gamma(\\mathbf{h})\\) permanece definido mesmo para processos com variância infinita onde a covariância \\(C(\\mathbf{h})\\) não existe, o que justifica a preferência de Matheron pelo variograma como ferramenta fundamental de análise estrutural Figura 3.3 .\n\n\nCódigo\nnugget &lt;- 2\npartial_sill &lt;- 8\nrange_val &lt;- 20\nsill &lt;- nugget + partial_sill\n\nh &lt;- seq(0, 30, by = 0.1)\n\ngamma &lt;- ifelse(h == 0, 0,\n         ifelse(h &lt;= range_val, \n                nugget + partial_sill * (1.5 * (h/range_val) - 0.5 * (h/range_val)^3),\n                sill))\n\ngamma[1] &lt;- nugget \n\ndf_vgm &lt;- data.frame(Distancia = h, Semivariancia = gamma)\n\n\ndf_vgm$Covariancia &lt;- sill - df_vgm$Semivariancia\n\ndf_comp &lt;- pivot_longer(df_vgm, cols = c(Semivariancia, Covariancia), names_to = \"Funcao\", values_to = \"Valor\")\nggplot(df_comp, aes(x = Distancia, y = Valor, color = Funcao)) +\n  geom_line(size = 1) +\n  scale_color_manual(values = c(\"Covariancia\" = \"darkred\", \"Semivariancia\" = \"steelblue\"),\n                     labels = c(expression(C(h)), expression(gamma(h)))) +\n  labs(y = \"Valor\", x = \"Distância (h)\", color = \"Função:\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFigura 3.3: Covariância vs. Semivariograma em Processo Estacionário.",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#sec-efeito_pepita",
    "href": "geostat.html#sec-efeito_pepita",
    "title": "3  Geoestatística",
    "section": "3.3 Efeito Pepita e suas Implicações na Predição",
    "text": "3.3 Efeito Pepita e suas Implicações na Predição\nA caracterização da continuidade espacial de um fenômeno é frequentemente realizada através do variograma experimental (ver secção seguinte). Em muitos casos, observa-se que, à medida que a distância de separação (\\(\\|\\mathbf{h}\\|\\)) entre dois pontos tende a zero, o valor do semivariograma (\\(\\gamma(\\mathbf{h})\\)) não converge para zero, mas sim para um valor positivo. Esta descontinuidade na origem é denominada Efeito Pepita (\\(c_0\\) ou nugget effect). O termo Efeito Pepita (ou Nugget Effect) tem origem histórica nas minas de ouro sul-africanas. Matheron observou que, mesmo quando duas amostras eram recolhidas muito próximas uma da outra, os seus valores podiam diferir devido à presença de pepitas, microscópicas de ouro distribuídas aleatoriamente.\nCressie (1993) define este parâmetro como a soma das variâncias das duas componentes descritas acima:\n\\[c_0 = \\lim_{\\|\\mathbf{h}\\| \\to 0} \\gamma(\\mathbf{h}) = \\text{Var}(\\eta(\\mathbf{s})) + \\text{Var}(\\epsilon(\\mathbf{s}))\\]\nEsta distinção permite-nos definir o “sinal” ou processo latente \\(S(\\mathbf{s})\\) que desejamos recuperar, livre do erro de medição instrumental:\n\\[S(\\mathbf{s}) = \\mu(\\mathbf{s}) + W(\\mathbf{s}) + \\eta(\\mathbf{s})\\]\nConsequentemente, os dados observados são compostos pelo sinal mais o erro de medição: \\(Y(\\mathbf{s}) = S(\\mathbf{s}) + \\epsilon(\\mathbf{s})\\). Esta formulação alinha-se com a Geoestatística Baseada em Modelos (Diggle, Tawn, e Moyeed 1998), que assume uma hierarquia:\n\nModelo de Processo: \\([S(\\cdot)]\\) (descreve a física do fenômeno).\nModelo de Dados: \\([Y(\\cdot) | S(\\cdot)]\\) (descreve a observação ruidosa desse fenômeno).\n\nCressie (1993) e Laslett (1994) destacam uma implicação fundamental desta decomposição. As equações de predição (Krigagem) devem ser ajustadas dependendo do nosso objetivo final:\n\nPredição do dado observável (\\(Y\\)): Se o objetivo é prever o valor que um sensor mediria no local \\(\\mathbf{s}_0\\) (incluindo o erro inerente ao sensor), utilizamos a Krigagem Exata. Este interpolador respeita os dados originais e incorpora todo o \\(c_0\\) na variabilidade estimada.\nPredição do processo latente (\\(S\\)): Se o objetivo é prever o valor físico real do fenômeno, filtrado do ruído instrumental, utilizamos a Krigagem com Suavização. Isto é feito subtraindo a variância do erro de medição (\\(\\text{Var}(\\epsilon)\\)) da diagonal da matriz de covariância do sistema de krigagem.\n\nSe ignorarmos esta distinção e tratarmos todo o efeito pepita como variabilidade natural (assumindo \\(\\text{Var}(\\epsilon)=0\\)), os nossos mapas serão desnecessariamente ruidosos (“rugosos”) e respeitarão erros de medição como se fossem verdades. Por outro lado, se tratarmos todo o pepita como erro, obteremos mapas mais suaves.\n\n\n\n\n\n\nImportanteO Problema da Identificabilidade\n\n\n\nÉ importante notar que, sem medições repetidas no mesmo local (co-localizadas), é impossível separar estatisticamente o quanto do \\(c_0\\) se deve a \\(\\eta(s)\\) (microescala) e o quanto se deve a \\(\\epsilon (s)\\) (erro). Cressie (1993) alerta que esta distinção é frequentemente uma escolha de modelagem baseada no conhecimento do equipamento, e não uma inferência puramente estatística.\n\n\n\n\n\n\n\n\nNotaTendência e Erro\n\n\n\nWackernagel (2003) demonstra que, em domínios pequenos, uma tendência local pode ser indistinguível de uma flutuação estocástica de baixa frequência: A estrutura determinística média de uma pessoa pode ser a estrutura de erro correlacionado de outra.",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#sec-variograma",
    "href": "geostat.html#sec-variograma",
    "title": "3  Geoestatística",
    "section": "3.4 Variograma e funções de covariância",
    "text": "3.4 Variograma e funções de covariância\nUma vez assumida a hipótese de estacionariedade, a inferência geoestatística exige a quantificação da dependência espacial do processo estocástico \\(Y(\\mathbf{s})\\). Diferentemente da estatística clássica, onde a correlação é frequentemente um escalar único, na geoestatística a dependência é modelada como uma função contínua do vetor de separação \\(\\mathbf{h} \\in \\mathbb{R}^d\\) entre pares de pontos.\nA caracterização desta estrutura é realizada através de duas ferramentas fundamentais, cuja aplicabilidade depende do nível de estacionariedade assumido: a Função de Covariância (Estacionariedade de 2.ª Ordem) e o Variograma (Estacionariedade Intrínseca).\nFunção de Covariância\nA função de covariância \\(C(\\mathbf{h})\\) quantifica a covariância linear entre os valores do processo estocástico \\(Y(\\mathbf{s})\\) em dois locais \\(\\mathbf{s}\\) e \\(\\mathbf{s} + \\mathbf{h}\\), separados por um vetor de distância \\(\\mathbf{h} \\in \\mathbb{R}^d\\):\n\\[C(\\mathbf{h}) = \\text{Cov}(Y(\\mathbf{s}), Y(\\mathbf{s} + \\mathbf{h})) = E[(Y(\\mathbf{s}) - \\mu)(Y(\\mathbf{s} + \\mathbf{h}) - \\mu)]\\]\nonde \\(\\mu\\) é a média estacionária do processo Cressie (1993). Sob a hipótese de estacionariedade de segunda ordem, a covariância depende apenas de \\(\\mathbf{h}\\) e é simétrica em relação à origem, ou seja, \\(C(\\mathbf{h}) = C(-\\mathbf{h})\\).\nComo descrito anteriormente, uma consequência fundamental desta definição é que a covariância na origem, \\(C(\\mathbf{0})\\), corresponde à variância a priori do processo, \\(\\sigma^2_Y\\), que se assume finita e constante em todo o domínio \\(D^G\\), \\(C(\\mathbf{0}) = \\text{Var}(Y(\\mathbf{s})) = \\sigma^2_Y\\).\nPara garantir a validade estatística das predições (Krigagem), a função de covariância deve ser positiva definida, o que implica que a variância de qualquer combinação linear das variáveis aleatórias seja não negativa George Matheron (1971).\nVariograma\nEm situações nas quais a variância do processo não é finita, a função de covariância não pode ser definida (por exemplo: movimento Browniano, topografia em grandes escalas, etc.). Esta condição de não-finitude, central na formulação da hipótese intrínseca por George Matheron (1971), não implica que um valor pontual seja infinito, mas sim que a dispersão a priori do processo cresce indefinidamente à medida que o domínio de observação se expande (\\(|D| \\to \\infty\\)). Um exemplo clássico é o movimento Browniano unidimensional (processo de Wiener), denotado por \\(Y(t)\\), em que a variância da posição da partícula no instante \\(t\\) é linearmente proporcional ao tempo decorrido, expressa por \\(\\text{Var}(Y(t)) = \\sigma^2 t\\). Consequentemente, num domínio temporal ilimitado (\\(t \\to \\infty\\)), a variância global do processo tende ao infinito (\\(\\sigma^2_{global} \\to \\infty\\)), tornando matematicamente impossível a definição de um patamar \\(C(0)\\) ou de uma função de covariância estacionária.\nNesses casos, onde a estacionariedade de segunda ordem é violada pela ausência de variância finita, utiliza-se o variograma \\(2\\gamma(\\mathbf{h})\\). Essa ferramenta baseia-se na hipótese de estacionaridade intrínseca introduzida por G. Matheron (1963), assumindo estacionariedade apenas para os incrementos do processo. Isto implica que os incrementos permanecem finitos e estáveis, mesmo quando a variância absoluta diverge. O variograma é definido como a variância da diferença entre os valores da variável em dois locais separados por um vetor \\(\\mathbf{h}\\) e, assumindo que a média dos incrementos é zero, equivale ao valor esperado do quadrado dessas diferenças:\n\\[2\\gamma(\\mathbf{h}) = \\text{Var}(Y(\\mathbf{s} + \\mathbf{h}) - Y(\\mathbf{s})) = E[(Y(\\mathbf{s} + \\mathbf{h}) - Y(\\mathbf{s}))^2]\\]\nO semivariograma \\(\\gamma(\\mathbf{h})\\) corresponde à metade do variograma. Sob a hipótese intrínseca, assume-se que o variograma depende apenas do vetor de separação \\(\\mathbf{h}\\), garantindo a caracterização da continuidade espacial mesmo sem variância global definida. Contudo, se o processo satisfizer a estacionariedade de segunda ordem (onde \\(C(\\mathbf{0})\\) existe e é finito), estabelece-se uma relação analítica direta entre o variograma e a função de covariância (ver a dedução na Eq. 3.4).\nElementos do Semivariograma\nAntes de procedermos à modelagem teórica, é crucial compreender os parâmetros que constituem o perfil de um variograma pois a correta identificação destes parâmetros condiciona diretamente a estrutura de covariância utilizada na predição espacial (Yamamoto e Landim 2013).\n\nAlcance (\\(a\\) ou Range)\n\nO alcance define o limite da dependência espacial. É a distância no eixo das abcissas (\\(\\|\\mathbf{h}\\|\\)) a partir da qual a correlação entre observações se torna desprezível e o variograma estabiliza. Em termos práticos, pontos separados por uma distância \\(\\|\\mathbf{h}\\| \\ge a\\) são considerados estatisticamente independentes (ou não correlacionados). Este parâmetro impõe o critério crítico para a amostragem: para capturar a estrutura do fenômeno, a malha de amostragem deve possuir um espaçamento inferior ao alcance. Caso contrário, qualquer interpolação entre os pontos será meramente especulativa, uma vez que, além desta fronteira, a predição reverte estatisticamente para a média global do processo.\n\nEfeito Pepita (\\(C_0\\) ou Nugget Effect)\n\nTeoricamente, pela definição de variância de um incremento nulo, \\(\\gamma(\\mathbf{0}) = 0\\). Contudo, o variograma experimental frequentemente exibe uma descontinuidade na origem, intercetando o eixo das ordenadas num valor positivo \\(C_0 &gt; 0\\). Como descrito na secção Seção 3.3, Cressie (1993) e Diggle, Tawn, e Moyeed (1998) decompõem este parâmetro na soma de duas fontes de variabilidade que operam em escalas sub-amostrais: a variabilidade de microescala e o erro de medição (\\(C_0 =\\text{Var}(\\eta(\\mathbf{s})) + \\text{Var}(\\epsilon(\\mathbf{s}))\\)). Embora seja um parâmetro a modelar, é desejável que a sua magnitude seja reduzida em comparação com a variância total (\\(C_0 &lt;C\\)), indicando que o ruído não domina o sinal espacial.\n\nContribuição (\\(C\\) ou Partial Sill)\n\nA Contribuição representa a componente da variância que é explicitamente explicada pela estrutura de dependência espacial. Corresponde à amplitude do crescimento do variograma, ou seja, a diferença entre o valor onde a função estabiliza e o ponto onde intercepta o eixo \\(Y\\) (Efeito Pepita). É neste segmento da curva que reside a informação espacial útil para a krigagem: quanto maior for o valor de \\(C\\) em relação a \\(C_0\\), mais forte e contínuo é o padrão espacial do fenômeno.\n\nPatamar (\\(C_0 + C\\) ou Sill)\n\nO Patamar é o valor assintótico onde a função \\(\\gamma(\\mathbf{h})\\) estabiliza. Sob a hipótese de estacionariedade de segunda ordem, este valor corresponde teoricamente à variância total a priori do processo (\\(\\sigma^2 = C(\\mathbf{0})\\)). Existe uma relação analítica de aditividade que conecta os elementos de variância descritos acima:\n\\[\\text{Patamar} = \\text{Efeito Pepita} (C_0) + \\text{Contribuição} (C)\\]\nSe o variograma não estabilizar num patamar e continuar a crescer indefinidamente, isso indica que o processo não possui variância finita (é intrínseco) ou que existe uma tendência (drift) não removida nos dados.\nInterpretação Gráfica\nPara visualizar estes componentes no gráfico do semivariograma, observa-se o comportamento da curva \\(\\gamma(\\mathbf{h})\\) em relação aos eixos cartesianos. O gráfico inicia-se no eixo Y à altura do Efeito Pepita (\\(C_0\\)). À medida que a distância \\(\\mathbf{h}\\) (eixo X) aumenta, a semivariância cresce com uma amplitude igual à Contribuição (\\(C\\)). A curva cessa o seu crescimento quando a distância atinge o Alcance (\\(a\\)), momento em que a função se torna horizontal, fixando-se no valor do Patamar (\\(C_0 + C\\)). Portanto, a altura total da curva representa a variabilidade total dos dados, a qual é particionada em variabilidade não explicada (pepita) e variabilidade estruturada (contribuição) Figura 3.4.\n\n\n\n\n\n\nNota\n\n\n\nUma discussão aprofundada sobre as implicações do efeito pepita na escolha entre Krigagem Exata e Krigagem com Suavização encontra-se na Seção 3.3.\n\n\n\n\nCódigo\nggplot(df_vgm, aes(x = Distancia, y = Semivariancia)) +\n  geom_line(color = \"black\", size = 1) +\n  geom_hline(yintercept = sill, linetype = \"dashed\", color = \"black\") +\n  geom_vline(xintercept = range_val, linetype = \"dotted\", color = \"black\") +\n  geom_hline(yintercept = nugget, linetype = \"dashed\", color = \"black\") +\n  annotate(\"text\", x = 25, y = sill + 0.5, label = \"Patamar (Sill)\", color = \"black\") +\n  annotate(\"text\", x = range_val + 0.5, y = 2, label = \"Alcance (Range)\", angle = 90, color = \"black\") +\n  geom_point(aes(x = 0, y = nugget), color = \"red\", size = 3) +\n  annotate(\"text\", x = 1, y = nugget - 0.5, label = \"Efeito Pepita (C0)\", color = \"black\", hjust = 0) +\n  annotate(\"curve\", \n           x = 1.5, y = nugget - 0.2,       \n           xend = 0.15, yend = nugget - 0.05, \n           arrow = arrow(length = unit(0.2, \"cm\"), type = \"closed\"), \n           color = \"black\", \n           curvature = 0.3) +\n  annotate(\"segment\", x = 28, xend = 28, y = nugget, yend = sill, \n           arrow = arrow(ends = \"both\", length = unit(0.2, \"cm\")), color = \"black\") +\n  annotate(\"text\", x = 28.5, y = (nugget + sill)/2 -2 , label = \"Contribuição (C)\", angle = 90, color = \"black\", hjust = 0) +\n  scale_y_continuous(limits = c(0, 12), breaks = seq(0, 12, 2)) +\n  scale_x_continuous(expand = c(0, 0), limits = c(0, 30)) +\n  labs(x = \"Distância de Separação (h)\", y = expression(gamma(h))) +\n  theme_classic()\n\n\n\n\n\n\n\n\nFigura 3.4: Elementos Teóricos do Semivariograma: Alcance, patamar, efeito pepita e contribuição.",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#estimadores-do-variograma",
    "href": "geostat.html#estimadores-do-variograma",
    "title": "3  Geoestatística",
    "section": "3.5 Estimadores do Variograma",
    "text": "3.5 Estimadores do Variograma\nA definição teórica do variograma, \\(2\\gamma(\\mathbf{h}) = E[(Y(\\mathbf{s}+\\mathbf{h}) - Y(\\mathbf{s}))^2]\\), pressupõe o conhecimento da distribuição de probabilidade conjunta do processo estocástico \\(Y(\\mathbf{s})\\). Contudo, na prática geoestatística, o investigador dispõe apenas de uma única realização finita do processo, materializada num conjunto discreto de observações amostrais \\(\\mathbf{y} = \\{y(\\mathbf{s}_1), y(\\mathbf{s}_2), \\dots, y(\\mathbf{s}_n)\\}\\). Consequentemente, a função teórica \\(\\gamma(\\mathbf{h})\\) é desconhecida e deve ser estimada empiricamente a partir destes dados. A qualidade desta estimativa é crítica, uma vez que o variograma experimental constitui a base para o ajuste do modelo teórico que alimentará as equações de krigagem.\nEstimador Experimental Clássico (Matheron)\nProposto por G. Matheron (1963), este estimador baseia-se no método dos momentos. Ele calcula a média dos quadrados das diferenças entre pares de pontos separados por um vetor \\(\\mathbf{h}\\) (dentro de uma determinada tolerância de distância e direção). O semivariograma experimental \\(\\hat{\\gamma}_{M}(\\mathbf{h})\\) é dado por:\n\\[\\hat{\\gamma}_{M}(\\mathbf{h}) = \\frac{1}{2|N(\\mathbf{h})|} \\sum_{(\\mathbf{s}_i, \\mathbf{s}_j) \\in N(\\mathbf{h})} (y(\\mathbf{s}_i) - y(\\mathbf{s}_j))^2 \\tag{3.6}\\]\nOnde \\(N(\\mathbf{h})\\) representa o conjunto de pares de localizações \\((\\mathbf{s}_i, \\mathbf{s}_j)\\) tal que a separação \\(\\mathbf{s}_i - \\mathbf{s}_j\\) se aproxima do vetor \\(\\mathbf{h}\\), e \\(|N(\\mathbf{h})|\\) denota a cardinalidade (número de pares) desse conjunto.\nEstimador Robusto (Cressie-Hawkins)\nEmbora o estimador clássico Eq. 3.6 seja não-viesado para processos Gaussianos, ele apresenta uma vulnerabilidade intrínseca: a elevação das diferenças ao quadrado, \\((y(\\mathbf{s}_i) - y(\\mathbf{s}_j))^2\\), amplifica desproporcionalmente o impacto de valores extremos. A presença de um único outlier ou erro grosseiro na amostragem pode inflar a variância estimada em determinadas classes de distância, distorcendo a estrutura de continuidade espacial e dificultando a modelagem do patamar e do alcance.\nEm resposta à sensibilidade do estimador clássico a dados contaminados e a distribuições de cauda longa, Cressie e Hawkins (1980) desenvolveram uma alternativa mais resiliente, conhecida como Estimador Robusto de Cressie-Hawkins. A premissa deste método reside na suavização das diferenças extremas através de uma transformação de raiz quadrada, aproximando a distribuição dos incrementos à normalidade antes do cálculo da média. A expressão para o estimador robusto \\(\\hat{\\gamma}_{CH}(\\mathbf{h})\\) é definida como:\n\\[\\hat{\\gamma}_{CH}(\\mathbf{h}) = \\frac{\\left( \\frac{1}{|N(\\mathbf{h})|} \\sum_{(\\mathbf{s}_i, \\mathbf{s}_j) \\in N(\\mathbf{h})} |y(\\mathbf{s}_i) - y(\\mathbf{s}_j)|^{1/2} \\right)^4}{0.457 + \\frac{0.494}{|N(\\mathbf{h})|}} \\tag{3.7}\\]\nO denominador na Eq. 3.7 atua como um fator de correção de viés para amostras finitas, garantindo a consistência estatística do estimador. A aplicação comparativa de ambos os estimadores constitui uma prática recomendada na fase de Análise Exploratória de Dados Espaciais (ESDA). Uma divergência significativa entre o perfil do variograma clássico e o do robusto especificamente, se \\(\\hat{\\gamma}_{M}(\\mathbf{h})\\) apresentar flutuações erráticas ou valores excessivamente elevados nas curtas distâncias em comparação com \\(\\hat{\\gamma}_{CH}(\\mathbf{h})\\), é um indicador forte da presença de outliers ou de não-normalidade severa nos dados Figura 3.5, sugerindo a necessidade de adoção da abordagem robusta para a modelagem ou verificar possíveis falhas resultantes da ação humana (Cressie 1993) .\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(ggplot2, gstat, sf, viridis, dplyr)\n\nset.seed(123)\ngrid_exemplo &lt;- expand.grid(x = 1:50, y = 1:50)\ngrid_sf_ex &lt;- st_as_sf(grid_exemplo, coords = c(\"x\", \"y\"))\n\nmodelo_verdadeiro &lt;- gstat::vgm(psill = 10, model = \"Exp\", range = 15, nugget = 2)\n\ng_sim &lt;- gstat::gstat(formula = z~1, locations = grid_sf_ex, dummy = TRUE, beta = 0, model = modelo_verdadeiro, nmax = 20)\nsimulacao &lt;- predict(g_sim, newdata = grid_sf_ex, nsim = 1)\n\n\n[using unconditional Gaussian simulation]\n\n\nCódigo\nsimulacao$z_contaminado &lt;- simulacao$sim1\n\n# 2. Introduzindo Outliers (Contaminação)\n# Adicionamos um erro grosseiro (+50) em apenas 0.2% dos dados\nset.seed(999) \nidx_outliers &lt;- sample(1:nrow(simulacao), 5)\nsimulacao$z_contaminado[idx_outliers] &lt;- simulacao$z_contaminado[idx_outliers] + 50\n\n#Variogramas\nvgm_classico &lt;- gstat::variogram(z_contaminado ~ 1, data = simulacao, cressie = FALSE)\nvgm_classico$Estimador &lt;- \"Clássico (Matheron)\"\n\nvgm_robusto &lt;- gstat::variogram(z_contaminado ~ 1, data = simulacao, cressie = TRUE)\nvgm_robusto$Estimador &lt;- \"Robusto (Cressie)\"\n\nvgm_comp &lt;- rbind(vgm_classico, vgm_robusto)\n\n\nggplot() +\n  geom_point(data = vgm_comp, aes(x = dist, y = gamma, color = Estimador), size = 3, alpha = 0.8) + \n  geom_line(data = vgm_comp, aes(x = dist, y = gamma, color = Estimador), \n            linewidth = 0.6, alpha = 0.6) +\n  stat_function(fun = function(h) 2 + 10 * (1 - exp(-h/15)), aes(linetype = \"Modelo Verdadeiro (Gerador)\"), \n                color = \"black\", linewidth = 1) +\n  scale_color_manual(values = c(\"Clássico (Matheron)\" = \"#D55E00\", \"Robusto (Cressie)\" = \"#0072B2\")) +\n  scale_linetype_manual(name = \"\", values = c(\"Modelo Verdadeiro (Gerador)\" = \"dashed\")) +\n  coord_cartesian(ylim = c(0, 25)) + \n  labs(x = \"Distância (h)\", \n       y = expression(Semivariância ~ gamma(h)), color=\"Estimador:\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFigura 3.5: Comparação de Robustez: O estimador de Matheron (laranja) é sensível aos outliers, superestimando a variância. O estimador de Cressie (azul) ignora a contaminação e ajusta-se quase perfeitamente ao Modelo Verdadeiro (tracejado).\n\n\n\n\n\nNa definição teórica, o variograma é uma função contínua calculada para vetores de distância exatos \\(\\mathbf{h}\\). Contudo, a realidade operacional dos levantamentos de campo impõe uma restrição fundamental: as amostras raramente estão dispostas numa grelha regular perfeita. Em dados reais, a probabilidade de encontrar múltiplos pares de pontos separados por uma distância vetorial exata (por exemplo, \\(\\|\\mathbf{h}\\| = 10.00\\) metros a \\(0^\\circ\\)) é, para todos os efeitos práticos, nula.\nSe tentássemos calcular o estimador \\(\\hat{\\gamma}(\\mathbf{h})\\) exigindo distâncias exatas, obteríamos apenas um par de pontos (ou nenhum) para cada distância única, resultando num gráfico caótico de ruído puro. Para viabilizar a inferência estrutural e garantir robustez estatística, é necessário proceder à regularização dos dados, agrupando os pares de pontos em classes de distância e direção, denominadas Lags (ou passos).\nCada ponto no variograma experimental representa, portanto, não uma distância única, mas a média estatística de todos os pares contidos num intervalo de tolerância \\([\\mathbf{h} - \\epsilon, \\mathbf{h} + \\epsilon]\\). A Figura 3.6 ilustra este processo de agrupamento: em torno de um ponto de referência \\(\\mathbf{s}_i\\) (a vermelho), definem-se anéis concêntricos com uma espessura definida. Todos os vizinhos que caem dentro de um determinado anel (a azul) são considerados como estando aproximadamente à mesma distância, contribuindo conjuntamente para o cálculo da semivariância média daquele lag.\nA definição correta da largura do lag e da tolerância angular constitui um compromisso delicado entre resolução e estabilidade:\n\nIntervalos demasiado estreitos: Resultam em classes com poucos pares (\\(|N(\\mathbf{h})|\\) baixo). Como a variância do estimador é inversamente proporcional ao número de pares, isto gera um variograma ruidoso, instável e de difícil interpretação.\nIntervalos demasiado largos: Suavizam excessivamente a estrutura espacial. Ao fazer a média de pares muito distantes entre si, mascara-se o comportamento do variograma nas curtas distâncias, o que pode levar a uma estimativa incorreta do Efeito Pepita e da microestrutura do fenômeno.\n\nAndre G. Journel e Huijbregts (1976) propõe, como regra empírica amplamente aceite, que o número de pares por lag não deve ser inferior a 30 para garantir a fiabilidade estatística da estimativa (Teorema do Limite Central). Adicionalmente, recomenda-se que a largura do lag seja coincidente com a distância média entre amostras vizinhas, maximizando assim o aproveitamento da informação disponível.\n\n\nCódigo\npacman::p_load(ggplot2,dplyr)\n\n\nset.seed(42)\nn_points &lt;- 60\ndf_points &lt;- data.frame(\n  id = 1:n_points,\n  x = runif(n_points, -10, 10),\n  y = runif(n_points, -10, 10)\n)\n\ncenter_pt &lt;- data.frame(x = 0, y = 0)\ndf_points$dist &lt;- sqrt((df_points$x - center_pt$x)^2 + (df_points$y - center_pt$y)^2)\nlag_width &lt;- 2.5   # Largura do Lag\nn_lags &lt;- 3        # Número de anéis para desenhar\n\ndf_points$lag_group &lt;- cut(df_points$dist, \n                           breaks = seq(0, 15, by = lag_width),\n                           labels = FALSE)\n\ntarget_lag &lt;- 2\ndf_points$status &lt;- case_when(\n  df_points$lag_group == target_lag ~ \"Pares no Lag Alvo\",\n  TRUE ~ \"Outros Pares\"\n)\n\ncreate_circle &lt;- function(r, center_x=0, center_y=0, npoints=100){\n  tt &lt;- seq(0, 2*pi, length.out = npoints)\n  data.frame(x = center_x + r * cos(tt), y = center_y + r * sin(tt), r = r)\n}\n\ncircles &lt;- do.call(rbind, lapply(seq(lag_width, lag_width*4, by=lag_width), create_circle))\nradius_inner &lt;- (target_lag - 1) * lag_width\nradius_outer &lt;- target_lag * lag_width\n\nggplot() +\n  annotate(\"path\", x=circles$x[circles$r==radius_outer], y=circles$y[circles$r==radius_outer], color=\"gray80\") +\n  annotate(\"path\", x=circles$x[circles$r==radius_inner], y=circles$y[circles$r==radius_inner], color=\"gray80\") +\n  geom_path(data = circles, aes(x, y, group = r), color = \"black\", linetype = \"dashed\", alpha = 0.5) +\n  geom_point(data = df_points, aes(x, y, color = status, shape = status), size = 3) +\n  geom_point(data = center_pt, aes(x, y), color = \"red\", size = 5, shape = 18) +\n  annotate(\"text\", x = 0.5, y = 0.5, label = \"s_i\", color = \"red\", vjust = -1, fontface=\"bold\") +\n  annotate(\"text\", x = 0, y = -lag_width * 1.5, label = \"Lag 1\") +\n  annotate(\"text\", x = 0, y = -lag_width * 2.5, label = paste(\"Lag\", target_lag, \"\\n(Intervalo Ativo)\")) +\n  annotate(\"segment\", x = 0, y = lag_width, xend = 0, yend = lag_width * 2, \n           arrow = arrow(length = unit(0.2, \"cm\"), ends = \"both\")) +\n  annotate(\"text\", x = 0.5, y = lag_width * 1.5, label = \"Largura\\ndo Lag\", hjust = 0, size = 3) +\n  scale_color_manual(values = c(\"Pares no Lag Alvo\" = \"blue\", \"Outros Pares\" = \"gray70\")) +\n  scale_shape_manual(values = c(\"Pares no Lag Alvo\" = 19, \"Outros Pares\" = 1)) +\n  coord_fixed(xlim = c(-8, 8), ylim = c(-8, 8)) +\n  theme_void() +\n  labs(color = \"Classificação:\", shape = \"Classificação\") +\n  theme(legend.position = \"bottom\", plot.title = element_text(hjust=0.5), plot.subtitle = element_text(hjust=0.5))\n\n\n\n\n\n\n\n\nFigura 3.6: Esquematização do cálculo do Variograma Experimental: Os pontos observados (y) não estão a distâncias exatas, pelo que são agrupados em anéis concêntricos (Lags). Todos os pontos na área azul contribuem para o cálculo da variância média daquele Lag específico.\n\n\n\n\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(gstat, sf, ggplot2, sp, patchwork)\n\n#Carregar dados exemplo (Meuse - metais pesados)\ndata(meuse)\n# Converter para objeto sf\nmeuse_sf &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\n\n#Variograma Cloud (Todos os pares de pontos possíveis)\n# Mostra a dispersão bruta das diferenças ao quadrado\nv_cloud &lt;- variogram(log(zinc) ~ 1, meuse_sf, cloud = TRUE)\n\np1 &lt;- ggplot(v_cloud, aes(x = dist, y = gamma)) +\n  geom_point(alpha = 0.2, size = 0.5) +\n  labs(title = \"Variograma Cloud (Bruto)\", \n       x = \"Distância\", y = \"Semivariância\") +\n  theme_minimal()\n\n#Variograma Experimental (Experimental / Binned)\n# Agrupa a nuvem em lags (classes de distância) para obter a média\nv_exp &lt;- variogram(log(zinc) ~ 1, meuse_sf, cutoff = 1500, width = 100)\n\np2 &lt;- ggplot(v_exp, aes(x = dist, y = gamma)) +\n  geom_point(size = 1, color = \"blue\") +\n  geom_text(aes(label = np), vjust = -0.5, size = 3) + # Número de pares\n  labs(title = \"Variograma Experimental (Médias)\", \n       subtitle = \"Números indicam pares por lag\",\n       x = \"Distância\", y = \"Semivariância\") +\n  theme_minimal()\n\np1 + p2\n\n\n\n\n\n\n\n\nFigura 3.7: Cálculo do Variograma Experimental: Nuvem (Cloud) vs. Experimental (Binned)\n\n\n\n\n\n\n\n\n\n\n\nImportanteSaiba mais\n\n\n\nPara compreender melhor os diferentes tipos de variograma e suas aplicações práticas, recomenda-se a leitura do Capítulo 3 do livro Scalon (2024).",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#modelagem-do-variograma",
    "href": "geostat.html#modelagem-do-variograma",
    "title": "3  Geoestatística",
    "section": "3.6 Modelagem do Variograma",
    "text": "3.6 Modelagem do Variograma\nO cálculo do variograma experimental, conforme detalhado na secção anterior, resulta num conjunto discreto de estimativas pontuais \\(\\hat{\\gamma}(\\mathbf{h}_k)\\) para distâncias de separação específicas. No entanto, o sistema de equações da Krigagem exige o conhecimento do valor da semivariância para qualquer distância contínua \\(\\mathbf{h}\\) dentro do domínio de estudo, e não apenas para os intervalos amostrados. Poder-se-ia intuir que uma simples interpolação linear ou spline entre os pontos experimentais seria suficiente para resolver esta lacuna. Contudo, tal procedimento é inválido e perigoso para a inferência.\nPara garantir a existência e unicidade da solução do sistema de Krigagem e, crucialmente, para assegurar que a variância de estimativa calculada seja sempre não-negativa (\\(\\sigma_E^2 \\ge 0\\)), a função utilizada para modelar a dependência espacial deve satisfazer a condição de definição condicionalmente negativa (no caso do variograma) ou definição positiva (no caso da covariância) (George Matheron 1971; Cressie 1993). Funções arbitrárias ou interpolações empíricas raramente satisfazem estas desigualdades. Consequentemente, a prática geoestatística impõe a substituição dos pontos experimentais por um modelo teórico paramétrico \\(\\gamma(\\mathbf{h}; \\boldsymbol{\\theta})\\) que seja válido (admissível). O processo de modelagem consiste, portanto, em ajustar uma curva teórica aos dados empíricos, estimando o vetor de parâmetros \\(\\boldsymbol{\\theta} = (C_0, C, a)\\) que melhor representa a estrutura de continuidade do fenômeno.\nEntre a vasta família de funções admissíveis (Entre a vasta família de funções admissíveis (ver Banerjee, Carlin, e Gelfand (2003), p. 27-28), três modelos isotrópicos destacam-se na literatura aplicada devido à sua interpretabilidade: o modelo Esférico, o Exponencial e o Gaussiano (Wackernagel 2003).\nModelo Esférico\nO Modelo Esférico descreve fenômenos com uma transição clara e abrupta entre a dependência espacial e a independência. O seu comportamento caracteriza-se por um crescimento quase linear na origem que se curva progressivamente até atingir o patamar exatamente na distância definida pelo alcance \\(a\\). Para uma distância escalar \\(h = \\|\\mathbf{h}\\|\\), é definido por:\n\\[\\gamma(h) = \\begin{cases} C_0 + c \\left( \\frac{3h}{2a} - \\frac{1}{2}\\left(\\frac{h}{a}\\right)^3 \\right) & \\text{se } 0 &lt; h \\le a \\\\ C_0 + c & \\text{se } h &gt; a \\end{cases} \\tag{3.8}\\]\nEm contraste, muitos fenômenos ambientais, como a dispersão de poluentes ou propriedades do solo, exibem uma continuidade mais suave e persistente, melhor descrita pelo Modelo Exponencial.\nModelo Exponencial\nDiferentemente do modelo esférico, o exponencial é assintótico: ele cresce rapidamente na origem mas nunca atinge o patamar, aproximando-se dele indefinidamente. A sua formulação é dada por\n\\[\\gamma(h) = C_0 + C(1 - \\exp(-h/a)) \\tag{3.9}\\]\nDevido a esta natureza assintótica, o parâmetro \\(a\\) a Eq. 3.9 não representa o alcance geométrico onde a correlação se anula, mas sim um parâmetro de escala. Convenciona-se, portanto, definir o Alcance Prático (\\(a' \\approx 3a\\)) como a distância na qual o variograma atinge \\(95\\%\\) do valor do patamar.\nModelo Gaussiano\nNo extremo da continuidade encontra-se o Modelo Gaussiano, utilizado para representar fenômenos extremamente regulares e infinitamente diferenciáveis. A sua característica distintiva é o comportamento parabólico na origem (\\(h^2\\)), indicando uma variação muito suave a curtas distâncias. A equação define-se como\n\\[\\gamma(h) = C_0 + c(1 - \\exp(-h^2/a^2)), \\tag{3.10}\\]\ncom um alcance prático de aproximadamente \\(\\sqrt{3}a\\). Laslett (1994) e Wackernagel (2003) demonstram que a utilização do modelo Gaussiano sem um efeito pepita (\\(C_0 = 0\\)) pode conduzir a instabilidades numéricas severas na inversão da matriz de krigagem (singularidade) e gerar artefactos irrealistas na predição, devendo a sua aplicação ser sempre acompanhada de uma componente de ruído, ainda que infinitesimal.\n\n\nCódigo\nnugget &lt;- 0\nsill_total &lt;- 10  # C0 + C\nalcance_pratico &lt;- 30 \n\nh &lt;- seq(0, 50, by = 0.5)\n\ngamma_sph &lt;- ifelse(h &lt;= alcance_pratico,\n                    nugget + sill_total * (1.5 * (h/alcance_pratico) - 0.5 * (h/alcance_pratico)^3),\n                    sill_total)\n\n#Modelo Exponencial \na_exp &lt;- alcance_pratico / 3\ngamma_exp &lt;- nugget + sill_total * (1 - exp(-h/a_exp))\n\n#Modelo Gaussiano\na_gau &lt;- alcance_pratico / sqrt(3)\ngamma_gau &lt;- nugget + sill_total * (1 - exp(-(h^2)/(a_gau^2)))\n\ndf_modelos &lt;- data.frame(Distancia = h,\n                         Esferico = gamma_sph,\n                         Exponencial = gamma_exp,\n                         Gaussiano = gamma_gau)\n\ndf_long &lt;- pivot_longer(df_modelos, cols = -Distancia, names_to = \"Modelo\", values_to = \"Gamma\")\n\nggplot(df_long, aes(x = Distancia, y = Gamma, color = Modelo, linetype = Modelo)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = sill_total, linetype = \"dashed\", color = \"black\") +\n  annotate(\"text\", x = 45, y = sill_total + 0.3, label = \"Patamar (Sill)\", color = \"black\") +\n  \n  scale_color_manual(values = c(\"Esferico\" = \"black\", \"Exponencial\" = \"red\", \"Gaussiano\" = \"blue\")) +\n  scale_linetype_manual(values = c(\"solid\", \"solid\", \"solid\")) +\n  \n  labs(x = \"Distância (h)\", y = expression(gamma(h))) +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFigura 3.8: Comparação dos Modelos Teóricos: O Esférico (linear na origem) atinge o patamar abruptamente. O Exponencial sobe rápido mas estabiliza lentamente (assintótico). O Gaussiano (parabólico na origem) representa fenômenos muito suaves.\n\n\n\n\n\nPara unificar estas abordagens, Guttorp e Gneiting (2006) defendem que modelos da família Matérn devem ser preferidos devido à sua flexibilidade superior. Enquanto outros modelos impõem uma suavidade fixa ao processo, a família Matérn possui um parâmetro de forma \\(\\nu\\) (suavidade) que permite aos dados ditar o grau de diferenciabilidade do campo aleatório \\(Y(\\mathbf{s})\\).\nComo definido anteriormente, para processos estacionários de segunda ordem, o semivariograma relaciona-se diretamente com a função de covariância através de \\(\\gamma(h) = C(\\mathbf{0}) - C(h)\\).\nA formulação da família Matérn distingue-se pela introdução de um parâmetro adicional que controla a suavidade do processo estocástico, permitindo que o modelo transite entre as formas exponencial e gaussiana de acordo com a evidência dos dados Sahu (2022).\n\\[\\gamma(h) = C_0 + C \\left[ 1 - \\frac{1}{2^{\\nu-1}\\Gamma(\\nu)} \\left(\\frac{2\\sqrt{\\nu} \\cdot h}{a}\\right)^\\nu K_\\nu\\left(\\frac{2\\sqrt{\\nu} \\cdot h}{a}\\right) \\right]\\]\nOnde,\n\n\\(C\\) (Contribuição ou Variância Estrutural) representa a parte do patamar explicada pela continuidade espacial. No limite \\(h \\to \\infty\\), o termo de correlação anula-se e o semivariograma estabiliza no patamar total \\(C_0 + C\\).\n\\(a\\) (Alcance) define a escala de distância da dependência. Tal como nos modelos exponencial e gaussiano, este parâmetro dita quão rápido a semivariância cresce em direção ao patamar.\n\\(\\nu\\) (Parâmetro de suavidade) dita a diferenciabilidade do campo aleatório \\(Y(\\mathbf{s})\\).\n\\(K_\\nu(\\cdot)\\) representa a função de Bessel modificada de segunda espécie de ordem \\(\\nu\\), que garante que o modelo seja válido (positivo definido) em espaços multidimensionais.\n\n\n\nCódigo\nmatern_semivgm &lt;- function(h, C0 = 0, C = 1, a = 10, nu = 0.5) {\n  if (h == 0) return(0)\n  \n  #correlação Matérn\n  # arg = (2 * sqrt(nu) * h) / a\n  arg &lt;- (2 * sqrt(nu) * h) / a\n  \n  term1 &lt;- 1 / (2^(nu - 1) * gamma(nu))\n  term2 &lt;- (arg)^nu\n  term3 &lt;- besselK(arg, nu)\n  \n  correlacao &lt;- term1 * term2 * term3\n  # gamma(h) = C0 + C * (1 - rho(h))\n  return(C0 + C * (1 - correlacao))\n}\n\nh_seq &lt;- seq(0, 50, by = 0.2)\ncontribuicao &lt;- 10\nalcance_a &lt;- 15 \n\ndf_matern &lt;- data.frame()\n\nfor (nu_val in c(0.5, 1.5, 10)) {\n  gamma_vals &lt;- sapply(h_seq, matern_semivgm, C0 = 0, C = contribuicao, a = alcance_a, nu = nu_val)\n  nome_modelo &lt;- case_when(\n    nu_val == 0.5 ~ \"nu = 0.5 (Exponencial)\",\n    nu_val == 1.5 ~ \"nu = 1.5 (Matern 3/2)\",\n    nu_val == 10  ~ \"nu = 10 (Aprox. Gaussiano)\"\n  )\n  \n  df_matern &lt;- rbind(df_matern, data.frame(h = h_seq, gamma = gamma_vals, Modelo = nome_modelo))\n}\n\nggplot(df_matern, aes(x = h, y = gamma, color = Modelo, linetype = Modelo)) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = contribuicao, linetype = \"dashed\", alpha = 0.5) +\n  scale_color_manual(values = c(\"nu = 0.5 (Exponencial)\" = \"red\", \n                                \"nu = 1.5 (Matern 3/2)\" = \"#009E73\", \n                                \"nu = 10 (Aprox. Gaussiano)\" = \"blue\")) +\n  labs(x = \"Distância de separação (h)\", \n       y = expression(gamma(h))) +\n  theme_classic() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\nFigura 3.9: A Família Matérn e a Flexibilidade de Suavidade (nu). O parâmetro nu controla o comportamento na origem: nu=0.5 recupera o modelo Exponencial, enquanto nu elevados aproximam-se do Gaussiano.\n\n\n\n\n\nA flexibilidade desta família reside no facto de englobar os modelos clássicos como casos particulares. Sahu (2022) destaca que quando \\(\\nu = 0.5\\), a função simplifica-se analiticamente para o modelo exponencial, \\(\\gamma(d) = C_0 + C(1 - \\exp(-h/a))\\), descrevendo processos contínuos mas rugosos (não diferenciáveis na origem). À medida que \\(\\nu \\to \\infty\\), a função converge para o modelo Gaussiano, descrevendo processos infinitamente suaves e diferenciáveis Figura 3.9.\nEsta capacidade de transitar entre o rugoso e o suave permite que os próprios dados informem o grau de regularidade do fenômeno, evitando suposições arbitrárias. Além disso, a modelagem respeita a Primeira lei da geografia de Tobler, assegurando que a semivariância cresça monotonicamente com a distância. Para o caso específico de \\(\\nu = 0.5\\), o alcance prático (onde se atinge \\(95\\%\\) do patamar) mantém a relação clássica de \\(3a\\), facilitando a interpretação dos parâmetros estimados.",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#diagnóstico-e-validação",
    "href": "geostat.html#diagnóstico-e-validação",
    "title": "3  Geoestatística",
    "section": "3.7 Diagnóstico e Validação",
    "text": "3.7 Diagnóstico e Validação\n\n3.7.1 Diagnóstico via Derivada na Origem (Teoria Espectral).\nA distinção visual entre modelos teóricos, particularmente entre as famílias Exponencial e Gaussiana (ou Matérn com diferentes parâmetros de suavidade), é frequentemente ambígua na presença de ruído experimental. Gorsich e Genton (2000) propõem uma metodologia objetiva baseada na análise da derivada do variograma na origem, \\(\\gamma'(0)\\), fundamentada na teoria espectral de campos aleatórios.\nPelo Teorema de Bochner, a função de covariância \\(C(\\mathbf{h})\\) de um campo aleatório estacionário e isotrópico em \\(\\mathbb{R}^d\\) possui uma representação espectral dada pela transformada de Hankel da densidade espectral \\(f(\\omega)\\). Para a dimensão \\(d=2\\), tem-se:\n\\[C(h) = 2\\pi \\int_0^{\\infty} J_0(\\omega h) f(\\omega) \\omega \\, d\\omega\\]\nOnde \\(J_0(\\cdot)\\) é a função de Bessel de primeira espécie de ordem zero. Sabemos que o semivariograma se relaciona com a covariância por \\(\\gamma(h) = C(0) - C(h)\\). Consequentemente, a derivada do semivariograma é o simétrico da derivada da covariância: \\(\\gamma'(h) = -C'(h)\\).\nA suavidade do processo estocástico (a existência de derivadas em média quadrática do processo \\(Y(\\mathbf{s})\\)) é determinada pela taxa de decaimento da densidade espectral \\(f(\\omega)\\) em altas frequências. Gorsich e Genton (2000) demonstram que o comportamento de \\(\\gamma'(h)\\) quando \\(h \\to 0\\) discrimina classes de diferenciabilidade.\nAnálise Assintótica dos Modelos\n1.Modelo Gaussiano: A função de covariância é dada por \\(C(h) = \\sigma^2 \\exp(-h^2/a^2)\\). A expansão de Taylor de segunda ordem em torno de zero é:\n\\[C(h) \\approx \\sigma^2 \\left( 1 - \\frac{h^2}{a^2} + O(h^4) \\right)\\]\nO semivariograma correspondente é \\(\\gamma(h) = \\sigma^2 - C(h) \\approx \\sigma^2 \\frac{h^2}{a^2}\\). A derivada em relação a \\(h\\) é: \\(\\gamma'(h) \\approx \\frac{2\\sigma^2 h}{a^2}\\). Tomando o limite na origem: \\(\\lim_{h \\to 0} \\gamma'(h) = 0\\). Uma derivada nula na origem implica que o processo \\(Y(\\mathbf{s})\\) é infinitamente diferenciável em média quadrática, caracterizando uma estrutura espacial extremamente suave.\n\nModelo Exponencial: A função de covariância é \\(C(h) = \\sigma^2 \\exp(-h/a)\\). A expansão de Taylor de primeira ordem é:\n\n\\[C(h) \\approx \\sigma^2 \\left( 1 - \\frac{h}{a} + O(h^2) \\right)\\]\nO semivariograma comporta-se como \\(\\gamma(h) \\approx \\sigma^2 \\frac{h}{a}\\). A derivada é: \\(\\gamma'(h) \\approx \\frac{\\sigma^2}{a}\\). Tomando o limite:\\(\\lim_{h \\to 0} \\gamma'(h) = \\frac{\\sigma^2}{a} &gt; 0\\). Uma derivada positiva constante na origem indica que o processo é contínuo em média quadrática, mas não diferenciável.\nA aplicação prática deste diagnóstico requer o uso de um estimador não-paramétrico para \\(\\gamma(h)\\) (como expansões de Bessel ou splines de suavização) e o cálculo numérico da sua derivada em \\(h=0\\).\n\n\nCódigo\n# Permite ver se as curvas de nível formam elipses\nplot(\n  variogram(log(zinc) ~ 1, meuse_sf, map = TRUE, cutoff = 1000, width = 50),\n  main = \"Mapa de Semivariância\"\n)\n\n#Variogramas Direcionais\n# alpha = direção (0=Norte, 45=Nordeste, 90=Leste, 135=Sudeste)\nv_dir &lt;- variogram(log(zinc) ~ 1, meuse_sf, alpha = c(0, 45, 90, 135))\n\nggplot(v_dir, aes(x = dist, y = gamma, color = factor(dir.hor))) +\n  geom_point() +\n  geom_line(linewidth = 0.5) +\n  labs(\n    title = \"Variogramas Direcionais\",\n    color = \"Direção (graus):\",\n    x = \"Distância\",\n    y = \"Semivariância\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n# Nota: Se as curvas divergirem muito em altura (Patamar) = Anisotropia Zonal\n# Se divergirem no alcance (distância onde estabiliza) = Anisotropia Geométrica\n\n\n\n\n\n\n\n\nFigura 3.10: Diagnóstico de Anisotropia: (a) mapa de semivariância e (b) variogramas direcionais\n\n\n\n\n\n\n\n\n\n\n\nFigura 3.11: Diagnóstico de Anisotropia: (a) mapa de semivariância e (b) variogramas direcionais",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#construção-de-modelos-válidos-via-médias-móveis",
    "href": "geostat.html#construção-de-modelos-válidos-via-médias-móveis",
    "title": "3  Geoestatística",
    "section": "3.8 Construção de Modelos Válidos via Médias Móveis",
    "text": "3.8 Construção de Modelos Válidos via Médias Móveis\nUma limitação fundamental na modelagem geoestatística é a necessidade de garantir que a função de variograma escolhida seja condicionalmente negativa definida (ou que a covariância seja positiva definida). O uso de funções arbitrárias pode levar a variâncias de krigagem negativas. Em vez de se restringir a uma lista fixa de modelos paramétricos pré-aprovados (como o Esférico ou Exponencial), Ver Hoef e Barry (1998) propõem uma abordagem construtiva baseada em médias móveis (convolução) que garante a validade matemática do modelo a priori.\nAssuma-se que o processo espacial \\(Y(\\mathbf{s})\\) é gerado pela suavização (convolução) de um ruído branco subjacente \\(W(\\mathbf{u})\\) através de uma função de ponderação ou kernel \\(g(\\cdot)\\) integrável ao quadrado.\nSeja \\(W(\\mathbf{u})\\) um processo de ruído branco em \\(\\mathbb{R}^d\\) tal que \\(E[dW(\\mathbf{u})] = 0\\), \\(\\text{Var}[dW(\\mathbf{u})] = d\\mathbf{u}\\) (ou seja, \\(\\text{Cov}(dW(\\mathbf{u}), dW(\\mathbf{v})) = 0\\) se \\(\\mathbf{u} \\neq \\mathbf{v}\\))\nO processo \\(Y(\\mathbf{s})\\) define-se como a integral estocástica:\n\\[Y(\\mathbf{s}) = \\int_{\\mathbb{R}^d} g(\\mathbf{u} - \\mathbf{s}) \\, dW(\\mathbf{u})\\]\nO nosso objetivo é encontrar a expressão teórica do variograma \\(2\\gamma(\\mathbf{h})\\) resultante deste processo construtivo.\n\\[2\\gamma(\\mathbf{h}) = \\text{Var}[Y(\\mathbf{s}+\\mathbf{h}) - Y(\\mathbf{s})] = E\\left[ (Y(\\mathbf{s}+\\mathbf{h}) - Y(\\mathbf{s}))^2 \\right]\\]\nSubstituímos \\(Y(\\cdot)\\) pela sua definição integral:\n\\[\n\\begin{aligned}\nY(\\mathbf{s}+\\mathbf{h}) - Y(\\mathbf{s}) &= \\int_{\\mathbb{R}^d} g(\\mathbf{u} - (\\mathbf{s}+\\mathbf{h})) \\, dW(\\mathbf{u}) - \\int_{\\mathbb{R}^d} g(\\mathbf{u} - \\mathbf{s}) \\, dW(\\mathbf{u}) \\\\\n&= \\int_{\\mathbb{R}^d} \\left[ g(\\mathbf{u} - \\mathbf{s} - \\mathbf{h}) - g(\\mathbf{u} - \\mathbf{s}) \\right] \\, dW(\\mathbf{u})\n\\end{aligned}\n\\]\nUma propriedade fundamental das integrais estocásticas em relação ao ruído branco (Movimento Browniano) é a Isometria de Ito, que estabelece que a variância da integral estocástica é igual à integral do quadrado da função determinística integranda:\n\\[\n\\text{Var}\\left[ \\int_{\\mathbb{R}^d} f(\\mathbf{u}) \\, dW(\\mathbf{u}) \\right] = \\int_{\\mathbb{R}^d} [f(\\mathbf{u})]^2 \\, d\\mathbf{u}\n\\]\n\\[\n2\\gamma(\\mathbf{h}) = \\int_{\\mathbb{R}^d} \\left[ g(\\mathbf{u} - \\mathbf{s} - \\mathbf{h}) - g(\\mathbf{u} - \\mathbf{s}) \\right]^2 \\, d\\mathbf{u}\n\\] Para demonstrar que o variograma depende apenas da separação \\(\\mathbf{h}\\) e não da localização \\(\\mathbf{s}\\), fazemos uma mudança de variável.\nSeja \\(\\mathbf{x} = \\mathbf{u} - \\mathbf{s}\\). Então, \\(d\\mathbf{x} = d\\mathbf{u}\\) e os limites de integração (\\(\\mathbb{R}^d\\)) permanecem inalterados.\n\\(g(\\mathbf{u} - \\mathbf{s}) \\rightarrow g(\\mathbf{x})\\) e \\(g(\\mathbf{u} - \\mathbf{s} - \\mathbf{h}) \\rightarrow g(\\mathbf{x} - \\mathbf{h})\\),\n\\[2\\gamma(\\mathbf{h}) = \\int_{\\mathbb{R}^d} \\left[ g(\\mathbf{x} - \\mathbf{h}) - g(\\mathbf{x}) \\right]^2 \\, d\\mathbf{x}\\]\nEsta equação demonstra que o variograma é a autoconvolução da diferença do kernel. A implicação teórica mais poderosa deste resultado é a garantia de validade:\nTeorema: Qualquer função \\(g(\\cdot)\\) que seja de quadrado integrável (\\(\\int g(\\mathbf{x})^2 d\\mathbf{x} &lt; \\infty\\)) gera automaticamente um modelo de variograma matematicamente válido (condicionalmente negativo definido). Isto elimina a necessidade de verificar a positividade da matriz de covariância a posteriori.",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#sec-anisotropia",
    "href": "geostat.html#sec-anisotropia",
    "title": "3  Geoestatística",
    "section": "3.9 Anisotropia",
    "text": "3.9 Anisotropia\nA hipótese de isotropia assume que a estrutura de dependência espacial do processo estocástico \\(Y(\\mathbf{s})\\) depende apenas da distância euclidiana entre os pontos, \\(\\|\\mathbf{h}\\|\\), e não da direção do vetor de separação \\(\\mathbf{h}\\). Isto implica que as isolinhas (ou isossuperfícies) do variograma \\(\\gamma(\\mathbf{h})\\) formam círculos (em \\(\\mathbb{R}^2\\)) ou esferas (em \\(\\mathbb{R}^3\\)).\nContudo, processos físicos geológicos e ambientais raramente são isotrópicos. A sedimentação, o fluxo de águas subterrâneas ou a dispersão eólica de poluentes criam direções preferenciais de continuidade. Quando a variabilidade espacial muda com a direção, o processo é denominado anisotrópico.\nA modelagem da anisotropia não requer a criação de novas funções de variograma, mas sim a aplicação de transformações lineares afins sobre o sistema de coordenadas, mapeando o espaço anisotrópico original num espaço isotrópico equivalente. Classificamos a anisotropia em três categorias fundamentais: Geométrica, Zonal e Mista(Yamamoto e Landim 2013).\n\n3.9.1 Anisotropia Geométrica\nA anisotropia geométrica ocorre quando o alcance (\\(a\\)) da dependência espacial varia com a direção, mas o patamar (\\(C_0 + C\\)) permanece constante em todas as direções. As isolinhas de semivariância formam elipses, cujos eixos principais correspondem às direções de maior e menor continuidade.\nSeja \\(\\gamma_{iso}(h; a)\\) um modelo isotrópico com alcance \\(a\\). Num processo anisotrópico geométrico em \\(\\mathbb{R}^2\\), definimos:\n\\(a_{max}\\): O alcance máximo (direção de maior continuidade).\n\\(a_{min}\\): O alcance mínimo (direção de menor variabilidade).\n\\(\\phi\\): O ângulo de azimute da direção de maior continuidade.\nA razão de anisotropia é definida como o escalar \\(\\lambda = a_{max} / a_{min} \\ge 1\\) (ou o seu inverso, dependendo da convenção de software, aqui usaremos a definição de estiramento).\nO objetivo é transformar o vetor de separação original \\(\\mathbf{h} = [h_x, h_y]^T\\) num vetor transformado \\(\\mathbf{h}'\\) tal que a estrutura se torne isotrópica com alcance padronizado (geralmente \\(a_{min}\\) ou 1). Esta transformação \\(\\mathbf{h}' = \\mathbf{A}\\mathbf{h}\\) compõe-se de uma rotação e um escalonamento.\n\nPasso 1: Rotação\n\nAlinhamos o sistema de coordenadas com os eixos principais da anisotropia através da matriz de rotação \\(\\mathbf{R}(\\phi)\\):\n\\[\\mathbf{R}(\\phi) = \\begin{bmatrix} \\cos\\phi & \\sin\\phi \\\\ -\\sin\\phi & \\cos\\phi \\end{bmatrix}\\]\n\nPasso 2: Escalonamento\n\nReduzimos a distância ao longo do eixo maior para que corresponda à escala do eixo menor. A matriz de escalonamento \\(\\mathbf{S}\\) é:\n\\[\\mathbf{S} = \\begin{bmatrix} 1/\\lambda & 0 \\\\ 0 & 1 \\end{bmatrix} \\quad \\text{onde } \\lambda = \\frac{a_{max}}{a_{min}}\\]\nEsta operação comprime o eixo maior, transformando a elipse de alcance num círculo de raio \\(a_{min}\\).\n\nPasso 3: Variograma Anisotrópico\n\nO vetor transformado é \\(\\mathbf{h}' = \\mathbf{S} \\mathbf{R}(\\phi) \\mathbf{h}\\). A matriz de transformação completa \\(\\mathbf{A}\\) é:\n\\[\\mathbf{A} = \\mathbf{S} \\mathbf{R}(\\phi) = \\begin{bmatrix} \\frac{\\cos\\phi}{\\lambda} & \\frac{\\sin\\phi}{\\lambda} \\\\ -\\sin\\phi & \\cos\\phi \\end{bmatrix}\\]\nO modelo de variograma anisotrópico \\(\\gamma_{aniso}(\\mathbf{h})\\) é obtido avaliando o modelo isotrópico na norma do vetor transformado:\n\\[\\gamma_{aniso}(\\mathbf{h}) = \\gamma_{iso}(\\|\\mathbf{A}\\mathbf{h}\\|; a_{min})\\]\nComo \\(\\mathbf{A}\\) é uma matriz não-singular, a transformação é bijectiva. Uma vez que \\(\\gamma_{iso}\\) é uma função condicionalmente negativa definida (CND) em \\(\\mathbb{R}^d\\), a composição \\(\\gamma_{iso}(\\|\\mathbf{A}\\cdot\\|)\\) preserva a propriedade CND, garantindo que o modelo anisotrópico é válido para a krigagem (Cressie 1993).\n\n\n3.9.2 Anisotropia Zonal\nA anisotropia zonal ocorre quando o patamar (variância total) varia consoante a direção. Isto é teoricamente problemático para a hipótese de estacionariedade de segunda ordem, pois implica que a covariância na origem \\(C(\\mathbf{0})\\) (a variância do processo) não é única.\nNa prática, isto ocorre quando a variabilidade vertical é muito superior à horizontal, de tal forma que o variograma vertical atinge um patamar muito mais alto do que o horizontal, ou o variograma horizontal parece nunca atingir o patamar total do processo.\nPara modelar a anisotropia zonal mantendo a validade do modelo, Andre G. Journel e Huijbregts (1976) propõem decompor o processo \\(Y(\\mathbf{s})\\) na soma de processos independentes (estruturas aninhadas), onde alguns componentes atuam apenas em subespaços do domínio.\nSeja \\(\\mathbf{h} = (h_x, h_y, h_z)\\). O modelo é construído como:\n\\[\\gamma(\\mathbf{h}) = \\gamma_{iso}(\\|\\mathbf{h}\\|) + \\gamma_{zonal}(|h_z|)\\]\nOnde:\n\\(\\gamma_{iso}(\\|\\mathbf{h}\\|)\\): É uma componente isotrópica (ou geometricamente anisotrópica) que contribui para a variabilidade em todas as direções.\n\\(\\gamma_{zonal}(|h_z|)\\): É uma componente que depende apenas da distância vertical \\(h_z\\).\n\nNa direção vertical (\\(h_x=0, h_y=0\\)), o vetor é \\(\\mathbf{h} = (0,0, h_z)\\). O variograma total é:\n\n\\[\\gamma(0,0, h_z) = \\gamma_{iso}(h_z) + \\gamma_{zonal}(h_z)\\]\nO patamar nesta direção é a soma dos patamares das duas estruturas: \\(C_{total} = C_{iso} + C_{zonal}\\).\n\nNa direção horizontal (\\(h_z=0\\)), o vetor é \\(\\mathbf{h} = (h_x, h_y, 0)\\). Como \\(\\gamma_{zonal}(0) = 0\\) (por definição de variograma na origem), a equação reduz-se a:\n\n\\[\\gamma(h_x, h_y, 0) = \\gamma_{iso}(\\sqrt{h_x^2 + h_y^2}) + 0\\]\nO patamar aparente na horizontal é apenas \\(C_{iso}\\).\nA componente \\(\\gamma_{zonal}\\) possui um alcance horizontal infinito. Modela-se como uma estrutura cujo alcance no plano \\(xy\\) tende a \\(\\infty\\), contribuindo para a variância total apenas quando existe separação vertical. Isto reflete a realidade de camadas sedimentares onde a variação litológica é intensa verticalmente, mas as propriedades persistem lateralmente por longas distâncias.\n\n\n3.9.3 Anisotropia Mista (ou Combinada)\nA anisotropia mista é a generalização que permite modelar sistemas complexos onde diferentes escalas de variação possuem diferentes direções de continuidade. Por exemplo, a microvariabilidade (curta distância) pode ser isotrópica, enquanto a tendência regional (longa distância) segue uma direção preferencial.\nAssume-se que o processo \\(Y(\\mathbf{s})\\) é a soma de \\(K\\) componentes ortogonais independentes (escalas espaciais), \\(Y(\\mathbf{s}) = \\sum_{k=1}^K Y_k(\\mathbf{s})\\).\nPela propriedade de aditividade da variância de variáveis independentes, o variograma total é a soma dos variogramas individuais:\n\\[\\gamma_{total}(\\mathbf{h}) = \\sum_{k=1}^{K} \\gamma_k(\\mathbf{h})\\]\nCada estrutura \\(\\gamma_k(\\mathbf{h})\\) pode ter a sua própria definição de anisotropia geométrica, com a sua própria matriz de transformação \\(\\mathbf{A}_k\\). A equação geral para a anisotropia mista é (Goovaerts (1997)):\n\\[\\gamma(\\mathbf{h}) = C_0 + \\sum_{k=1}^{K} C_k \\cdot \\rho_k\\left( \\|\\mathbf{A}_k \\mathbf{h}\\| \\right)\\]\nOnde:\n\\(C_0\\) é feito pepita (geralmente isotrópico, pois é ruído); \\(C_k\\) é contribuição da estrutura \\(k\\); \\(\\rho_k(\\cdot)\\) é função de correlação básica; \\(\\mathbf{A}_k\\) é matriz de transformação específica para a estrutura \\(k\\).",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#diagnóstico-da-anisotropia",
    "href": "geostat.html#diagnóstico-da-anisotropia",
    "title": "3  Geoestatística",
    "section": "3.10 Diagnóstico da Anisotropia",
    "text": "3.10 Diagnóstico da Anisotropia\nA anisotropia em um processo espacial \\(Y(\\mathbf{s})\\) manifesta-se quando a estrutura de dependência espacial varia consoante a direção. Como discutido na Seção 3.9, a modelagem correta da anisotropia é crucial para garantir a precisão da predição espacial (krigagem) e a validade estatística das inferências. O diagnóstico da anisotropia envolve a detecção e a caracterização da dependência direcional, tipicamente através da análise de variogramas direcionais e mapas de variograma.\n\n3.10.1 Variogramas Direcionais\nA forma mais comum de diagnosticar a anisotropia é calcular variogramas experimentais \\(\\hat{\\gamma}(\\mathbf{h})\\) para diferentes direções do vetor de separação \\(\\mathbf{h}\\). Em vez de considerar todas as distâncias omnidirecionais, restringimos o cálculo a pares de pontos cuja separação vetorial cai dentro de setores angulares específicos (Andre G. Journel e Huijbregts 1976; Isaaks, Srivastava, et al. 1989).\nSeja \\(\\theta\\) o ângulo de direção (azimute) e \\(\\Delta\\theta\\) a tolerância angular (meia-janela). O estimador do variograma direcional para a direção \\(\\theta\\) é dado por:\n\\[\\hat{\\gamma}(h, \\theta) = \\frac{1}{2|N(h, \\theta)|} \\sum_{(\\mathbf{s}_i, \\mathbf{s}_j) \\in N(h, \\theta)} (y(\\mathbf{s}_i) - y(\\mathbf{s}_j))^2\\]\nOnde \\(N(h, \\theta)\\) é o conjunto de pares de locais \\((\\mathbf{s}_i, \\mathbf{s}_j)\\) tal que a distância \\(\\|\\mathbf{s}_i - \\mathbf{s}_j\\| \\approx h\\) e o ângulo do vetor \\(\\mathbf{s}_i - \\mathbf{s}_j\\) está em \\([\\theta - \\Delta\\theta, \\theta + \\Delta\\theta]\\).\nTipicamente, calculam-se variogramas para quatro direções principais: \\(0^\\circ\\) (Norte-Sul), \\(45^\\circ\\) (Nordeste-Sudoeste), \\(90^\\circ\\) (Leste-Oeste) e \\(135^\\circ\\) (Sudeste-Noroeste) Scalon (2024). A comparação visual destes variogramas permite identificar o tipo de anisotropia (ver seção Seção 3.9):\n\nAnisotropia Geométrica: Se os variogramas atingem o mesmo patamar (\\(C_0 + C\\)) mas com alcances diferentes (\\(a(\\theta)\\)), estamos perante uma anisotropia geométrica. O alcance varia com a direção segundo uma elipse.\nAnisotropia Zonal: Se os variogramas estabilizam em patamares diferentes dependendo da direção, ou se numa direção específica o variograma não estabiliza (indicando uma tendência), temos anisotropia zonal.\nAnisotropia mista se ocorre a anisotropia geometrica e zonal.\n\nUma ferramenta visual poderosa para detetar anisotropia é o mapa de variograma ou superfície de variograma. Em vez de traçar curvas \\(\\gamma(h)\\) para direções discretas, representamos \\(\\hat{\\gamma}(\\mathbf{h})\\) como uma superfície em função das coordenadas do vetor de separação \\(\\mathbf{h} = (h_x, h_y)\\) (Goovaerts 1997).\nO mapa é construído calculando a semivariância média para células de uma grelha no espaço dos vetores de separação. O centro do mapa corresponde a \\(\\mathbf{h} = (0,0)\\) (semivariância zero). As cores ou curvas de nível representam a magnitude de \\(\\gamma(\\mathbf{h})\\).\nIsotropia: é o contrario da anisotropia.Aqui, as curvas de nível formam círculos concêntricos em redor da origem.\n\nAnisotropia Geométrica: As curvas de nível formam elipses. O eixo maior da elipse no mapa de variograma corresponde à direção de menor variabilidade (maior continuidade), que é a direção do alcance máximo (\\(a_{max}\\)). O eixo menor corresponde à direção de maior variabilidade (alcance mínimo \\(a_{min}\\)).\nAnisotropia Zonal: As curvas de nível não fecham ou mostram comportamentos muito distintos em direções ortogonais, indicando diferenças nos patamares.\n\nPara anisotropia geométrica, pode-se ajustar modelos teóricos aos variogramas direcionais experimentais e plotar os alcances estimados \\(a(\\theta)\\) num diagrama polar (Rose Diagram). A forma resultante deve aproximar-se de uma elipse descrita pela equação polar:\n\\[a(\\theta) = \\frac{a_{max} a_{min}}{\\sqrt{a_{min}^2 \\cos^2(\\theta - \\phi) + a_{max}^2 \\sin^2(\\theta - \\phi)}}\\]\nOnde \\(\\phi\\) é o ângulo da direção de maior continuidade (eixo maior). Este diagnóstico permite estimar os parâmetros da transformação de coordenadas (rotação \\(\\phi\\) e razão de anisotropia \\(\\lambda = a_{max}/a_{min}\\)) necessários para corrigir a anisotropia e aplicar a krigagem num espaço isotrópico equivalente (Chiles e Delfiner 2012).",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#sec-ajuste-diagnostico",
    "href": "geostat.html#sec-ajuste-diagnostico",
    "title": "3  Geoestatística",
    "section": "3.11 Ajuste de Modelos de semivariograma",
    "text": "3.11 Ajuste de Modelos de semivariograma\nA inferência da estrutura de dependência espacial é uma etapa crítica na geoestatística. Dado um conjunto de dados observados \\(\\mathbf{y} = (y(\\mathbf{s}_1), \\dots, y(\\mathbf{s}_n))^\\top\\), em locais \\(\\mathbf{s}_1, \\dots, \\mathbf{s}_n\\), o objetivo é estimar o vetor de parâmetros \\(\\boldsymbol{\\theta}\\) (efeito pepita, patamar, alcance, suavidade) de um modelo de variograma teórico \\(2\\gamma(\\mathbf{h}; \\boldsymbol{\\theta})\\) que descreva adequadamente o processo estocástico subjacente.\n\n3.11.1 Mínimos Quadrados Ponderados\nO método de mínimos quadrados ordinários é inadequada para o ajuste de variogramas devido à heterocedasticidade intrínseca dos estimadores. Cressie (1985) formalizou a dedução da variância assintótica do estimador de Matheron, justificando a necessidade de pesos.\nAssumindo que o campo aleatório \\(Y(\\mathbf{s})\\) é um processo Gaussiano estacionário. Definamos a variável aleatória da diferença entre dois pontos separados pelo vetor \\(\\mathbf{h}\\) como \\(D(\\mathbf{h}) = Y(\\mathbf{s} + \\mathbf{h}) - Y(\\mathbf{s})\\), pela hipótese estacionaridade intrínseca, esta diferença tem média zero e variância definida pelo variograma teórico: \\(D(\\mathbf{h}) \\sim \\mathcal{N}(0, 2\\gamma(\\mathbf{h}))\\). O estimador de variograma baseia-se no quadrado desta diferença. Vamos padronizar \\(D(\\mathbf{h})\\) dividindo pelo desvio padrão \\(\\sqrt{2\\gamma(\\mathbf{h})}\\) para obter uma normal padrão \\(Y | X \\sim \\mathcal{N}(0,1)\\):\n\\[\\frac{D(\\mathbf{h})}{\\sqrt{2\\gamma(\\mathbf{h})}} \\sim \\mathcal{N}(0, 1)\\]\nElevando ambos os lados ao quadrado, obtemos uma variável que segue uma distribuição Qui-quadrado com 1 grau de liberdade (\\(\\chi^2_1\\)):\n\\[\\left( \\frac{D(\\mathbf{h})}{\\sqrt{2\\gamma(\\mathbf{h})}} \\right)^2 \\sim \\chi^2_1 \\implies [D(\\mathbf{h})]^2 \\sim 2\\gamma(\\mathbf{h}) \\cdot \\chi^2_1\\]\nSabemos que a variância de uma variável \\(\\chi^2_1\\) é exatamente 2. Usando a propriedade de variância \\(\\text{Var}(kX) = k^2 \\text{Var}(X)\\), a variância da diferença quadrática para um único par de pontos é:\n\\[\n\\begin{aligned}\n\\text{Var}\\left[ (Y(\\mathbf{s}+\\mathbf{h}) - Y(\\mathbf{s}))^2 \\right] &= \\text{Var}\\left[ 2\\gamma(\\mathbf{h}) \\cdot \\chi^2_1 \\right] \\\\\n&= [2\\gamma(\\mathbf{h})]^2 \\cdot \\text{Var}(\\chi^2_1) \\\\\n&= 4[\\gamma(\\mathbf{h})]^2 \\cdot 2 \\\\\n&= 8[\\gamma(\\mathbf{h})]^2\n\\end{aligned}\n\\]\nO estimador do semivariograma \\(\\hat{\\gamma}(\\mathbf{h})\\) para um lag \\(\\mathbf{h}\\) é a média de \\(N(\\mathbf{h})\\) diferenças quadráticas, dividida por 2. Assumindo independência aproximada entre os pares (necessário para a derivação dos pesos Cressie (1985)):\n\\[\n\\begin{aligned}\n\\text{Var}[\\hat{\\gamma}(\\mathbf{h})] &= \\text{Var}\\left[ \\frac{1}{2 N(\\mathbf{h})} \\sum_{i=1}^{N(\\mathbf{h})} (Y(\\mathbf{s}_i) - Y(\\mathbf{s}*j))^2 \\right] \\\\\n&= \\frac{1}{4 [N(\\mathbf{h})]^2} \\sum_{i=1}^{N(\\mathbf{h})} \\text{Var}\\left[ (Y(\\mathbf{s}_i) - Y(\\mathbf{s}_j))^2 \\right] \\quad \\text{(Soma de variâncias)} \\\\\n&= \\frac{1}{4 [N(\\mathbf{h})]^2} \\cdot N(\\mathbf{h}) \\cdot 8[\\gamma(\\mathbf{h})]^2 \\quad \\text{(Substituindo o resultado anterior)} \\\\\n&= \\frac{2[\\gamma(\\mathbf{h})]^2}{N(\\mathbf{h})}\n\\end{aligned}\n\\]\nPelo princípio de Aitken, os pesos ótimos são o inverso da variância (\\(w_k = 1/\\text{Var}_k\\)) (McBratney e Webster 1986; Cressie 1985). Ignorando a constante 2 (que não afeta a minimização) temos, \\(w_k = \\frac{N(\\mathbf{h}_k)}{[\\gamma(\\mathbf{h}_k \\boldsymbol{\\theta})]^2}\\). Substituindo os pesos na soma dos erros quadráticos, obtemos a função a ser minimizada para encontrar \\(\\boldsymbol{\\theta}\\):\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\theta}}_{WLS} &= \\arg \\min_{\\boldsymbol{\\theta}} \\sum_{k=1}^{K} w(\\mathbf{h}_k) \\left[ \\hat{\\gamma}(\\mathbf{h}_k) - \\gamma(\\mathbf{h}_k; \\boldsymbol{\\theta}) \\right]^2, \\quad   w(\\mathbf{h}_k) = \\frac{|N(\\mathbf{h}_k)|}{[\\gamma(\\mathbf{h}_k; \\boldsymbol{\\theta})]^2}\n\\end{aligned}\n\\]\nonde \\(K\\) é número total de lags (classes de distância) considerados; \\(N(\\mathbf{h}_k)\\) é número de pares de pontos no lag \\(k\\). \\(\\hat{\\gamma}(\\mathbf{h}_k)\\) é o valor do semivariograma experimental (observado); \\(\\gamma(\\mathbf{h}_k; \\boldsymbol{\\theta})\\) é o valor do modelo teórico (predito); \\([\\gamma(\\mathbf{h}_k; \\boldsymbol{\\theta})]^{-2}\\) (implícito no denominador) penaliza fortemente erros em distâncias curtas (onde \\(\\gamma\\) é pequeno), garantindo que o modelo se ajuste bem na origem, o que é crucial para a Krigagem.",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#mínimos-quadrados-generalizados-explícitos-glse",
    "href": "geostat.html#mínimos-quadrados-generalizados-explícitos-glse",
    "title": "3  Geoestatística",
    "section": "3.12 Mínimos Quadrados Generalizados Explícitos (GLSE)",
    "text": "3.12 Mínimos Quadrados Generalizados Explícitos (GLSE)\nO método de mínimos quadrados ponderados (WLS) pressupõe que as estimativas do variograma em diferentes lags são estatisticamente independentes (\\(\\text{Cov}(\\hat{\\gamma}(\\mathbf{h}_i), \\hat{\\gamma}(\\mathbf{h}_j)) = 0\\) para \\(i \\neq j\\)). Na realidade, como os mesmos dados espaciais são reutilizados para calcular múltiplos lags, existe uma correlação significativa entre eles. Genton (1998) formalizou o problema como um ajuste de Mínimos Quadrados Generalizados, deduzindo a matriz de covariância completa \\(\\boldsymbol{\\Sigma}_{\\hat{\\gamma}}\\) dos estimadores.\nPara deduzir a covariância entre dois estimadores do variograma, reescrevemos primeiro o estimador de Matheron em notação matricial. Seja \\(\\mathbf{y}\\) o vetor de observações e \\(\\mathbf{A}(\\mathbf{h})\\) a matriz de incidência espacial para o lag \\(\\mathbf{h}\\) (uma matriz esparsa que seleciona os pares de pontos separados por \\(\\mathbf{h}\\)). O estimador pode ser expresso como uma forma quadrática:\n\\[2\\hat{\\gamma}(\\mathbf{h}) = \\frac{1}{|N(\\mathbf{h})|} \\mathbf{y}^\\top \\mathbf{A}(\\mathbf{h}) \\mathbf{y}\\]\nA matriz \\(\\mathbf{A}(\\mathbf{h})\\) é definida tal que \\(\\mathbf{y}^\\top \\mathbf{A}(\\mathbf{h}) \\mathbf{y} = \\sum_{(\\mathbf{s}_i, \\mathbf{s}_j) \\in N(\\mathbf{h})} (y(\\mathbf{s}_i) - y(\\mathbf{s}_j))^2\\).\nPara calcular a covariância entre as estimativas em dois lags distintos, \\(\\mathbf{h}_u\\) e \\(\\mathbf{h}_v\\), recorremos a um teorema fundamental para formas quadráticas de vetores Gaussianos. Se \\(\\mathbf{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}_y)\\), então a covariância entre duas formas quadráticas \\(\\mathbf{y}^\\top \\mathbf{A} \\mathbf{y}\\) e \\(\\mathbf{y}^\\top \\mathbf{B} \\mathbf{y}\\) é dada por:\n\\[\\text{Cov}(\\mathbf{y}^\\top \\mathbf{A} \\mathbf{y}, \\mathbf{y}^\\top \\mathbf{B} \\mathbf{y}) = 2 \\text{tr}(\\mathbf{A} \\boldsymbol{\\Sigma}_y \\mathbf{B} \\boldsymbol{\\Sigma}_y) + 4\\boldsymbol{\\mu}^\\top \\mathbf{A} \\boldsymbol{\\Sigma}_y \\mathbf{B} \\boldsymbol{\\mu}\\]\nAssumindo um processo de média zero (ou trabalhando com resíduos), o termo da média anula-se. Aplicando este teorema às matrizes de incidência espacial \\(\\mathbf{A}(\\mathbf{h}_u)\\) e \\(\\mathbf{A}(\\mathbf{h}_v)\\), obtemos a covariância entre os estimadores do variograma (elemento \\(uv\\) da matriz \\(\\boldsymbol{\\Sigma}_{\\hat{\\gamma}}\\)):\n\\[\\begin{aligned}\n[\\boldsymbol{\\Sigma}_{\\hat{\\gamma}}]_{uv} &= \\text{Cov}(2\\hat{\\gamma}(\\mathbf{h}_u), 2\\hat{\\gamma}(\\mathbf{h}_v)) \\\\\n&= \\text{Cov}\\left( \\frac{1}{|N(\\mathbf{h}_u)|} \\mathbf{y}^\\top \\mathbf{A}(\\mathbf{h}_u) \\mathbf{y}, \\frac{1}{|N(\\mathbf{h}_v)|} \\mathbf{y}^\\top \\mathbf{A}(\\mathbf{h}_v) \\mathbf{y} \\right) \\\\\n&= \\frac{1}{|N(\\mathbf{h}_u)| |N(\\mathbf{h}_v)|} \\text{Cov}(\\mathbf{y}^\\top \\mathbf{A}(\\mathbf{h}_u) \\mathbf{y}, \\mathbf{y}^\\top \\mathbf{A}(\\mathbf{h}_v) \\mathbf{y}) \\\\\n&= \\frac{2}{|N(\\mathbf{h}_u)| |N(\\mathbf{h}_v)|} \\text{tr}\\left( \\mathbf{A}(\\mathbf{h}_u) \\boldsymbol{\\Sigma}_y(\\boldsymbol{\\theta}) \\mathbf{A}(\\mathbf{h}_v) \\boldsymbol{\\Sigma}_y(\\boldsymbol{\\theta}) \\right)\n\\end{aligned}\\]\nA matriz \\(\\boldsymbol{\\Sigma}_{\\hat{\\gamma}}\\) captura explicitamente a interdependência estatística entre os lags, dependendo tanto da geometria da amostragem (via matrizes \\(\\mathbf{A}\\)) quanto da estrutura de covariância real dos dados (\\(\\boldsymbol{\\Sigma}_y(\\boldsymbol{\\theta})\\)).\nA função objetivo a ser minimizada no método GLSE, que leva em conta esta estrutura de correlação completa, é a distância de Mahalanobis entre o vetor de estimativas empíricas \\(\\hat{\\boldsymbol{\\gamma}}\\) e o vetor do modelo teórico \\(\\boldsymbol{\\gamma}(\\boldsymbol{\\theta})\\):\n\\[\\begin{aligned}\n\\hat{\\boldsymbol{\\theta}}_{GLSE} &= \\arg \\min_{\\boldsymbol{\\theta}} (\\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}))^\\top [\\boldsymbol{\\Sigma}_{\\hat{\\gamma}}(\\boldsymbol{\\theta})]^{-1} (\\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}))\n\\end{aligned}\\]\nonde \\(\\hat{\\boldsymbol{\\gamma}}\\) é o vetor contendo as estimativas \\(\\hat{\\gamma}(\\mathbf{h}_k)\\) para todos os \\(K\\) lags; \\(\\boldsymbol{\\gamma}(\\boldsymbol{\\theta})\\) é o vetor correspondente dos valores teóricos; \\([\\boldsymbol{\\Sigma}_{\\hat{\\gamma}}(\\boldsymbol{\\theta})]^{-1}\\) é a inversa da matriz de covariância dos estimadores, que atua como uma matriz de pesos generalizada, penalizando não apenas a variância (elementos diagonais, como no WLS), mas também a redundância de informação entre lags correlacionados (elementos fora da diagonal). Este método é iterativo, pois a matriz de pesos depende dos próprios parâmetros \\(\\boldsymbol{\\theta}\\) que estamos a estimar.\n\n3.12.1 Métodos de Máxima Verossimilhança (ML) e Máxima Verossimilhança Restrita (REML)\nEnquanto o método dos Mínimos Quadrados Ponderados (WLS) minimiza a distância entre o variograma experimental e o modelo teórico, os métodos baseados em verossimilhança operam diretamente sobre o vetor de dados observados \\(\\mathbf{y(s)}\\), sem a necessidade de calcular estatísticas intermédias (como a semivariância experimental). Lark (2000) e Diggle, Tawn, e Moyeed (1998) argumentam que esta abordagem é teoricamente mais eficiente, especialmente quando os dados são escassos ou a amostragem é irregular, pois utiliza toda a informação contida na distribuição conjunta dos dados.\nAssuma-se que o campo aleatório \\(Y(\\mathbf{s})\\) segue um modelo linear misto, composto por uma tendência determinística (\\(\\mathbf{X}\\boldsymbol{\\beta}\\) - média ) e um componente estocástico espacialmente correlacionado (\\(\\boldsymbol{\\eta}\\)). Seja \\(\\mathbf{y} = (y(\\mathbf{s}_1), \\dots, y(\\mathbf{s}_n))^\\top\\) o vetor de observações:\n\\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\eta}\\]\nonde \\(\\mathbf{X}\\) é a matriz de desenho (\\(n \\times p\\)) contendo as covariáveis (coordenadas, elevação, etc.); \\(\\boldsymbol{\\beta}\\) é o vetor (\\(p \\times 1\\)) de parâmetros de tendência desconhecidos (efeitos fixos) e, \\(\\boldsymbol{\\eta}\\) é o vetor de resíduos aleatórios, assumido seguir uma distribuição normal multivariada com média zero e matriz de covariância \\(\\mathbf{V}(\\boldsymbol{\\theta})\\).\nA matriz de covariância \\(\\mathbf{V}(\\boldsymbol{\\theta})\\) é parametrizada pelo vetor \\(\\boldsymbol{\\theta}\\) (alcance, patamar, efeito pepita) que desejamos estimar. O elemento \\((i,j)\\) desta matriz é dado por:\n\\[[\\mathbf{V}(\\boldsymbol{\\theta})]_{ij} = \\text{Cov}(Y(\\mathbf{s}_i), Y(\\mathbf{s}_j)) = C(\\mathbf{s}_i - \\mathbf{s}_j; \\boldsymbol{\\theta})\\]\nA função de densidade de probabilidade conjunta para o vetor \\(\\mathbf{y(s)}\\), dado os parâmetros \\(\\boldsymbol{\\beta}\\) e \\(\\boldsymbol{\\theta}\\), é:\n\\[f(\\mathbf{y} | \\boldsymbol{\\beta}, \\boldsymbol{\\theta}) = (2\\pi)^{-n/2} |\\mathbf{V}(\\boldsymbol{\\theta})|^{-1/2} \\exp\\left( -\\frac{1}{2} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top \\mathbf{V}(\\boldsymbol{\\theta})^{-1} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\right)\\]\nA função de log-verossimilhança (\\(L_{ML}\\)) é o logaritmo natural desta densidade. Para estimar \\(\\boldsymbol{\\theta}\\), primeiro perfilamos a verossimilhança em relação a \\(\\boldsymbol{\\beta}\\). O estimador de Máxima Verossimilhança para \\(\\boldsymbol{\\beta}\\), fixado \\(\\boldsymbol{\\theta}\\), é o estimador de Mínimos Quadrados Generalizados (GLS):\n\\[\\hat{\\boldsymbol{\\beta}}_{GLS} = (\\mathbf{X}^\\top \\mathbf{V}(\\boldsymbol{\\theta})^{-1} \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{V}(\\boldsymbol{\\theta})^{-1} \\mathbf{y}\\]\nSubstituindo \\(\\hat{\\boldsymbol{\\beta}}_{GLS}\\) na equação da densidade, obtemos a verossimilhança perfilada a ser maximizada em relação a \\(\\boldsymbol{\\theta}\\):\n\\[\\begin{aligned}\nL_{ML}(\\boldsymbol{\\theta}) &= -\\frac{n}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\mathbf{V}(\\boldsymbol{\\theta})| - \\frac{1}{2}(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{GLS})^\\top \\mathbf{V}(\\boldsymbol{\\theta})^{-1} (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{GLS})\n\\end{aligned}\\]\nCressie (1993) e Marchant e Lark (2007) destacam que existe uma tendência nos dados (\\(\\mu(\\mathbf{s}) \\neq \\text{constante}\\)), o estimador de Máxima Verossimilhança (ML) subestima a variância e o variograma, pois assume que os parâmetros da tendência (\\(\\boldsymbol{\\beta}\\)) são conhecidos, ignorando os graus de liberdade perdidos na sua estimação.\nPara corrigir este viés, utiliza-se a Máxima Verossimilhança Restrita (REML).\nSeja \\(\\mathbf{K}\\) uma matriz de contrastes de dimensão \\(n \\times (n-p)\\) tal que suas colunas geram o espaço ortogonal ao espaço das colunas de \\(\\mathbf{X}\\).\n\\[\\mathbf{K}^\\top \\mathbf{X} = \\mathbf{0} \\quad \\text{e} \\quad \\text{posto}(\\mathbf{K}) = n-p\\]\nDefinimos o vetor de contrastes de erro transformados como \\(\\mathbf{w} = \\mathbf{K}^\\top \\mathbf{y}\\). A distribuição de \\(\\mathbf{w}\\) depende apenas de \\(\\boldsymbol{\\theta}\\) e não de \\(\\boldsymbol{\\beta}\\):\n\\[\n\\begin{aligned}\nE[\\mathbf{w}] &= E[\\mathbf{K}^\\top (\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\eta})] = \\mathbf{K}^\\top \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{K}^\\top E[\\boldsymbol{\\eta}] = \\mathbf{0} \\cdot \\boldsymbol{\\beta} + \\mathbf{0} = \\mathbf{0} \\\\\n\\text{Var}[\\mathbf{w}] &= \\mathbf{K}^\\top \\text{Var}[\\mathbf{y}] \\mathbf{K} = \\mathbf{K}^\\top \\mathbf{V}(\\boldsymbol{\\theta}) \\mathbf{K}\n\\end{aligned}\n\\]\nA log-verossimilhança baseada nestes contrastes (Verossimilhança Restrita) é:\n\\[L_R(\\boldsymbol{\\theta}) = -\\frac{1}{2} \\ln|\\mathbf{K}^\\top \\mathbf{V}(\\boldsymbol{\\theta}) \\mathbf{K}| - \\frac{1}{2} \\mathbf{w}^\\top (\\mathbf{K}^\\top \\mathbf{V}(\\boldsymbol{\\theta}) \\mathbf{K})^{-1} \\mathbf{w} + \\text{constante}\\]\nA implementação computacional direta desta fórmula é ineficiente devido à necessidade de construir a matriz \\(\\mathbf{K}\\). Contudo, Harville (1977) provou duas identidades algébricas fundamentais que relacionam os componentes da verossimilhança restrita com as matrizes originais:\n\nIdentidade do Determinante: \\(\\ln|\\mathbf{K}^\\top \\mathbf{V} \\mathbf{K}| = \\ln|\\mathbf{V}| + \\ln|\\mathbf{X}^\\top \\mathbf{V}^{-1} \\mathbf{X}| - \\ln|\\mathbf{X}^\\top \\mathbf{X}| - 2\\ln|\\mathbf{K}|\\)\n\nIgnorando os termos que não dependem de \\(\\boldsymbol{\\theta}\\) (\\(\\mathbf{X}\\) e \\(\\mathbf{K}\\) são fixos), o termo relevante é \\(\\ln|\\mathbf{V}| + \\ln|\\mathbf{X}^\\top \\mathbf{V}^{-1} \\mathbf{X}|\\).\n\nIdentidade da Forma Quadrática: \\(\\mathbf{w}^\\top (\\mathbf{K}^\\top \\mathbf{V} \\mathbf{K})^{-1} \\mathbf{w} = (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{GLS})^\\top \\mathbf{V}^{-1} (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{GLS})\\)\n\nIsto demonstra que a forma quadrática nos contrastes é equivalente à soma ponderada dos quadrados dos resíduos GLS no espaço original.\nSubstituindo estas identidades na equação de \\(L_R\\), obtemos a função objetivo do REML apresentada por Marchant e Lark (2007):\n\\[\n\\begin{aligned}\nL_{REML}(\\boldsymbol{\\theta}) \\propto -\\frac{1}{2} \\Bigg( & \\underbrace{\\ln|\\mathbf{V}(\\boldsymbol{\\theta})|}_{\\text{Ajuste da Covariância}} + \\underbrace{(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{GLS})^\\top \\mathbf{V}(\\boldsymbol{\\theta})^{-1} (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{GLS})}_{\\text{Ajuste aos Resíduos}} + \\underbrace{\\ln|\\mathbf{X}^\\top \\mathbf{V}(\\boldsymbol{\\theta})^{-1} \\mathbf{X}|}_{\\text{Penalidade de Complexidade}} \\Bigg)\n\\end{aligned}\n\\tag{3.11}\\]\nO termo adicional \\(\\ln|\\mathbf{X}^\\top \\mathbf{V}^{-1} \\mathbf{X}|\\) atua como uma penalidade. Quanto maior a incerteza na estimação da tendência (refletida na variância do estimador \\(\\hat{\\boldsymbol{\\beta}}\\), que é proporcional a \\((\\mathbf{X}^\\top \\mathbf{V}^{-1} \\mathbf{X})^{-1}\\)), maior será este termo logarítmico (pois estamos a tomar o log da inversa da variância, ou seja, da precisão/informação). Esta penalidade corrige a subestimação da variância inerente ao método ML, fornecendo estimativas não-viesadas para o variograma.\n\n\n3.12.2 REML Robusto\nO estimador REML padrão assume que os dados seguem uma distribuição Gaussiana. Consequentemente, a função de log-verossimilhança inclui um termo quadrático (a distância de Mahalanobis dos resíduos) que penaliza desvios em relação à média. Na presença de outliers (valores provenientes de um processo de contaminação com caudas pesadas), este termo quadrático cresce rapidamente, dominando a função de verossimilhança e forçando o modelo a inflacionar a variância (efeito pepita ou patamar) para acomodar os dados anómalos.\nMarchant e Lark (2007), baseando-se no trabalho de Richardson e Welsh (1995), propõem o REML Robusto, que substitui a norma \\(L_2\\) (quadrática) por uma função de perda robusta (Huber) aplicada aos resíduos decorrelacionados.\nEm dados geoestatísticos, os resíduos não são independentes; são correlacionados pela estrutura espacial \\(\\mathbf{V}(\\boldsymbol{\\theta})\\). Para aplicar uma função de robustez (que geralmente assume independência), é necessário primeiro decorrelacionar os resíduos. Seja o modelo linear misto \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\eta}\\), com \\(\\boldsymbol{\\eta} \\sim N(\\mathbf{0}, \\mathbf{V}(\\boldsymbol{\\theta}))\\). Definimos a raiz quadrada inversa da matriz de covariância, \\(\\mathbf{V}^{-1/2}\\), tal que \\(\\mathbf{V}^{-1/2} \\mathbf{V} (\\mathbf{V}^{-1/2})^T = \\mathbf{I}\\).\nO vetor de resíduos padronizados e decorrelacionados \\(\\boldsymbol{\\epsilon}^*\\) é dado por:\n\\[\\boldsymbol{\\epsilon}^*(\\boldsymbol{\\beta}, \\boldsymbol{\\theta}) = \\mathbf{V}(\\boldsymbol{\\theta})^{-1/2} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\]\nDesta forma, sob a hipótese nula (sem contaminação), \\(\\boldsymbol{\\epsilon}^* \\sim N(\\mathbf{0}, \\mathbf{I})\\).\nNo REML padrão, \\(\\boldsymbol{\\beta}\\) é estimado por GLS (\\(\\hat{\\boldsymbol{\\beta}}_{GLS}\\)), que minimiza a soma dos quadrados de \\(\\boldsymbol{\\epsilon}^*\\). No REML Robusto, \\(\\hat{\\boldsymbol{\\beta}}_{R}\\) é obtido minimizando uma função de perda robusta \\(\\rho_c\\) sobre estes resíduos transformados.\nDado um \\(\\boldsymbol{\\theta}\\) fixo, \\(\\hat{\\boldsymbol{\\beta}}_{R}\\) é o vetor que minimiza:\n\\[Q_{\\beta}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\rho_c \\left( [\\mathbf{V}^{-1/2} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})]_i \\right)\\]\nOnde \\(\\rho_c(\\cdot)\\) é a função de Huber (definida abaixo). Esta etapa garante que a estimativa da tendência (média local) não seja “puxada” por valores extremos, fornecendo uma base estável para a estimativa da variância.\nA função objetivo a ser minimizada para estimar \\(\\boldsymbol{\\theta}\\) substitui o termo quadrático da verossimilhança restrita (Robust Estimation of a Location Parameter) pela soma dos escores de Huber dos resíduos decorrelacionados, calculados com base no \\(\\hat{\\boldsymbol{\\beta}}_{R}\\).\nA função objetivo negativa de log-verossimilhança robusta é definida como:\n\\[L_{Rob}(\\boldsymbol{\\theta}) = \\underbrace{\\frac{1}{2}\\ln|\\mathbf{V}(\\boldsymbol{\\theta})| + \\frac{1}{2}\\ln|\\mathbf{X}^T \\mathbf{V}(\\boldsymbol{\\theta})^{-1} \\mathbf{X}|}_{\\text{Penalidades de Complexidade e Viés (Inalteradas)}} + \\underbrace{\\sum_{i=1}^{n} \\rho_c \\left( \\left[ \\mathbf{V}(\\boldsymbol{\\theta})^{-1/2} (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{R}) \\right]_i \\right)}_{\\text{Termo de Ajuste Robusto aos Resíduos}}\\]\nonde \\(\\mathbf{V}(\\boldsymbol{\\theta})^{-1/2}\\) garante que a métrica de distância considere a correlação espacial. Um outlier espacial não é apenas um valor alto, mas um valor que difere do que seria esperado dada a sua vizinhança. A decorrelação expõe estes valores. \\(\\hat{\\boldsymbol{\\beta}}_{R}\\) é o estimador robusto da tendência. Se usássemos o GLS padrão aqui, o viés na média propagar-se-ia para a variância. \\(\\rho_c(u)\\) (Função de Huber) é uma função híbrida que limita a influência de resíduos grandes. É definida por:\n\\[\n\\rho_c(u) = \\begin{cases}\n\\frac{1}{2}u^2 & \\text{se } |u| \\le c \\\\\nc|u| - \\frac{1}{2}c^2 & \\text{se } |u| &gt; c\n\\end{cases}\n\\]\n\\(|u| \\le c\\), para resíduos pequenos (dados normais), o método comporta-se como o REML padrão (máxima eficiência estatística sob normalidade). \\(|u| &gt; c\\), para resíduos grandes (outliers), a penalidade cresce linearmente em vez de quadraticamente. Isso significa que a derivada da função de perda (a função de influência \\(\\psi(u)\\)) torna-se constante, limitando o peso que uma única observação anômala pode exercer sobre a estimativa dos parâmetros \\(\\boldsymbol{\\theta}\\); \\(c\\) define o limiar entre dados normais e anómalos. Valores comuns situam-se entre \\(1.345\\) e \\(2.0\\). Marchant e Lark (2007) recomendam testar diferentes valores de \\(c\\) e selecionar aquele que minimiza o erro na validação cruzada, pois o grau de contaminação é desconhecido a priori.\nAo minimizar \\(L_{Rob}(\\boldsymbol{\\theta})\\), obtemos estimativas dos parâmetros do variograma (alcance, patamar) que representam a estrutura espacial dominante do processo \\(Y(\\mathbf{s})\\), ignorando as perturbações locais causadas por contaminação.\n\n\nCódigo\npacman::p_load(gt)\n#Calcular Variograma Experimental\nv_exp &lt;- variogram(log(zinc) ~ 1, meuse_sf)\n\n#Definir um modelo inicial (Chute inicial)\n# psill = patamar parcial, range = alcance, nugget = efeito pepita\nmodelo_inicial &lt;- vgm(psill = 0.5, model = \"Sph\", range = 900, nugget = 0.1)\n\n# Ajuste Automático (Mínimos Quadrados Ponderados - WLS)\nmodelo_ajustado &lt;- fit.variogram(v_exp, model = modelo_inicial)\n\nmodelo_ajustado%&gt;%\n  knitr::kable()\n\n\n\n\n\n\n\nmodel\npsill\nrange\nkappa\nang1\nang2\nang3\nanis1\nanis2\n\n\n\n\nNug\n0.0506602\n0.0000\n0.0\n0\n0\n0\n1\n1\n\n\nSph\n0.5906056\n897.0044\n0.5\n0\n0\n0\n1\n1\n\n\n\n\n\nFigura 3.12: Ajuste do modelo teórico sobre o experimental usando WLS\n\n\n\n\nCódigo\npreds &lt;- variogramLine(modelo_ajustado, maxdist = max(v_exp$dist))\n\nggplot() +\n  geom_point(data = v_exp, aes(x = dist, y = gamma), size = 3) +\n  geom_line(data = preds, aes(x = dist, y = gamma),\n            color = \"red\", linewidth =.5) +\n  labs(\n    title = \"Modelo Esférico\",\n    subtitle = paste0(\n      \"Nugget: \", round(modelo_ajustado$psill[1], 3),\n      \" | Sill Total: \", round(sum(modelo_ajustado$psill), 3),\n      \" | Range: \", round(modelo_ajustado$range[2], 1)\n    ),\n    x = \"Distância\",\n    y = \"Semivariância\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigura 3.13: Ajuste do modelo teórico sobre o experimental usando WLS",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#sec-metodos_predicao",
    "href": "geostat.html#sec-metodos_predicao",
    "title": "3  Geoestatística",
    "section": "3.13 Métodos de Predição",
    "text": "3.13 Métodos de Predição\nA predição espacial é o objetivo central da geoestatística: estimar o valor da variável regionalizada \\(Y(\\mathbf{s}_0)\\) num local não amostrado \\(\\mathbf{s}_0 \\in D^G\\), baseando-se nos valores observados \\(\\mathbf{y} = (y(\\mathbf{s}_1), \\dots, y(\\mathbf{s}_n))^\\top\\) em locais vizinhos. O método universalmente consagrado para esta tarefa é a Krigagem (termo cunhado por Matheron em homenagem a D.G. Krige).\nA Krigagem é definida como o Melhor Estimador Linear Não Viciado (BLUE - Best Linear Unbiased Estimator) da variável \\(Y(\\mathbf{s}_0)\\). Vamos decompor esta definição para compreender a sua fundamentação teórica (Cressie 1993; Wackernagel 2003).\n\nEstimador Linear: O preditor \\(\\hat{Y}(\\mathbf{s}_0)\\) é uma combinação linear ponderada das observações disponíveis:\n\n\\[\n\\hat{Y}(\\mathbf{s}_0) = \\sum_{i=1}^{n} \\lambda_i Y(\\mathbf{s}_i) = \\boldsymbol{\\lambda}^\\top \\mathbf{Y}\n\\]\nonde \\(\\lambda_i\\) são os pesos atribuídos a cada observação \\(Y(\\mathbf{s}_i)\\) e \\(\\boldsymbol{\\lambda} = (\\lambda_1, \\dots, \\lambda_n)^\\top\\) é o vetor de pesos. O objetivo da krigagem é determinar os pesos ótimos \\(\\lambda_i\\).\n\nNão Viciado (Unbiasedness): Exige-se que, em média, o estimador não cometa erros sistemáticos. A esperança do erro de predição deve ser nula:\n\n\\[\nE[\\hat{Y}(\\mathbf{s}_0) - Y(\\mathbf{s}_0)] = 0 \\implies E[\\hat{Y}(\\mathbf{s}_0)] = E[Y(\\mathbf{s}_0)]\n\\]\nSubstituindo a forma linear:\n\\[E\\left[ \\sum_{i=1}^{n} \\lambda_i Y(\\mathbf{s}_i) \\right] = \\sum_{i=1}^{n} \\lambda_i E[Y(\\mathbf{s}_i)] = E[Y(\\mathbf{s}_0)]\\]\nEsta condição impõe restrições sobre os pesos \\(\\lambda_i\\), dependendo do modelo assumido para a média \\(E[Y(\\mathbf{s})]\\) (média constante, tendência polinomial, etc.).\n\nMelhor (Best - Minimum Variance): Entre todos os estimadores lineares não viciados possíveis (aqueles que satisfazem as condições acima), a krigagem escolhe aquele que minimiza a variância do erro de predição (ou variância de estimação).\n\nO erro de estimação é \\(\\varepsilon = \\hat{Y}(\\mathbf{s}_0) - Y(\\mathbf{s}_0)\\). A variância deste erro, que queremos minimizar, é:\n\\[\n\\sigma_E^2(\\mathbf{s}_0) = \\text{Var}(\\hat{Y}(\\mathbf{s}_0) - Y(\\mathbf{s}_0)) = E\\left[ (\\hat{Y}(\\mathbf{s}_0) - Y(\\mathbf{s}_0))^2 \\right]\n\\]\nA minimização desta função objetivo quadrática em relação aos pesos \\(\\lambda_i\\), sujeita às restrições de não enviesamento, é realizada através do método dos Multiplicadores de Lagrange, resultando no Sistema de Krigagem.\nPara minimizar a variância do erro sob restrições lineares, construímos a função de Lagrange. A forma geral do sistema depende das suposições sobre a média \\(\\mu(\\mathbf{s})\\). Contudo, a estrutura fundamental deriva da expansão da variância do erro em termos da covariância espacial \\(C(\\mathbf{h})\\) ou do variograma \\(\\gamma(\\mathbf{h})\\).\n\\[\n\\begin{aligned}\n\\sigma_E^2 &= \\text{Var}\\left( \\sum_{i=1}^n \\lambda_i Y(\\mathbf{s}_i) - Y(\\mathbf{s}_0) \\right) \\\\\n&= \\text{Var}\\left( \\sum_{i=1}^n \\lambda_i Y(\\mathbf{s}_i) \\right) + \\text{Var}(Y(\\mathbf{s}_0)) - 2\\text{Cov}\\left( \\sum_{i=1}^n \\lambda_i Y(\\mathbf{s}_i), Y(\\mathbf{s}_0) \\right) \\\\\n&= \\sum_{i=1}^n \\sum_{j=1}^n \\lambda_i \\lambda_j C(\\mathbf{s}_i, \\mathbf{s}_j) + C(0) - 2 \\sum_{i=1}^n \\lambda_i C(\\mathbf{s}_i, \\mathbf{s}_0)\n\\end{aligned}\n\\tag{3.12}\\] onde \\(C(\\mathbf{s}_i, \\mathbf{s}_j) = \\text{Cov}(Y(\\mathbf{s}_i), Y(\\mathbf{s}_j))\\) é a covariância entre dados, e \\(C(0) = \\sigma^2\\) é a variância a priori do processo (Cressie 1993).\nA minimização desta expressão (com restrições) conduz a um sistema de equações lineares da forma \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\), onde:\n\n\\(\\mathbf{A}\\): Matriz de covariâncias (ou variogramas) entre as observações amostrais (redundância de informação).\n\\(\\mathbf{x}\\): Vetor das incógnitas (os pesos \\(\\lambda_i\\) e os multiplicadores de Lagrange).\n\\(\\mathbf{b}\\): Vetor de covariâncias (ou variogramas) entre as observações e o ponto a estimar \\(\\mathbf{s}_0\\) (proximidade da informação).\n\n\n3.13.1 Krigagem Simples (KS)\nA Krigagem Simples (KS) é a forma mais básica, assumindo que a média do processo \\(E[Y(\\mathbf{s})] = \\mu\\) é conhecida e constante em todo o domínio (Andre G. Journel e Huijbregts 1976). Como a média \\(\\mu\\) é conhecida, o estimador baseia-se nos resíduos em relação à média:\n\\[\\hat{Y}_{KS}(\\mathbf{s}_0) = \\mu + \\sum_{i=1}^n \\lambda_i (Y(\\mathbf{s}_i) - \\mu)\\]\nNão enviesamento\nO estimador é não viciado por construção para quaisquer pesos \\(\\lambda_i\\), pois \\(E[Y(\\mathbf{s}_i) - \\mu] = 0\\).\n\\[E[\\hat{Y}_{KS}(\\mathbf{s}_0) - Y(\\mathbf{s}_0)] = \\mu + \\sum \\lambda_i E[Y(\\mathbf{s}_i) - \\mu] - E[Y(\\mathbf{s}_0)] = \\mu + 0 - \\mu = 0\\] Portanto, não existem restrições de soma de pesos (sem multiplicadores de Lagrange).\nVariância mínima\nPara minimizar \\(\\sigma_E^2\\) derivamos Eq. 3.12 em relação a cada peso \\(\\lambda_k\\) (para \\(k=1,\\ldots, n\\)) e igualamos a zero:\n\nDerivada do termo linear:\n\n\\[\n  \\frac{\\partial}{\\partial \\lambda_k} \\left( -2 \\sum_{i=1}^n \\lambda_i C(\\mathbf{s}_i, \\mathbf{s}_0) \\right) = -2 C(\\mathbf{s}_k, \\mathbf{s}_0)\n\\]\n\nDerivada do termo quadrático (dupla soma):\n\nAqui, a derivada gera dois termos idênticos devido à simetria da covariância e à regra do produto:\n\\[\n  \\frac{\\partial}{\\partial \\lambda_k} \\left( \\sum_{i=1}^n \\sum_{j=1}^n \\lambda_i \\lambda_j C(\\mathbf{s}_i, \\mathbf{s}_j) \\right)   = \\sum_{j=1}^n \\lambda_j C(\\mathbf{s}_k, \\mathbf{s}_j) + \\sum_{i=1}^n \\lambda_i C(\\mathbf{s}_i, \\mathbf{s}_k)\n\\] Como \\(C(\\mathbf{s}_i, \\mathbf{s}_k) = C(\\mathbf{s}_k, \\mathbf{s}_i)\\), as duas somas são iguais.\n\nA derivada de \\(C(\\mathbf{s}_0, \\mathbf{s}_0)\\) (constante) é zero.\n\nAssim, temos:\n\\[\n\\begin{aligned}\n\\frac{\\partial \\sigma_E^2}{\\partial \\lambda_k} &= \\sum_{j=1}^n \\lambda_j C(\\mathbf{s}_k, \\mathbf{s}_j) + \\sum_{i=1}^n \\lambda_i C(\\mathbf{s}_i, \\mathbf{s}_k) - 2C(\\mathbf{s}_k, \\mathbf{s}_0) = 0\\\\\n&\\text{Como as duas somas são idênticas, podemos combiná-las}\\\\\n2 \\sum_{j=1}^n \\lambda_j C_{kj} - 2C_{k0} &= 0 \\implies \\sum_{j=1}^n \\lambda_j C_{kj} = C_{k0}, \\quad \\forall k=1,\\dots,n\n\\end{aligned}\n\\]\nEm notação matricial: \\(\\mathbf{\\Sigma} \\boldsymbol{\\lambda} = \\mathbf{c}\\), onde \\(\\mathbf{\\Sigma}\\) é a matriz de covariância dados-dados e \\(\\mathbf{c}\\) é o vetor de covariância dados-alvo. A solução é: \\(\\boldsymbol{\\lambda} = \\mathbf{\\Sigma}^{-1} \\mathbf{c}\\).\nA Krigagem Simples é raramente usada na prática pois o conhecimento exato da média global \\(\\mu\\) é incomum. No entanto, é o fundamento teórico para métodos mais avançados como a Krigagem Gaussiana e Simulação condicional.\n\n\nCódigo\npacman::p_load(stars, gstat, ggplot2, sf, patchwork, viridis)\n\ndata(meuse)\ndata(meuse.area) \n\nmeuse_sf &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\n\narea_sf &lt;- st_polygon(list(as.matrix(meuse.area))) |&gt; \n  st_sfc(crs = 28992) |&gt; \n  st_as_sf()\n\n#Criar Grid de Predição \n# Precisamos definir ONDE queremos estimar\ngrid_pred &lt;- st_bbox(area_sf) |&gt;       \n  st_as_stars(dx = 40, dy = 40) |&gt;     # Define a resolução\n  st_crop(area_sf)                     # RECORTA usando o polígono\n\n\n#Ajuste do Variograma\nv_ord &lt;- variogram(log(zinc) ~ 1, meuse_sf)\nm_ord &lt;- fit.variogram(v_ord, vgm(0.6, \"Sph\", 900, 0.05))\n\nmedia_conhecida &lt;- mean(log(meuse$zinc))\n\n#Krigagem Simples\n# O resultado terá duas camadas: var1.pred e var1.var\nks &lt;- krige(log(zinc) ~ 1, \n            locations = meuse_sf, \n            newdata = grid_pred, \n            model = m_ord, \n            beta = media_conhecida, debug.level = 0)\n\n# Mapa de Predição\nks_masked &lt;- ks[area_sf]\np1 &lt;- ggplot() +\n  geom_stars(data = ks_masked, aes(fill = var1.pred)) +\n  geom_sf(data = area_sf, fill = NA, color = \"black\", linewidth = 0.5) +\n  scale_fill_viridis_c(option = \"B\", na.value = \"transparent\") +\n  labs(title = \"Predição (Log Zinc)\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Mapa de Variância (Erro)\n# A variável de erro se chama 'var1.var'\np2 &lt;- ggplot() +\n  geom_stars(data = ks_masked, aes(fill = var1.var)) +\n geom_sf(data = area_sf, fill = NA, color = \"black\", size = 0.5) + # Contorno\n  scale_fill_viridis_c(option = \"B\", na.value = \"transparent\") + \n  labs(title = \"Variância de Krigagem\", fill = \"Var\", x = NULL, y = NULL) +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\np1 + p2\n\n\n\n\n\n\n\n\nFigura 3.14: Krigagem simples: (a) predição e (b) variância de krigagem\n\n\n\n\n\n\n\n3.13.2 Krigagem Ordinária (KO)\nA Krigagem Ordinária (KO) assume que a média é constante (\\(E[Y(\\mathbf{s})] = \\mu\\)) mas desconhecida (Cressie 1993). O modelo deve estimar a média implicitamente localmente, adaptando-se a flutuações locais do nível da variável.\nO estimador é uma combinação linear direta:\n\\[\\hat{Y}_{KO}(\\mathbf{s}_0) = \\sum_{i=1}^n \\lambda_i Y(\\mathbf{s}_i)\\]\nPara garantir o não enviesamento sem conhecer \\(\\mu\\):\n\\[E[\\hat{Y}_{KO}(\\mathbf{s}_0)] = \\sum \\lambda_i E[Y(\\mathbf{s}_i)] = \\mu \\sum \\lambda_i = \\mu \\implies \\sum_{i=1}^n \\lambda_i = 1 \\tag{3.13}\\]\nPara que o erro seja zero para qualquer \\(\\mu\\), é necessário impor a restrição \\(\\sum_{i=1}^n \\lambda_i = 1\\).\nA esperança na Eq. 3.13 pode ser rescrita por:\n\\[E[\\hat{Y}_{KO} - Y_0] = \\sum \\lambda_i E[Y_i] - E[Y_0] = \\mu \\sum \\lambda_i - \\mu = \\mu \\left( \\sum_{i=1}^n \\lambda_i - 1 \\right)\\]\nComo nosso problema agora não é simplesmente encontrar o mínimo de $ _E^2(_1, …, _n)$, e sim, encontrar o mínimo sujeito a uma restrição, isto é,\n\\[\n\\begin{aligned}\n& \\underset{\\lambda_1, \\dots, \\lambda_n}{\\text{minimizar}}\n& & \\sigma_E^2(\\lambda_1, ..., \\lambda_n) \\\\\n& \\text{sujeito a}\n& & \\sum_{i=1}^n \\lambda_i - 1 = 0\n\\end{aligned}\n\\] temos que somos obrigados a introduzir uma nova variável, o multiplicador \\(\\nu\\), e criamos a função Lagrangiana \\(L\\), que combina a função objetivo original e a restrição. O fator 2 é pura conveniência algébrica, pois cancela o 2 que surgirá das derivadas, simplificando as equações finais.\nO Multiplicador de Lagrange é a ferramenta matemática usada para transformar um problema de otimização com restrições em um problema de otimização sem restrições, facilitando a solução. Assim, temos:\n\\[\n\\begin{aligned}\nL(\\boldsymbol{\\lambda}, \\nu) &= \\sigma_E^2 + 2\\nu \\left( \\sum_{i=1}^n \\lambda_i - 1 \\right)\\\\\nL(\\boldsymbol{\\lambda}, \\nu) &= \\sum_i \\sum_j \\lambda_i \\lambda_j C_{ij} + C(0) - 2 \\sum_i \\lambda_i C_{i0} + 2\\nu \\left( \\sum_i \\lambda_i - 1 \\right)\n\\end{aligned}\n\\]\nDerivando em relação a \\(\\lambda_k\\) e \\(\\nu\\) obtemos:\n\n\\(\\frac{\\partial L}{\\partial \\lambda_k} = 2 \\sum_{j=1}^n \\lambda_j C_{kj} - 2 C_{k0} + 2\\nu = 0 \\implies \\sum_{j=1}^n \\lambda_j C_{kj} + \\nu = C_{k0}\\)\n\\(\\frac{\\partial L}{\\partial \\nu} = 2 \\left( \\sum_{i=1}^n \\lambda_i - 1 \\right) = 0 \\implies \\sum_{i=1}^n \\lambda_i = 1\\)\n\nIsto resulta num sistema de Krigagem Ordinária equações:\nO sistema de \\((n+1)\\) equações resultante (em termos de covariograma \\(C(\\cdot)\\)) é:\n\\[\\begin{cases}\n\\sum_{j=1}^n \\lambda_j  C_{kj} + \\nu = C_{k0}, & i=1, \\dots, n \\\\\n\\sum_{j=1}^n \\lambda_j = 1\n\\end{cases}\\]\nEm notação matricial \\(\\mathbf{A}_{KO} \\mathbf{x}_{KO} = \\mathbf{b}_{KO}\\):\n\\[\n\\begin{bmatrix}\nC_{11} & \\dots & C_{1n} & 1 \\\\\n\\vdots & \\ddots & \\vdots & \\vdots \\\\\nC_{n1} & \\dots & C_{nn} & 1 \\\\\n1 & \\dots & 1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\lambda_1 \\\\ \\vdots \\\\ \\lambda_n \\\\ \\nu\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nC_{10} \\\\ \\vdots \\\\ C_{n0} \\\\ 1\n\\end{bmatrix}\n\\] A Krigagem Ordinária é robusta a tendências locais (drift) se a vizinhança de krigagem for restrita, pois reestima a média localmente em cada janela de busca.\nVariância de Krigagem\nPara obter a variância de krigagem ordinária \\(\\sigma_{KO}^2\\), substituímos a relação de otimalidade (\\(\\sum \\lambda_j C_{ij} = C_{i0} - \\nu\\)) na expressão original da variância do erro:\n\\[\n\\begin{aligned}\n\\sigma_{KO}^2 &= \\sum_{i=1}^n \\lambda_i \\underbrace{\\left( \\sum_{j=1}^n \\lambda_j C_{ij} \\right)}_{C_{i0} - \\nu} + C(0) - 2 \\sum_{i=1}^n \\lambda_i C_{i0} \\\\\n&= \\sum_{i=1}^n \\lambda_i (C_{i0} - \\nu) + C(0) - 2 \\sum_{i=1}^n \\lambda_i C_{i0} \\\\\n&= \\sum_{i=1}^n \\lambda_i C_{i0} - \\nu \\underbrace{\\sum_{i=1}^n \\lambda_i}_{1} + C(0) - 2 \\sum_{i=1}^n \\lambda_i C_{i0} \\\\\n&= C(0) - \\sum_{i=1}^n \\lambda_i C_{i0} - \\nu\n\\end{aligned}\n\\]\nEm termos de variograma (\\(\\gamma (h)\\)), e usando a relação \\(C(h) = C(0) - \\gamma(h)\\), a expressão equivalente é (Andre G. Journel e Huijbregts 1976):\n\\[\\sigma_{KO}^2 = \\sum_{i=1}^n \\lambda_i \\gamma(\\mathbf{s}_i - \\mathbf{s}_0) + \\nu - \\gamma(\\mathbf{0})\\]\nNote-se que \\(\\nu\\) é o multiplicador de Lagrange, que pode ser interpretado como o custo em variância de não conhecermos a média verdadeira.\nSe assumirmos que \\(\\gamma(\\mathbf{0}) = 0\\) (o variograma teórico na origem é nulo, o efeito pepita \\(C_0\\) é o limite quando \\(h \\to 0\\)), a expressão simplifica-se. Em termos matriciais:\n\\[\\sigma_{KO}^2(\\mathbf{s}_0) = \\mathbf{b}_{KO}^\\top \\mathbf{x}_{KO} = \\sum_{i=1}^n \\lambda_i \\gamma_{i0} + \\nu\\]\nA variância de krigagem depende apenas da configuração geométrica das amostras e do modelo de variograma, mas não depende dos valores observados \\(y(\\mathbf{s}_i)\\) (propriedade de homocedasticidade da krigagem linear sob normalidade). O mapa de variância de krigagem reflete a densidade da amostragem:\n\nÉ zero nos locais amostrados (se não houver efeito pepita/erro de medição e usarmos krigagem exata).\nAumenta à medida que nos afastamos dos pontos de dados.\nDepende da estrutura espacial: modelos com maior efeito pepita ou menor alcance resultam em maior incerteza de predição.\n\n\n\nCódigo\npacman::p_load(stars, gstat, ggplot2, sf, patchwork, viridis, sp)\n\n\ndata(meuse)\ndata(meuse.area) \n\nmeuse_sf &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\n\narea_sf &lt;- st_polygon(list(as.matrix(meuse.area))) |&gt; \n  st_sfc(crs = 28992) |&gt; \n  st_as_sf()\n\n#Criar Grid de Predição \n# Precisamos definir ONDE queremos estimar\ngrid_pred &lt;- st_bbox(area_sf) |&gt;       \n  st_as_stars(dx = 40, dy = 40) |&gt;     \n  st_crop(area_sf)                     \n\n#Krigagem\n# modelo_ajustado &lt;- vgm(0.5, \"Sph\", 900, 0.1)\n\nkrigagem &lt;- krige(\n  log(zinc) ~ 1,\n  locations = meuse_sf,\n  newdata   = grid_pred,\n  model     = modelo_ajustado, debug.level = 0\n)\n\n# var1.pred = Valor Estimado \n# var1.var = Variância de Krigagem (Erro)\n# Mapa 1: Predição\np1 &lt;- ggplot() +\n  geom_stars(data = krigagem, aes(fill = var1.pred)) +\n  geom_sf(data = area_sf, fill = NA, color = \"black\", size = 0.5) + # Contorno\n  scale_fill_viridis_c(option = \"B\", name = \"Log(Zinc)\", na.value = \"transparent\") +\n  labs(title = \"Predição\") +\n  theme_void()+\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Mapa 2: Variância (Erro)\np2 &lt;- ggplot() +\n  geom_stars(data = krigagem, aes(fill = var1.var)) +\n  geom_sf(data = area_sf, fill = NA, color = \"black\", size = 0.5) +\n  scale_fill_viridis_c(option = \"B\", name = \"Variância\", na.value = \"transparent\") +\n  labs(title = \"Incerteza (Variância)\") +\n  theme_minimal() + #para colocar coordenadas\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title = element_blank()\n        )\n\np1 + p2\n\n\n\n\n\n\n\n\nFigura 3.15: Krigagem Ordinária: (a) predição e (b) variância de krigagem\n\n\n\n\n\n\n\n3.13.3 Krigagem Universal (KU)\nQuando a média do processo não é constante, mas apresenta uma tendência sistemática (drift) que pode ser modelada como uma função determinística das coordenadas (ex: \\(E[Y(\\mathbf{s})] = \\mu(\\mathbf{s}) = \\beta_0 + \\beta_1 x + \\beta_2 y\\)), utilizamos a Krigagem Universal (KU) (George Matheron 1971).\nAssumimos que a média é uma combinação linear de \\(L+1\\) funções de base conhecidas \\(f_l(\\mathbf{s})\\) (monómios):\n\\[\\mu(\\mathbf{s}) = \\sum_{l=0}^L a_l f_l(\\mathbf{s}), \\quad \\text{com } f_0(\\mathbf{s}) \\equiv 1\\] Isto pode ser reescrito como: \\[\n\\begin{aligned}\n&Y(\\mathbf{s}) = \\mu(\\mathbf{s}) + \\delta(\\mathbf{s}), \\quad \\text{com } E[\\delta(\\mathbf{s})] = 0, \\quad \\text{Cov}(\\delta(\\mathbf{s}_i), \\delta(\\mathbf{s}_j)) = C(\\mathbf{s}_i, \\mathbf{s}_j)\\\\\n&\\mu(\\mathbf{s}) = \\sum_{l=0}^{L} a_l f_l(\\mathbf{s}), \\quad f_0(\\mathbf{s}) \\equiv 1\\\\\n&\\hat{Y}(\\mathbf{s}_0) = \\sum_{i=1}^n \\lambda_i Y(\\mathbf{s}_i)\n\\end{aligned}\n\\] Condição de não enviesamento:\n\\[\n\\begin{aligned}\nE[\\hat{Y}(\\mathbf{s}_0) - Y(\\mathbf{s}_0)] &= \\sum_{i=1}^n \\lambda_i \\mu(\\mathbf{s}_i) - \\mu(\\mathbf{s}_0)\\\\\n&= \\sum_{i=1}^n \\lambda_i \\sum_{l=0}^L a_l f_l(\\mathbf{s}_i) - \\sum_{l=0}^L a_l f_l(\\mathbf{s}_0)\\\\\n&= \\sum_{l=0}^L a_l \\left[ \\sum_{i=1}^n \\lambda_i f_l(\\mathbf{s}_i) - f_l(\\mathbf{s}_0) \\right] \\overset{!}{=} 0\n\\end{aligned}\n\\]\nComo isto deve ser verdade para quaisquer coeficientes \\(a_l\\) desconhecidos, os termos entre parênteses devem ser zero individualmente. Isto gera \\(L+1\\) restrições:\n\\[\\sum_{i=1}^n \\lambda_i f_l(\\mathbf{s}_i) = f_l(\\mathbf{s}_0), \\quad \\text{para } l=0, \\dots, L\\]\nVariância do erro:\n\\[\n\\begin{aligned}\n\\sigma_E^2 &= \\text{Var}\\left(\\hat{Y}(\\mathbf{s}_0) - Y(\\mathbf{s}_0)\\right)\\\\\n&= \\text{Var}\\left(\\sum_{i=1}^n \\lambda_i \\delta(\\mathbf{s}_i) - \\delta(\\mathbf{s}_0)\\right)\\\\\n&= \\sum_{i=1}^n \\sum_{j=1}^n \\lambda_i \\lambda_j C(\\mathbf{s}_i, \\mathbf{s}_j) - 2\\sum_{i=1}^n \\lambda_i C(\\mathbf{s}_i, \\mathbf{s}_0) + C(\\mathbf{s}_0, \\mathbf{s}_0)\n\\end{aligned}\n\\]\nDe igual modo como feito na KO, introduzimos \\(L+1\\) multiplicadores de Lagrange \\(\\nu_l\\).\n\\[\n\\begin{aligned}\nL(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) &= \\sigma_E^2 - 2\\sum_{l=0}^L \\nu_l \\left(\\sum_{i=1}^n \\lambda_i f_l(\\mathbf{s}_i) - f_l(\\mathbf{s}_0)\\right)\\\\\n&= \\sum_{i=1}^n\\sum_{j=1}^n \\lambda_i\\lambda_j C_{ij} - 2\\sum_{i=1}^n \\lambda_i C_{i0} + C_{00} - 2\\sum_{l=0}^L \\nu_l\\left(\\sum_{i=1}^n \\lambda_i f_{il} - f_{0l}\\right)\n\\end{aligned}\n\\] Para procedermos com a minimização da variância, novamente calculamos as derivadas em relação a \\(\\lambda_i\\) e \\(\\nu_l\\) :\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\lambda_k} &= \\frac{\\partial}{\\partial \\lambda_k}\\left(\\sum_{i=1}^n\\sum_{j=1}^n \\lambda_i\\lambda_j C_{ij}\\right) - 2C_{k0} - 2\\sum_{l=0}^L \\nu_l f_{kl}\\\\\n&= \\sum_{j=1}^n \\lambda_j C_{kj} + \\sum_{i=1}^n \\lambda_i C_{ik} - 2C_{k0} - 2\\sum_{l=0}^L \\nu_l f_{kl}\\\\\n2\\sum_{j=1}^n \\lambda_j C_{kj} - 2C_{k0} - 2\\sum_{l=0}^L \\nu_l f_{kl} &=  0\\\\\n\\sum_{j=1}^n \\lambda_j C_{kj} - \\sum_{l=0}^L \\nu_l f_l(\\mathbf{s}_k) &= C_{k0}, \\quad k = 1,\\dots,n\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\nu_l} = -2\\left(\\sum_{i=1}^n \\lambda_i f_l(\\mathbf{s}_i) - f_l(\\mathbf{s}_0)\\right) &= 0\\\\\n\\sum_{i=1}^n \\lambda_i f_l(\\mathbf{s}_i) &= f_l(\\mathbf{s}_0), \\quad l = 0,\\dots,L\n\\end{aligned}\n\\]\nPara a notação matricial definimos:\n\\[\n\\begin{aligned}\n&\\mathbf{C}_{nn} = [C_{ij}]_{n\\times n}, \\quad \\mathbf{F}_{n\\times(L+1)} = [f_l(\\mathbf{s}_i)]\\\\\n&\\mathbf{c}_{n0} = [C_{i0}]_{n\\times 1}, \\quad \\mathbf{f}_{L0} = [f_l(\\mathbf{s}_0)]_{(L+1)\\times 1}\\\\\n&\\boldsymbol{\\lambda} = [\\lambda_1, \\dots, \\lambda_n]^\\top, \\quad \\boldsymbol{\\nu} = [\\nu_0, \\dots, \\nu_L]^\\top\n\\end{aligned}\n\\] e obtemos:\n\\[\n\\begin{bmatrix}\n\\mathbf{C}_{nn} & -\\mathbf{F} \\\\\n-\\mathbf{F}^\\top & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{\\lambda} \\\\ \\boldsymbol{\\nu}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{c}_{n0} \\\\ -\\mathbf{f}_{L0}\n\\end{bmatrix}\n\\] É comum alguns autores definem \\(\\mu_l = -\\nu_l\\), obtendo:\n\\[\n\\begin{bmatrix}\n\\mathbf{C}_{nn} & \\mathbf{F} \\\\\n\\mathbf{F}^\\top & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{\\lambda} \\\\ \\boldsymbol{\\mu}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{c}_{n0} \\\\ \\mathbf{f}_{L0}\n\\end{bmatrix}\n\\]\nOnde \\(\\mathbf{F}\\) contém os valores das funções de deriva nos pontos amostrais.\nA variância mínima é dada por:\n\\[\n\\sigma_{\\min}^2 = C_{00} - \\sum_{i=1}^n \\lambda_i C_{i0} + \\sum_{l=0}^L \\nu_l f_l(\\mathbf{s}_0)\n\\]\nA matriz \\(\\mathbf{C}_{nn}\\) deve representar a covariância dos resíduos \\(\\delta(\\mathbf{s})\\), não dos dados originais \\(Y(\\mathbf{s})\\). Como os resíduos são desconhecidos, na prática estima-se frequentemente o variograma na direção ortogonal à tendência ou usa-se REML para estimar o variograma dos resíduos diretamente.\nQuando \\(L=0\\) (apenas \\(f_0(\\mathbf{s})=1\\)), recupera-se o sistema da KO com um multiplicador \\(\\nu_0\\).\n\n\nCódigo\npacman::p_load(stars, gstat, ggplot2, sf, patchwork, viridis)\n\ndata(meuse)\ndata(meuse.grid)\n\n\nmeuse_sf &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992, remove = FALSE)\n\ngrid_sf &lt;- st_as_sf(meuse.grid, coords = c(\"x\", \"y\"), crs = 28992, remove = FALSE)\n\n#fórmula: ~ x + y (tendência linear nas coordenadas)\nv_uni &lt;- variogram(log(zinc) ~ x + y, meuse_sf)\nm_uni &lt;- fit.variogram(v_uni, vgm(0.5, \"Sph\", 800, 0.05))\n\n#Krigagem Universal\nku_points &lt;- krige(log(zinc) ~ x + y, \n                   locations = meuse_sf, \n                   newdata = grid_sf, \n                   model = m_uni, debug.level = 0)\n\n# dx e dy definem o tamanho do pixel (40m para o dataset meuse)\nku_stars &lt;- st_rasterize(ku_points, dx = 40, dy = 40)\n\n# Mapa de Predição\np1 &lt;- ggplot() +\n  geom_stars(data = ku_stars, aes(fill = var1.pred)) +\n  scale_fill_viridis_c(option = \"B\", na.value = \"transparent\") +\n  labs(title = \"Krigagem Universal (Pred)\", fill = \"Log(Zn)\", x = NULL, y = NULL) +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Mapa de Variância\np2 &lt;- ggplot() +\n  geom_stars(data = ku_stars, aes(fill = var1.var)) +\n  scale_fill_viridis_c(option = \"B\", na.value = \"transparent\") +\n  labs(title = \"Variância (Erro)\", fill = \"Var\", x = NULL, y = NULL) +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\np1 + p2\n\n\n\n\n\n\n\n\nFigura 3.16: Krigagem Universal: (a) predição e (b) variância de krigagem\n\n\n\n\n\n\n\n3.13.4 Krigagem com Deriva Externa (KED)\nA Krigagem com Deriva Externa (KED) é uma variante da Krigagem Universal onde a tendência \\(\\mu(\\mathbf{s})\\) não é modelada por coordenadas, mas sim por uma ou mais variáveis auxiliares (covariáveis) \\(x_k(\\mathbf{s})\\) conhecidas exaustivamente em todo o domínio (ex: um Modelo Digital de Elevação para interpolar temperatura ou precipitação) (Goovaerts 1997).\nO modelo para a média é: \\(E[Y(\\mathbf{s})] = a_0 + a_1 x_1(\\mathbf{s})\\)\n\\[\n\\begin{aligned}\n&Y(\\mathbf{s}) = \\mu(\\mathbf{s}) + \\delta(\\mathbf{s}), \\quad \\text{com } E[\\delta(\\mathbf{s})] = 0, \\quad \\text{Cov}(\\delta(\\mathbf{s}_i), \\delta(\\mathbf{s}_j)) = C(\\mathbf{s}_i, \\mathbf{s}_j)\\\\\n&\\mu(\\mathbf{s}) = a_0 + a_1 x(\\mathbf{s}) \\quad \\text{(modelo linear com covariável } x\\text{)}\\\\\n&\\hat{Y}(\\mathbf{s}_0) = \\sum_{i=1}^n \\lambda_i Y(\\mathbf{s}_i)\n\\end{aligned}\n\\] Condição de não enviesamento:\n\\[\n\\begin{aligned}\nE[\\hat{Y}(\\mathbf{s}_0) - Y(\\mathbf{s}_0)] &= \\sum_{i=1}^n \\lambda_i \\mu(\\mathbf{s}_i) - \\mu(\\mathbf{s}_0)\\\\\n&= \\sum_{i=1}^n \\lambda_i (a_0 + a_1 x(\\mathbf{s}_i)) - (a_0 + a_1 x(\\mathbf{s}_0))\\\\\n&= a_0\\left(\\sum_{i=1}^n \\lambda_i - 1\\right) + a_1\\left(\\sum_{i=1}^n \\lambda_i x(\\mathbf{s}_i) - x(\\mathbf{s}_0)\\right) \\overset{!}{=} 0\n\\end{aligned}\n\\] Para que valha para quaisquer \\(a_0, a_1\\) desconhecidos é necessário que:\n\\[\n\\sum_{i=1}^n \\lambda_i = 1 \\quad \\text{e} \\quad \\sum_{i=1}^n \\lambda_i x(\\mathbf{s}_i) = x(\\mathbf{s}_0)\n\\]\nVariância do erro\n\\[\n\\begin{aligned}\n\\sigma_E^2 &= \\text{Var}\\left(\\hat{Y}(\\mathbf{s}_0) - Y(\\mathbf{s}_0)\\right)\\\\\n&= \\text{Var}\\left(\\sum_{i=1}^n \\lambda_i \\delta(\\mathbf{s}_i) - \\delta(\\mathbf{s}_0)\\right)\\\\\n&= \\sum_{i=1}^n\\sum_{j=1}^n \\lambda_i\\lambda_j C_{ij} - 2\\sum_{i=1}^n \\lambda_i C_{i0} + C_{00}\n\\end{aligned}\n\\] Novamente, introduzindo multiplicadores \\(\\nu_0\\) e \\(\\nu_1\\) e para minimizar derivamos derivamos:\n\\[\n\\begin{aligned}\nL(\\boldsymbol{\\lambda}, \\nu_0, \\nu_1) &= \\sigma_E^2 - 2\\nu_0\\left(\\sum_{i=1}^n \\lambda_i - 1\\right) - 2\\nu_1\\left(\\sum_{i=1}^n \\lambda_i x(\\mathbf{s}_i) - x(\\mathbf{s}_0)\\right)\\\\\n&= \\sum_{i=1}^n\\sum_{j=1}^n \\lambda_i\\lambda_j C_{ij} - 2\\sum_{i=1}^n \\lambda_i C_{i0} + C_{00} - 2\\nu_0\\left(\\sum_{i=1}^n \\lambda_i - 1\\right) - 2\\nu_1\\left(\\sum_{i=1}^n \\lambda_i x_i - x_0\\right)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\lambda_k} &= \\frac{\\partial}{\\partial \\lambda_k}\\left(\\sum_{i=1}^n\\sum_{j=1}^n \\lambda_i\\lambda_j C_{ij}\\right) - 2C_{k0} - 2\\nu_0 - 2\\nu_1 x_k\\\\\n&= \\sum_{j=1}^n \\lambda_j C_{kj} + \\sum_{i=1}^n \\lambda_i C_{ik} - 2C_{k0} - 2\\nu_0 - 2\\nu_1 x_k\\\\\n&= 2\\sum_{j=1}^n \\lambda_j C_{kj} - 2C_{k0} - 2\\nu_0 - 2\\nu_1 x_k = 0\\\\\n\\sum_{j=1}^n \\lambda_j C_{kj} - \\nu_0 - \\nu_1 x_k &= C_{k0}, \\quad k = 1,\\dots,n\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\nu_0} &= -2\\left(\\sum_{i=1}^n \\lambda_i - 1\\right) = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^n \\lambda_i = 1\\\\\n\\frac{\\partial L}{\\partial \\nu_1} &= -2\\left(\\sum_{i=1}^n \\lambda_i x_i - x_0\\right) = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^n \\lambda_i x_i = x_0\n\\end{aligned}\n\\] Novamente definimos:\n\\[\n\\begin{aligned}\n&\\mathbf{C}_{nn} = [C_{ij}]_{n\\times n}, \\quad \\mathbf{X} = [1 \\ x_i]_{n\\times 2}, \\quad \\mathbf{c}_{n0} = [C_{i0}]_{n\\times 1}\\\\\n&\\mathbf{x}_0 = \\begin{bmatrix} 1 \\\\ x_0 \\end{bmatrix}_{2\\times 1}, \\quad \\boldsymbol{\\lambda} = [\\lambda_1, \\dots, \\lambda_n]^\\top, \\quad \\boldsymbol{\\nu} = [\\nu_0, \\nu_1]^\\top\n\\end{aligned}\n\\] para obter o sistema:\n\\[\n\\begin{bmatrix}\n\\mathbf{C}_{nn} & -\\mathbf{X} \\\\\n-\\mathbf{X}^\\top & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{\\lambda} \\\\ \\boldsymbol{\\nu}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{c}_{n0} \\\\ -\\mathbf{z}_0\n\\end{bmatrix}\n\\]\nAlternativamente, com \\(\\boldsymbol{\\mu} = -\\boldsymbol{\\nu}\\):\n\\[\n\\begin{bmatrix}\n\\mathbf{C}_{nn} & \\mathbf{X} \\\\\n\\mathbf{X}^\\top & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{\\lambda} \\\\ \\boldsymbol{\\mu}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{c}_{n0} \\\\ \\mathbf{x}_0\n\\end{bmatrix}\n\\] Variância mínima da krigagem:\n\\[\n\\sigma_{\\text{KED}}^2 = C_{00} - \\sum_{i=1}^n \\lambda_i C_{i0} + \\nu_0 + \\nu_1 x_0\n\\] A KED é um caso particular da KU onde as funções de base são substituídas por covariáveis conhecidas. Para p covariáveis:\n\\[\n\\mu(\\mathbf{s}) = a_0 + \\sum_{k=1}^p a_k x_k(\\mathbf{s}) \\Rightarrow \\sum_{i=1}^n \\lambda_i x_k(\\mathbf{s}_i) = x_k(\\mathbf{s}_0), \\ k=0,\\dots,p\n\\] com \\(x_0(\\mathbf{s}) \\equiv 1\\).\nUma vantagem da KED é que ela incorpora eficientemente informação auxiliar correlacionada com a variável primária, melhorando a precisão da interpolação quando a covariável captura padrões de larga escala. Note que esta covariável deve ser conhecida em todos os pontos (amostrados e não amostrados). Covariância \\(C\\) deve representar a dependência espacial dos resíduos após remover o efeito da covariável\n\n\nCódigo\npacman::p_load(stars, gstat, ggplot2, sf, patchwork, viridis)\n\ndata(meuse)\ndata(meuse.grid)\n\n# Precisamos garantir que a coluna 'dist' (nossa covariável x_k) esteja presente\nmeuse_sf &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\n\n#Preparar o Grid (Newdata)\n# Aqui usamos covariavel 'dist' pois TAMBÉM precisa existir para todos os pixels\ngrid_sf &lt;- st_as_sf(meuse.grid, coords = c(\"x\", \"y\"), crs = 28992)\n\ngrid_stars &lt;- st_rasterize(grid_sf, dx = 40, dy = 40)\n\n#Variograma\nv_ked &lt;- variogram(log(zinc) ~ dist, meuse_sf)\nm_ked &lt;- fit.variogram(v_ked, vgm(0.5, \"Exp\", 800, 0.05))\n\nplot(v_ked, m_ked, main = \"Variograma (KED)\")\n\n# Krigagem com Deriva Externa (KED)\nked &lt;- krige(log(zinc) ~ dist, \n             locations = meuse_sf, \n             newdata = grid_stars, \n             model = m_ked, debug.level = 0)\n\n#\n# Predição\np1 &lt;- ggplot() +\n  geom_stars(data = ked, aes(fill = var1.pred)) +\n  scale_fill_viridis_c(option = \"plasma\", na.value = \"transparent\") +\n  labs(title = \"KED Predição (Covariável: Dist)\", \n       subtitle = \"Tendência guiada pela distância ao rio\",\n       fill = \"Log(Zn)\", x = NULL, y = NULL) +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(size = 9))\n\n# Variância (Erro)\np2 &lt;- ggplot() +\n  geom_stars(data = ked, aes(fill = var1.var)) +\n  scale_fill_viridis_c(option = \"cividis\", na.value = \"transparent\") +\n  labs(title = \"Variância KED\", fill = \"Var\", x = NULL, y = NULL) +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\np1 + p2\n\n\n\n\n\n\n\n\nFigura 3.17: Krigagem com deriva externa: (a) predição e (b) variância de krigagem\n\n\n\n\n\n\n\n\n\n\n\nFigura 3.18: Krigagem com deriva externa: (a) predição e (b) variância de krigagem\n\n\n\n\n\n\n\n3.13.5 Co-Krigagem\nA Co-Krigagem (CK) é a extensão multivariada da krigagem. Utiliza-se quando queremos estimar uma variável primária \\(Y_1(\\mathbf{s}_0)\\) utilizando a correlação espacial não só com as suas próprias amostras, mas também com amostras de uma ou mais variáveis secundárias \\(Y_2(\\mathbf{s}), \\dots, Y_k(\\mathbf{s})\\), que estão correlacionadas espacialmente com \\(Y_1\\) (co-regionalização) (Ver Hoef e Barry 1998; Wackernagel 2003). Assim, temos:\n\\[\n\\begin{aligned}\n&Y_1(\\mathbf{s}) = \\mu_1 + \\delta_1(\\mathbf{s}), \\quad E[\\delta_1(\\mathbf{s})] = 0 \\\\\n&Y_2(\\mathbf{s}) = \\mu_2 + \\delta_2(\\mathbf{s}), \\quad E[\\delta_2(\\mathbf{s})] = 0 \\\\\n&\\text{Cov}(\\delta_a(\\mathbf{s}_i), \\delta_b(\\mathbf{s}_j)) = C_{ab}(\\mathbf{s}_i, \\mathbf{s}_j), \\quad a,b = 1,2\\\\\n&\\hat{Y}_1(\\mathbf{s}_0) = \\sum_{i=1}^{n_1} \\lambda_{1i} Y_1(\\mathbf{s}_{1i}) + \\sum_{j=1}^{n_2} \\lambda_{2j} Y_2(\\mathbf{s}_{2j})\n\\end{aligned}\n\\]\nCondição de não enviesamento:\n\\[\n\\begin{aligned}\nE[\\hat{Y}_1(\\mathbf{s}_0) - Y_1(\\mathbf{s}_0)] &= \\sum_{i=1}^{n_1} \\lambda_{1i} \\mu_1 + \\sum_{j=1}^{n_2} \\lambda_{2j} \\mu_2 - \\mu_1 \\\\\n&= \\mu_1\\left(\\sum_{i=1}^{n_1} \\lambda_{1i} - 1\\right) + \\mu_2 \\sum_{j=1}^{n_2} \\lambda_{2j} \\overset{!}{=} 0\n\\end{aligned}\n\\] Como sempre para que valha para quaisquer \\(\\mu_1, \\mu_2\\) desconhecidos, definimos:\n\\[\n\\sum_{i=1}^{n_1} \\lambda_{1i} = 1 \\quad \\text{e} \\quad \\sum_{j=1}^{n_2} \\lambda_{2j} = 0\n\\]\nVariância do erro:\n\\[\n\\begin{aligned}\n\\sigma_E^2 &= \\text{Var}\\left(\\hat{Y}_1(\\mathbf{s}_0) - Y_1(\\mathbf{s}_0)\\right) \\\\\n&= \\text{Var}\\left(\\sum_{i=1}^{n_1} \\lambda_{1i} \\delta_1(\\mathbf{s}_{1i}) + \\sum_{j=1}^{n_2} \\lambda_{2j} \\delta_2(\\mathbf{s}_{2j}) - \\delta_1(\\mathbf{s}_0)\\right) \\\\\n&= \\sum_{i=1}^{n_1}\\sum_{k=1}^{n_1} \\lambda_{1i}\\lambda_{1k} C_{11}(\\mathbf{s}_{1i}, \\mathbf{s}_{1k}) \\\\\n&\\quad + \\sum_{j=1}^{n_2}\\sum_{l=1}^{n_2} \\lambda_{2j}\\lambda_{2l} C_{22}(\\mathbf{s}_{2j}, \\mathbf{s}_{2l}) \\\\\n&\\quad + 2\\sum_{i=1}^{n_1}\\sum_{j=1}^{n_2} \\lambda_{1i}\\lambda_{2j} C_{12}(\\mathbf{s}_{1i}, \\mathbf{s}_{2j}) \\\\\n&\\quad - 2\\sum_{i=1}^{n_1} \\lambda_{1i} C_{11}(\\mathbf{s}_{1i}, \\mathbf{s}_0) - 2\\sum_{j=1}^{n_2} \\lambda_{2j} C_{12}(\\mathbf{s}_{2j}, \\mathbf{s}_0) \\\\\n&\\quad + C_{11}(\\mathbf{s}_0, \\mathbf{s}_0)\n\\end{aligned}\n\\] Definindo o Langrangiano temos:\n\\[\n\\begin{aligned}\nL(\\boldsymbol{\\lambda}_1, \\boldsymbol{\\lambda}_2, \\nu_1, \\nu_2) &= \\sigma_E^2 - 2\\nu_1\\left(\\sum_{i=1}^{n_1} \\lambda_{1i} - 1\\right) - 2\\nu_2\\sum_{j=1}^{n_2} \\lambda_{2j}\n\\end{aligned}\n\\] cujas derivadas são:\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\lambda_{1i}} = 2\\sum_{k=1}^{n_1} \\lambda_{1k} C_{11}(\\mathbf{s}_{1i}, \\mathbf{s}_{1k}) + 2\\sum_{j=1}^{n_2} \\lambda_{2j} C_{12}(\\mathbf{s}_{1i}, \\mathbf{s}_{2j}) - 2C_{11}(\\mathbf{s}_{1i}, \\mathbf{s}_0) - 2\\nu_1 &= 0\\\\\n\\sum_{k=1}^{n_1} \\lambda_{1k} C_{11}(\\mathbf{s}_{1i}, \\mathbf{s}_{1k}) + \\sum_{j=1}^{n_2} \\lambda_{2j} C_{12}(\\mathbf{s}_{1i}, \\mathbf{s}_{2j}) - \\nu_1 &= C_{11}(\\mathbf{s}_{1i}, \\mathbf{s}_0)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\lambda_{2j}} = 2\\sum_{l=1}^{n_2} \\lambda_{2l} C_{22}(\\mathbf{s}_{2j}, \\mathbf{s}_{2l}) + 2\\sum_{i=1}^{n_1} \\lambda_{1i} C_{12}(\\mathbf{s}_{1i}, \\mathbf{s}_{2j}) - 2C_{12}(\\mathbf{s}_{2j}, \\mathbf{s}_0) - 2\\nu_2 &= 0\\\\\n\\sum_{l=1}^{n_2} \\lambda_{2l} C_{22}(\\mathbf{s}_{2j}, \\mathbf{s}_{2l}) + \\sum_{i=1}^{n_1} \\lambda_{1i} C_{12}(\\mathbf{s}_{1i}, \\mathbf{s}_{2j}) - \\nu_2 &= C_{12}(\\mathbf{s}_{2j}, \\mathbf{s}_0)\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\nu_1} &= -2\\left(\\sum_{i=1}^{n_1} \\lambda_{1i} - 1\\right) = 0 \\quad \\Rightarrow \\quad\\sum_{i=1}^{n_1} \\lambda_{1i} = 1 \\\\\n\\frac{\\partial L}{\\partial \\nu_2} &= -2\\sum_{j=1}^{n_2} \\lambda_{2j} = 0 \\quad \\Rightarrow \\quad \\sum_{j=1}^{n_2} \\lambda_{2j} = 0\n\\end{aligned}\n\\]\nNovamente, definimos:\n\\[\n\\begin{aligned}\n&\\mathbf{C}_{11} = [C_{11}(\\mathbf{s}_{1i}, \\mathbf{s}_{1k})]_{n_1 \\times n_1}, \\quad\n\\mathbf{C}_{22} = [C_{22}(\\mathbf{s}_{2j}, \\mathbf{s}_{2l})]_{n_2 \\times n_2} \\\\\n&\\mathbf{C}_{12} = [C_{12}(\\mathbf{s}_{1i}, \\mathbf{s}_{2j})]_{n_1 \\times n_2}, \\quad\n\\mathbf{C}_{21} = \\mathbf{C}_{12}^\\top \\\\\n&\\mathbf{c}_{10} = [C_{11}(\\mathbf{s}_{1i}, \\mathbf{s}_0)]_{n_1 \\times 1}, \\quad \\quad\n\\mathbf{c}_{20} = [C_{12}(\\mathbf{s}_{2j}, \\mathbf{s}_0)]_{n_2 \\times 1} \\\\\n&\\mathbf{1}_{n_1} = [1, \\dots, 1]^\\top_{n_1 \\times 1}, \\quad \\quad \\quad \\:\n\\mathbf{1}_{n_2} = [1, \\dots, 1]^\\top_{n_2 \\times 1} \\\\\n&\\boldsymbol{\\lambda}_1 = [\\lambda_{11}, \\dots, \\lambda_{1n_1}]^\\top, \\quad \\quad \\:\\:\n\\boldsymbol{\\lambda}_2 = [\\lambda_{21}, \\dots, \\lambda_{2n_2}]^\\top\n\\end{aligned}\n\\] para obter o seguinte sistema:\n\\[\n\\begin{bmatrix}\n\\mathbf{C}_{11} & \\mathbf{C}_{12} & -\\mathbf{1}_{n_1} & \\mathbf{0}_{n_1} \\\\\n\\mathbf{C}_{21} & \\mathbf{C}_{22} & \\mathbf{0}_{n_2} & -\\mathbf{1}_{n_2} \\\\\n\\mathbf{1}_{n_1}^\\top & \\mathbf{0}_{n_2}^\\top & 0 & 0 \\\\\n\\mathbf{0}_{n_1}^\\top & \\mathbf{1}_{n_2}^\\top & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{\\lambda}_1 \\\\ \\boldsymbol{\\lambda}_2 \\\\ \\nu_1 \\\\ \\nu_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{c}_{10} \\\\ \\mathbf{c}_{20} \\\\ 1 \\\\ 0\n\\end{bmatrix}\n\\] Variância mínima da co-krigagem:\n\\[\n\\sigma_{\\text{CK}}^2 = C_{11}(\\mathbf{s}_0, \\mathbf{s}_0) - \\left(\\sum_{i=1}^{n_1} \\lambda_{1i} C_{11}(\\mathbf{s}_{1i}, \\mathbf{s}_0) + \\sum_{j=1}^{n_2} \\lambda_{2j} C_{12}(\\mathbf{s}_{2j}, \\mathbf{s}_0)\\right) + \\nu_1\n\\]\n\n\n\n\n\n\nImportante\n\n\n\n\nPara garantir que a matriz de covariâncias cruzadas seja positiva definida, as covariâncias devem ser modeladas como:\n\n\\[\nC_{ab}(\\mathbf{h}) = \\sum_{k=1}^K B_{ab}^k \\rho_k(\\mathbf{h})\n\\] onde \\(\\rho_k(\\mathbf{h})\\) são funções de correlação básicas e \\([B_{ab}^k]\\) são matrizes de coregionalização definidas não-negativas.\n\nComo descrito nas seções anteriores em geral, \\(C_{12}(\\mathbf{h}) = C_{21}(-\\mathbf{h})\\). Para processos estacionários, \\(C_{12}(\\mathbf{h}) = C_{21}(\\mathbf{h})\\) se a covariância cruzada é simétrica.\nQuando as variáveis secundárias estão disponíveis apenas nos mesmos locais que a primária (ou em grades finas), simplificações são possíveis.\nA abordagem condicional de Cressie e Zammit-Mangion (2016) oferece uma alternativa flexível para construir modelos de covariância multivariados válidos e assimétricos. Cressie e Zammit-Mangion (2016) modela a distribuição conjunta condicionando \\(Y_1\\) a \\(Y_2\\), evitando a necessidade de modelar explicitamente a covariância cruzada:\n\n\\[Y_1(\\mathbf{s}) = \\mu_1(\\mathbf{s}) + \\beta(\\mathbf{s})[Y_2(\\mathbf{s}) - \\mu_2(\\mathbf{s})] + \\delta(\\mathbf{s})\\] onde \\(\\beta(\\mathbf{s})\\) é um coeficiente espacialmente variante.\n\n\n\n\nCódigo\npacman::p_load(stars, gstat, ggplot2, sf, patchwork, viridis)\n\ndata(meuse)\ndata(meuse.grid)\n\n\nmeuse_sf &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\ngrid_sf &lt;- st_as_sf(meuse.grid, coords = c(\"x\", \"y\"), crs = 28992)\ngrid_stars &lt;- st_rasterize(grid_sf, dx = 40, dy = 40)\n\n# Adicionamos as variáveis uma por uma ao objeto gstat\n\n# Variável 1 (Primária): Zinco\ng &lt;- gstat(NULL, id = \"zinc\", formula = log(zinc) ~ 1, data = meuse_sf); g\n# Variável 2 (Secundária): Chumbo (Lead)\ng1 &lt;- gstat(g, id = \"lead\", formula = log(lead) ~ 1, data = meuse_sf);g1\n\n#Variograma Cruzado\n# O gstat calcula automaticamente: Var(Zn), Var(Pb) e Cov(Zn, Pb)\nv_cross &lt;- variogram(g1)\n\n# Plotar os variogramas (Diretos e Cruzado) para inspeção\nplot(v_cross, main = \"Variogramas Cruzados: Zinco x Chumbo\")\n\n#Ajuste do Modelo Linear de Coregionalização (LMC)\n# fit.lmc ajusta o modelo aos variogramas direto e cruzado simultaneamente\nmodel_base &lt;- vgm(0.6, \"Exp\", 800, 0.05)\nm_cross &lt;- fit.lmc(v_cross, g, model = model_base)\n\n\nplot(v_cross, m_cross, main = \"Ajuste do LMC (Zn + Pb)\")\n\n#Realizar a Co-Krigagem\nck &lt;- predict(m_cross, newdata = grid_stars)\n\n# Predição\np1 &lt;- ggplot() +\n  geom_stars(data = ck, aes(fill = zinc.pred)) +\n  scale_fill_viridis_c(option = \"plasma\", na.value = \"transparent\") +\n  labs(title = \"Co-Krigagem Ordinária (Pred)\", \n       subtitle = \"Zinco auxiliado por Chumbo\",\n       fill = \"Log(Zn)\", x = NULL, y = NULL) +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(size = 9))\n\n# Variância\np2 &lt;- ggplot() +\n  geom_stars(data = ck, aes(fill = zinc.var)) +\n  scale_fill_viridis_c(option = \"cividis\", na.value = \"transparent\") +\n  labs(title = \"Variância CK\", fill = \"Var\", x = NULL, y = NULL) +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\np1 + p2\n\n\n\n\n\n\n\n\nFigura 3.19: Co-Krigagem: (a) predição e (b) variância de krigagem\n\n\n\n\n\n\n\n\n\n\n\nFigura 3.20: Co-Krigagem: (a) predição e (b) variância de krigagem\n\n\n\n\n\n\n\n\n\n\n\nFigura 3.21: Co-Krigagem: (a) predição e (b) variância de krigagem",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#avaliação-da-performance-preditiva",
    "href": "geostat.html#avaliação-da-performance-preditiva",
    "title": "3  Geoestatística",
    "section": "3.14 Avaliação da Performance Preditiva",
    "text": "3.14 Avaliação da Performance Preditiva\nUma vez ajustado o modelo de variograma e realizada a krigagem, é fundamental verificar se as predições são precisas e se a incerteza (variância de krigagem) está bem calibrada. Para tal, recorre-se a métodos de reamostragem e estatísticas de consistência, sendo a validação cruzada (leave-one-out) a mais usada.\n\n3.14.1 Validação Cruzada\nA validação cruzada leave-one-out é o método padrão para verificar a consistência entre a incerteza predita pelo modelo (variância de krigagem) e o erro real de predição. A estatística para este diagnóstico é a Razão de Desvio Quadrático Médio (MSDR - Mean Squared Deviation Ratio), discutida em profundidade por McBratney e Webster (1986) e Lark (2000).\nSeja \\(Y(\\mathbf{s}_i)\\) o valor observado no local \\(\\mathbf{s}_i\\). Retiramos este ponto do conjunto de dados e realizamos a krigagem usando os \\(n-1\\) vizinhos restantes para obter a estimativa \\(\\hat{Y}_{-i}(\\mathbf{s}_i)\\) e a variância de krigagem associada \\(\\sigma_E^2(\\mathbf{s}_i)\\).\nErro de predição\nDefinimos o erro de predição (resíduo) como: \\[\\varepsilon_i = Y(\\mathbf{s}_i) - \\hat{Y}_{-i}(\\mathbf{s}_i)\\]\nSob a hipótese nula de que o modelo de variograma \\(\\gamma(\\mathbf{h}; \\boldsymbol{\\theta})\\) e as premissas de estacionariedade estão corretos, o estimador de krigagem é não-viesado e a variância do erro é exata. Se assumirmos adicionalmente que o processo é Gaussiano:\n\\[\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_E^2(\\mathbf{s}_i))\\]\nPara avaliar a validade da variância, padronizamos o erro dividindo-o pelo desvio padrão estimado (variância de krigagem). Definimos o resíduo padronizado \\(\\theta_i\\):\n\\[\\theta_i = \\frac{Y(\\mathbf{s}_i) - \\hat{Y}_{-i}(\\mathbf{s}_i)}{\\sigma_k(\\mathbf{s}_i)}\\]\nDada a distribuição de \\(\\varepsilon_i\\), a variável \\(\\theta_i\\) deve seguir uma distribuição normal padrão, \\(\\theta_i \\sim \\mathcal{N}(0, 1)\\). Consequentemente, o quadrado do resíduo padronizado segue uma distribuição Qui-quadrado com 1 grau de liberdade:\n\\[\\theta_i^2 \\sim \\chi^2_{(1)}\\]\nA esperança de uma variável \\(\\chi^2_{(1)}\\) é igual aos seus graus de liberdade. Portanto, o valor esperado teórico para o quadrado do erro padronizado é:\n\\[E[\\theta_i^2] = \\text{Var}(\\theta_i) + (E[\\theta_i])^2 = 1 + 0 = 1\\]\nA MSDR é definida como a média amostral dos quadrados dos resíduos padronizados sobre todos os pontos de validação \\(n\\):\n\\[\\text{MSDR} = \\frac{1}{n} \\sum_{i=1}^n \\theta_i^2 = \\frac{1}{n} \\sum_{i=1}^n \\frac{[Y(\\mathbf{s}_i) - \\hat{Y}_{-i}(\\mathbf{s}_i)]^2}{\\sigma_E^2(\\mathbf{s}_i)}\\]\nA análise do valor de MSDR permite diagnosticar a calibração da incerteza do modelo:\n\nConsistência (\\(\\text{MSDR} \\approx 1\\)):\n\nA variância média dos erros reais (numerador) é aproximadamente igual à variância média predita pela krigagem (denominador). O modelo descreve corretamente a variabilidade espacial.\n\nSubestimação da Incerteza (\\(\\text{MSDR} &gt; 1\\)):\n\n\\[\\frac{1}{n}\\sum \\varepsilon_i^2 &gt; \\frac{1}{n}\\sum \\sigma_E^2\\]\nO erro real é sistematicamente maior do que o modelo prevê. Isso ocorre frequentemente quando o variograma subestima o efeito pepita ou a variância total (patamar), ou quando há outliers não modelados.\n\nSuperestimação da Incerteza (\\(\\text{MSDR} &lt; 1\\)):\n\n\\[\\frac{1}{n}\\sum \\varepsilon_i^2 &lt; \\frac{1}{n}\\sum \\sigma_E^2\\]\nA variância de krigagem é maior do que o erro efetivo. O modelo assume uma variabilidade espacial ou um ruído (pepita) superior ao que realmente existe nos dados. O modelo é “pessimista” ou conservador.\nCom base na fundamentação teórica de Journel & Huijbregts (1978), Cressie (1993), Goovaerts (1997) e Chilès & Delfiner (1999), apresento a secção detalhada sobre Anisotropia, mantendo o rigor matemático e a dedução das transformações lineares necessárias para a modelagem.\n\n\nCódigo\npacman::p_load(stars, gstat, ggplot2, sf, patchwork, viridis, dplyr, tibble, gt)\n\ndata(meuse)\nmeuse_sf &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\n\nv_ord &lt;- variogram(log(zinc) ~ 1, meuse_sf)\nm_ord &lt;- fit.variogram(v_ord, vgm(0.6, \"Exp\", 800, 0.05))\n\n#Executar Validação Cruzada (Leave-One-Out)\n# A função krige.cv faz o loop automaticamente.\ncv_results &lt;- krige.cv(log(zinc) ~ 1, \n                       locations = meuse_sf, \n                       model = m_ord, debug.level = 0); \n\ncv_results|&gt;\n  head()|&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar1.pred\nvar1.var\nobserved\nresidual\nzscore\nfold\ngeometry\n\n\n\n\n6.833605\n0.1614145\n6.929517\n0.0959118\n0.2387267\n1\nPOINT (181072 333611)\n\n\n6.786747\n0.1604868\n7.039660\n0.2529135\n0.6313240\n2\nPOINT (181025 333558)\n\n\n6.291128\n0.1836067\n6.461468\n0.1703405\n0.3975334\n3\nPOINT (181165 333537)\n\n\n6.051561\n0.2405948\n5.549076\n-0.5024847\n-1.0244238\n4\nPOINT (181298 333484)\n\n\n5.576407\n0.1717014\n5.594711\n0.0183041\n0.0441734\n5\nPOINT (181307 333330)\n\n\n5.455516\n0.2381790\n5.638355\n0.1828387\n0.3746419\n6\nPOINT (181390 333260)\n\n\n\n\n\nCódigo\n#Estatísticas de Diagnóstico\n\n# MSDR (Razão de Desvio Quadrático Médio)\nmsdr &lt;- mean(cv_results$zscore^2)\n\n# ME (Erro Médio) - Deve ser próximo de 0 (viés)\nme &lt;- mean(cv_results$residual)\n\n# RMSE (Raiz do Erro Quadrático Médio)\nrmse &lt;- sqrt(mean(cv_results$residual^2))\n\nresultados &lt;- tibble(\n  Indicador = c(\"Mean Error (Viés)\", \n                \"RMSE (Precisão)\", \n                \"MSDR (Calibração)\"),\n  Valor = round(c(me, rmse, msdr), 4)\n)\n\nresultados|&gt;\n  knitr::kable()\n\n\n\n\n\nIndicador\nValor\n\n\n\n\nMean Error (Viés)\n0.0021\n\n\nRMSE (Precisão)\n0.3935\n\n\nMSDR (Calibração)\n0.8657\n\n\n\n\n\nCódigo\n# Interpretação automática simples\nif(msdr &gt; 1.1) {\n  cat(\"MSDR &gt; 1: Subestimação da incerteza (Variograma muito 'otimista' ou outliers).\\n\")\n} else if(msdr &lt; 0.9) {\n  cat(\"MSDR &lt; 1: Superestimação da incerteza (Variograma muito 'pessimista').\\n\")\n} else {\n  cat(\"MSDR ~ 1: Incerteza bem calibrada.\\n\")\n}\n\n\nMSDR &lt; 1: Superestimação da incerteza (Variograma muito 'pessimista').\n\n\nCódigo\np1 &lt;- ggplot(cv_results, aes(x = observed, y = var1.pred)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Acurácia: Observado vs Predito\",\n       subtitle = paste(\"RMSE:\", round(rmse, 3)),\n       x = \"Log(Zinc) Observado\", y = \"Log(Zinc) Predito\") +\n  theme_bw()\n\n#Histograma dos Z-Scores (Deve parecer uma Normal(0,1))\np2 &lt;- ggplot(cv_results, aes(x = zscore)) +\n  geom_histogram(aes(y = ..density..), bins = 20, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = \"red\", size = 1) +\n  labs(title = \"Calibração: Z-Scores\",\n       subtitle = paste(\"MSDR:\", round(msdr, 3), \"(Ideal = 1.0)\"),\n       x = \"Resíduo Padronizado\", y = \"Densidade\") +\n  theme_bw()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nCom base na sua solicitação, elaborei esta seção detalhada sobre Krigagem Indicatriz. Mantive a notação rigorosa \\(Y(\\mathbf{s})\\), integrei a evolução histórica e matemática com base nos documentos fornecidos (de 1983 a 2025) e diferenciei o método das abordagens lineares anteriores.",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#krigagem-indicatriz-ik",
    "href": "geostat.html#krigagem-indicatriz-ik",
    "title": "3  Geoestatística",
    "section": "3.15 Krigagem Indicatriz (IK)",
    "text": "3.15 Krigagem Indicatriz (IK)\nAs técnicas de krigagem discutidas anteriormente (Simples, Ordinária, Universal) são classificadas como estimadores lineares. Elas são ótimas para estimar o valor esperado da variável \\(Y(\\mathbf{s}_0)\\) sob a condição de que a distribuição dos dados seja razoavelmente simétrica (dados normais) e livre de outliers extremos. No entanto, em geociências e estudos ambientais, frequentemente lidamos com distribuições altamente assimétricas (ex: concentrações de ouro ou poluentes) onde a média não é uma boa medida de tendência central e a variância de krigagem não reflete a verdadeira incerteza local, pois é independente dos valores dos dados (propriedade da homocedasticidade) (André G. Journel 1983).\nA Krigagem Indicatriz (IK), introduzida por André G. Journel (1983), representa uma mudança de paradigma: em vez de estimar o valor da variável \\(Y(\\mathbf{s}_0)\\) diretamente, estimamos a Função de Distribuição Cumulativa Condicional (ccdf) local em \\(\\mathbf{s}_0\\).\nA importância fundamental da IK em relação à KO ou KS reside na sua natureza não-paramétrica. Ela não assume normalidade dos dados e é robusta a outliers, pois transforma os dados em binários (0 ou 1) baseados em limiares de corte (cut-offs). Um valor extremamente alto não explode a estimativa, pois é tratado apenas como acima do corte (Carvalho e Deutsch 2017).\nTransformada Indicadora e a ccdf\nSeja \\(Y(\\mathbf{s})\\) uma variável regionalizada e \\(y_k\\) um valor de corte (threshold) escolhido. A transformada indicadora \\(I(\\mathbf{s}; y_k)\\) é definida como uma variável binária:\n\\[\nI(\\mathbf{s}; z_k) = \\begin{cases}\n1, & \\text{se } Y(\\mathbf{s}) \\le y_k \\\\\n0, & \\text{se } Y(\\mathbf{s}) &gt; y_k\n\\end{cases}\n\\]\nPara variáveis categóricas (como espécies biológicas ou litologia), a definição muda para uma igualdade estrita (\\(= y_k\\)), conforme descrito por Guimaraes et al. (2012).\nA propriedade estatística que fundamenta a IK é que a esperança da variável indicadora é igual à probabilidade cumulativa:\n\\[E[I(\\mathbf{s}; y_k)] = 1 \\cdot P(Y(\\mathbf{s}) \\le y_k) + 0 \\cdot P(Y(\\mathbf{s}) &gt; y_k) = P(Y(\\mathbf{s}) \\le y_k) = F(y_k)\\]\nPortanto, ao estimar o valor esperado do indicador num local não amostrado \\(\\mathbf{s}_0\\), estamos estimando a probabilidade de que a variável real seja menor ou igual ao corte \\(y_k\\), condicionada aos dados vizinhos \\((n)\\). Esta é a ccdf local:\n\\[\\hat{F}(u; y_k | (n)) = E^*[I(\\mathbf{s}_0; y_k) | (n)] = \\text{Prob}^* \\{ Y(\\mathbf{s}_0) \\le y_k | (n) \\}\\]\nEstimador e o Sistema de Krigagem Indicatriz\nA estimativa da probabilidade para um corte \\(y_k\\) é uma combinação linear dos indicadores observados nos locais \\(\\mathbf{s}_i\\):\n\\[\\hat{I}(\\mathbf{s}_0; y_k) = \\sum_{i=1}^{n} \\lambda_i(y_k) I(\\mathbf{s}_i; y_k)\\]\nNote que os pesos \\(\\lambda_i(y_k)\\) dependem do corte \\(y_k\\). Isso significa que a estrutura de continuidade espacial (variograma) pode mudar para diferentes teores. O ouro de alto teor pode ter uma continuidade espacial muito menor (alcance curto) do que o ouro de baixo teor (alcance longo). Esta flexibilidade é uma vantagem crucial sobre a Krigagem Ordinária, que assume um único variograma para todo o processo (Mohammadpour et al. 2019).\nO sistema de equações para obter os pesos é idêntico ao da Krigagem Ordinária, mas aplicado aos dados transformados e ao Variograma Indicador \\(\\gamma_I(\\mathbf{h}; z_k)\\):\n\\[\\gamma_I(\\mathbf{h}; y_k) = \\frac{1}{2} E \\left[ \\{ I(\\mathbf{s} + \\mathbf{h}; y_k) - I(\\mathbf{s}; y_k) \\}^2 \\right]\\]\nO sistema \\(\\mathbf{A}_{IK} \\mathbf{x}_{IK} = \\mathbf{b}_{IK}\\) para cada corte \\(y_k\\) é:\n\\[\n\\begin{cases}\n\\sum_{j=1}^n \\lambda_j(z_k) \\gamma_I(\\mathbf{s}_i - \\mathbf{s}_j; z_k) + \\nu(z_k) = \\gamma_I(\\mathbf{s}_i - \\mathbf{s}_0; z_k), & i=1, \\dots, n \\\\\n\\sum_{j=1}^n \\lambda_j(z_k) = 1\n\\end{cases}\n\\]\nMyers (1994) classifica a IK como uma extensão não-linear que evita as premissas fortes de distribuição bivariada exigidas pela Krigagem Disjuntiva, embora exija a hipótese de estacionariedade forte para a inferência da ccdf.\nSoft Kriging: Incorporando Incertezas\nUma generalização poderosa da IK é a capacidade de incorporar dados imprecisos ou qualitativos, conhecidos como Soft Data. Andre G. Journel (1986) formalizou o conceito de Soft Kriging.\nEnquanto um dado exato em \\(\\mathbf{s}_i\\) é codificado como um degrau abrupto na ccdf (0 ou 1), um dado suave (ex: o valor está entre \\(a\\) e \\(b\\)) pode ser codificado como uma probabilidade \\(y(\\mathbf{s}_i; y_k) \\in [0, 1]\\). O estimador se torna:\n\\[\\hat{I}(\\mathbf{s}_0; y_k) = \\sum_{i \\in \\text{hard}} \\lambda_i I(\\mathbf{s}_i; y_k) + \\sum_{j \\in \\text{soft}} \\nu_j y(\\mathbf{s}_j; y_k)\\]\nUngaro et al. (2008) expandem este conceito utilizando Simple Indicator Kriging with Varying Local Means (SIK-VLM), onde a média local varia conforme mapas de solo auxiliares, permitindo distinguir anomalias naturais de contaminação antrópica.\nEstatísticas E-type e Risco\nA IK resulta em um conjunto de probabilidades discretas para \\(K\\) cortes: \\(\\hat{F}(y_1), \\hat{F}(y_2), \\dots, \\hat{F}(y_K)\\). Para obter uma estimativa de valor (teor) único para o local \\(\\mathbf{s}_0\\), calculamos a esperança da ccdf, conhecida como estimativa E-type:\n\\[\\hat{Y}_{E\\text{-type}}(\\mathbf{s}_0) \\approx \\sum_{k=0}^{K} \\bar{y}_k \\left[ \\hat{F}(y_{k+1}) - \\hat{F}(y_k) \\right]\\]\nOnde \\(\\bar{y}_k\\) é a média da classe entre os cortes \\(y_k\\) e \\(y_{k+1}\\). Carvalho e Deutsch (2017) destaca a importância de modelar corretamente as caudas da distribuição (abaixo do primeiro corte e acima do último), sugerindo ajustes hiperbólicos para evitar subestimativa de valores extremos.\nAlém da média, a IK permite quantificar o risco de decisão. Juang e Lee (1998) desenvolvem as equações para calcular a probabilidade de Falso Positivo (\\(\\alpha\\), classificar como perigoso o que é seguro) e Falso Negativo (\\(\\beta\\), classificar como seguro o que é perigoso), cruciais para remediação ambiental.\nDesafios e Desenvolvimentos Recentes\n\nComo as krigagens são independentes para cada corte, pode ocorrer que \\(P(Y \\le 10) &lt; P(Y \\le 5)\\), o que é matematicamente impossível. Algoritmos de correção (médias ascendentes/descendentes) são aplicados a posteriori (Deutsch e Journel 1997).\nPara reduzir o esforço computacional, Hill (1998) propõem a Median Indicator Kriging, assumindo que o variograma da mediana é representativo para todos os cortes. Embora eficiente, Mohammadpour et al. (2019) argumentam que para anomalias sutis, deve-se usar variogramas específicos definidos por modelos Multifractais (FMIK) para separar corretamente as populações geológicas.\nO avanço mais recente, proposto por Ji et al. (2025), introduz a Ordered Indicator Kriging com parâmetros de campo. Este método adapta a anisotropia da krigagem localmente baseada na estrutura geológica (dobras, falhas) e na lógica deposicional (proximal-distal), superando a limitação de estacionaridade da IK em ambientes complexos.\n\n\n\nCódigo\npacman::p_load(stars, gstat, ggplot2, sf, viridis, dplyr)\n\ndata(meuse)\ndata(meuse.grid)\n\n\n#Preparação dos Dados\n# Definir o corte (Threshold). Vamos usar o 3º quartil do Zinco como \"perigo\".\nthreshold &lt;- quantile(meuse$zinc, 0.75) \nmeuse$zinc_ind &lt;- ifelse(meuse$zinc &gt; threshold, 1, 0) # 1 se perigoso, 0 se seguro\n\nmeuse_sf &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\ngrid_sf &lt;- st_as_sf(meuse.grid, coords = c(\"x\", \"y\"), crs = 28992)\ngrid_stars &lt;- st_rasterize(grid_sf, dx = 40, dy = 40)\n\n\n#Variograma Indicador\n\nv_ind &lt;- variogram(zinc_ind ~ 1, meuse_sf)\nm_ind &lt;- fit.variogram(v_ind, vgm(0.15, \"Exp\", 600, 0.05))\n\nplot(v_ind, m_ind, main = \"Variograma Indicador (Zinco &gt; Q75)\")\n\n\n#Krigagem Indicatriz (Ordinária)\n\n# O resultado (var1.pred) será a PROBABILIDADE de ser 1 (acima do corte)\nik &lt;- krige(zinc_ind ~ 1, \n            locations = meuse_sf, \n            newdata = grid_stars, \n            model = m_ind, debug.level = 0)\n\n\nik$probabilidade &lt;- pmin(pmax(ik$var1.pred, 0), 1)\n\nggplot() +\n  geom_stars(data = ik, aes(fill = probabilidade)) +\n  geom_sf(data = meuse_sf, aes(color = as.factor(zinc_ind)), size = 1) +\n  scale_fill_viridis_c(option = \"turbo\", name = \"Prob. &gt; Corte\", na.value = \"transparent\") +\n  scale_color_manual(values = c(\"white\", \"black\"), name = \"Dados Reais\", labels = c(\"&lt;= Corte\", \"&gt; Corte\"))+\n  labs(title = \"\",\n       subtitle = \"Probabilidade do teor de Zinco exceder o limiar de 75%\",\n       x = NULL, y = NULL) +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"right\")\n\n\n\n\n\n\n\n\nFigura 3.22: Krigagem Indicatriz\n\n\n\n\n\n\n\n\n\n\n\nFigura 3.23: Krigagem Indicatriz",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#desafios-computacionais-e-a-abordagem-spde",
    "href": "geostat.html#desafios-computacionais-e-a-abordagem-spde",
    "title": "3  Geoestatística",
    "section": "3.16 Desafios Computacionais e a Abordagem SPDE",
    "text": "3.16 Desafios Computacionais e a Abordagem SPDE\nImagine que você é um climatologista modelando a temperatura da superfície do mar no Atlântico Norte usando dados de satélite. Você tem \\(n = 100.000\\) pontos de observação \\(\\mathbf{s}_1, \\dots, \\mathbf{s}_n\\) e deseja prever a temperatura em locais não medidos e entender a variabilidade espacial.\nNa geoestatística, assumimos que o vetor de temperaturas observadas \\(\\mathbf{y}\\) é uma realização de um Campo Aleatório Gaussiano (GRF) \\(x(\\mathbf{s})\\). A dependência espacial é regida por uma Matriz de Covariância densa \\(\\mathbf{\\Sigma}\\), onde o elemento \\(\\Sigma_{ij}\\) descreve a correlação entre \\(\\mathbf{s}_i\\) e \\(\\mathbf{s}_j\\).\nUm Campo Aleatório Gaussiano (GRF - Gaussian Random Field), denotado por \\(\\{x(\\mathbf{s}) : \\mathbf{s} \\in D^G \\subset \\mathbb{R}^d\\}\\), é um processo estocástico onde, para qualquer conjunto finito de locais \\(\\{\\mathbf{s}_1, \\dots, \\mathbf{s}_n\\}\\), o vetor \\(\\mathbf{x} = (x(\\mathbf{s}_1), \\dots, x(\\mathbf{s}_n))^\\top\\) segue uma distribuição Normal Multivariada:\n\\[\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\]\nOnde \\(\\boldsymbol{\\mu}\\) é o vetor de médias e \\(\\boldsymbol{\\Sigma}\\) é a matriz de covariância, cujos elementos são dados por uma função de covariância definida positiva \\(C(\\cdot)\\):\n\\[\\Sigma_{ij} = \\text{Cov}(x(\\mathbf{s}_i), x(\\mathbf{s}_j)) = C(\\|\\mathbf{s}_i - \\mathbf{s}_j\\|)\\]\nA função \\(C\\) é função de covariância da Matérn, que controla a variância e a suavidade do campo. Note que ao longo do texto descrevemos funções de semivariograma, mas poderiamos ter o feito para funções de covariância.\nPara estimar parâmetros (como o alcance da correlação) via Máxima Verossimilhança, precisamos calcular a log-verossimilhança:\n\\[\\log L(\\theta) \\propto -\\frac{1}{2} \\log |\\mathbf{\\Sigma}| - \\frac{1}{2} \\mathbf{y}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{y}\\]\nIsso exige calcular o determinante \\(|\\mathbf{\\Sigma}|\\) e a inversa \\(\\mathbf{\\Sigma}^{-1}\\). O método padrão é a Fatoração de Cholesky (\\(\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top\\)), que decompõe a matriz \\(\\boldsymbol{\\Sigma}\\) em um produto \\(\\boldsymbol{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top\\), onde \\(\\mathbf{L}\\) é uma matriz triangular inferior única com elementos diagonais estritamente positivos.\nA matriz \\(\\mathbf{\\Sigma}\\) é densa. Em processos espaciais, tudo se correlaciona com tudo (Primeira Lei da Geografia), mesmo que a correlação seja ínfima a longas distâncias.\nO problema é que a matriz \\(\\boldsymbol{\\Sigma}\\) é densa (quase todos os elementos são não-nulos, pois a correlação decai com a distância mas nunca é exatamente zero). Portanto, \\(\\Sigma_{ij} \\neq 0\\) para quase todos os pares. Como consequência, o custo computacional da fatoração de Cholesky para uma matriz densa seja de ordem \\(\\mathcal{O}(n^3)\\) e o custo de armazenamento (memória) de ordem \\(\\mathcal{O}(n^2)\\). Para \\(n=100.000\\), isso exige da ordem de \\(10^{15}\\) operações e cerca de 80 GB de RAM apenas para armazenar a matriz em precisão dupla. Isso torna a geoestatística clássica computacionalmente proibitiva para grandes conjuntos de dados (Abdulah et al. 2023).\nA solução reside na mudança de paradigma: modelar a Precisão (dependência local) em vez da Covariância (dependência global).\n\nCovariância (\\(\\mathbf{\\Sigma}\\)) descreve a correlação marginal. Se o ponto A afeta B, e B afeta C, então A está correlacionado com C. O grafo de dependência é completo (denso).\nPrecisão (\\(\\mathbf{Q} = \\mathbf{\\Sigma}^{-1}\\)) descreve a correlação condicional. Se fixarmos o valor de B, A e C tornam-se independentes (propriedade de Markov). Em um Campo Aleatório de Markov Gaussiano (GMRF), um ponto só interage com seus vizinhos imediatos. A matriz é esparsa (cheia de zeros).\n\nPara construir uma matriz esparsa \\(\\mathbf{Q}\\) que corresponda a um modelo de covariância contínuo e válido, recorre-se a abordagem Stochastic Partial Differential Equation ( SPDE ou Equação Diferencial Parcial Estocástica).\n\n3.16.1 Equação Diferencial Parcial Estocástica (SPDE)\nUma SPDE é uma equação diferencial onde um ou mais termos são processos estocásticos. No contexto da geoestatística, usamos uma SPDE linear para descrever como o campo \\(x(\\mathbf{s})\\) é gerado a partir de um ruído branco.\nWhittle (1954) provou um campo aleatório Gaussiano estacionário \\(x(\\mathbf{s})\\) em \\(\\mathbb{R}^d\\) com função de covariância Matérn é a solução estacionária da seguinte Equação Diferencial Parcial Estocástica (SPDE) linear fracionária impulsionada por um ruído branco Gaussiano:\n\\[(\\kappa^2 - \\Delta)^{\\alpha/2} (\\tau x(\\mathbf{s})) = \\mathcal{W}(\\mathbf{s}), \\quad \\mathbf{s} \\in \\mathbb{R}^d \\tag{3.14}\\]\nOnde:\n\n\\(\\Delta\\) (Laplaciano) é operador diferencial definido como \\(\\Delta = \\sum_{i=1}^d \\frac{\\partial^2}{\\partial s_i^2}\\). Mede a concavidade local.\n\\(\\mathcal{W}(\\mathbf{s})\\) (Ruído Branco Espacial) é um processo estocástico Gaussiano com média zero e densidade espectral constante \\(S_{\\mathcal{W}}(\\boldsymbol{\\omega}) = 1\\) em todas as frequências. Para quaisquer funções de teste \\(f, g \\in L^2(\\mathbb{R}^d)\\), a covariância é dada pelo produto interno: \\(\\text{Cov}(\\langle f, \\mathcal{W} \\rangle, \\langle g, \\mathcal{W} \\rangle) = \\int f(\\mathbf{s})g(\\mathbf{s}) d\\mathbf{s}\\).\n\\(\\kappa\\) é parâmetro de escala espacial (\\(&gt;0\\)). Controla o decaimento da correlação. Relacionado ao Alcance (\\(\\rho\\)) por \\(\\rho = \\sqrt{8\\nu}/\\kappa\\).\n\\(\\alpha\\) é parâmetro de suavidade que determina a ordem da equação diferencial. Relacionado ao parâmetro \\(\\nu\\) da Matérn por \\(\\alpha = \\nu + d/2\\)\n\n\\(\\tau\\) é parâmetro de escala da variância. Controla a magnitude das flutuações. Relacionado à variância marginal \\(\\sigma^2\\).\nPara provar que a solução da equação acima realmente gera uma covariância Matérn, utilizamos a análise espectral no domínio da frequência (Lindgren, Rue, e Lindström 2011). Que fundamenta-se na transformada de Fourier \\(\\mathcal{F}\\), que consiste em converter uma função do domínio espacial \\(\\mathbf{s}\\) para o domínio da frequência \\(\\boldsymbol{\\omega}\\). Uma propriedade crucial é como ela afeta o operador Laplaciano:\n\\[\\mathcal{F}(\\Delta f(\\mathbf{s})) = -\\|\\boldsymbol{\\omega}\\|^2 \\hat{f}(\\boldsymbol{\\omega})\\]\nAplicando a transformada de Fourier em ambos os lados da SPDE (Eq. 3.14), temos:\n\\[\n\\begin{aligned}\n\\mathcal{F}\\left[ (\\kappa^2 - \\Delta)^{\\alpha/2} (\\tau x(\\mathbf{s})) \\right] &= \\mathcal{F}[\\mathcal{W}(\\mathbf{s})] \\\\\n\\tau (\\kappa^2 - \\mathcal{F}[\\Delta])^{\\alpha/2} \\hat{x}(\\boldsymbol{\\omega}) &= \\hat{\\mathcal{W}}(\\boldsymbol{\\omega}) \\\\\n\\tau (\\kappa^2 + \\|\\boldsymbol{\\omega}\\|^2)^{\\alpha/2} \\hat{x}(\\boldsymbol{\\omega}) &= \\hat{\\mathcal{W}}(\\boldsymbol{\\omega})\n\\end{aligned}\n\\]\nIsolamos \\(\\hat{x}(\\boldsymbol{\\omega})\\) para encontrar a função de transferência do sistema:\n\\[\\hat{x}(\\boldsymbol{\\omega}) = \\frac{1}{\\tau (\\kappa^2 + \\|\\boldsymbol{\\omega}\\|^2)^{\\alpha/2}} \\hat{\\mathcal{W}}(\\boldsymbol{\\omega})\\]\nA densidade espectral de potência \\(S_x(\\boldsymbol{\\omega})\\) do campo \\(x\\) é dada pelo quadrado do módulo da função de transferência multiplicado pela densidade espectral do ruído de entrada (que é 1):\n\\[\\begin{aligned}\nS_x(\\boldsymbol{\\omega}) &= \\left| \\frac{1}{\\tau (\\kappa^2 + \\|\\boldsymbol{\\omega}\\|^2)^{\\alpha/2}} \\right|^2 \\cdot S_{\\mathcal{W}}(\\boldsymbol{\\omega}) \\\\\nS_x(\\boldsymbol{\\omega}) &= \\frac{1}{\\tau^2 (\\kappa^2 + \\|\\boldsymbol{\\omega}\\|^2)^{\\alpha}}\n\\end{aligned}\\]\nEste resultado, \\(S_x(\\boldsymbol{\\omega}) \\propto (\\kappa^2 + \\|\\boldsymbol{\\omega}\\|^2)^{-\\alpha}\\), é exatamente a definição da densidade espectral de um campo isotrópico com covariância da classe Matérn em \\(\\mathbb{R}^d\\), com parâmetro de suavidade \\(\\nu = \\alpha - d/2\\) Lindgren, Bolin, e Rue (2022).\nDiscretização via Método de Elementos Finitos\nA SPDE é uma equação definida no contínuo. Para implementá-la computacionalmente e obter a almejada esparsidade, precisamos discretizar o domínio espacial \\(D^G\\). Lindgren, Rue, e Lindström (2011) propõem o uso do Método de Elementos Finitos (FEM), uma técnica numérica para encontrar soluções aproximadas de equações diferenciais parciais. O método consiste em subdividir um domínio contínuo em um conjunto de subdomínios discretos (elementos, formando uma malha) e aproximar a solução da equação como uma combinação linear de funções de base simples definidas nesses elementos (Cressie, Sainsbury-Dale, e Zammit-Mangion 2022).\nA Malha e as Funções de Base\nSubdividimos o domínio \\(D^G\\) em uma malha (mesh) de triângulos não sobrepostos. Seja \\(V\\) o número de vértices (nós) da malha. Definimos uma aproximação do campo contínuo \\(x(\\mathbf{s})\\) como:\n\\[x(\\mathbf{s}) \\approx \\sum_{k=1}^V w_k \\psi_k(\\mathbf{s})\\]\nOnde:\n\n\\(w_k\\) são pesos aleatórios (Gaussianos) associados a cada vértice \\(k\\). O objetivo da inferência passa a ser encontrar a distribuição conjunta do vetor de pesos \\(\\mathbf{w} = (w_1, \\dots, w_V)^\\top\\).\n\\(\\psi_k(\\mathbf{s})\\) são funções de base determinísticas, lineares por partes. A função \\(\\psi_k\\) vale 1 no vértice \\(k\\), decai linearmente para 0 nos vértices vizinhos e é identicamente nula em todo o restante do domínio. Esta propriedade de suporte compacto é crucial para a esparsidade.\n\nNão podemos substituir a aproximação diretamente na SPDE \\((\\kappa^2 - \\Delta)x = \\mathcal{W}\\) (assumindo \\(\\alpha=2\\) para simplificar, o que equivale a \\(\\nu=1\\) em 2D), pois a segunda derivada \\(\\Delta\\) de funções lineares por partes resulta em funções indefinidas (Deltas de Dirac) nas arestas dos triângulos.\nPara contornar isso, usamos a formulação fraca (ou variacional): multiplicamos a SPDE por uma função de teste (escolhemos a própria base \\(\\psi_i\\)) e integramos sobre o domínio \\(\\Omega\\):\n\\[\\int_{\\Omega} \\psi_i(\\mathbf{s}) (\\kappa^2 - \\Delta) x(\\mathbf{s}) \\, d\\mathbf{s} = \\frac{1}{\\tau} \\int_{\\Omega} \\psi_i(\\mathbf{s}) \\mathcal{W}(\\mathbf{s}) \\, d\\mathbf{s}, \\quad \\text{para } i=1,\\dots,V\\]\nPara resolver o termo com o Laplaciano (\\(\\int \\psi_i \\Delta x\\)), aplicamos a Primeira Identidade de Green, que é a generalização da integração por partes para dimensões superiores. Assumindo condições de fronteira de Neumann (derivada normal nula na borda do domínio), a identidade transfere uma derivada do campo \\(x\\) para a função de teste \\(\\psi_i\\):\n\\[-\\int_{\\Omega} \\psi_i \\Delta x \\, d\\mathbf{s} = \\int_{\\Omega} \\nabla \\psi_i \\cdot \\nabla x \\, d\\mathbf{s}\\]\nSubstituindo a expansão \\(x(\\mathbf{s}) = \\sum_{j=1}^V w_j \\psi_j(\\mathbf{s})\\) na equação integral:\n\\[\n\\begin{aligned}\n\\int_{\\Omega} \\psi_i \\left[ \\kappa^2 \\sum_j w_j \\psi_j \\right] d\\mathbf{s} + \\int_{\\Omega} \\nabla \\psi_i \\cdot \\left[ \\sum_j w_j \\nabla \\psi_j \\right] d\\mathbf{s} &= \\frac{1}{\\tau} \\int_{\\Omega} \\psi_i \\mathcal{W} d\\mathbf{s} \\\\\n\\sum_{j=1}^V w_j \\underbrace{\\int_{\\Omega} \\kappa^2 \\psi_i \\psi_j d\\mathbf{s}}_{\\text{Termo dependente de C}} + \\sum_{j=1}^V w_j \\underbrace{\\int_{\\Omega} \\nabla \\psi_i \\cdot \\nabla \\psi_j d\\mathbf{s}}_{\\text{Termo dependente de G}} &= \\frac{1}{\\tau} \\int_{\\Omega} \\psi_i \\mathcal{W} d\\mathbf{s}\n\\end{aligned}\n\\]\nIsso nos permite definir as matrizes esparsas fundamentais que dependem apenas da geometria da malha:\n\nMatriz de Massa (\\(\\mathbf{C}\\)): \\(C_{ij} = \\int_{\\Omega} \\psi_i(\\mathbf{s}) \\psi_j(\\mathbf{s}) \\, d\\mathbf{s} = \\langle \\psi_i, \\psi_j \\rangle\\).\nMatriz de Rigidez (\\(\\mathbf{G}\\)): \\(G_{ij} = \\int_{\\Omega} \\nabla \\psi_i(\\mathbf{s}) \\cdot \\nabla \\psi_j(\\mathbf{s}) \\, d\\mathbf{s} = \\langle \\nabla \\psi_i, \\nabla \\psi_j \\rangle\\).\n\nA equação matricial resultante para o vetor de pesos \\(\\mathbf{w}\\) é:\n\\[(\\kappa^2 \\mathbf{C} + \\mathbf{G}) \\mathbf{w} = \\tilde{\\mathbf{W}}\\]\nonde \\(\\tilde{\\mathbf{W}}\\) é um vetor Gaussiano com média zero. Pela propriedade do ruído branco espacial, a matriz de covariância de \\(\\tilde{\\mathbf{W}}\\) é igual à matriz de massa \\(\\mathbf{C}\\) (pois \\(\\text{Cov}(\\int f \\mathcal{W}, \\int g \\mathcal{W}) = \\int fg\\)).\nNosso objetivo final é encontrar a matriz de precisão \\(\\mathbf{Q}\\) dos pesos \\(\\mathbf{w}\\). Da equação linear derivada acima, definimos a matriz do sistema como \\(\\mathbf{K} = \\kappa^2 \\mathbf{C} + \\mathbf{G}\\). Temos então: \\(\\mathbf{K}\\mathbf{w} = \\tilde{\\mathbf{W}}\\)\nA matriz de covariância de \\(\\mathbf{w}\\), denotada por \\(\\boldsymbol{\\Sigma}_w\\), é dada por:\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}_w &= \\text{Var}(\\mathbf{w}) \\\\\n&= \\text{Var}(\\mathbf{K}^{-1}\\tilde{\\mathbf{W}}) \\\\\n&= \\mathbf{K}^{-1} \\text{Var}(\\tilde{\\mathbf{W}}) \\mathbf{K}^{-T} \\quad \\text{(Propriedade da variância linear)} \\\\\n&= \\mathbf{K}^{-1} \\mathbf{C} \\mathbf{K}^{-T} \\quad \\text{(Pois Var}(\\tilde{\\mathbf{W}}) = \\mathbf{C})\n\\end{aligned}\n\\]\nA matriz de precisão \\(\\mathbf{Q}\\) é, por definição, a inversa da matriz de covariância:\n\\[\n\\begin{aligned}\n\\mathbf{Q} &= \\boldsymbol{\\Sigma}_w^{-1} \\\\\n&= (\\mathbf{K}^{-1} \\mathbf{C} \\mathbf{K}^{-T})^{-1} \\\\\n&= \\mathbf{K}^T \\mathbf{C}^{-1} \\mathbf{K}\n\\end{aligned}\n\\]\nAs matrizes \\(\\mathbf{K}\\) e \\(\\mathbf{C}\\) são esparsas, pois as funções de base \\(\\psi_i\\) e \\(\\psi_j\\) só se sobrepoem se os vértices \\(i\\) e \\(j\\) forem vizinhos na malha. Se não forem vizinhos, as integrais são zero. No entanto, a inversa de uma matriz esparsa (\\(\\mathbf{C}^{-1}\\)) é, em geral, densa. Se usássemos \\(\\mathbf{C}^{-1}\\) na fórmula acima, \\(\\mathbf{Q}\\) se tornaria densa, destruindo todo o benefício computacional da abordagem.\nPara resolver isso, Lindgren, Rue, e Lindström (2011) utilizam a técnica de Mass Lumping (comum em métodos numéricos): substituímos a matriz de massa consistente \\(\\mathbf{C}\\) por uma matriz diagonal \\(\\tilde{\\mathbf{C}}\\), onde os elementos diagonais são a soma das linhas de \\(\\mathbf{C}\\):\n\\[\\tilde{C}_{ii} = \\sum_j C_{ij} = \\int_{\\Omega} \\psi_i(\\mathbf{s}) d\\mathbf{s}\\]\nComo a inversa de uma matriz diagonal é trivial e também diagonal (portanto, esparsa), podemos calcular a precisão final preservando a esparsidade. Substituindo \\(\\mathbf{K} = \\kappa^2 \\mathbf{C} + \\mathbf{G}\\) e usando a aproximação diagonal \\(\\tilde{\\mathbf{C}}\\), obtemos a matriz de precisão explícita para \\(\\alpha=2\\):\n\\[\\mathbf{Q}_{\\text{SPDE}} = \\tau^2 (\\kappa^2 \\tilde{\\mathbf{C}} + \\mathbf{G})^T \\tilde{\\mathbf{C}}^{-1} (\\kappa^2 \\tilde{\\mathbf{C}} + \\mathbf{G})\\]\nA matriz \\(\\mathbf{Q}_{\\text{SPDE}}\\) é construída apenas por somas e multiplicações de matrizes esparsas e diagonais. Portanto, \\(\\mathbf{Q}_{\\text{SPDE}}\\) é esparsa. Esta esparsidade permite o uso de algoritmos eficientes de fatoração de Cholesky esparsa, reduzindo a complexidade computacional de \\(\\mathcal{O}(n^3)\\) para aproximadamente \\(\\mathcal{O}(n^{3/2})\\) em problemas espaciais 2D. Isso viabiliza a análise bayesiana (via INLA) (Bakka et al. 2018; Lindgren e Rue 2015) ou a estimação de máxima verossimilhança (via ExaGeoStatR) para grandes conjuntos de dados geoestatísticos (Big Data) que eram anteriormente intratáveis (Abdulah et al. 2023).\n\n\nCódigo\npacman::p_load(sf, ggplot2)\n\nset.seed(123)\nfronteira &lt;- st_polygon(list(rbind(c(0,0), c(10,0), c(10,10), c(0,10), c(0,0))))\npontos &lt;- st_sample(fronteira, size = 30)\n\nborda_externa &lt;- st_buffer(st_sfc(fronteira), dist = 2) \npontos_borda &lt;- st_sample(st_cast(borda_externa, \"LINESTRING\"), size = 20)\ntodos_pontos &lt;- c(pontos, pontos_borda)\n\nmalha &lt;- st_triangulate(st_combine(todos_pontos)) |&gt; \n  st_collection_extract(\"POLYGON\") |&gt; \n  st_sf()\n\nggplot() +\n  geom_sf(data = malha, fill = NA, color = \"grey60\", size = 0.3) +\n  geom_sf(data = fronteira, fill = NA, color = \"blue\", size = 1) +\n  geom_sf(data = pontos, color = \"red\", size = 2) +\n  theme_void() +\n  labs(title = \"\",\n       subtitle = \"(1) Triângulos cinza sao elementos finitos; (2) pontos vermelhos \\nsão dados observados; (3) linha azul é o domínio de estudo\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5, color = \"grey40\"))\n\n\n\n\n\n\n\n\nFigura 3.24: Conceito de Malha (Mesh) para SPDE. A precisão é maior onde há dados e menor nas bordas.\n\n\n\n\n\n\n\n\n\n\n\nImportanteAprofundamento Teórico e Prático\n\n\n\nAlém dos métodos abordados aqui, existem diversas variantes de Krigagem desenvolvidas para lidar com características específicas dos dados. Entre elas destacam-se: a Krigagem Lognormal (aplicada aos logaritmos dos dados); a Krigagem Multi-Gaussiana (aplicada após a transformação normal score), que é uma generalização da lognormal; a Krigagem de Postos (Rank Kriging, baseada na transformação uniforme); Krigagem Disjuntiva, entre outras. Para maior detalhes e aplicações, recomenda-se a leitura do capítulo 4 de Deutsch e Journel (1997).\nMuitas vezes, a aplicação correta de transformações nos dados é suficiente para resolver problemas de estacionaridade ou não-normalidade. Aos interessados neste tópico, recomenda-se a leitura do capítulo 3 de Yamamoto e Landim (2013).\nPor fim, para uma visão histórica completa da geoestatística e um guia prático de ferramentas computacionais, sugere-se o capítulo 3 de Scalon (2024). A obra abrange o uso do R/RStudio, análise exploratória, outros tipos de semivariogramas, validação de modelos e inclui um tutorial detalhado sobre o pacote geoR.",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#pacote-gstat",
    "href": "geostat.html#pacote-gstat",
    "title": "3  Geoestatística",
    "section": "3.17 Pacote gstat",
    "text": "3.17 Pacote gstat\nO pacote gstat (Pebesma 2004; Gräler, Pebesma, e Heuvelink 2016) é de autoria do professor Edzer Pebesma, junto com o pacote geoR (Paulo Justiniano Ribeiro Jr e Diggle 2025; Paulo J. Ribeiro Jr e Diggle 2006) da autoria dos professores Paulo Justiniano Ribeiro Jr e Peter J. Diggle se estabeleceram como a pacotes de referência no ambiente R para a modelagem geoestatística. Aqui é descrito apenas o pacote gstat. Aos interressados na descrição completa do pacote geoR sugere-se o capítulo 3 de Scalon (2024).\nEmbora suas raízes estejam no antigo pacote sp, as versões contemporâneas do gstat integram-se nativamente com os pacotes sf (Simple Features) e stars (Spatiotemporal Arrays), permitindo um fluxo de trabalho moderno e computacionalmente eficiente.\nUtilizaremos o conjunto de dados meuse (concentração de metais pesados na planície de inundação do rio Meuse, Holanda) para demonstração.\nPreparação do Ambiente e Dados\nPara a execução de rotinas geoestatísticas, dois componentes geométricos são imprescindíveis:\n\nDados Observados (Suporte Pontual): As amostras coletadas em campo.\nMalha de Predição (Suporte de Área/Grade): O domínio espacial discretizado onde as estimativas serão realizadas.\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\npacman::p_load(gstat, sf, stars, ggplot2, viridis, dplyr,gt, gridExtra)\n\n\n#Carregamento e conversão dos dados amostrais\ndata(meuse)\n# O CRS 28992 refere-se à projeção holandesa Amersfoort / RD New, vc nos seus dados usaraa crs=4326\nmeuse_sf &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\nglimpse(meuse_sf)\n\n\nRows: 155\nColumns: 13\n$ cadmium  &lt;dbl&gt; 11.7, 8.6, 6.5, 2.6, 2.8, 3.0, 3.2, 2.8, 2.4, 1.6, 1.4, 1.8, …\n$ copper   &lt;dbl&gt; 85, 81, 68, 81, 48, 61, 31, 29, 37, 24, 25, 25, 93, 31, 27, 8…\n$ lead     &lt;dbl&gt; 299, 277, 199, 116, 117, 137, 132, 150, 133, 80, 86, 97, 285,…\n$ zinc     &lt;dbl&gt; 1022, 1141, 640, 257, 269, 281, 346, 406, 347, 183, 189, 251,…\n$ elev     &lt;dbl&gt; 7.909, 6.983, 7.800, 7.655, 7.480, 7.791, 8.217, 8.490, 8.668…\n$ dist     &lt;dbl&gt; 0.00135803, 0.01222430, 0.10302900, 0.19009400, 0.27709000, 0…\n$ om       &lt;dbl&gt; 13.6, 14.0, 13.0, 8.0, 8.7, 7.8, 9.2, 9.5, 10.6, 6.3, 6.4, 9.…\n$ ffreq    &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ soil     &lt;fct&gt; 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ lime     &lt;fct&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1…\n$ landuse  &lt;fct&gt; Ah, Ah, Ah, Ga, Ah, Ga, Ah, Ab, Ab, W, Fh, Ag, W, Ah, Ah, W, …\n$ dist.m   &lt;dbl&gt; 50, 30, 150, 270, 380, 470, 240, 120, 240, 420, 400, 300, 20,…\n$ geometry &lt;POINT [m]&gt; POINT (181072 333611), POINT (181025 333558), POINT (18…\n\n\nCriação da Malha de Predição (Grid)\nA krigagem precisa de locais para onde serão feitas as predições. Para tal criaremos uma grade regular (raster) baseada nos limites da área.\n\n\nCódigo\n# O pacote traz um polígono 'meuse.area', vamos convertê-lo para sf\n\ndata(meuse.area)\n\nlimite_sf &lt;- st_polygon(list(as.matrix(meuse.area))) |&gt; \n  st_sfc(crs = 28992) |&gt; \n  st_sf() #provavelmente vc terá um arquivo shapfile, use arquivo .shp\n\n\n#Gerar a Grade Regular (Rasterização Vetorial)\n# st_make_grid cria a geometria. 'cellsize' define a resolução (ex: 40x40 metros)\n\ngrid_vetorial &lt;- st_make_grid(limite_sf, cellsize = 40, what = \"centers\") |&gt;\n  st_as_sf() |&gt;\n  st_filter(limite_sf) # Recorta a grade para ficar apenas dentro do polígono\n\n\n#Conversão para STARS (Mais eficiente para o gstat)\n\ngrid_stars &lt;- st_as_stars(st_bbox(limite_sf), dx = 40, dy = 40)\ngrid_stars &lt;- st_crop(grid_stars, limite_sf) # Mascara o que está fora do limite\n\n\nggplot() +\n  geom_sf(data = grid_vetorial, color=\"white\") + # vc pode trocar 'grid_vetorial' por 'grid_stars'\n  geom_sf(data = meuse_sf, color = \"red\", size = 0.5) +# e aqui trocar 'geom_sf' por geom_stars(data=grid_stars)\n  geom_sf(data=limite_sf, color=\"black\", fill=NA)+\n  labs(title = \"Domínio de Predição (Grade) e Amostras (Vermelho)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nAnálise exploratória espacial\nAntes de modelar, é necessário verificar a existência de dependência espacial.\nhscat: Gráficos de Dispersão Defasados\nA função hscat (h-scatterplots) confronta o valor de \\(Z(s)\\) com \\(Z(s+h)\\). Se houver estrutura espacial, espera-se que, para distâncias (\\(h\\)) pequenas, os pontos se alinhem à diagonal (alta correlação). Conforme \\(h\\) aumenta, a nuvem deve se dispersar.\n\nformula: Define a variável e/ou variável resposta (ex: log(zinc) ~ 1) (recomenda-se transformação logarítmica para dados assimétricos de concentração).\ndata: O objeto espacial contendo as observações.\nbreaks: Vetor numérico definindo os limites dos intervalos de distância (lags).\n\n\n\nCódigo\nhscat(log(zinc) ~ 1, data=meuse_sf, breaks = c(0, 100, 200, 400, 800))\n\n\n\n\n\n\n\n\nFigura 3.25: Dispersão espacial do log(Zinco) em diferentes lags\n\n\n\n\n\nModelagem da Covariância\nA etapa central da geoestatística é a determinação da função de semivariância \\(\\gamma(h)\\), que quantifica a dissimilaridade espacial.\n\nO Variograma Experimental: variogram\n\nA função variogram calcula a semivariância média para classes de distância discretas a partir dos dados observados.\n\nSintaxe: variogram(object, locations, ...)\nArgumentos:\n\n\nformula: Para Krigagem Ordinária, utiliza-se z ~ 1 (média constante). Para Krigagem Universal, define-se a tendência, ex:z ~ x + y, para as demais krigagens consulte a seção Seção 3.13.\ncutoff: A distância máxima de investigação. Por convenção, limita-se a 1/3 da diagonal da área de estudo para garantir representatividade amostral nos lags.\nwidth: A largura do intervalo de classe (tamanho do lag).\ncloud: Se TRUE, retorna a nuvem variográfica (semivariância de todos os pares individuais), útil para detecção de outliers.\nmap: Se TRUE, gera um mapa variográfico para inspeção de anisotropia.\nalpha: Direção em graus (ex: c(0, 45, 90, 135)) para investigar anisotropia.\n\n\n\nCódigo\n# Variograma Omnidirecional (Isotrópico)\nv_exp &lt;- variogram(log(zinc) ~ 1, meuse_sf, cutoff = 1200, width = 100)\n\nplot(v_exp, main = \"Semivariograma Experimental\", \n     xlab = \"Distância (m)\", ylab = \"Semivariância\")\n\n\n\n\n\n\n\n\n\nDefinição do Modelo Teórico: vgm\nO variograma experimental fornece pontos discretos. A krigagem exige uma função contínua e positiva definida. A função vgm estrutura este modelo.\n\npsill: Patamar parcial (variância estrutural).\nmodel: Família da curva.\n\n“Sph” (Esférico): Crescimento linear na origem, atinge patamar definido.\n“Exp” (Exponencial): Crescimento abrupto, atinge patamar assintoticamente.\n“Gau” (Gaussiano): Suave na origem (parabólico), indica alta continuidade.\n\nrange: Alcance.\nnugget: Efeito pepita (erro na origem).\n\nPara visualizar as famílias disponíveis, utiliza-se show.vgms().\n\n\nCódigo\n# Definição inicial baseada na inspeção visual do gráfico anterior\n\nmodelo_inicial &lt;- vgm(psill = 0.6, model = \"Sph\", range = 800, nugget = 0.05) \n\n#range 800 porque parece se estabilizar nele\n# no mesmo lugar que se estabiliza range 800, temos uma patamar (psill) de ~ 0.6 a 0.7\n#começa meio como linha reta obliquo, então aparenta ser esférico (Sph)\n# se olhar para onde se interceta o eixo y parece ser ~0.05 esse é efeito pepita (nugget)\n\n\nAjuste de Parâmetros: fit.variogram\nA função fit.variogram ajusta os parâmetros do modelo teórico (vgm) aos pontos experimentais (v_exp) utilizando Mínimos Quadrados Ponderados (WLS). O método pondera mais fortemente os lags iniciais (menores distâncias), que contêm mais pares de pontos e são cruciais para a interpolação.\n\n\nCódigo\nmodelo_ajustado &lt;- fit.variogram(v_exp, modelo_inicial)\nmodelo_ajustado|&gt;\n  knitr::kable()\n\n\n\n\n\n\n\nmodel\npsill\nrange\nkappa\nang1\nang2\nang3\nanis1\nanis2\n\n\n\n\nNug\n0.0635564\n0.000\n0.0\n0\n0\n0\n1\n1\n\n\nSph\n0.6042690\n979.887\n0.5\n0\n0\n0\n1\n1\n\n\n\n\n\nFigura 3.26: Ajuste do modelo teórico\n\n\n\n\nCódigo\nplot(v_exp, modelo_ajustado)\n\n\n\n\n\n\n\n\nFigura 3.27: Ajuste do modelo teórico\n\n\n\n\n\nExtração de Valores: variogramLine\nSe desejar reproduzir a curva teórica no ggplot2, esta função gera os dados da linha.\n\n\nCódigo\nlinha_teorica &lt;- variogramLine(modelo_ajustado, maxdist = 1200)\n\nhead(linha_teorica) |&gt;\n  knitr::kable()\n\n\n\n\n\ndist\ngamma\n\n\n\n\n0.001200\n0.0635575\n\n\n6.031345\n0.0691354\n\n\n12.061489\n0.0747128\n\n\n18.091634\n0.0802894\n\n\n24.121779\n0.0858648\n\n\n30.151924\n0.0914384\n\n\n\n\n\nKrigagem: Interpolação espacial: krige\nA função krige seleciona automaticamente o método adequado (Simples, Ordinária, Universal) baseando-se nos argumentos fornecidos.\n\nSintaxe: krige(formula, locations, newdata, model)\nArgumentos:\n\nformula: log(zinc) ~ 1 indica ausência de variáveis explicativas exógenas (apenas intercepto).\nlocations: Objeto sf com os dados observados.\nnewdata: Objeto sf ou stars com os locais de predição.\nmodel: O modelo de variograma ajustado.\nblock: (Opcional) Se fornecido um vetor, ex: c(40, 40), realiza Krigagem de Bloco, estimando o valor médio dentro da célula, resultando em mapas mais suaves e menor variância de predição.\n\n\n\n\nCódigo\n# Execução da Krigagem Ordinária\n\nkrigagem_ord &lt;- krige(log(zinc) ~ 1, \n                      locations = meuse_sf,  # Dados\n                      newdata =grid_vetorial ,  # Grade de destino, vc poderia usar tambem grid_stars\n                      model = modelo_ajustado, debug.level = 0) # Modelo espacial\n\n\nO objeto resultante (krigagem_ord) contém duas variáveis fundamentais:\n\nvar1.pred: O valor predito (estimativa).\nvar1.var: A variância da krigagem (incerteza da estimativa).\n\nVisualização de Mapas de Predição e Incerteza\nA apresentação correta dos resultados exige a exibição da estimativa juntamente com sua incerteza associada.\n\nCódigo\npacman::p_load(stars)\n\nkrigagem_raster &lt;- st_rasterize(krigagem_ord) |&gt;\n  st_crop( limite_sf)  #corta apenas a area do shapfile\n#\ng1 &lt;- ggplot() +\n  geom_stars(data = krigagem_raster, aes(fill = var1.pred, x = x, y = y)) + \n  scale_fill_viridis_c(option = \"B\", name = \"log(Zn)\", na.value = \"transparent\") +\n  geom_sf(data = limite_sf, fill = NA, color = \"black\") +\n  labs(title = \"Predição (Superfície Raster)\") +\n  theme_minimal()\n\ng2 &lt;- ggplot() +\n  geom_stars(data = krigagem_raster, aes(fill = sqrt(var1.var), x = x, y = y)) + \n  scale_fill_viridis_c(option = \"B\", name = \"SD\", na.value = \"transparent\") +\n  geom_sf(data = limite_sf, fill = NA, color = \"black\") +\n  labs(title = \"Incerteza (Desvio Padrão)\") +\n  theme_minimal()+\n  theme(axis.title = element_blank()) #remove os eixos x e y (veja no da esquerda existem porque não removemos)\n\ng1; g2\n\n\n\n\n\n\n\n\n\n\nFigura 3.28: Mapas de Predição e Incerteza (Desvio Padrão)\n\n\n\n\n\n\n\n\n\n\n\nFigura 3.29: Mapas de Predição e Incerteza (Desvio Padrão)\n\n\n\n\n\n\nAlternativamente poderia fazer:\n\n\nCódigo\n# st_bbox pega os limites da área\nbb &lt;- st_bbox(limite_sf) # onde limite_sf é seu shapfile\n\n# Criar objeto stars vazio com resolução de 40m\ngrid_raster &lt;- st_as_stars(bb, dx = 40, dy = 40)\n\n# Recortar (Crop) o raster usando o polígono limite\ngrid_raster &lt;- st_crop(grid_raster, limite_sf)\n\n\nkrigagem_direta &lt;- krige(log(zinc) ~ 1, \n                         locations = meuse_sf, \n                         newdata = grid_raster, \n                         model = modelo_ajustado, debug.level = 0)\n\nggplot() +\n  geom_stars(data = krigagem_direta, aes(fill = var1.pred)) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"log(Zn)\", na.value = \"transparent\") +\n  geom_sf(data = limite_sf, fill = NA, color = \"black\", size = 0.8) +\n  labs(title = \"Krigagem Ordinária\") +\n  theme_void() +\n  coord_sf(expand = FALSE) # Remove espaços em branco extras nas margens\n\n\n\n\n\n\n\n\n\nValidação Cruzada: krige.cv\nPara aferir a qualidade preditiva do modelo, utiliza-se a função krige.cv. Esta função executa o procedimento leave-one-out (ou k-fold), que remove um ponto, estima-o com os vizinhos, compara o real com o estimado, repete para todos.\n\nSintaxe: krige.cv(formula, locations, model, nfold, ...)\nArgumentos:\n\nnfold: Se omitido ou igual ao número de observações, faz leave-one-out. Se definido (ex: 5 ou 10), faz validação cruzada em k-partes.\n\n\n\n\nCódigo\nvalidacao &lt;- krige.cv(log(zinc) ~ 1, locations = meuse_sf, model = modelo_ajustado, debug.level = 0)\n\n\nExtração de Métricas de Diagnóstico\n\n\nCódigo\n# Resíduo = Observado - Predito\n# Z-score = Resíduo / Desvio Padrão da Krigagem\n\nmetricas &lt;- validacao |&gt;\n  st_drop_geometry() |&gt;\n  summarise(\n    ME = mean(residual),              # Erro Médio (Viés): Ideal -&gt; 0\n    RMSE = sqrt(mean(residual^2)),    # Acurácia: Ideal -&gt; Baixo\n    MSNE = mean(zscore),              # Viés Normalizado: Ideal -&gt; 0\n    RMSNE = sqrt(mean(zscore^2))      # Consistência da Variância: Ideal -&gt; 1\n  )\n\n\nmetricas|&gt;\n  knitr::kable()\n\n\n\n\n\nME\nRMSE\nMSNE\nRMSNE\n\n\n\n\n-0.0004748\n0.3978521\n-0.0003306\n0.8972854\n\n\n\n\n\nSimulação Estocástica\nA Krigagem suaviza a realidade. Para aplicações que exigem a reprodução da textura real da variabilidade (ex: fluxo em meios porosos, análise de risco ambiental), utiliza-se simulação.\nBasta adicionar o argumento nsim e definir nmax (para Simulação Sequencial Gaussiana local).\n\n\nCódigo\n# nsim = 4 gera quatro mapas possíveis da realidade\nsimulacoes &lt;- krige(log(zinc) ~ 1, \n                    locations = meuse_sf, \n                    newdata = grid_stars, \n                    model = modelo_ajustado, \n                    nsim = 4, \n                    nmax = 30, debug.level = 0)\n\ndados_df &lt;- as.data.frame(merge(simulacoes))\n\nggplot() +\n  geom_stars(data = merge(simulacoes)) +\ngeom_contour(data=dados_df,aes(x=x, y=y,z=var1),color=\"white\",size=0.2,alpha=0.5)+ #linhas de contorno (isolines)\n  facet_wrap(~attributes) +\n  geom_sf(data = limite_sf, fill = NA, color = \"black\", size = 0.5) +\n  scale_fill_viridis_c(option = \"inferno\", name = \"log(Zn)\", na.value = \"transparent\") +\n  theme_minimal() +\n  labs(title = \"Simulação com Curvas de Nível\")+\n  theme(axis.title = element_blank())\n\n\n\n\n\n\n\n\nFigura 3.30: Cenários equiprováveis (Realizações)",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#pacote-automap",
    "href": "geostat.html#pacote-automap",
    "title": "3  Geoestatística",
    "section": "3.18 Pacote automap",
    "text": "3.18 Pacote automap\nEnquanto no pacote gstat o usuário deve fornecer estimativas iniciais (chutes) para os parâmetros do variograma (patamar, alcance e efeito pepita) e testar manualmente diferentes funções de covariância (Esférico, Exponencial, Matérn, etc.), o pacote automap foi desenvolvido para automatizar essas etapas de ajuste e krigagem.\nBaseado na metodologia descrita por Hiemstra et al. (2008), o pacote é ideal para situações onde se deseja realizar interpolação espacial sem a necessidade de definir manualmente os parâmetros iniciais, ou quando é necessário processar múltiplos conjuntos de dados em lote. O pacote estima os valores iniciais a partir dos dados e itera sobre diferentes modelos para encontrar o melhor ajuste baseado na menor soma dos quadrados dos resíduos.\nAjuste Automático do Variograma: autofitVariogram\nEsta função elimina a necessidade de tentativa e erro manual. Ao contrário da função fit.variogram do gstat, que exige parâmetros iniciais, a autofitVariogram calcula esses valores automaticamente: o alcance inicial é definido como 0,10 vezes a diagonal da área dos dados (bounding box), o efeito pepita inicial é o mínimo da semivariância amostral, e o patamar é uma média entre o máximo e a mediana da semivariância . O algoritmo então ajusta e testa automaticamente os modelos Esférico, Exponencial, Gaussiano e Stein (Matérn), retornando aquele com o melhor ajuste estatístico . No canto inferior direito mostra o modelo ajustado e respetivos parâmetros.\n\n\nCódigo\npacman::p_load(automap, sf, gstat)\n\ndata(meuse)\nmeuse_sf &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\n\nvariograma_auto &lt;- autofitVariogram(log(zinc) ~ 1, input_data = as(meuse_sf, \"Spatial\"))\n\nplot(variograma_auto)\n\n\n\n\n\n\n\n\n\nInterpolação Automática: autoKrige\nA função autoKrige é chama internamente a autofitVariogram para ajustar o modelo e, em seguida, utiliza esse modelo otimizado para realizar a predição espacial nos novos locais . Isso resolve o problema de ter que passar manualmente os parâmetros do variograma para a função de krigagem. Ela suporta Krigagem Ordinária (padrão), Universal (inserindo covariáveis na fórmula) e Krigagem de Bloco .\n\n\nCódigo\npacman::p_load(stars, sp)\ndata(meuse)\ndata(meuse.grid)\n\nmeuse_sf &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\nmeuse_grid_sf &lt;- st_as_sf(meuse.grid, coords = c(\"x\", \"y\"), crs = 28992)\nmeuse_grid_stars &lt;- st_rasterize(meuse_grid_sf, dx = 40, dy = 40)\n\nkrigagem &lt;- autoKrige(log(zinc)~1, \n                      input_data = meuse_sf, \n                      new_data = meuse_grid_stars, debug.level = 0)\n\nplot(krigagem)\n\n\n\n\n\n\n\n\n\nExtraindo o resultado para plotar com ggplot\n\n\nCódigo\npacman::p_load(patchwork)\n\nresultado_sf &lt;- st_as_sf(krigagem$krige_output)\n\np1 &lt;- ggplot(resultado_sf) +\n  geom_sf(aes(fill = var1.pred), color = NA) + # color=NA é crucial aqui\n  scale_fill_viridis_c(option = \"plasma\", name = \"Log(Zn)\") +\n  labs(title = \"Predição\") + \n  theme_void()+\n  theme(plot.title = element_text(hjust = 0.5))\n\np2 &lt;- ggplot(resultado_sf) +\n  geom_sf(aes(fill = sqrt(var1.var)), color = NA) + \n  scale_fill_viridis_c(option = \"cividis\", name = \"SD\") +\n  labs(title = \"Erro Padrão\") + \n  theme_minimal()+\n  theme(plot.title = element_text(hjust = 0.5))\n\np1 + p2\n\n\n\n\n\n\n\n\n\nValidação Cruzada Automática: autoKrige.cv\nPara garantir que a automação não comprometeu a qualidade da predição, a função autoKrige.cv realiza a validação cruzada. Ela ajusta o variograma automaticamente e aplica a validação (leave-one-out ou k-fold) usando a função krige.cv do gstat. Isso permite verificar rapidamente se a estratégia automática está gerando resíduos aceitáveis sem a necessidade de configurar loops manuais de validação.\nPara avaliar a qualidade do modelo ajustado automaticamente, utiliza-se a autoKrige.cv. Esta função ajusta o variograma aos dados e, em seguida, utiliza a função krige.cv do gstat para realizar a validação cruzada (cross-validation)7. Ela suporta tanto o método leave-one-out quanto o k-fold (validar subconjuntos de dados) através do argumento nfold8.\n\n\nCódigo\n# Validação cruzada automática (10-fold) para KO\ncv_ordinaria &lt;- autoKrige.cv(log(zinc) ~ 1,\n                             input_data = as(meuse_sf, \"Spatial\"),\n                             nfold = 10)\n\n# Validação cruzada automática para Krigagem Universal (usando distância como covariável)\ncv_universal &lt;- autoKrige.cv(log(zinc) ~ sqrt(dist),\n                             input_data = as(meuse_sf, \"Spatial\"),\n                             nfold = 10, debug.level = 0)\n\n# Resumo dos resíduos\nsummary(cv_ordinaria$krige.cv_output)\n\n\nObject of class SpatialPointsDataFrame\nCoordinates:\n             min    max\ncoords.x1 178605 181390\ncoords.x2 329714 333611\nIs projected: TRUE \nproj4string :\n[+proj=sterea +lat_0=52.1561605555556 +lon_0=5.38763888888889\n+k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +units=m +no_defs]\nNumber of points: 155\nData attributes:\n   var1.pred        var1.var         observed        residual        \n Min.   :4.891   Min.   :0.1151   Min.   :4.727   Min.   :-0.969586  \n 1st Qu.:5.369   1st Qu.:0.1564   1st Qu.:5.288   1st Qu.:-0.199327  \n Median :5.862   Median :0.1808   Median :5.787   Median :-0.002832  \n Mean   :5.882   Mean   :0.1895   Mean   :5.886   Mean   : 0.004181  \n 3rd Qu.:6.347   3rd Qu.:0.2017   3rd Qu.:6.514   3rd Qu.: 0.218404  \n Max.   :7.265   Max.   :0.5400   Max.   :7.517   Max.   : 1.387144  \n     zscore               fold       \n Min.   :-2.251356   Min.   : 1.000  \n 1st Qu.:-0.467268   1st Qu.: 4.000  \n Median :-0.006538   Median : 6.000  \n Mean   : 0.010897   Mean   : 5.768  \n 3rd Qu.: 0.515972   3rd Qu.: 8.000  \n Max.   : 3.108253   Max.   :10.000  \n\n\nComparação de Modelos: compare.cv\nDado que a escolha entre Krigagem Ordinária e Universal pode ser difícil, a função compare.cv permite comparar diretamente os resultados de múltiplas validações cruzadas. Ela gera diagnósticos estatísticos (como RMSE e Correlação) e gráficos espaciais de bolhas (bubble plots) para identificar visualmente qual abordagem automática produziu menores erros. O argumento plot.diff destaca onde um modelo supera o outro.\n\n\nCódigo\ncomparacao &lt;- compare.cv(cv_ordinaria, cv_universal,\n                         col.names = c(\"Ordinária\", \"Universal\"),\n                         bubbleplots = TRUE, # Gera os gráficos na janela de plotagem\n                         plot.diff = FALSE)   \n\n\n\n\n\n\n\n\n\nCódigo\nprint(comparacao$spatial)\n\n\nNULL\n\n\nCódigo\ncomparacao1 &lt;- compare.cv(cv_ordinaria, cv_universal,\n                         col.names = c(\"Ordinária\", \"Universal\"),\n                         bubbleplots = FALSE, \n                         plot.diff = FALSE)   \n\ncomparacao1 |&gt;\n  knitr::kable()\n\n\n\n\n\n\nOrdinária\nUniversal\n\n\n\n\nmean_error\n0.004181\n0.001329\n\n\nme_mean\n0.0007103\n0.0002258\n\n\nMAE\n0.2933\n0.2709\n\n\nMSE\n0.1551\n0.1442\n\n\nMSNE\n0.8247\n1.088\n\n\ncor_obspred\n0.8378\n0.8499\n\n\ncor_predres\n0.06733\n-0.0531\n\n\nRMSE\n0.3938\n0.3797\n\n\nRMSE_sd\n0.5455\n0.526\n\n\nURMSE\n0.3938\n0.3797\n\n\niqr\n0.4177\n0.3999\n\n\n\n\n\nIntervalos de Predição de Posição: posPredictionInterval\nEsta função oferece uma ferramenta prática para tomada de decisão baseada na incerteza da krigagem automática. Ela calcula a posição do intervalo de predição (padrão 95%) em relação a um valor limite (cutoff). O mapa resultante classifica as áreas como potencialmente acima, potencialmente abaixo ou indistinguível do limite, facilitando a interpretação de riscos sem exigir cálculos manuais de intervalos de confiança.\n\n\nCódigo\n#Extrair os resultados da krigagem e remover NA\nresultado_pontos &lt;- st_as_sf(krigagem$krige_output, as_points = TRUE)\nresultado_limpo &lt;- resultado_pontos[!is.na(resultado_pontos$var1.pred), ]\n\nkrigagem_pontos &lt;- krigagem\nkrigagem_pontos$krige_output &lt;- resultado_limpo\n\nintervalos &lt;- posPredictionInterval(krigagem_pontos, \n                                    p = 95, \n                                    value = 6.0)\n\nplot(intervalos, main = \"Classificação vs Limiar (6.0)\")\n\n\n\n\n\n\n\n\nFigura 3.31: Áreas estatisticamente acima ou abaixo do limiar (Log(Zn) = 6.0)",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "geostat.html#pacote-geor",
    "href": "geostat.html#pacote-geor",
    "title": "3  Geoestatística",
    "section": "3.19 Pacote geoR",
    "text": "3.19 Pacote geoR\nVeja no capítulo 3 de Scalon (2024).\n\n\n\n\nAbdulah, Sameh, Yuxiao Li, Jian Cao, Hatem Ltaief, David E Keyes, Marc G Genton, e Ying Sun. 2023. “Large-scale environmental data science with ExaGeoStatR”. Environmetrics 34 (1): e2770.\n\n\nBakka, Haakon, Håvard Rue, Geir-Arne Fuglstad, Andrea Riebler, David Bolin, Janine Illian, Elias Krainski, Daniel Simpson, e Finn Lindgren. 2018. “Spatial modeling with R-INLA: A review”. Wiley Interdisciplinary Reviews: Computational Statistics 10 (6): e1443.\n\n\nBanerjee, Sudipto, Bradley P Carlin, e Alan E Gelfand. 2003. Hierarchical modeling and analysis for spatial data. Chapman; Hall/CRC.\n\n\nCarvalho, Dhaniel, e CV Deutsch. 2017. “An overview of multiple indicator kriging”. Geostatistics Lessons 7.\n\n\nChiles, Jean-Paul, e Pierre Delfiner. 2012. Geostatistics: modeling spatial uncertainty. John Wiley & Sons.\n\n\nCressie, Noel. 1985. “Fitting variogram models by weighted least squares”. Journal of the international Association for mathematical Geology 17 (5): 563–86.\n\n\n———. 1989. “Geostatistics”. The American Statistician 43 (4): 197–202.\n\n\n———. 1990. “The origins of kriging”. Mathematical geology 22 (3): 239–52.\n\n\n———. 1991. “Geostatistical analysis of spatial data”. Spatial statistics and digital image analysis 1991: 87–108.\n\n\n———. 1993. Statistics for spatial data. John Wiley & Sons.\n\n\nCressie, Noel, e Douglas M Hawkins. 1980. “Robust estimation of the variogram: I”. Journal of the international Association for Mathematical Geology 12 (2): 115–25.\n\n\nCressie, Noel, e Matthew T Moores. 2022. “Spatial statistics”. Em Encyclopedia of mathematical geosciences, 1–11. Springer.\n\n\nCressie, Noel, Matthew Sainsbury-Dale, e Andrew Zammit-Mangion. 2022. “Basis-function models in spatial statistics”. Annual Review of Statistics and Its Application 9 (1): 373–400.\n\n\nCressie, Noel, e Andrew Zammit-Mangion. 2016. “Multivariate spatial covariance models: a conditional approach”. Biometrika 103 (4): 915–35.\n\n\nDeutsch, Clayton V., e André G. Journel. 1997. GSLIB: Geostatistical Software Library and User’s Guide. 2º ed. New York: Oxford University Press.\n\n\nDiggle, Peter J, Jonathan A Tawn, e Rana A Moyeed. 1998. “Model-based geostatistics”. Journal of the Royal Statistical Society Series C: Applied Statistics 47 (3): 299–350.\n\n\nEcker, Mark D. 2003. “Geostatistics: past, present and future”. Encyclopedia of Life Support Systems (EOLSS), 50614–506.\n\n\nGenton, Marc G. 1998. “Variogram fitting by generalized least squares using an explicit formula for the covariance structure”. Mathematical Geology 30 (4): 323–45.\n\n\nGoovaerts, Pierre. 1997. Geostatistics for natural resources evaluation. Oxford university press.\n\n\nGorsich, David J, e Marc G Genton. 2000. “Variogram model selection via nonparametric derivative estimation”. Mathematical geology 32 (3): 249–70.\n\n\nGräler, Benedikt, Edzer Pebesma, e Gerard Heuvelink. 2016. “Spatio-temporal interpolation using gstat”.\n\n\nGuimaraes, Ricardo JPS, Corina C Freitas, Luciano V Dutra, Carlos A Felgueiras, Sandra C Drummond, Sandra HC Tibiriçá, Guilherme Oliveira, e Omar S Carvalho. 2012. “Use of indicator kriging to investigate schistosomiasis in minas gerais state, Brazil”. Journal of Tropical Medicine 2012 (1): 837428.\n\n\nGuttorp, Peter, e Tilmann Gneiting. 2006. “Studies in the history of probability and statistics XLIX on the Matérn correlation family”. Biometrika 93 (4): 989–95.\n\n\nHarville, David A. 1977. “Maximum likelihood approaches to variance component estimation and to related problems”. Journal of the American statistical association 72 (358): 320–38.\n\n\nHiemstra, P. H., E. J. Pebesma, C. J. W. Twenh\"ofel, e G. B. M. Heuvelink. 2008. “Real-time automatic interpolation of ambient gamma dose rates from the Dutch Radioactivity Monitoring Network”. Computers & Geosciences.\n\n\nHill, Donna. 1998. “Comparison of median indicator kriging with full indicator kriging in the analysis of spatial data”.\n\n\nIsaaks, Edward H, R Mohan Srivastava, et al. 1989. “Applied geostatistics”.\n\n\nJi, Guangjun, Zizhao Cai, Keyan Xiao, Yan Lu, e Qian Wang. 2025. “Ordered Indicator Kriging Interpolation Method with Field Variogram Parameters for Discrete Variables in the Aquifers of Quaternary Loose Sediments”. Water 17 (21): 3116.\n\n\nJournel, Andre G. 1986. “Constrained interpolation and qualitative information—the soft kriging approach”. Mathematical Geology 18 (3): 269–86.\n\n\nJournel, Andre G, e Charles J Huijbregts. 1976. “Mining geostatistics”.\n\n\nJournel, André G. 1983. “Nonparametric estimation of spatial distributions”. Journal of the International Association for Mathematical Geology 15 (3): 445–68.\n\n\nJuang, Kai-Wei, e Dar-Yuan Lee. 1998. “Simple indicator kriging for estimating the probability of incorrectly delineating hazardous areas in a contaminated site”. Environmental Science & Technology 32 (17): 2487–93.\n\n\nKrige, Danie, e Wynand Kleingeld. 2005. “The genesis of geostatistics in gold and diamond industries”. Em Space, Structure and Randomness: Contributions in Honor of Georges Matheron in the Field of Geostatistics, Random Sets and Mathematical Morphology, 5–16. Springer.\n\n\nLark, RM. 2000. “Estimating variograms of soil properties by the method-of-moments and maximum likelihood”. European Journal of Soil Science 51 (4): 717–28.\n\n\nLaslett, Geoffrey M. 1994. “Kriging and splines: an empirical comparison of their predictive performance in some applications”. Journal of the American Statistical Association 89 (426): 391–400.\n\n\nLindgren, Finn, David Bolin, e Håvard Rue. 2022. “The SPDE approach for Gaussian and non-Gaussian fields: 10 years and still running”. Spatial Statistics 50: 100599.\n\n\nLindgren, Finn, e Håvard Rue. 2015. “Bayesian spatial modelling with R-INLA”. Journal of statistical software 63: 1–25.\n\n\nLindgren, Finn, Håvard Rue, e Johan Lindström. 2011. “An explicit link between Gaussian fields and Gaussian Markov random fields: the stochastic partial differential equation approach”. Journal of the Royal Statistical Society Series B: Statistical Methodology 73 (4): 423–98.\n\n\nMarchant, BP, e RM Lark. 2007. “Robust estimation of the variogram by residual maximum likelihood”. Geoderma 140 (1-2): 62–72.\n\n\nMatheron, G. 1963. “\" Principles of geostatistics\", Economic Geology, 58, pp 1246-1266”.\n\n\nMatheron, George. 1971. “The theory of regionalised variables and its applications”. Les Cahiers du Centre de Morphologie Mathématique 5: 212.\n\n\nMcBratney, AB, e R Webster. 1986. “Choosing functions for semi-variograms of soil properties and fitting them to sampling estimates”. Journal of soil Science 37 (4): 617–39.\n\n\nMohammadpour, Mahyadin, Abbas Bahroudi, Maysam Abedi, Gholamreza Rahimipour, Golnaz Jozanikohan, e Farzaneh Mami Khalifani. 2019. “Geochemical distribution mapping by combining number-size multifractal model and multiple indicator kriging”. Journal of Geochemical Exploration 200: 13–26.\n\n\nMyers, Donald E. 1994. “Spatial interpolation: an overview”. Geoderma 62 (1-3): 17–28.\n\n\nNhancololo, A. M., Wélson A. Oliveira, Fernandes A. C. Pereira, Bruno Montoani Silva, e João Domingos Scalon. 2024. “Comparison between the laboratory method and the Stolf penetrometer in soil density analysis: a study using geostatistical approaches”. Sigmae 13 (1): 63–78. https://doi.org/10.29327/2520355.13.1-7.\n\n\nOliver, MA, e R Webster. 2014. “A tutorial guide to geostatistics: Computing and modelling variograms and kriging”. Catena 113: 56–69.\n\n\nPebesma, Edzer J. 2004. “Multivariable geostatistics in S: the gstat package”. Computers & geosciences 30 (7): 683–91.\n\n\nRibeiro Jr, Paulo J, e Peter J Diggle. 2006. “geoR: Package for Geostatistical Data Analysis an illustrative session”. Artificial Intelligence 1: 1–24.\n\n\nRibeiro Jr, Paulo Justiniano, e Peter Diggle. 2025. geoR: Analysis of Geostatistical Data. https://doi.org/10.32614/CRAN.package.geoR.\n\n\nRichardson, Alice M, e Alan H Welsh. 1995. “Robust restricted maximum likelihood in mixed linear models”. Biometrics, 1429–39.\n\n\nSahu, Sujit. 2022. Bayesian modeling of spatio-temporal data with R. Chapman; Hall/CRC.\n\n\nScalon, João Domingos. 2024. Análise de Dados Espaciais com Aplicações em R. Lavras: Ed. UFLA.\n\n\nUngaro, F, F Ragazzi, R Cappellin, e P Giandon. 2008. “Arsenic concentration in the soils of the Brenta Plain (Northern Italy): mapping the probability of exceeding contamination thresholds”. Journal of Geochemical Exploration 96 (2-3): 117–31.\n\n\nVer Hoef, Jay M, e Ronald Paul Barry. 1998. “Constructing and fitting models for cokriging and multivariable spatial prediction”. Journal of Statistical Planning and Inference 69 (2): 275–94.\n\n\nWackernagel, Hans. 2003. Multivariate geostatistics: an introduction with applications. Springer Science & Business Media.\n\n\nWhittle, Peter. 1954. “On stationary processes in the plane”. Biometrika, 434–49.\n\n\nYamamoto, Jorge Kazuo, e Paulo M. Barbosa Landim. 2013. Geoestatística: conceitos e aplicações. São Paulo: Oficina de Textos.",
    "crumbs": [
      "Geostatística",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geoestatística</span>"
    ]
  },
  {
    "objectID": "lattice_data.html",
    "href": "lattice_data.html",
    "title": "4  Dados de Área",
    "section": "",
    "text": "4.1 Representação espacial e construção de estruturas de vizinhança\nA análise de dados de área (ou lattice data) lida com processos estocásticos cujo domínio espacial é fixo, discreto e contável. Denotamos esse domínio por \\(D^L\\). Enquanto na geoestatística (Capítulo 3) o suporte é contínuo, permitindo observações em qualquer localização \\(\\mathbf{s} \\in D^G\\), nos dados de área as observações estão ancoradas em unidades espaciais predefinidas e não sobrepostas, como regiões administrativas, células de uma grelha ou zonas censitárias.\nSeja \\(\\{D_i\\}_{i=1}^{n}\\) uma coleção finita de \\(n\\) unidades espaciais (por exemplo, municípios, distritos ou pixels). Os dados de área são definidos como uma coleção de variáveis aleatórias indexadas por essas unidades: \\(\\{Y(\\mathbf{s}_i): \\mathbf{s}_i \\in D^L\\}\\), onde \\(D^L = \\{\\mathbf{s}_1, \\dots, \\mathbf{s}_n\\}\\) é um subconjunto fixo e contável do espaço Euclidiano \\(\\mathbb{R}^d\\) (Cressie e Moores 2022). A incerteza reside exclusivamente no valor do atributo \\(Y(\\mathbf{s}_i)\\), e não na localização \\(\\mathbf{s}_i\\) (que é fixa e conhecida), diferenciando-se dos processos pontuais (Capítulo 5). Para as \\(n\\) regiões, o vetor de observações é \\(\\mathbf{y} = (y_1, \\dots, y_n)^\\top\\), onde \\(y_i \\equiv y(\\mathbf{s}_i)\\).\nConforme destacado por Besag (1974), a dependência espacial neste contexto não é necessariamente governada por uma métrica de distância Euclidiana contínua, como na geoestatística (Capítulo 3), mas sim pela topologia ou estrutura de vizinhança definida entre as unidades discretas \\(\\{D_i\\}\\).\nA questão inferencial central também se desloca. Em vez de interpolar (prever) um valor em um local não observado \\(s_0\\) (krigagem, Capítulo 3), o foco passa a ser compreender e quantificar como o valor observado na unidade \\(D_i\\) é influenciado pelos valores nas unidades vizinhas \\(\\{D_j\\}\\) (interação espacial). Por simplicidade, as unidades são frequentemente denotadas apenas por seus índices \\(i\\) e \\(j\\).\nUma característica fundamental dos dados de área é a agregação. O valor observado \\(y_i\\) na unidade \\(i\\) é tipicamente o resultado da integração (ou média) de um processo contínuo latente \\(Y(\\mathbf{s})\\) sobre a área geográfica \\(A_i\\) daquela unidade. Formalmente, se \\(Y(\\mathbf{s})\\) representa, por exemplo, densidade ou intensidade, então:\n\\[\ny_i = \\int_{A_i} Y(\\mathbf{s}) \\, d\\mathbf{s} \\: \\text{(para contagens ou volumes)}, \\: \\text{ ou }\ny_i = \\frac{1}{|A_i|} \\int_{A_i} Y(\\mathbf{s}) \\, d\\mathbf{s} \\: \\text{(para médias ou intensidades)},\n\\] onde \\(|A_i|\\) é a área da região \\(i\\).\nEsta natureza agregada implica que a inferência estatística é condicional à partição específica do espaço (\\(A_1 \\cup \\dots \\cup A_n\\)). Alterar essa partição (escala ou limites) pode alterar as propriedades estatísticas (média, variância, correlação) dos dados. Este fenômeno é conhecido como o Problema da Unidade de Área Modificável (MAUP) (Openshaw 1984). Estatisticamente, a agregação introduz uma heterocedasticidade intrínseca: unidades com áreas \\(|A_i|\\) ou populações-base diferentes terão variâncias de amostragem distintas, um aspecto que deve ser cuidadosamente considerado na modelagem da matriz de covariância \\(\\mathbf{\\Sigma}\\).\nA representação dos dados de área pode se dar em estruturas regulares (grelhas ou grids) ou irregulares (divisões políticas ou administrativas, Seção 2.5). Grids regulares são comuns em análise de imagens, sensoriamento remoto e dados climáticos, onde cada célula (pixel) tem uma forma e tamanho constantes, facilitando a computação e a definição de vizinhança. Polígonos irregulares, que representam entidades como municípios ou distritos, são comuns em ciências sociais e saúde pública. A heterogeneidade no tamanho e forma dessas regiões introduz desafios adicionais, como a já mencionada variância desigual e a definição não trivial de proximidade (Cressie e Chan 1989).\nPara modelar a dependência espacial, é fundamental definir formalmente como as unidades se relacionam. Essa relação baseia-se na matriz de pesos espaciais ou matriz de vizinhança:\n\\[\n\\mathbf{W}_{n \\times n} =\n\\begin{bmatrix}\nw_{11} & w_{12} & \\cdots & w_{1n} \\\\\nw_{21} & w_{22} & \\cdots & w_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nw_{n1} & w_{n2} & \\cdots & w_{nn}\n\\end{bmatrix},\n\\]\nonde cada elemento \\(w_{ij}\\) quantifica a conexão espacial entre a unidade \\(j\\) e unidade \\(i\\). Por convenção, assume-se que \\(w_{ii} = 0\\), impedindo que uma unidade seja vizinha de si (Anselin 2001). Note ainda que é comum descrever a vizinhança entre unidades \\(i\\) e \\(j\\) se existe, simplesmente \\(i\\sim j\\), para referir que \\(w_{ij} \\neq 0\\) (Besag e Kooperberg 1995).\nA construção de \\(\\mathbf{W}\\) envolve duas etapas conceituais distintas: 1) a definição da topologia ou critério de vizinhança (quem é vizinho de quem); e 2) a ponderação (a intensidade atribuída a cada conexão). Enquanto a primeira é predominantemente geométrica, a segunda frequentemente envolve uma operação de normalização, crucial para a estabilidade numérica e interpretabilidade dos modelos.\nDiferente das séries temporais, onde a dependência é unidirecional e sequencial (o passado influencia o futuro), nos dados de área a dependência é multidirecional e simultânea. A unidade \\(i\\) influencia \\(j\\), que influencia \\(k\\), que pode, por sua vez, influenciar \\(i\\) novamente através de outras conexões, criando um sistema de feedback espacial.",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#sec-lattice",
    "href": "lattice_data.html#sec-lattice",
    "title": "4  Dados de Área",
    "section": "",
    "text": "4.1.1 Critérios de Vizinhança\nA definição operacional de proximidade ou vizinhança é um passo fundamental e teórico. Anselin (2002) discutem os critérios mais comuns:\n\nContiguidade (Adjacência): Baseia-se no compartilhamento de fronteiras (ver Seção 2.2).\n\nTorre (Rook): As unidades \\(i\\) e \\(j\\) são vizinhas se compartilham um segmento de fronteira (aresta). Formalmente, \\(\\text{dim}(\\partial A_i \\cap \\partial A_j) = 1\\).\nRainha (Queen): As unidades \\(i\\) e \\(j\\) são vizinhas se compartilham qualquer ponto de fronteira, seja um vértice ou uma aresta. Formalmente, \\(A_i \\cap \\partial A_j \\neq \\emptyset\\). Este critério é mais abrangente e é particularmente útil para malhas irregulares, pois evita que unidades que se tocam apenas em um canto (como municípios separados por um rio que se encontram em uma confluência) sejam consideradas desconectadas. Por exemplo, ao estudar a propagação de um fenômeno social entre municípios, dois que são separados por um rio mas cujos centros urbanos estão próximos na confluência podem ter intensa interação. Usar o critério Rook os trataria como isolados, enquanto o critério Queen capturaria essa potencial conexão, resultando em uma matriz de conectividade mais robusta e evitando subestimar a dependência espacial.\n\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(sf, spdep, ggplot2, patchwork, dplyr, geodata)\n\n# Baixar dados dos EUA (Nível 1 = Estados)\nusa_sf &lt;- tryCatch({\n  # Tenta baixar direto\n  usa_vect &lt;- geodata::gadm(country = \"USA\", level = 1, path = tempdir(), version=\"latest\")\n  sf::st_as_sf(usa_vect)\n}, error = function(e) {\n  message(\"Erro ao baixar dados, gadm está com problemas, baixa direto no site: https://gadm.org/maps.html.\")\n})\n\n# FILTRAR apenas Utah, Colorado, Arizona, New Mexico\nfour_corners &lt;- usa_sf %&gt;% \n  filter(NAME_1 %in% c(\"Utah\", \"Colorado\", \"Arizona\", \"New Mexico\")) %&gt;%\n  st_make_valid()\n\n# Extrair centroides para o grafo\ncoords &lt;- suppressWarnings(st_coordinates(st_centroid(four_corners))) #suppressWarnings() era para tirar\ncoords_df &lt;- as.data.frame(coords)\n\n\n# Queen (Rainha)\n\n#identificar quais polígonos são vizinhos e constroir lista de vizinhos\nnb_queen &lt;- poly2nb(four_corners, queen = TRUE)\n\n#criar segmentos de reta ligando os centroides das áreas vizinhas\nnb_lines_queen &lt;- nb2lines(nb_queen, coords = coords, as_sf = TRUE)\nst_crs(nb_lines_queen) &lt;- st_crs(four_corners) # atribuir a nb_lines_queen CRS igual do four_corners\n\n# Rook (Torre)\nnb_rook &lt;- poly2nb(four_corners, queen = FALSE)\nnb_lines_rook &lt;- nb2lines(nb_rook, coords = coords, as_sf = TRUE)\nst_crs(nb_lines_rook) &lt;- st_crs(four_corners)\n\ntheme_comp &lt;- theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        legend.position = \"bottom\")+\n  theme(legedn.title=element_text(hjust=0.5))\n\n# Queen\np_queen &lt;- ggplot() +\n  geom_sf(data = four_corners, fill = \"white\", color = \"gray20\", linewidth = 0.5) +\n  geom_sf(data = nb_lines_queen, aes(color = \"Conexão (Queen)\"), linewidth = 1.2) +\n  geom_point(data = coords_df, aes(X, Y), size = 3) +\n  geom_sf_text(data = four_corners, aes(label = NAME_1), size = 3, nudge_y = -0.5, nudge_x=1) +\n  scale_color_manual(values = \"steelblue\", name = \"\") +\n  labs(title = \"Critério Rainha (Queen)\", \n       subtitle = \"Utah, Colorado, Arizona, New Mexico\\n tem 1 ponto em comum\") +\n  theme_comp\n\n# Rook\np_rook &lt;- ggplot() +\n  geom_sf(data = four_corners, fill = \"white\", color = \"gray20\", linewidth = 0.5) +\n  geom_sf(data = nb_lines_rook, aes(color = \"Conexão (Rook)\"), linewidth = 1.2) +\n  geom_point(data = coords_df, aes(X, Y), size = 3) +\n  geom_sf_text(data = four_corners, aes(label = NAME_1), size = 3, nudge_y = -0.5, nudge_x=1) +\n  scale_color_manual(values = \"firebrick\", name = \"\") +\n  labs(title = \"Critério Torre (Rook)\", \n       subtitle = \"Utah e New Mexico; Colorado e Arizona\\n śo tem 1 ponto em comum\") +\n  theme_comp\n\np_queen + p_rook\n\n\n\n\n\n\n\n\nFigura 4.1: Comparação das Estruturas de Vizinhança Queen (Rainha) e Rook (Torre) entre Utah, Colorado, Arizona, New Mexico (Estados Unidos)\n\n\n\n\n\n\nBaseado em distância:\n\n\\(k\\)-Vizinhos mais próximos (\\(k\\)-NN): Define como vizinhos de \\(i\\), as \\(k\\) unidades cujos centroides (ou outro ponto representativo) estão mais próximos, segundo a distância euclidiana. Garante que cada unidade tenha exatamente \\(k\\) vizinhos, criando uma matriz esparsa e evitando ilhas de desconexão (Anselin 2001).\n\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(sf, spdep, ggplot2, geobr, dplyr, patchwork)\n\n# Baixar mapa municipal de Mato Grosso (MT)\nmt_sf &lt;- read_municipality(code_muni = \"MT\", year = 2020, showProgress = FALSE)\n\ncoords_mt &lt;- suppressWarnings(st_coordinates(st_centroid(mt_sf)))\n\ntheme_map &lt;- theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\", size = 12),\n        plot.subtitle = element_text(hjust = 0.5, size = 14))\n\n\n\n\nCódigo\n# Calcular os k=4 vizinhos mais próximos\nk &lt;- 4\nknn_nb &lt;- knearneigh(coords_mt, k = k)\nnb_knn &lt;- knn2nb(knn_nb)\n\n# Converter para linhas espaciais para plotar\nlines_knn &lt;- nb2lines(nb_knn, coords = coords_mt, as_sf = TRUE)\nst_crs(lines_knn) &lt;- st_crs(mt_sf)\n\n# Plot\nggplot() +\n  geom_sf(data = mt_sf, fill = \"gray95\", color = \"gray80\") +\n  geom_sf(data = lines_knn, color = \"purple\", linewidth = 0.5, alpha = 0.6) +\n  geom_point(data = as.data.frame(coords_mt), aes(X, Y), size = 0.8) +\n  labs(title = paste0(\"k-Vizinhos Mais Próximos (k=\", k, \")\"),\n       subtitle = \"Cada município conecta-se aos 4 centroides mais próximos\") +\n  theme_map\n\n\n\n\n\n\n\n\nFigura 4.2: Vizinhança k-NN (k=4) em Mato Grosso.\n\n\n\n\n\n-   Limiar de distância (*Threshold*): $w_{ij} = 1$ se $d_{ij} \\le d_{\\max}$, e $0$ caso contrário, onde $d_{ij}$ é a distância entre centroides.\n\n\nCódigo\n# Para precisão, vamos projetar para SIRGAS 2000 / Brazil Polyconic (EPSG 5880) para usar metros.\nmt_proj &lt;- st_transform(mt_sf, 5880)\ncoords_proj &lt;- st_coordinates(st_centroid(mt_proj))\n\n# Definir raio de 120 km (120000 metros)\ndist_nb &lt;- dnearneigh(coords_proj, 0, 120000)\n\n# Converter para linhas\nlines_dist &lt;- nb2lines(dist_nb, coords = coords_proj, as_sf = TRUE)\nst_crs(lines_dist) &lt;- st_crs(mt_proj)\n\nggplot() +\n  geom_sf(data = mt_proj, fill = \"gray95\", color = \"gray80\") +\n  geom_sf(data = lines_dist, color = \"darkorange\", linewidth = 0.5, alpha = 0.6) +\n  geom_point(data = as.data.frame(coords_proj), aes(X, Y), size = 0.8) +\n  labs(title = \"Limiar de Distância Fixa (120 km)\",\n       subtitle = \"Conexões apenas se d &lt; 120km (Note as ilhas isoladas)\") +\n  theme_map\n\n\n\n\n\n\n\n\nFigura 4.3: Vizinhança por Limiar de Distância (120km).\n\n\n\n\n\n-   Decaimento por distância: Atribui pesos que decrescem com a distância, ex: $w_{ij} = d_{ij}^{-\\alpha}$ ou $w_{ij} = \\exp(-\\beta d_{ij})$. Atribui maior influência a unidades mais próximas.\n\n\nCódigo\n# Identificar Cuiabá\nid_cuiaba &lt;- which(mt_sf$name_muni == \"Cuiabá\")\n\n# Calcular distâncias de Cuiabá para TODOS os outros municípios\nnb_all &lt;- dnearneigh(coords_proj, 0, 900000) # Raio grande para pegar quase todo estado\ndists &lt;- nbdists(nb_all, coords_proj)\n\n# Calcular Pesos (Inverso da Distância: 1/d)\nweights_list &lt;- lapply(dists, function(x) 1/(x/1000)) # /1000 para km\n\n# Preparar dados apenas para Cuiabá para visualização\nvizinhos_cuiaba &lt;- nb_all[[id_cuiaba]]\npesos_cuiaba &lt;- weights_list[[id_cuiaba]]\n\n# Criar linhas saindo de Cuiabá\n\nlines_cuiaba &lt;- vector(\"list\", length(vizinhos_cuiaba))\n\nfor(i in seq_along(vizinhos_cuiaba)) {\n  dest_idx &lt;- vizinhos_cuiaba[i]\n  lines_cuiaba[[i]] &lt;- st_linestring(rbind(coords_proj[id_cuiaba,], coords_proj[dest_idx,]))\n}\n\nsf_decay &lt;- st_sf(peso = pesos_cuiaba, geometry = st_sfc(lines_cuiaba), crs = 5880)\n\nggplot() +\n  geom_sf(data = mt_proj, fill = \"gray95\", color = \"white\") +\n  geom_sf(data = sf_decay, aes(color = peso, linewidth = peso), alpha = 0.8) +\n  geom_point(aes(x=coords_proj[id_cuiaba,1], y=coords_proj[id_cuiaba,2]), color=\"red\", size=3) +\n  scale_color_viridis_c(option = \"magma\", name = \"Peso (1/d)\") +\n  scale_linewidth(range = c(0.1, 2), guide = \"none\") +\n  labs(title = \"Decaimento por Distância (Foco: Cuiabá)\",\n       subtitle = \"A espessura e cor indicam a força da influência\") +\n  theme_map\n\n\n\n\n\n\n\n\nFigura 4.4: Decaimento por Distância Inversa a partir de Cuiabá.\n\n\n\n\n\n\nVizinhança econômica ou social: Harris, Moffat, e Kravtsova (2011) argumentam que a contiguidade física pode ser insuficiente ou enganosa em muitos contextos. Em estudos regionais, a conexão funcional frequentemente supera a proximidade geográfica. Por exemplo, no Brasil, um município do agronegócio no Centro-Oeste (ex.: Sorriso/MT) pode estar economicamente mais conectado aos portos de Santos (SP) ou Paranaguá (PR) por onde escoa sua produção do que aos municípios geograficamente adjacentes em seu próprio estado que possuem economias de base diferente. Da mesma forma, para análises de mercado de trabalho ou inovação, a região metropolitana de São Paulo pode ter uma interação mais intensa com polos tecnológicos como Campinas ou até com outros centros globais do que com municípios vizinhos de baixa intensidade tecnológica. Matrizes baseadas em fluxos (comerciais, migratórios, de passageiros), similaridade socioeconômica (PIB per capita, estrutura produtiva) ou redes de infraestrutura (rodovias, linhas de voo) são, portanto, alternativas teóricas mais ricas e adequadas a fenômenos específicos.\n\n\n\nCódigo\n# Carregar mapa do Brasil (Estados)\nbr_states &lt;- read_state(year = 2020, showProgress = FALSE)\n\n# Coordenadas aproximadas das cidades de interesse\n# (Sorriso-MT, Santos-SP, Paranaguá-PR)\n\ncidades_df &lt;- data.frame(\n  cidade = c(\"Sorriso (MT)\", \"Porto de Santos (SP)\", \"Porto de Paranaguá (PR)\"),\n  lat = c(-12.5427, -23.9618, -25.5205),\n  lon = c(-55.7211, -46.3322, -48.5095),\n  tipo = c(\"Origem\", \"Destino\", \"Destino\")\n)\n\ncidades_sf &lt;- st_as_sf(cidades_df, coords = c(\"lon\", \"lat\"), crs = 4326)\n\n# Criar conexões (Arcos)\nsorriso_coords &lt;- subset(cidades_df, cidade == \"Sorriso (MT)\")\n\ndestinos &lt;- subset(cidades_df, tipo == \"Destino\")\n\nconexoes &lt;- lapply(1:nrow(destinos), function(i) {\n  st_linestring(rbind(\n    c(sorriso_coords$lon, sorriso_coords$lat),\n    c(destinos$lon[i], destinos$lat[i])\n  ))\n})\n\nconexoes_sf &lt;- st_sf(geometry = st_sfc(conexoes), crs = 4326)\n\n# Plot\nggplot() +\n  geom_sf(data = br_states, fill = \"gray95\", color = \"white\") +\n  # Destacar Estados envolvidos\n  geom_sf(data = subset(br_states, abbrev_state %in% c(\"MT\", \"SP\", \"PR\")), \n          fill = \"gray85\", color = \"white\") +\n  \n  # Linhas de Fluxo (Curvas para indicar movimento/distância)\n  geom_curve(data = data.frame(x1 = sorriso_coords$lon, y1 = sorriso_coords$lat,\n                               x2 = destinos$lon, y2 = destinos$lat),\n             aes(x = x1, y = y1, xend = x2, yend = y2),\n             color = \"darkgreen\", size = 1, curvature = 0.2, \n             arrow = arrow(length = unit(0.03, \"npc\"))) +\n  \n  geom_point(data = cidades_df, aes(x = lon, y = lat, color = tipo), size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  \n  geom_text(data = cidades_df, aes(x = lon, y = lat, label = cidade), \n            vjust = -1, fontface = \"bold\", size = 3, nudge_x=8, nudge_y=-2) + #usei nudge pra mover legenda\n  labs(title = \"Vizinhança Econômica (Fluxo de Commodities)\",\n       subtitle = \"A conexão funcional supera a proximidade geográfica\") +\n  theme_void() +\n  theme(legend.position = \"none\", plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigura 4.5: Vizinhança Econômica/Funcional: O Agronegócio conectando Sorriso-MT aos Portos.\n\n\n\n\n\n\n\n4.1.2 Matriz de pesos espaciais (\\(\\mathbf{W}\\)) e normalização\nA matriz binária de adjacência \\(\\mathbf{W} = [w_{ij}]_{n \\times n}\\) (com elementos 0 ou 1) é frequentemente transformada em uma matriz de pesos para refletir a intensidade relativa das conexões. A escolha dos pesos é exógena ao modelo (ou seja, deve ser definida a priori com base em teoria ou no desenho do estudo) e tem implicações na estimação e interpretação (H. Kelejian e Piras 2017).\nA necessidade de normalização surge por razões estatísticas e de interpretação. Em modelos autorregressivos espaciais (por serem vistos mais adiante), o parâmetro de dependência \\(\\rho\\) deve geralmente estar em um intervalo que garanta a invertibilidade da matriz \\(( \\mathbf{I} - \\rho \\mathbf{W} )\\). Se \\(\\mathbf{W}\\) não for normalizada, os autovalores podem ser muito grandes ou desiguais, restringindo o espaço paramétrico válido para \\(\\rho\\) a um intervalo desconhecido e difícil de interpretar. A normalização estabiliza o comportamento numérico do modelo.\n\nNormalização por linha (row-standardization)\n\nÉ a abordagem mais comum. Cada peso é dividido pela soma da linha correspondente:\n\\[ w_{ij}^{r} = \\frac{w_{ij}}{\\sum_{j=1}^n w_{ij}}.\\]\nO resultado é que cada linha de \\(\\mathbf{W}^{r}\\) soma 1. A operação \\(\\mathbf{W}^{r}\\mathbf{y}\\) gera uma defasagem espacial (spatial lag) que é interpretado como a média ponderada dos valores dos vizinhos de cada unidade. Esta normalização equaliza a capacidade de receber influência de cada unidade, independentemente do seu número de vizinhos. Garante também que o maior autovalor de \\(\\mathbf{W}^{r}\\) seja 1, facilitando a definição do intervalo \\((-1, 1)\\) para \\(\\rho\\) em modelos SAR.\nExemplo: Considere uma matriz de vizinhança/adjacência binária \\(\\mathbf{W}\\) para quatro unidades/estados (A, B, C, D), onde estado A é vizinho de B e C; B é vizinha apenas de A; C é vizinho apenas de A e, D é isolado.\nA normalização por linha transforma a matriz da seguinte forma:\n\\[\n\\mathbf{W} =\n\\begin{array}{c|cccc}\n& A & B & C & D \\\\\n\\hline\nA & 0 & 1 & 1 & 0 \\\\\nB & 1 & 0 & 0 & 0 \\\\\nC & 1 & 0 & 0 & 0 \\\\\nD & 0 & 0 & 0 & 0 \\\\\n\\end{array}\n\\quad \\rightarrow \\quad\n\\mathbf{W}^{r} =\n\\begin{array}{c|cccc}\n& A & B & C & D \\\\\n\\hline\nA & 0 & 0.5 & 0.5 & 0 \\\\\nB & 1 & 0 & 0 & 0 \\\\\nC & 1 & 0 & 0 & 0 \\\\\nD & 0 & 0 & 0 & 0 \\\\\n\\end{array}\n\\]\nCada unidade recebe uma influência total igual a 1 de seus vizinhos. A unidade A (com dois vizinhos) recebe 50% de sua influência de B e 50% de C. B e C (cada um com um único vizinho) recebem 100% de sua influência de A. D não recebe influência. A defasagem espacial para a unidade A, \\((\\mathbf{W}^{r}\\mathbf{y})_A\\), é \\(0.5 \\cdot y_B + 0.5 \\cdot y_C\\), a média simples dos valores de seus vizinhos.\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(sf, spdep, geobr, dplyr)\n\n# Carregar mapa do estado de Sergipe \nse_sf &lt;- read_municipality(code_muni = \"SE\", year = 2020, showProgress = FALSE)\n\n# Criar vizinhança (Queen)\nnb &lt;- poly2nb(se_sf, queen = TRUE)\n\n# Criar Matriz Binária (0 e 1)\n# Necessária para os cálculos manuais de Coluna, Espectral e CAR\nW_binaria &lt;- nb2mat(nb, style = \"B\", zero.policy = TRUE)\n\npaste(\"Dimensão da Matriz W:\", nrow(W_binaria), \"x\", ncol(W_binaria))\n\n\n[1] \"Dimensão da Matriz W: 75 x 75\"\n\n\n\n\nCódigo\n# Normalização por linha, style = \"W\"\nlw_row &lt;- nb2listw(nb, style = \"W\", zero.policy = TRUE)\n\n# Extrair a matriz de pesos para verificação\nW_row &lt;- listw2mat(lw_row)\n\n# A soma dos pesos de cada linha deve ser 1 (para quem tem vizinhos)\nsoma_linhas &lt;- rowSums(W_row)\nprint(head(soma_linhas)) # Deve mostrar 1, 1, 1... (por baixo), \n\n\n1 2 3 4 5 6 \n1 1 1 1 1 1 \n\n\nCódigo\n                        #os de cima sao indices que identificam Municipios\n\n\n\nNormalização por Coluna (Column-Standardization)\n\nCada peso é dividido pela soma da coluna correspondente:\n\\[ w_{ij}^{c} = \\frac{w_{ij}}{\\sum_{i=1}^n w_{ij}}.\\]\nEsta abordagem equaliza a capacidade de emitir influência de cada unidade. Enquanto a normalização por linha controla o impacto recebido, a normalização por coluna controla o impacto causado.\nExemplo: Usando a mesma matriz \\(\\mathbf{W}\\) definida anteriormente, a normalização por coluna resulta em:\n\\[\n\\mathbf{W} =\n\\begin{array}{c|cccc}\n& A & B & C & D \\\\\n\\hline\nA & 0 & 1 & 1 & 0 \\\\\nB & 1 & 0 & 0 & 0 \\\\\nC & 1 & 0 & 0 & 0 \\\\\nD & 0 & 0 & 0 & 0 \\\\\n\\end{array}\n\\quad \\rightarrow \\quad\n\\mathbf{W}^{c} =\n\\begin{array}{c|cccc}\n& A & B & C & D \\\\\n\\hline\nA & 0 & 1 & 1 & 0 \\\\\nB & 0.5 & 0 & 0 & 0 \\\\\nC & 0.5 & 0 & 0 & 0 \\\\\nD & 0 & 0 & 0 & 0 \\\\\n\\end{array}\n\\]\nA influência total que cada unidade emite é normalizada para 1. A unidade A é alvo da influência de B e C; portanto, a coluna A (influência emitida para A) soma 2 (vinda de B e C). Cada conexão para A recebe peso \\(1/2\\). A unidade B emite influência apenas para A (coluna B soma 1), logo, a conexão de A para B recebe peso 1. Assim, a defasagem espacial agora é um vetor onde o valor para cada unidade é a soma dos valores das unidades que ela influencia, ponderada pela intensidade. Para a unidade A, \\((\\mathbf{W}^{c}\\mathbf{y})_A = 1 \\cdot y_B + 1 \\cdot y_C\\). Esta abordagem é menos comum, mas pode ser relevante em modelos de difusão ou análise de redes, onde o out-degree (influência emitida) é um objeto de interesse central.\n\n\nCódigo\n# Calcular a soma de cada coluna da matriz binária\ncol_somas &lt;- colSums(W_binaria)\n# Proteção contra divisão por zero (caso haja ilhas)\ncol_somas[col_somas == 0] &lt;- 1 \n\n# Dividir cada elemento pela soma da sua coluna\n# A função sweep aplica a operação na MARGIN=2 (colunas)\nW_col &lt;- sweep(W_binaria, MARGIN = 2, STATS = col_somas, FUN = \"/\")\n\n# A soma da primeira coluna deve ser 1\npaste(\"Soma da Coluna 1:\", sum(W_col[,1]))\n\n\n[1] \"Soma da Coluna 1: 1\"\n\n\n\nNormalização espectral (ou por autovalor máximo): Para preservar as proporções relativas originais entre os pesos (especialmente importante em matrizes baseadas em distância), normaliza-se toda a matriz por seu autovalor de maior módulo, \\(\\lambda_{max}\\):\n\n\\[ \\mathbf{W}^{spectral} = \\frac{\\mathbf{W}^0}{\\lambda_{max}}.\\]\nEsta abordagem mantém a simetria da matriz (se originalmente simétrica) e preserva o significado físico original dos pesos (ex., um decaimento por distância). É recomendada por autores como H. H. Kelejian e Prucha (2010) e Elhorst et al. (2014) para evitar distorções na estrutura de dependência.\n\n\nCódigo\n# Calcular autovalores da matriz binária\nautovalores &lt;- eigen(W_binaria, only.values = TRUE)$values\n\n# Encontrar o maior autovalor absoluto (Raio Espectral)\nlambda_max &lt;- max(abs(autovalores))\n\n# Normalizar a matriz\nW_spec &lt;- W_binaria / lambda_max\n\n# O maior autovalor da nova matriz deve ser 1\nprint(paste(\"Novo Lambda Max:\", max(abs(eigen(W_spec, only.values=TRUE)$values))))\n\n\n[1] \"Novo Lambda Max: 1\"\n\n\n\nNormalização de variância escalar (para modelos CAR): Em modelos autorregressivos condicionais (CAR) bayesianos, busca-se frequentemente uma matriz simétrica para definir uma matriz de precisão válida. Uma normalização comum é:\n\n\\[\\mathbf{W}^{CAR} = \\mathbf{D}^{-1/2} \\mathbf{W} \\mathbf{D}^{-1/2},\\]\nonde \\(\\mathbf{D}\\) é uma matriz diagonal com \\(d_{ii} = \\sum_j w_{ij}\\). Esta forma estabiliza a variância e preserva a simetria.\n\n\nCódigo\n# Fórmula: D^(-1/2) * W * D^(-1/2)\n\n# Obter número de vizinhos (D) de cada área\nnum_vizinhos &lt;- rowSums(W_binaria)\n\n# Calcular a matriz diagonal inversa da raiz quadrada (D^-1/2)\n# Se vizinhos = 0, mantemos 0 para evitar Infinito\ninv_sqrt_D &lt;- ifelse(num_vizinhos &gt; 0, 1 / sqrt(num_vizinhos), 0)\nM_diag &lt;- diag(inv_sqrt_D)\n\n# Multiplicação Matricial (%*%)\nW_car &lt;- M_diag %*% W_binaria %*% M_diag\n\n# Visualizar o canto da matriz (Note que ela é simétrica)\nprint(round(W_car[1:5, 1:5], 3))\n\n\n      [,1]  [,2] [,3] [,4] [,5]\n[1,] 0.000 0.192    0    0    0\n[2,] 0.192 0.000    0    0    0\n[3,] 0.000 0.000    0    0    0\n[4,] 0.000 0.000    0    0    0\n[5,] 0.000 0.000    0    0    0\n\n\n\n\n4.1.3 Críticas e escolha da matriz \\(\\mathbf{W}\\)\nA escolha de \\(\\mathbf{W}\\) é frequentemente o ponto mais subjetivo e crítico da modelagem espacial. H. Kelejian e Piras (2017) e Elhorst et al. (2014) apresentam críticas à aplicação da normalização por linha:\n\nPerda da interpretação de distância: Se \\(\\mathbf{W}\\) é baseada no inverso da distância (\\(w_{ij} = d_{ij}^{-\\alpha}\\)), a normalização por linha destrói a estrutura de decaimento absoluto. Uma unidade central com muitos vizinhos próximos (\\(\\sum_j w_{ij}\\) grande) terá seus pesos reduzidos drasticamente, enquanto uma unidade periférica com poucos vizinhos distantes (\\(\\sum_j w_{ij}\\) pequeno) terá seus pesos inflacionados.\nIndução de assimetria: Uma matriz de contiguidade ou distância é frequentemente simétrica (\\(w_{ij} = w_{ji}\\)). A normalização por linha gera uma matriz assimétrica (\\(w_{ij}^{r} \\neq w_{ji}^{r}\\)), o que pode ser contra-intuitivo para noções de vizinhança e complica a interpretação em alguns modelos.\nA Falácia da seleção por \\(R^2\\): Uma prática comum é escolher a matriz \\(\\mathbf{W}\\) (ou seu critério de construção) que maximiza uma medida de ajuste como o \\(R^2\\) ou a verossimilhança do modelo. H. Kelejian e Piras (2017) demonstram analiticamente que este procedimento é enviesado. Eles provam que o \\(R^2\\) é maximizado quando os pesos se aproximam de uma matriz de pesos uniformes. Nesse cenário, o parâmetro espacial \\(\\hat{\\rho}\\) absorve toda a variação, e os coeficientes das covariáveis \\(\\hat{\\boldsymbol{\\beta}}\\) colapsam para zero, produzindo um modelo sem poder explicativo real.\n\nAssim, a seleção de \\(\\mathbf{W}\\) deve ser guiada pela teoria substantiva do fenômeno em estudo. Quando várias especificações são plausíveis, pode-se usar:\n\nCritérios de seleção de modelo: Como proposto por Zhang e Yu (2018), que adaptam um critério do tipo \\(C_p\\) de Mallows para selecionar a matriz dentro de um conjunto candidato, visando minimizar o erro de previsão. Em sua forma clássica, o \\(C_p\\) de Mallows fornece uma estimativa do erro quadrático médio de previsão para um modelo de regressão com \\(p\\) parâmetros (Colin L. Mallows 1973; Cohn L. Mallows 1995). Sua expressão é dada por:\n\n\\[\nC_p = \\frac{\\text{SSE}_p}{\\hat{\\sigma}^2} - n + 2p,\n\\]\nonde = \\(\\text{SSE}_p\\) é a soma dos quadrados dos resíduos do modelo candidato; \\(\\hat{\\sigma}^2\\) é uma estimativa não viciada da variância do erro do modelo mais completo (ou do modelo considerado verdadeiro); \\(n\\) é o número de observações e, \\(p\\) é o número de parâmetros do modelo (incluindo o intercepto), que atua como penalização pela complexidade.\nUm valor menor de \\(C_p\\) indica um melhor equilíbrio entre qualidade de ajuste (SSE baixo) e parcimônia (penalidade \\(p\\) baixa), guiando a seleção do modelo.\nZhang e Yu (2018) estende este princípio para modelos de defasagem espacial (SAR). A ideia central é tratar cada matriz candidata \\(\\mathbf{W}_k\\) como um modelo distinto. Para um SAR da forma \\(\\mathbf{y} = \\rho_k \\mathbf{W}_k \\mathbf{y} + \\mathbf{X} \\boldsymbol{\\beta}_k + \\boldsymbol{\\varepsilon}_k\\), uma estatística \\(C_p\\) adaptada é derivada.\nEssa adaptação considera que a complexidade efetiva do modelo espacial não depende apenas do número de covariáveis em \\(\\mathbf{X}\\), mas também da estrutura de dependência induzida por \\(\\mathbf{W}_k\\) e do parâmetro espacial \\(\\rho_k\\). O traço da matriz de projeção (ou hat matrix) do modelo SAR, \\(\\text{tr}(\\mathbf{H}_k)\\), que generaliza o número de parâmetros \\(p\\), é tipicamente utilizado na penalização. A estatística resultante pode ser aproximada por:\n\\[\nC_p(\\mathbf{W}_k) \\approx \\frac{\\text{SSE}_k}{\\hat{\\sigma}^2} - n + 2 \\, \\text{tr}(\\mathbf{H}_k),\n\\]\nonde \\(\\text{SSE}_k\\) e \\(\\text{tr}(\\mathbf{H}_k)\\) são calculados para o modelo estimado com a matriz \\(\\mathbf{W}_k\\). A matriz \\(\\mathbf{W}_k\\) que minimiza \\(C_p(\\mathbf{W}_k)\\) no conjunto candidato é então selecionada.\nZhang e Yu (2018) demonstra que este procedimento é assintoticamente ótimo no sentido de minimizar o erro quadrático médio de previsão, mesmo que a verdadeira matriz de pesos (geradora dos dados) não esteja incluída no conjunto \\(\\{\\mathbf{W}_1, \\ldots, \\mathbf{W}_K\\}\\).\n\nMédia de modelos: Uma evolução natural deste paradigma é reconhecer a incerteza inerente à escolha de uma única matriz. Em vez de selecionar um único \\(\\mathbf{W}_k\\), a abordagem de média de modelos combina as previsões de todos os modelos candidatos, atribuindo-lhes pesos que refletem seu suporte empírico. Miao et al. (2025) estendem esta lógica para modelos espaciais multivariados (MSAR). Eles propõem estimar pesos \\(\\pi_k\\) para cada modelo (cada um com sua matriz \\(\\mathbf{W}_k\\)) de modo a minimizar o risco de predição. A previsão final é uma média ponderada:\n\n\\[\n\\hat{\\mathbf{y}}^* = \\sum_{k=1}^K \\pi_k \\, \\hat{\\mathbf{y}}_k,\n\\]\nonde \\(\\hat{\\mathbf{y}}_k\\) é a previsão do modelo com matriz \\(\\mathbf{W}_k\\). Esta estratégia geralmente produz previsões mais robustas e estáveis do que qualquer modelo individual, pois incorpora a incerteza sobre a estrutura de dependência espacial correta.",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#implicações-estatísticas-da-discretização-espacial",
    "href": "lattice_data.html#implicações-estatísticas-da-discretização-espacial",
    "title": "4  Dados de Área",
    "section": "4.2 Implicações estatísticas da discretização espacial",
    "text": "4.2 Implicações estatísticas da discretização espacial\nA agregação de um processo contínuo em unidades de área discretas introduz desafios inferenciais profundos que vão além do MAUP.\n\nO MAUP possui duas dimensões Openshaw (1984) . O efeito de escala refere-se à mudança nos resultados ao se alterar o nível de agregação (ex.: de bairros para municípios). O efeito de zoneamento refere-se à mudança nos resultados ao se redesenhar os limites das unidades no mesmo nível de agregação. Ambos podem alterar ou até inverter o sinal de correlações e parâmetros espaciais, pois a discretização atua como um filtro não linear na estrutura de covariância do processo subjacente.\nEm dados de contagem (ex.: casos de doença), a variabilidade observada é uma combinação da variação do processo espacial latente de risco (\\(Y(\\mathbf{s})\\)) e da variação inerente ao mecanismo de amostragem (ex.: distribuição de Poisson). Em áreas com populações pequenas, a flutuação amostral pode dominar, criando padrões espúrios. Cressie e Chan (1989) mostram, no estudo da Síndrome da Morte Súbita Infantil (SIDS), como a heterogeneidade do tamanho da população-base pode gerar autocorrelação espacial aparente. Modelos hierárquicos que incorporam um offset populacional ou usam distribuições como a Binomial negativa são essenciais para separar esses efeitos.\nA agregação tende a alisar a variação local de um processo contínuo, podendo atenuar hotspots reais. Além disso, como alertado por Reich, Hodges, e Zadnik (2006), a inclusão de termos de dependência espacial (como um processo CAR ou SAR) para capturar correlação nos resíduos pode introduzir colinearidade com as covariáveis fixas do modelo, inflando a variância das estimativas dos coeficientes \\(\\boldsymbol{\\beta}\\) e complicando a inferência.\nA estrutura de dependência inferida é altamente condicional à matriz \\(\\mathbf{W}\\) especificada. Duas observações fisicamente próximas, mas separadas por uma fronteira administrativa que não é considerada no critério de vizinhança, serão modeladas como independentes. Portanto, a dependência espacial estimada é, em grande parte, uma função da discretização e da definição de vizinhança adotada, e não apenas uma propriedade intrínseca do fenômeno (Hodges e Reich 2010).",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#sec-esda",
    "href": "lattice_data.html#sec-esda",
    "title": "4  Dados de Área",
    "section": "4.3 Análise exploratória em dados de área",
    "text": "4.3 Análise exploratória em dados de área\nA Análise Exploratória de Dados Espaciais (ESDA – Exploratory Spatial Data Analysis) é definida por Anselin (1995) como um conjunto de técnicas destinadas a: (i) descrever e visualizar distribuições espaciais; (ii) identificar localizações atípicas (spatial outliers); (iii) detectar padrões de associação espacial (clusters); e (iv) sugerir regimes de heterogeneidade espacial. Diferentemente da estatística descritiva clássica, a ESDA não assume independência entre as observações. O seu objetivo central é, justamente, quantificar a natureza e a intensidade da dependência espacial, que é definida pela estrutura de vizinhança \\(\\mathbf{W}=[w_{ij}]_{n \\times n}\\) (ver Seção 4.1). A ESDA é uma extensão da Análise Exploratória de Dados (EDA) para o contexto espacial, mantendo seu caráter visual e robusto, mas com a adição fundamental do mapa como ferramenta central para responder a perguntas como “onde estão esses casos no mapa?” ou “quais áreas nesta sub-região atendem a critérios específicos de atributo?” R. H. Haining, Wise, e Ma (1998) .\nEm dados de área (lattice), onde \\(y_i\\) representa um valor agregado na unidade discreta \\(i\\), a análise divide-se fundamentalmente em duas categorias: indicadores globais (que resumem o padrão de todo o mapa num único escalar) e indicadores locais (que decompõem a estrutura de dependência para cada \\(i\\)-unidade individualmente). A implementação prática da ESDA frequentemente ocorre em ambientes de Sistemas de Informação Geográfica (GIS), que integram capacidades de visualização cartográfica, gestão de dados e análise estatística interativa, como exemplificado pelo sistema SAGE descrito por R. H. Haining, Wise, e Ma (1998). Este capítulo, assim como feito nos outros capítulos, usaremos o R/Rstudio.\n\n4.3.1 Estatísticas globais de autocorrelação\nAs estatísticas globais testam a hipótese nula de aleatoriedade espacial completa (CSR - Complete Spatial Randomness). Sob \\(H_0\\), os valores \\(\\{y_i\\}\\) são distribuídos aleatoriamente pelas localizações fixas, sem respeitar a topologia definida por \\(\\mathbf{W}\\).\nÍndice I de Moran\nO Índice de Moran (\\(I\\)) é a medida de autocorrelação espacial mais amplamente utilizada, introduzida formalmente por Moran (1950) . A sua estrutura é análoga ao coeficiente de correlação de Pearson, mas ponderada pela matriz de pesos espaciais. Para um vetor de observações \\(\\mathbf{y} = (y_1, \\dots, y_n)^\\top\\) com \\(n\\) unidades:\n\\[\nI = \\frac{n}{S_0} \\cdot \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij} (y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n (y_i - \\bar{y})^2} = \\frac{n}{S_0} \\cdot \\frac{\\mathbf{z}^\\top \\mathbf{W} \\mathbf{z}}{\\mathbf{z}^\\top \\mathbf{z}},\n\\tag{4.1}\\]\nonde \\(\\mathbf{z} = (y_1 - \\bar{y}, \\dots, y_n - \\bar{y})^\\top\\) é o vetor dos desvios em relação à média \\(\\bar{y}\\), \\(w_{ij}\\) são os elementos da matriz de pesos espaciais \\(\\mathbf{W}\\) (tipicamente normalizada por linha, ver Seção 4.1.2), e \\(S_0 = \\sum_{i=1}^n \\sum_{j=1}^n w_{ij}\\) é a soma de todos os pesos (que iguala \\(n\\) no caso de normalização por linha).\nO valor esperado de \\(I\\) sob \\(H_0\\) é \\(E[I] = -1/(n-1)\\), que tende a zero quando \\(n\\) aumenta.\n\n\\(I &gt; E[I]\\): Indica autocorrelação espacial positiva (agrupamento de valores semelhantes no espaço).\n\\(I &lt; E[I]\\): Indica autocorrelação espacial negativa (dispersão perfeita ou padrão de xadrez).\n\nA inferência é geralmente realizada através de uma abordagem de permutação condicional (Monte Carlo), uma vez que a aproximação à normalidade depende de pressupostos assintóticos que podem não se verificar em matrizes de pesos irregulares, como discutido por Getis (1995).\nPropriedades estatísticas do Índice de Moran\nEmbora “não exista” um teste uniformemente mais poderoso (UMP) para autocorrelação espacial em todos os cenários, Tiefelsdorf (2000) demonstrou que o \\(I\\) de Moran é um teste Localmente Melhor Invariante (LBI). Isso significa que, na vizinhança da hipótese nula (\\(\\rho \\approx 0\\)), a função de poder do \\(I\\) de Moran possui a inclinação mais acentuada em comparação a outros testes. Isso torna o \\(I\\) de Moran a ferramenta mais sensível para detectar pequenos desvios da aleatoriedade, sendo eficaz tanto contra hipóteses alternativas de processos autorregressivos (AR) quanto de médias móveis (MA)18.\nBurridge (1980) provou que o \\(I\\) de Moran é assintoticamente equivalente a um teste de Multiplicador de Lagrange (LM) para processos gaussianos. A estatística LM, calculada a partir da função de verossimilhança restrita, é proporcional ao valor de \\(I\\), compartilhando assim as propriedades de eficiência computacional dos testes LM, que exigem estimação apenas sob a hipótese nula e são mais conservadores que os testes de Razão de Verossimilhança (LR).\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(sf, spdep, ggplot2, patchwork, dplyr, geobr)\n\n#Carregar dados: Malha de Minas Gerais (MG)\nmg_sf &lt;- read_municipality(code_muni = \"MG\", year = 2020, showProgress = FALSE)\n#simulando dados\n\ncoords &lt;- st_coordinates(st_centroid(mg_sf))\nset.seed(123)\nmg_sf$indicador &lt;- (-coords[,2]) * 10 + rnorm(nrow(mg_sf), mean = 0, sd = 15)\n\n# Definir Vizinhança e Pesos\n# Vizinhança Queen\nnb &lt;- poly2nb(mg_sf, queen = TRUE)\n\n# normalizar por linha (style W)\nlw &lt;- nb2listw(nb, style = \"W\", zero.policy = TRUE) #recomendo usar sempre zero.policy = TRUE\n\n# Cálculo do Índice de Moran\n# A) Teste Rápido para variavel de interresse \"indicador\"\nmoran_analitico &lt;- moran.test(mg_sf$indicador, listw=lw, randomisation = TRUE)\nprint(moran_analitico)\n\n\n\n    Moran I test under randomisation\n\ndata:  mg_sf$indicador  \nweights: lw    \n\nMoran I statistic standard deviate = 29.13, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.6193722870     -0.0011737089      0.0004537973 \n\n\nCódigo\n# B) Teste Monte Carlo (Robusto)\n# Simula 999 permutações aleatórias\nmoran_mc &lt;- moran.mc(mg_sf$indicador, listw=lw, nsim = 999)\n\nprint(moran_mc)\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  mg_sf$indicador \nweights: lw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.61937, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nInterpretação\nComo normalizamos a matriz \\(\\mathbf{W}\\) por linha, o intervalo do índice de Moran é limitado, tipicamente variando entre -1 e 1.\nO valor da estatística I de Moran calculado foi de aproximadamente 0,619. Este valor positivo e de magnitude elevada indica uma forte autocorrelação espacial positiva, sugerindo que municípios com indicador socioeconômico alto tendem a estar geograficamente agrupados com outros de indicador alto (neste caso, no Sul), enquanto municípios com indicador baixo formam aglomerados com seus vizinhos de indicador baixo (neste caso, no Norte).\nPara validar se este agrupamento é meramente fruto do acaso, compara-se o valor observado com o valor esperado sob a hipótese nula de aleatoriedade espacial completa (Expectation), que neste caso é -0,0012 (calculado como \\(-1/(n-1) = -1/(853-1) \\approx -0,0012\\)). A diferença substancial entre o valor observado (Statistic \\(\\approx 0,62\\)) e o esperado (-0,0012) fornece a evidência primária de que o processo gerador dos dados não é aleatório. A confirmação estatística desta observação na abordagem analítica é dada pelo desvio padrão padronizado (standard deviate ou Z-score) de 29,13. Este valor é extremamente alto muito além do corte crítico de 1,96 para 95% de confiança resultando em um valor-p virtualmente nulo (\\(&lt; 2.2e-16\\)). Isso nos permite rejeitar a hipótese nula com um nível de confiança de 95% (considerando que fixamos nível de significância em 5%) e confirmar a existência de dependência espacial significativa.\nA abordagem via simulação de Monte Carlo fortalece essa conclusão, sendo tecnicamente preferível por não depender de pressupostos de normalidade distributiva dos dados. O resultado mostra que, ao realizar 999 permutações aleatórias dos valores do indicador pelo mapa de Minas Gerais, a estatística observada nos dados originais (statistic = 0,61937) foi superior a absolutamente todas as simulações geradas, ocupando a posição máxima (observed rank) de 1000. Isso resulta em um pseudo valor-p de 0,001, indicando que a probabilidade de se obter um padrão espacial tão ou mais organizado quanto este por mero acaso é de apenas 1 em 1000. Portanto, conclui-se que a variável analisada apresenta dependência espacial positiva significativa.\nÍndice C de Geary\nProposto por Robert Charles Geary e desenvolvido por Cliff e Ord (1981), este indicador (\\(c\\)) foca na dissimilaridade quadrática entre vizinhos, em vez da covariância (produto cruzado):\n\\[\nc = \\frac{(n-1)}{2 S_0} \\cdot \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij} (y_i - y_j)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}.\n\\]\nEnquanto o \\(I\\) de Moran é uma medida de covariância global, o \\(c\\) de Geary assemelha-se ao variograma da geoestatística (Seção 3.4), medindo a variância local das diferenças.\n\n\\(0 &lt; c &lt; 1\\): Autocorrelação positiva (vizinhos são similares, diferenças ao quadrado são pequenas).\n\\(c &gt; 1\\): Autocorrelação negativa (vizinhos são dissimilares).\n\\(c \\approx 1\\): Ausência de autocorrelação espacial.\n\nAnselin (2001) nota que o \\(I\\) de Moran é mais sensível a tendências globais e clusters, enquanto o \\(c\\) de Geary é mais sensível a diferenças locais e outliers espaciais.\n\n\nCódigo\npacman::p_load(ggspatial)\n#Cálculo do Índice C de Geary\n\n# A) Teste Analítico\ngeary_analitico &lt;- geary.test(mg_sf$indicador, listw=lw, randomisation = TRUE)\n\n# B) Teste Monte Carlo\nset.seed(123)\ngeary_mc &lt;- geary.mc(mg_sf$indicador, listw=lw, nsim = 999)\n\nprint(geary_analitico)\n\n\n\n    Geary C test under randomisation\n\ndata:  mg_sf$indicador \nweights: lw   \n\nGeary C statistic standard deviate = 26.149, p-value &lt; 2.2e-16\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n      0.377124962       1.000000000       0.000567393 \n\n\nCódigo\nprint(geary_mc)\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  mg_sf$indicador \nweights: lw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.37712, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nCódigo\n# Calculamos o Geary Local (localC) para ver onde vizinhos diferem muito.\n# Valores altos no mapa indicam vizinhos muito diferentes (outliers locais).\nmg_sf$geary_local &lt;- localC(mg_sf$indicador, listw=lw)\n\n# Mapa da Variável Original\np1 &lt;- ggplot(mg_sf) +\n  geom_sf(aes(fill = indicador), color = NA) +\n  scale_fill_viridis_c(option = \"magma\", name = \"Valor\") +\n  labs(title = \"A. Variável Original\", subtitle = \"Padrão Norte-Sul\") +\n  theme_void()+ \n  annotation_scale(location = \"bl\", width_hint = 0.3, bar_cols = c(\"black\", \"white\")) +\n  annotation_north_arrow(location = \"tl\", which_north = \"true\", \n                         pad_x = unit(0.2, \"in\"), pad_y = unit(0.2, \"in\"),\n                         style = north_arrow_fancy_orienteering)\n\n# Mapa de Geary Local\np2 &lt;- ggplot(mg_sf) +\n  geom_sf(aes(fill = geary_local), color = NA) +\n  scale_fill_viridis_c(option = \"cividis\", name = \"Local C\") +\n  labs(title = \"B. Dissimilaridade Local (Geary)\", \n       subtitle = \"Áreas claras = Vizinhos muito diferentes\") +\n  theme_void()+ \n  annotation_scale(location = \"bl\", width_hint = 0.3, bar_cols = c(\"black\", \"white\")) +\n  annotation_north_arrow(location = \"tl\", which_north = \"true\", \n                         pad_x = unit(0.2, \"in\"), pad_y = unit(0.2, \"in\"),\n                         style = north_arrow_fancy_orienteering)\n\np1 + p2\n\n\n\n\n\n\n\n\nFigura 4.6: Índice C de Geary: Teste Global e Mapa de Dissimilaridade Local.\n\n\n\n\n\nInterpretação\nA análise do Índice C de Geary confirma a existência de autocorrelação espacial positiva fort. O valor estatístico observado (Geary C statistic 0,377) situa-se substancialmente abaixo do valor esperado sob a hipótese de aleatoriedade espacial (Expectation = 1,0). Como \\(0&lt;c&lt;1\\), conclui-se que a variância local (diferença entre vizinhos) é muito menor do que a variância global, ou seja, vizinhos tendem a ser parecidos.\nA significância estatística deste padrão é atestada pelo elevado desvio padrão padronizado (standard deviate = 26,15), que resulta em um valor-p baixo (p-value &lt; 2.2e-16). Isso permite rejeitar a hipótese nula com um nível de confiança de 95% e confirmar que a similaridade entre as unidades espaciais não é aleatória.\nA validação via Monte Carlo reforça o resultado obtido. A estatística observada (statistic = 0,377) foi mais extrema (menor) do que todas as 999 permutações aleatórias geradas, ocupando a primeira posição no ranking de dissimilaridade (observed rank = 1). Isso gera um pseudo valor-p significativo (p-value = 0,001), confirmando que a probabilidade de tal padrão de aglomeração surgir ao acaso é desprezível (1 em 1000).\n\n\n4.3.2 Suporte das estatísticas \\(I\\) e \\(c\\)\nÉ comum assumir incorretamente que o Índice de Moran varia estritamente no intervalo \\([-1, 1]\\), tal como o coeficiente de correlação de Pearson. No entanto, o suporte (intervalo de valores possíveis) das estatísticas de autocorrelação espacial não é fixo (Scalon 2024); ele é intrinsecamente dependente da topologia da rede definida por \\(\\mathbf{W}=[w_{ij}]_{n \\times n}\\) (matriz de pesos ou vizinhança) e pode ultrapassar esses limites dependendo da geometria dos vizinhos.\nSuporte do Índice de Moran\nJong, Sprenger, e Veen (1984) discutem este problema abordando-o através da álgebra matricial. Eles demonstram que os valores extremos (limites mínimo e máximo) de \\(I\\) correspondem aos autovalores da matriz de pesos, sujeitos à restrição de que os dados são centrados na média (\\(\\mathbf{z}^\\top \\mathbf{1} = 0\\), ver Eq. 4.1).\nPara uma dada matriz de pesos \\(\\mathbf{W}\\), os limites inferior (\\(I_{min}\\)) e superior (\\(I_{max}\\)), necessários para definir o suporte do índice de Moran são dados por:\n\\[\nI_{min} = \\frac{n}{S_0} \\lambda_{min} \\quad \\text{e} \\quad I_{max} = \\frac{n}{S_0} \\lambda_{max}\n\\]\nOnde \\(\\lambda_{min}\\) e \\(\\lambda_{max}\\) são, respectivamente, o menor e o maior autovalor da matriz de pesos projetada no subespaço ortogonal ao vetor constante.\nSe \\(\\mathbf{W}\\) for simétrica, estes são simplesmente os autovalores extremos (menor e maior autovalor) de \\(\\mathbf{W}\\) (excluindo o autovalor trivial associado à média constante). Caso \\(\\mathbf{W}\\) seja assimétrica (comum em vizinhança por \\(k\\)-vizinhos), os autovalores são calculados a partir da parte simétrica da matriz, \\(\\mathbf{S} = \\frac{1}{2}(\\mathbf{W} + \\mathbf{W}^\\top)\\).\nExtremos do Índice de Geary\nDe forma análoga, Jong, Sprenger, e Veen (1984) derivaram os limites para a estatística \\(c\\) de Geary reformulando o seu numerador como uma forma quadrática. Os valores extremos são definidos como:\n\\[\nc_{min} = \\frac{n-1}{2 S_0} \\gamma_{min} \\quad \\text{e} \\quad c_{max} = \\frac{n-1}{2 S_0} \\gamma_{max}\n\\]\nNeste caso, \\(\\gamma_{min}\\) e \\(\\gamma_{max}\\) representam os autovalores extremos (menor e maior autovalor) de uma matriz auxiliar \\(\\mathbf{B}\\), cujos elementos \\(b_{ij}\\) são construídos combinando a estrutura de conectividade com os totais marginais dos pesos6:\n\\[\nb_{ij} = (R_i + K_j)\\delta_{ij} - 2w_{ij}\n\\]\nSendo \\(R_i\\) a soma da linha \\(i\\) de \\(\\mathbf{W}\\), \\(K_j\\) a soma da coluna \\(j\\), e \\(\\delta_{ij}\\) o delta de Kronecker (1 se \\(i=j\\), 0 caso contrário).\nO cálculo destes limites exatos é crucial para a validação estatística. Se utilizarmos uma aproximação teórica (como a distribuição Normal) que atribua probabilidade a valores fora do intervalo \\([I_{min}, I_{max}]\\), estaremos cometendo um erro de especificação, atribuindo probabilidade a eventos impossíveis para aquela configuração espacial específica. Além disso, Jong, Sprenger, e Veen (1984) notam que a simetria desses limites fornece informação sobre a estrutura da rede; por exemplo, grafos do tipo estrela possuem limites muito distintos de reticulados regulares (grids), o que afeta a interpretação da magnitude da autocorrelação.\n\n\n4.3.3 Estatísticas Locais\nA dependência espacial raramente é estacionária sobre todo o domínio \\(D^L\\). Anselin (1995) introduziu o conceito de LISA (Local Indicators of Spatial Association) para decompor a estatística global em contribuições individuais \\(I_i\\), satisfazendo duas propriedades:\n\nO indicador \\(I_i\\) permite avaliar a significância do padrão espacial local em torno da unidade \\(i\\).\nA soma dos indicadores locais é proporcional à estatística global (ex: \\(\\sum_i I_i \\propto I\\)).\n\nÍndice I de Moran local\nO Moran local (\\(I_i\\)) avalia a correlação entre o valor de uma unidade e a média dos seus vizinhos (o spatial lag) Figura 4.7. É definido como:\n\\[\nI_i = \\frac{(y_i - \\bar{y})}{S^2} \\cdot \\sum_{j=1}^n w_{ij} (y_j - \\bar{y}),\n\\]\nonde \\(S^2 = \\sum_{j=1}^n (y_j - \\bar{y})^2 / n\\) é a variância amostral.\nEsta estatística é a base para o mapa de clusters LISA, que classifica cada localidade estatisticamente significativa em quatro quadrantes, baseados no sinal de \\(z_i = (y_i - \\bar{y})\\) (valor local) e do seu defasamento espacial (spatial lag) \\(\\sum_j w_{ij} (y_j - \\bar{y})\\) (valor dos vizinhos):\n\nAlto-Alto (High-High | HH): Este quadrante (superior direito) representa o regime de associação espacial positiva onde uma unidade com valor acima da média é circundada por vizinhos que também possuem valores altos. Estatisticamente, identifica-se aqui a formação de clusters (agrupamentos) de alta intensidade, conhecidos como hot spots. A sua presença sugere fortes fenômenos de contágio ou transbordamento (spillover), indicando que os fatores que elevam a variável numa localidade estão também presentes e ativos na sua vizinhança imediata.\nBaixo-Baixo (Low-Low | LL): Define-se pelo agrupamento de uma unidade com valor abaixo da média cercada por \\(j\\) vizinhos que compartilham essa característica de baixa intensidade. Este padrão, denominado cold spot, indica também autocorrelação espacial positiva, mas na direção oposta aos hot spots. Podendo refletir barreiras geográficas à difusão de um fenômeno ou áreas estruturalmente desfavorecidas (ou protegidas, dependendo se a variável é benéfica ou maléfica) em bloco.\nAlto-Baixo (High-Low | HL): Caracteriza-se por uma unidade com valor alto que está isolada em meio a uma vizinhança de valores predominantemente baixos, configurando um outlier espacial. Este padrão de autocorrelação negativa sugere que o processo gerador de dados na unidade central é distinto do seu entorno. Frequentemente indica vulnerabilidades locais específicas, erros de medição, ou um surto localizado que, por algum motivo de contenção, não transbordou para as regiões adjacentes.\nBaixo-Alto (Low-High | LH): Refere-se a uma unidade com valor abaixo da média que está cercada por vizinhos com valores altos. Como um outlier espacial inverso, esta configuração indica autocorrelação negativa. Pode sinalizar a eficácia de políticas de contenção locais (resiliência), subnotificação de dados em relação aos vizinhos, ou características intrínsecas que tornam a unidade impermeável à influência do seu entorno de alta intensidade.\n\n\n\nCódigo\npacman::p_load(tidyverse,sf,spdep,geobr,patchwork, ggtext, ggspatial)  \n\n\n# Cálculo do Moran Local\nloc_m &lt;- localmoran(mg_sf$indicador, listw=lw)\n\n# Preparar dados para plotagem\nmg_sf$z_score &lt;- as.numeric(scale(mg_sf$indicador)) \nmg_sf$lag_z   &lt;- lag.listw(lw, mg_sf$z_score)    \nmg_sf$p_value &lt;- loc_m[, 5] # P-valor do teste local\n\n# Classificação dos Quadrantes\nsig_level &lt;- 0.05\n\nmg_sf &lt;- mg_sf %&gt;% \n  mutate(quadrante = case_when(\n    p_value &gt; sig_level ~ \"NS\",\n    z_score &gt; 0 & lag_z &gt; 0 ~ \"HH\",\n    z_score &lt; 0 & lag_z &lt; 0 ~ \"LL\",\n    z_score &gt; 0 & lag_z &lt; 0 ~ \"HL\",\n    z_score &lt; 0 & lag_z &gt; 0 ~ \"LH\"\n  )) %&gt;%\n  mutate(quadrante = factor(quadrante, \n                            levels = c(\"HH\", \"LL\", \"HL\", \"LH\", \"NS\"),\n                            labels = c(\"Alto-Alto (HH)\", \"Baixo-Baixo (LL)\", \n                                       \"Alto-Baixo (HL)\", \"Baixo-Alto (LH)\", \n                                       \"Não Significativo\")))\n\n# Cores\ncores_lisa &lt;- c(\n  \"Alto-Alto (HH)\" = \"#FF0000\",    \n  \"Baixo-Baixo (LL)\" = \"#0000FF\", \n  \"Alto-Baixo (HL)\" = \"#FFA500\",  \n  \"Baixo-Alto (LH)\" = \"#87CEFA\",  \n  \"Não Significativo\" = \"#eeeeee\"\n)\n\n# Mapa\ng_map &lt;- ggplot(mg_sf) +\n  geom_sf(aes(fill = quadrante), color = \"black\", size = 0.05) +\n  scale_fill_manual(values = cores_lisa) +\n  theme_void() +\n  labs(title = \"A. LISA\",\n       subtitle = \"Identificação de regimes locais (p &lt; 0.05)\", \n       fill=\"Legenda\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\"))+\n  \n  annotation_scale(\n    location = \"bl\",           \n    width_hint = 0.3,          \n    bar_cols = c(\"black\", \"white\"), \n    text_family = \"sans\"       \n  ) +\n  \n  annotation_north_arrow(\n    location = \"tl\",           \n    which_north = \"true\",      \n    pad_x = unit(0.2, \"in\"),   \n    pad_y = unit(0.2, \"in\"),   \n    style = north_arrow_fancy_orienteering \n  )\n\n#\ng_scatter &lt;- ggplot(mg_sf, aes(x = z_score, y = lag_z)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_point(aes(color = quadrante), size = 1.5, alpha = 0.6) +\n  # Linha de regressão (Moran Global)\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", size = 0.8) +\n  # Anotações dos Quadrantes\n  annotate(\"text\", x = 2, y = 2, label = \"HH\", color = \"red\", fontface=\"bold\") +\n  annotate(\"text\", x = -2, y = -2, label = \"LL\", color = \"blue\", fontface=\"bold\") +\n  annotate(\"text\", x = 2, y = -1, label = \"HL\", color = \"orange\", fontface=\"bold\") +\n  annotate(\"text\", x = -2, y = 1, label = \"LH\", color = \"#87CEFA\", fontface=\"bold\") +\n  scale_color_manual(values = cores_lisa) +\n  labs(title = \"B. Diagrama de Dispersão\",\n       subtitle = paste(\"I de Moran Global:\", round(moran.test(mg_sf$indicador, lw)$estimate[1], 3)),\n       x = \"Valor Padronizado (y)\",\n       y = \"Defasagem Espacial (Wy)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 12, face = \"bold\"))\n\n\ng_map + g_scatter\n\n\n\n\n\n\n\n\nFigura 4.7: Clusters Espaciais LISA (Local Moran) para Minas Gerais\n\n\n\n\n\nInterpretação\nOs resultados (Figura 4.7) confirmam e localizam a intensa polarização espacial sugerida pelas estatísticas globais. O mapa de LISA (Figura 4.7 A) decompõe a dependência, evidenciando um aglomerado do tipo Alto-Alto (HH) na porção sul do estado (em vermelho), onde municípios com altos valores do indicador estão circundados por vizinhos com valores igualmente elevados. Em contrapartida, observa-se um vasto aglomerado do tipo Baixo-Baixo (LL) dominando as regiões Norte e Nordeste (em azul), caracterizando um regime espacial de valores baixos cercados por vizinhos também com baixos valores.\nO Diagrama de dispersão de Moran (Figura 4.7 B) mostra a quase totalidade das observações significativas (pontos coloridos) se concentra nos quadrantes de associação positiva (HH e LL). A escassez de pontos nos quadrantes de transição ou outliers espaciais (HL e LH) reflete a alta continuidade do fenômeno e a existência de fronteiras nítidas entre os regimes. A inclinação da reta de regressão, correspondente ao I de Moran Global de 0,619, atesta que o valor de um município é um forte preditor positivo da média de seus vizinhos, validando estatisticamente o gradiente Norte-Sul presente nos dados.\nEstatísticas Getis-Ord (\\(G_i\\) e \\(G_i^*\\))\nDesenvolvidas por Getis e Ord (1992) e Getis (2010), estas estatísticas focam especificamente na deteção de agrupamentos de valores altos ou baixos baseados em distância, sem a componente de covariância negativa do Moran.\n\n\\(G_i\\) (sem auto-inclusão): Mede a concentração de valores na vizinhança, excluindo \\(y_i\\).\n\\(G_i^*\\) (com auto-inclusão): Mede a concentração incluindo o valor da própria unidade \\(i\\):\n\\[\nG_i^* = \\frac{\\sum_{j=1}^n w_{ij} y_j}{\\sum_{j=1}^n y_j},\n\\]\n\nonde \\(\\mathbf{W}\\) é tipicamente uma matriz de pesos baseada em distância binária (1 se \\(d_{ij} &lt; d_{\\max}\\), 0 caso contrário).\nUm valor de \\(G_i^*\\) significativamente positivo (Z-score alto) indica um hot spot (agrupamento de valores altos); um valor significativamente negativo indica um cold spot. Diferentemente do Moran local, o \\(G_i^*\\) não distingue um outlier espacial de um cluster fraco, sendo uma medida pura de intensidade local.\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, sf, spdep, geobr, ggspatial)\n\n#Cálculo do Getis-Ord Gi*\n# A função retorna os Z-scores (desvios padrão)\ngi_star &lt;- localG(mg_sf$indicador, listw=lw)\n\n# Adicionar ao mapa\nmg_sf$gi_zscore &lt;- as.numeric(gi_star)\n\n# Classificação para o Mapa (Níveis de Confiança)\n# Baseado na distribuição Normal Padrão\nmg_sf$classificacao &lt;- case_when(\n  mg_sf$gi_zscore &gt;= 1.96  ~ \"Hot Spot (95%)\",\n  mg_sf$gi_zscore &lt;= -1.96 ~ \"Cold Spot (95%)\",\n  TRUE ~ \"Não Significativo\"\n)\n\ncores_gi &lt;- c(\n  \"Hot Spot (95%)\" = \"#d7191c\",\n  \"Cold Spot (95%)\" = \"#2c7bb6\",\n  \"Não Significativo\" = \"gray90\"\n)\n\nggplot(mg_sf) +\n  geom_sf(aes(fill = classificacao), color = \"white\", size = 0.05) +\n  scale_fill_manual(values = cores_gi, name = \"Intensidade (Gi*)\") +\n  theme_void() +\n  labs(title = \"Análise de Hot Spots (Getis-Ord Gi*)\",\n       subtitle = \"Identificação de aglomerados de alta e baixa intensidade\") +\n  theme(plot.title = element_text(size = 14, face = \"bold\"),\n        legend.position = \"right\") +\n  \n  annotation_scale(\n    location = \"bl\",           \n    width_hint = 0.3,          \n    bar_cols = c(\"black\", \"white\"), \n    text_family = \"sans\"       \n  ) +\n  \n  annotation_north_arrow(\n    location = \"tl\",           \n    which_north = \"true\",      \n    pad_x = unit(0.2, \"in\"),   \n    pad_y = unit(0.2, \"in\"),   \n    style = north_arrow_fancy_orienteering \n  )\n\n\n\n\n\n\n\n\nFigura 4.8: Análise de Hot Spots (Getis-Ord Gi*) para Minas Gerais.\n\n\n\n\n\nInterpretação\nO mapa (Figura 4.8) de análise de Hot Spots (Getis-Ord Gi) mostra uma clara polarização na distribuição espacial dos valores de intensidade em Minas Gerais. Observa-se a formação de um aglomerado significativo de alta intensidade (Hot Spot), representado em vermelho, concentrado na região sul do estado. Essa área indica uma aglomeração de municípios com valores elevados cercados por vizinhos também com valores elevados, sugerindo uma zona de maior atividade ou ocorrência do fenômeno analisado com 95% de confiança.\nEm contrapartida, a região norte e nordeste do estado é caracterizada por um aglomerado de baixa intensidade (Cold Spot), representado em azul. Essa configuração espacial aponta para uma área onde predominam municípios com valores baixos, rodeados por outros de características similares, indicando uma depressão ou menor intensidade do fenômeno nesta porção do território. As áreas em cinza, classificadas como “Não Significativo”, representam regiões de transição ou de aleatoriedade espacial, onde não se verifica um padrão de aglomeração significativo nem para altas nem para baixas intensidades.",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#sec-risco",
    "href": "lattice_data.html#sec-risco",
    "title": "4  Dados de Área",
    "section": "4.4 Riscos e Suavização",
    "text": "4.4 Riscos e Suavização\nA visualização de dados de área brutos, especialmente taxas (ex: incidência de doenças), sofre de instabilidade intrínseca da variância, um problema amplamente discutido por Cressie e Chan (1989) na análise de SIDS (Síndrome de Morte Súbita Infantil).\nMapas de risco e razão padronizada (SMR)\nPara dados de contagem \\(O_i\\) (observados) com uma população em risco \\(P_i\\), a taxa bruta é \\(r_i = O_i / P_i\\). Em epidemiologia, utiliza-se frequentemente a razão de morbidade/mortalidade padronizada (SMR - Standardized Mortality/Morbidity Ratio):\n\\[\n\\text{SMR}_i = \\frac{O_i}{E_i},\n\\]\nonde \\(E_i = P_i \\cdot \\bar{r}\\) é o número esperado de casos sob a hipótese de uma taxa constante global \\(\\bar{r} = \\sum_i O_i / \\sum_i P_i\\).\nEm áreas com população \\(P_i\\) pequena, a variância da taxa \\(r_i\\) é altíssima. Um único caso adicional pode duplicar a taxa, criando outliers espúrios que dominam visualmente o mapa coroplético.\nPara mitigar a instabilidade, utiliza-se a suavização Bayesiana empírica, onde a taxa estimada \\(\\theta_i\\) é uma média ponderada entre a taxa local (instável) e a média global (estável) ou a média da vizinhança local (Banerjee 2016).\n\\[\n\\hat{\\theta}_i^{EB} = \\gamma_i \\, r_i + (1 - \\gamma_i) \\, \\bar{r}.\n\\]\nO peso \\(\\gamma_i \\in [0, 1]\\) depende da população \\(P_i\\): áreas populosas têm \\(\\gamma_i \\approx 1\\) (confia-se no dado local), áreas despovoadas têm \\(\\gamma_i \\approx 0\\) (o valor é encolhido em direção à média). Besag e Green (1993) estendem isto para modelos hierárquicos completos (BYM), utilizando a estrutura de vizinhança \\(\\mathbf{W}\\) para suavizar localmente.\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, sf, spdep, geobr, patchwork, viridis, ggspatial)\n\nmg_sf &lt;- read_municipality(code_muni = \"MG\", year = 2020, showProgress = FALSE)\n\n#Vamos simulação de um dataframe de dados brutos (Como viria do DATASUS)\n# Cenário: Incidência de uma doença rara.\n# O usuário teria uma tabela com: ID do Município, População em Risco, Nº de Casos.\n\nset.seed(999)\n\n# A) Simular População (ni):\npopulacao_simulada &lt;- floor(rlnorm(nrow(mg_sf), meanlog = 9, sdlog = 1.2))\n\n# B) Definir um Risco Latente (Verdadeiro, mas desconhecido):\n\ncoords &lt;- st_coordinates(st_centroid(mg_sf))\npadrao_norte_sul &lt;- (coords[,2] - min(coords[,2])) / (max(coords[,2]) - min(coords[,2]))\n\nrisco_real &lt;- 0.0005 * (1 + (padrao_norte_sul * 2)) # Risco varia de 0.05% a 0.15%\n\n# Simular Nº de Casos (yi):\n# Os casos são uma realização de um processo de Poisson: Casos ~ Poisson(Pop * Risco)\ncasos_simulados &lt;- rpois(nrow(mg_sf), lambda = populacao_simulada * risco_real)\n\n# D) Unir ao mapa \nmg_dados &lt;- mg_sf %&gt;%\n  mutate(\n    populacao = populacao_simulada,  \n    casos = casos_simulados          #Óbitos ou Doentes\n  )\n\n# Cálculo das Taxas \n\n# Passo 1: Calcular Taxa Bruta (Incidência por 10.000 habitantes)\n\nmg_dados &lt;- mg_dados %&gt;%\n  mutate(taxa_bruta = (casos / populacao) * 10000)\n\n# Passo 2: Suavização Bayesiana Empírica Local \n\nnb &lt;- poly2nb(mg_dados, queen = TRUE)\n\n# A função EBlocal precisa dos CASOS (ri) e da POPULAÇÃO (ni)\n\neb_resultado &lt;- EBlocal(ri = mg_dados$casos, \n                        ni = mg_dados$populacao, \n                        nb = nb, \n                        zero.policy = TRUE)\n\n# Adicionamos a taxa suavizada ao mapa (multiplicando por 10k para ficar na mesma escala)\nmg_dados$taxa_suavizada &lt;- eb_resultado$est * 10000\n\n\n# Definir limites iguais para garantir que as cores representem os mesmos valores\nescala_limites &lt;- range(c(mg_dados$taxa_bruta, mg_dados$taxa_suavizada), na.rm = TRUE)\n\n\np1 &lt;- ggplot(mg_dados) +\n  geom_sf(aes(fill = taxa_bruta), color = NA) +\n  scale_fill_viridis_c(option = \"turbo\", limits = escala_limites, name = \"Taxa/10k\") +\n  theme_void() +\n  labs(title = \"A. Taxa Bruta (Dados Observados)\",\n       subtitle = \"Ruído excessivo em municípios pequenos\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\"))+\n  \n  annotation_scale(\n    location = \"bl\",           \n    width_hint = 0.3,          \n    bar_cols = c(\"black\", \"white\"), \n    text_family = \"sans\"       \n  ) +\n  \n  annotation_north_arrow(\n    location = \"tl\",           \n    which_north = \"true\",      \n    pad_x = unit(0.2, \"in\"),   \n    pad_y = unit(0.2, \"in\"),   \n    style = north_arrow_fancy_orienteering \n  )\n\np2 &lt;- ggplot(mg_dados) +\n  geom_sf(aes(fill = taxa_suavizada), color = NA) +\n  scale_fill_viridis_c(option = \"turbo\", limits = escala_limites, name = \"Taxa/10k\") +\n  theme_void() +\n  labs(title = \"B. Taxa Suavizada (Empirical Bayes)\",\n       subtitle = \"Padrão espacial real recuperado\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\")) +\n  \n  annotation_scale(\n    location = \"bl\",           \n    width_hint = 0.3,          \n    bar_cols = c(\"black\", \"white\"), \n    text_family = \"sans\"       \n  ) +\n  \n  annotation_north_arrow(\n    location = \"tl\",           \n    which_north = \"true\",      \n    pad_x = unit(0.2, \"in\"),   \n    pad_y = unit(0.2, \"in\"),   \n    style = north_arrow_fancy_orienteering \n  )\n\np1 + p2\n\n\n\n\n\n\n\n\nFigura 4.9: Comparação: (A) Taxa Bruta Instável vs. (B) Taxa Suavizada (Empirical Bayes Local).\n\n\n\n\n\nInterpretação\nA análise comparativa entre as taxas brutas (Figura 4.9 A) e as taxas suavizadas por meio do método Bayesiano Empírico Local (Figura 4.9 B) evidencia a eficácia deste último na correção de distorções estatísticas. O mapa das taxas brutas (Figura 4.9 A) apresenta um padrão ruidoso e fragmentado, com valores extremos (muito altos ou muito baixos) dispersos aleatoriamente, refletindo a instabilidade das estimativas em municípios com pequenas populações.\nEm contraste, o mapa das taxas suavizadas (Figura 4.9 B) mostra com clareza a estrutura espacial subjacente do fenômeno, destacando um gradiente norte-sul consistente. Ao incorporar informações da vizinhança, o método Bayesiano Empírico estabiliza as estimativas, permitindo a identificação de padrões geográficos reais que estavam obscurecidos pelo ruído nos dados brutos.",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#diagnóstico-de-dependência-espacial",
    "href": "lattice_data.html#diagnóstico-de-dependência-espacial",
    "title": "4  Dados de Área",
    "section": "4.5 Diagnóstico de dependência espacial",
    "text": "4.5 Diagnóstico de dependência espacial\nA ESDA não se aplica apenas aos dados brutos (\\(\\mathbf{y}\\)), mas é crucial no diagnóstico de modelos de regressão. Como alertado por Cressie e Chan (1989), a detecção de autocorrelação espacial em \\(\\mathbf{y}\\) não implica necessariamente um processo espacial intrínseco (como contágio); pode ser resultado de heterogeneidade não observada ou variáveis omitidas que possuem padrão espacial.\nO procedimento padrão, detalhado por Anselin (2001), envolve:\n\nAjustar um modelo de regressão (mínimos quadrados ordinários-OLS): \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, : \\mathbf{y} \\| \\mathbf{X} \\sim N(\\mathbf{\\mu}, \\Sigma)\\).\nCalcular os resíduos \\(\\hat{\\boldsymbol{\\varepsilon}} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\).\nAplicar o teste \\(I\\) de Moran sobre os resíduos \\(\\hat{\\boldsymbol{\\varepsilon}}\\) utilizando uma matriz \\(\\mathbf{W}\\) escolhida.\n\nSe \\(I\\) for significativo, o pressuposto de independência dos erros é violado. No entanto, Zhang e Yu (2018) demonstram que a validade deste diagnóstico depende criticamente da escolha correta de \\(\\mathbf{W}\\). O uso de uma matriz incorreta pode falhar em detetar a dependência residual ou indicar falsamente a necessidade de um modelo espacial.\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(sf, spdep, ggplot2, patchwork, viridis)\n\n\nif (!exists(\"mg_dados\")) {\n  mg_dados &lt;- read_municipality(code_muni = \"MG\", year = 2020, showProgress = FALSE)\n  coords &lt;- st_coordinates(st_centroid(mg_dados))\n  mg_dados$taxa_bruta &lt;- (-coords[,2] * 10) + rnorm(nrow(mg_dados), 0, 5)\n}\n\n# Criar uma variável explicativa X aleatória (sem padrão espacial)\nset.seed(123)\nmg_dados$variavel_x &lt;- rnorm(nrow(mg_dados))\n\n#Ajuste do Modelo de Regressão Linear (OLS)\nmodelo_ols &lt;- lm(taxa_bruta ~ variavel_x, data = mg_dados)\n\n# Extrair os resíduos do modelo\nmg_dados$residuos &lt;- residuals(modelo_ols)\n\n#Teste I de Moran nos Resíduos\n# Necessário recriar a lista de pesos (W)\nnb &lt;- poly2nb(mg_dados, queen = TRUE)\nlw &lt;- nb2listw(nb, style = \"W\", zero.policy = TRUE)\n\n# Função para resíduos de regressão (lm.morantest)\nmoran_residuos &lt;- lm.morantest(modelo_ols, lw, alternative = \"two.sided\")\n\nprint(moran_residuos)\n\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = taxa_bruta ~ variavel_x, data = mg_dados)\nweights: lw\n\nMoran I statistic standard deviate = 9.3389, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\nsample estimates:\nObserved Moran I      Expectation         Variance \n    0.1977385849    -0.0011812437     0.0004536919 \n\n\nCódigo\np1 &lt;- ggplot(mg_dados) +\n  geom_sf(aes(fill = residuos), color = NA) +\n  scale_fill_distiller(palette = \"RdBu\", name = \"Resíduos\") +\n  labs(title = \"A. Mapa dos Resíduos OLS\", \n       subtitle = \"Padrão visível (não aleatório)\") +\n  \n  annotation_scale(\n    location = \"bl\",           \n    width_hint = 0.3,          \n    bar_cols = c(\"black\", \"white\"), \n    text_family = \"sans\"       \n  ) +\n  \n  annotation_north_arrow(\n    location = \"tl\",           \n    which_north = \"true\",      \n    pad_x = unit(0.2, \"in\"),   \n    pad_y = unit(0.2, \"in\"),   \n    style = north_arrow_fancy_orienteering \n  )+\n  theme_void()\n\n#\nmg_dados$residuos_z &lt;- scale(mg_dados$residuos)\nmg_dados$lag_residuos &lt;- lag.listw(lw, mg_dados$residuos_z)\n\np2 &lt;- ggplot(mg_dados, aes(x = residuos_z, y = lag_residuos)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_hline(yintercept = 0, linetype=\"dashed\") +\n  geom_vline(xintercept = 0, linetype=\"dashed\") +\n  labs(title = \"B. Moran dos Resíduos\",\n       subtitle = paste(\"I de Moran =\", round(moran_residuos$statistic, 3)),\n       x = \"Resíduos (Z)\", y = \"Lag Espacial dos Resíduos\") +\n  theme_bw()\n\np1 + p2\n\n\n\n\n\n\n\n\nFigura 4.10: Diagnóstico de Resíduos: (A) Mapa dos Resíduos e (B) Diagrama de Moran dos Resíduos.\n\n\n\n\n\nInterpretação\nA análise dos resíduos do modelo de regressão linear clássico aponta para a violação do pressuposto de independência dos resíduos. A estatística I de Moran observada nos resíduos foi de aproximadamente 0,198, um valor substancialmente superior à esperança matemática sob aleatoriedade (-0,001). A magnitude dessa dependência é confirmada pelo desvio padrão padronizado de 9,34, resultando em um valor-p baixo (&lt; 2.2e-16), o que nos leva a rejeitar a hipótese nula de ausência de autocorrelação espacial com alto grau de confiança.\nVisualmente, essa dependência é observada na Figura 4.10. Figura 4.10 A), a distribuição espacial dos resíduos não se assemelha a um ruído branco aleatório; ao contrário, observam-se nítidos aglomerados de resíduos positivos (em vermelho) e negativos (em azul), indicando que o modelo subestima ou superestima sistematicamente os valores em regiões vizinhas. Figura 4.10 B) reforça esse diagnóstico através da inclinação positiva da reta de regressão entre os resíduos e sua defasagem espacial. Essa estrutura remanescente nos erros sugere que a variável explicativa aleatória introduzida foi incapaz de capturar o padrão espacial da variável resposta, transferindo essa estrutura não modelada para o termo de erro.\nComo próximo passo, é o pesquisador abandonaria o estimador de Mínimos Quadrados Ordinários, cujos testes de significância tornaram-se inválidos, e adotar modelos de regressão espacial que incorporem explicitamente a estrutura de dependência na matriz de covariância ou na média.\nLimitações da ESDA\n\nOs valores das estatísticas de Moran ou Geary podem mudar drasticamente com a alteração da escala de agregação ou do desenho das zonas. Um cluster identificado a nível municipal pode desaparecer a nível estadual (MAUP).\nO cálculo de estatísticas locais (LISA) envolve a realização de \\(n\\) testes de hipótese simultâneos. Sem correção (como Bonferroni ou False Discovery Rate), a probabilidade de encontrar clusters falsos positivos (erro Tipo I) aumenta com o tamanho da amostra.\nA descoberta de estrutura espacial nos resíduos via ESDA pode levar à inclusão de termos espaciais (CAR/SAR) que competem com as covariáveis fixas pela explicação da variância (colinearidade espacial), enviesando a inferência sobre os efeitos causais.\nUnidades na periferia do mapa têm menos vizinhos (efeito da borda), o que distorce o cálculo dos momentos das estatísticas locais e globais, exigindo correções ou simulações de Monte Carlo para inferência válida (Besag 1975).",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#sec-gmrf",
    "href": "lattice_data.html#sec-gmrf",
    "title": "4  Dados de Área",
    "section": "4.6 Fundamentos probabilísticos: GMRF, matrizes de precisão e operadores Laplacianos discretos",
    "text": "4.6 Fundamentos probabilísticos: GMRF, matrizes de precisão e operadores Laplacianos discretos\nEsta seção estabelece a ponte formal entre a teoria dos processos estocásticos contínuos e a implementação computacional eficiente em dados de área, fundamentando-se na estrutura probabilística dos Campos Aleatórios de Markov.\nCampos Aleatórios Gaussianos: GRF vs. GMRF\nUm campo aleatório Gaussiano (GRF - Gaussian Random Field) é um processo estocástico \\(\\{Y(\\mathbf{s}): \\mathbf{s} \\in D\\}\\) tal que, para qualquer conjunto finito de localizações \\(\\{\\mathbf{s}_1, \\dots, \\mathbf{s}_n\\}\\), o vetor \\((Y(\\mathbf{s}_1), \\dots, Y(\\mathbf{s}_n))^\\top\\) segue uma distribuição normal multivariada (Cressie e Moores 2022). A distinção fundamental na modelagem espacial reside em como essa estrutura de dependência é parametrizada:\n\nGRF Padrão (Geoestatística): Especificado por sua função de média \\(\\mu(\\mathbf{s})\\) e sua função de covariância \\(C(\\mathbf{s}_i, \\mathbf{s}_j) = \\text{Cov}(Y(\\mathbf{s}_i), Y(\\mathbf{s}_j))\\). Isto gera uma matriz de covariância \\(\\mathbf{\\Sigma}\\) densa, onde cada par de locais possui uma correlação teoricamente não nula, definida por uma função de decaimento (ex.: exponencial, Matérn). O custo computacional para inferência (como cálculo da verossimilhança) é \\(\\mathcal{O}(n^3)\\) devido à necessidade de fatorar uma matriz \\(n \\times n\\) densa.\nCampo aleatório de Markov Gaussiano (GMRF): É um GRF definido sobre um domínio discreto \\(D^L\\) (um grafo ou lattice), especificado diretamente pela sua matriz de precisão \\(\\mathbf{Q} = \\mathbf{\\Sigma}^{-1}\\). A propriedade que define um GMRF é a independência condicional espacial (Håvard Rue e Held 2005; Håvard Rue, Martino, e Chopin 2009):\n\n\\[\nY_i \\perp Y_j \\mid \\mathbf{Y}_{-ij} \\iff Q_{ij} = 0 \\quad \\text{para} \\quad i \\neq j.\n\\]\nOu seja, dadas as observações de todos os outros locais, \\(Y_i\\) e \\(Y_j\\) são independentes se e somente se não forem vizinhos no grafo (ou se a estrutura de dependência direta for modelada como nula). Esta é a formalização probabilística da propriedade de Markov: o valor em um local depende apenas dos valores em seus vizinhos diretos.\nPropriedade de Markov em grafos e esparsidade da matriz de precisão\nA propriedade de Markov espacial formaliza a noção intuitiva de vizinhança. Seja \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) o grafo que representa o domínio espacial discreto, onde \\(\\mathcal{V}=\\{1, \\dots, n\\}\\) são os vértices (unidades de área) e \\(\\mathcal{E}\\) são as arestas que definem vizinhanças. Um vetor aleatório \\(\\mathbf{Y} = (Y_1, \\dots, Y_n)^\\top\\) é um GMRF em relação a \\(\\mathcal{G}\\) se:\n\\[\np(Y_i \\mid \\mathbf{Y}_{-i}) = p(Y_i \\mid \\mathbf{Y}_{\\partial i}),\n\\]\nonde \\(\\partial i = \\{j: (i, j) \\in \\mathcal{E}\\}\\) é o conjunto de vizinhos de \\(i\\). Esta propriedade local é equivalente à esparsidade da matriz de precisão \\(\\mathbf{Q}\\): \\(Q_{ij} = 0\\) para todo par \\((i, j)\\) tal que \\(j \\notin \\partial i\\) e \\(j \\neq i\\).\nExemplo: Considere 5 regiões onde cada região é vizinha apenas da anterior e da seguinte (assumindo que as regiões estão em fila única). A matriz de precisão teria a forma:\n\\[\n\\mathbf{Q} =\n\\begin{bmatrix}\n* & * & 0 & 0 & 0 \\\\\n* & * & * & 0 & 0 \\\\\n0 & * & * & * & 0 \\\\\n0 & 0 & * & * & * \\\\\n0 & 0 & 0 & * & *\n\\end{bmatrix},\n\\]\nonde * denota um elemento não nulo. Para dados de área bidimensionais com vizinhança por contiguidade, cada região tem tipicamente entre 4 e 8 vizinhos, resultando em uma matriz \\(\\mathbf{Q}\\) com \\(\\mathcal{O}(n)\\) elementos não nulos, em contraste com os \\(\\mathcal{O}(n^2)\\) de uma matriz densa. Esta esparsidade permite o uso de algoritmos numéricos de álgebra linear esparsa (como a fatoração de Cholesky esparsa), reduzindo o custo computacional da inferência de \\(\\mathcal{O}(n^3)\\) para aproximadamente \\(\\mathcal{O}(n^{3/2})\\) em domínios 2D, viabilizando métodos como a inferência Bayesiana aproximada via INLA (Håvard Rue, Martino, e Chopin 2009).\nO Laplaciano do grafo\nA estrutura da matriz de precisão \\(\\mathbf{Q}\\) em modelos espaciais está intrinsecamente ligada ao conceito de Laplaciano discreto. O operador Laplaciano contínuo \\(\\nabla^2\\) mede a divergência do gradiente, ou a curvatura local de uma função. Num grafo, seu análogo mede a diferença entre o valor num nó e a média dos valores dos seus vizinhos, atuando como um quantificador de suavidade ou rugosidade local do campo.\nA matriz Laplaciana \\(\\mathbf{L}\\) de um grafo não direcionado é definida como \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{W},\\) onde \\(\\mathbf{W}=[w_{ij}]_{n \\times n}\\) é a matriz de adjacência/vizinhança (com \\(w_{ij}=1\\) se \\(i\\) e \\(j\\) são vizinhos) e \\(\\mathbf{D}\\) é a matriz diagonal dos graus (com \\(D_{ii} = \\sum_j w_{ij}\\), o número de vizinhos de \\(i\\)). Para um vetor \\(\\mathbf{y} = (y_1, \\dots, y_n)^\\top\\), a forma quadrática associada ao Laplaciano é:\n\\[\\mathbf{y}^\\top \\mathbf{L} \\mathbf{y} = \\sum_{(i,j) \\in \\mathcal{E}} (y_i - y_j)^2.\\]\nEsta equação demonstra que \\(\\mathbf{y}^\\top \\mathbf{L} \\mathbf{y}\\) é uma soma de diferenças quadráticas entre todos os pares de vizinhos. Um valor baixo indica que \\(\\mathbf{y}\\) é suave sobre o grafo (vizinhos têm valores similares), enquanto um valor alto indica um campo rugoso ou heterogêneo.\nPropriedades espectrais e singularidade\nAs propriedades espectrais (autovalores e autovetores) de \\(\\mathbf{L}\\) revelam características fundamentais da conectividade do sistema.\n\n\\(\\mathbf{L}\\) é sempre semidefinida positiva, ou seja, \\(\\mathbf{y}^\\top \\mathbf{L} \\mathbf{y} \\geq 0\\) para todo \\(\\mathbf{y}\\).\nO menor autovalor de \\(\\mathbf{L}\\) é sempre \\(\\lambda_1 = 0\\), e seu autovetor correspondente é o vetor constante \\(\\mathbf{1} = (1, \\dots, 1)^\\top\\). Isto decorre diretamente do fato de que \\(\\mathbf{L}\\mathbf{1} = \\mathbf{0}\\) (a soma de cada linha é zero).\nA existência do autovalor zero implica que \\(\\mathbf{L}\\) é uma matriz singular (não invertível). Estatisticamente, isso significa que uma distribuição com matriz de precisão proporcional a \\(\\mathbf{L}\\), como \\(\\mathbf{Y} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{L}^-)\\) (onde \\(\\mathbf{L}^-\\) denota uma inversa generalizada), é imprópria. Ela define uma densidade de probabilidade válida apenas no subespaço ortogonal ao vetor constante (ou seja, para contrastes entre os \\(y_i\\)), pois a variância na direção do nível médio global é infinita.",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#sec-modelos_classicos",
    "href": "lattice_data.html#sec-modelos_classicos",
    "title": "4  Dados de Área",
    "section": "4.7 Modelos mais comuns em dados de área: CAR, ICAR, SAR e BYM",
    "text": "4.7 Modelos mais comuns em dados de área: CAR, ICAR, SAR e BYM\nA modelagem de dados de área (ou lattice data) fundamenta-se na incorporação explícita da estrutura de vizinhança definida pela matriz de pesos espaciais \\(\\mathbf{W}=[w_{ij}]_{n \\times n}\\) (ver Seção 4.1) no mecanismo gerador de dados. A literatura distingue duas formas canônicas de especificar esta dependência: a especificação condicional (CAR), que modela a distribuição de uma área dados os seus vizinhos, e a especificação simultânea (SAR), que modela o sistema de equações de feedback instantâneo entre todas as áreas [Cressie (1993)].\n\n4.7.1 Modelo Condicional Autorregressivo (CAR)\nIntroduzido por Besag (1974), o modelo autorregressivo condicional (CAR - Conditional Autoregressive) especifica cada observação \\(Y_i\\) como uma função linear dos valores de seus vizinhos mais um termo de erro independente, mas a inferência é baseada na distribuição condicional.\nSeja \\(Y_i\\) a variável aleatória na unidade \\(i\\) e \\(\\mathbf{Y}_{-i} = \\{Y_j : j \\neq i\\}\\) o conjunto de todas as outras observações excluindo \\(i\\). O modelo CAR é definido por uma família de distribuições condicionais Gaussianas:\n\\[\nY_i = \\mu_i  + \\sum_{j \\neq i} w_{ij}(Y_j - \\mu_j) + \\epsilon_i,\n\\tag{4.2}\\]\nonde \\(\\mu_i\\) é uma tendência determinística (tendência, geralmente \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\)); \\(w_{ij}\\) são os pesos espaciais normalizados, com \\(w_{ij} \\neq 0\\) apenas se \\(j\\) é vizinho de \\(i\\) e, \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)\\) são erros independentes.\nAqui, o valor esperado em \\(i\\), condicional aos seus vizinhos, é a média global ajustada por uma média ponderada dos desvios dos seus vizinhos em relação à média global.\nA partir Eq. 4.2, deriva-se a distribuição condicional que caracteriza o CAR:\n\\[\nY_i \\mid \\mathbf{Y}_{-i} \\sim \\mathcal{N}\\left( \\mu_i + \\sum_{j \\neq i} w_{ij}(Y_j - \\mu_j),\\, \\sigma_i^2 \\right), \\: \\: \\mathbb{E} [Y_i \\mid \\mathbf{Y}_{-i}]=\\mu_i + \\sum_{j \\neq i} w_{ij}(Y_j - \\mu_j), \\:\\: \\text{Var}[Y_i \\mid \\mathbf{Y}_{-i}] =\\sigma^2_i\n\\]\nPara que estas condicionais definam uma distribuição conjunta válida \\(\\mathbf{Y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\sigma})\\), o Teorema de Hammersley-Clifford impõe condições de simetria. Besag (1974) mostrou que a matriz de precisão conjunta \\(\\Sigma^-1=\\mathbf{Q} = [Q_{ij}]_{n \\times n}\\) deve ser simétrica e positiva definida. A distribuição conjunta é dada por:\n\\[\n\\mathbf{Y} \\sim \\mathcal{N}_n(\\boldsymbol{\\mu}, \\mathbf{Q}_{CAR}), \\quad \\text{onde} \\quad \\mathbf{Q}_{CAR} = \\mathbf{M}^{-1} (\\mathbf{I} - \\rho \\mathbf{W})\\: \\text{ se a inversa existir }\n\\]\nAqui, \\(\\Sigma= \\mathbf{M}(\\mathbf{I} - \\rho \\mathbf{W})^{-1}\\), \\(\\mathbf{M}_{n \\times n} =\\text{diag}(\\sigma_1^2, \\dots, \\sigma_n^2), \\: \\boldsymbol{\\mu} = [\\mu_1, \\mu_2, \\ldots, \\mu_n]^\\top\\) e \\(\\rho\\) parâmetro espacial desconhecido. Para garantir a simetria de \\(\\mathbf{Q}_{CAR}\\), é necessário que \\(\\frac{w_{ij}}{\\sigma_i^2} = \\frac{w_{ji}}{\\sigma_j^2}\\). Note ainda que \\(w_{ii}=0, \\, Q_{ii} = 1/\\sigma_i^2 &gt;0, \\: Q_{ij}= -\\rho w_{ij}/\\sigma_i^2\\: i \\neq j\\). Lembre-se que quando \\(w_{ij} \\neq 0\\), pode-se escrever \\(i \\sim j\\) (Besag e Kooperberg 1995). Note que sem perda de generalidade, em várias literaturas tem se assumido que \\(\\mu=\\mu_i=\\mu_j=0\\), removendo o efeito da tendência global.\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialreg, spdep, modelsummary,knitr, kableExtra, texreg,ggplot2, patchwork, sf)\n\n#Preparação dos Dados\nif (!exists(\"mg_dados\")) {\n  mg_dados &lt;- geobr::read_municipality(code_muni = \"MG\", year = 2020, showProgress = FALSE)\n  coords &lt;- st_coordinates(st_centroid(mg_dados))\n  set.seed(123)\n  mg_dados$taxa_bruta &lt;- (-coords[,2] * 10) + rnorm(nrow(mg_dados), 0, 5)\n  mg_dados$variavel_x &lt;- rnorm(nrow(mg_dados))\n}\n\n# Matriz de Pesos Espaciais\nnb &lt;- poly2nb(mg_dados, queen = TRUE)\nlw &lt;- nb2listw(nb, style = \"W\", zero.policy = TRUE)\n\n#Ajuste dos Modelos\n# Regressão linear classica \nmod_ols &lt;- lm(taxa_bruta ~ variavel_x, data = mg_dados)\n\n# Modelo CAR (Incorpora dependência espacial condicional)\n# family = \"CAR\" ajusta via Máxima Verossimilhança\nmod_car &lt;- spautolm(taxa_bruta ~ variavel_x, \n                    data = mg_dados, \n                    listw = lw, \n                    family = \"CAR\")\n\n# Função auxiliar para formatar valor\nformat_coef &lt;- function(est, se, pval) {\n  stars &lt;- case_when(pval &lt; 0.001 ~ \"***\", pval &lt; 0.01 ~ \"**\", pval &lt; 0.05 ~ \"*\", TRUE ~ \"\")\n  paste0(format(round(est, 3), nsmall=3), \" (\", format(round(se, 3), nsmall=3), \")\", stars)\n}\n\n\nsum_ols &lt;- summary(mod_ols)\ncoef_ols &lt;- sum_ols$coefficients\nres_ols &lt;- c(\n  format_coef(coef_ols[1,1], coef_ols[1,2], coef_ols[1,4]), # Intercepto\n  format_coef(coef_ols[2,1], coef_ols[2,2], coef_ols[2,4]), # Variavel X\n  \"-\",                                                      # Lambda (Não existe no OLS)\n  round(AIC(mod_ols), 1)                                    # AIC\n)\n\n\nsum_car &lt;- summary(mod_car)\ncoef_car &lt;- sum_car$Coef\n# Lambda (parametro espacial) e seu SE\nlambda_val &lt;- mod_car$lambda\nlambda_se  &lt;- mod_car$lambda.se\n# Teste Z para o Lambda (aproximado)\nlambda_p   &lt;- 2 * (1 - pnorm(abs(lambda_val / lambda_se)))\n\nres_car &lt;- c(\n  format_coef(coef_car[1,1], coef_car[1,2], coef_car[1,4]), # Intercepto\n  format_coef(coef_car[2,1], coef_car[2,2], coef_car[2,4]), # Variavel X\n  format_coef(lambda_val, lambda_se, lambda_p),             # Lambda\n  round(AIC(mod_car), 1)                                    # AIC\n)\n\n#\ntabela_final &lt;- data.frame(\n  Parametro = c(\"Intercepto\", \"Variável X\", \"Lambda (Espacial)\", \"AIC\"),\n  OLS = res_ols,\n  CAR = res_car\n)\n\n#Gerar Tabela Bonita (HTML/LaTeX)\nkbl(tabela_final, \n    format = \"latex\", \n    booktabs = TRUE, \n    align = \"lcc\", \n    caption = NULL) %&gt;% \n  kable_styling(latex_options = c(\"HOLD_position\"), \n                full_width = FALSE, \n                position = \"center\") %&gt;%\n  add_header_above(c(\" \" = 1, \"Modelos\" = 2)) %&gt;%\n  footnote(general = \"* p&lt;0.05; ** p&lt;0.01; *** p&lt;0.001. Valores em parênteses são erros-padrão.\")\n\n\nCódigo\n# Diagnóstico dos Resíduos\nmg_dados$resid_ols &lt;- residuals(mod_ols)\nmg_dados$resid_car &lt;- residuals(mod_car)\n\n# Teste de Moran\nmoran_car &lt;- moran.test(mg_dados$resid_car, listw=lw)\nprint(paste(\"I de Moran (Resíduos CAR):\", round(moran_car$estimate[1], 3), \n            \"| p-valor:\", round(moran_car$p.value, 3)))\n\n\nCódigo\np1 &lt;- ggplot(mg_dados) +\n  geom_sf(aes(fill = resid_ols), color = NA) +\n  scale_fill_distiller(palette = \"RdBu\", name = \"Resíduos\") +\n  labs(title = \"A. Resíduos OLS\", subtitle = \"Dependência visível\") +\n  \n  annotation_scale(\n    location = \"bl\",           \n    width_hint = 0.3,          \n    bar_cols = c(\"black\", \"white\"), \n    text_family = \"sans\"       \n  ) +\n  \n  annotation_north_arrow(\n    location = \"tl\",           \n    which_north = \"true\",      \n    pad_x = unit(0.2, \"in\"),   \n    pad_y = unit(0.2, \"in\"),   \n    style = north_arrow_fancy_orienteering \n  )+\n  theme_void()\n\np2 &lt;- ggplot(mg_dados) +\n  geom_sf(aes(fill = resid_car), color = NA) +\n  scale_fill_distiller(palette = \"RdBu\", name = \"Resíduos\") +\n  labs(title = \"B. Resíduos CAR\", subtitle = \"Padrão removido\") +\n  \n  annotation_scale(\n    location = \"bl\",           \n    width_hint = 0.3,          \n    bar_cols = c(\"black\", \"white\"), \n    text_family = \"sans\"       \n  ) +\n  \n  annotation_north_arrow(\n    location = \"tl\",           \n    which_north = \"true\",      \n    pad_x = unit(0.2, \"in\"),   \n    pad_y = unit(0.2, \"in\"),   \n    style = north_arrow_fancy_orienteering \n  )+\n  theme_void()\n\np1 + p2\n\n\n\n\nTabela 4.1: \n\n\n[1] “I de Moran (Resíduos CAR): -0.187 | p-valor: 1”\n\n\n\nInterpretação\nA tabela de resultados demonstra que o modelo CAR proporcionou uma redução substancial no critério de informação AIC (de 5275 para 5199), indicando um ajuste melhor em relação a regressão linear (OLS). O parâmetro espacial \\(\\lambda\\) (Lambda), estimado em 0,733, foi significativo (\\(p &lt; 0,001\\)), confirmando que a vizinhança exerce influência determinante no processo, capturando a variabilidade que o modelo linear clássico falhou em explicar. Note-se ainda que a variável explicativa aleatória permaneceu não significativa em ambos os modelos, como esperado, mas o erro-padrão e o intercepto foram ajustados para refletir a incerteza real do sistema.\nQuanto ao diagnóstico final, o teste de Moran aplicado aos resíduos do modelo CAR resultou em um índice de -0,187 com um valor-p de 1 (não significativo). Indica a rejeição da hipótese de aglomeração espacial positiva (clusters) nos erros. O valor-p unitário sugere que a forte autocorrelação positiva, anteriormente presente no OLS, foi absorvida pela estrutura condicional do modelo, restando resíduos que, estatisticamente, não apresentam mais o padrão de agrupamento que invalidava a inferência anterior.",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#modelo-car-intrínseco-icar",
    "href": "lattice_data.html#modelo-car-intrínseco-icar",
    "title": "4  Dados de Área",
    "section": "4.8 Modelo CAR Intrínseco (ICAR)",
    "text": "4.8 Modelo CAR Intrínseco (ICAR)\nUm caso limite do modelo CAR ocorre quando \\(\\rho \\to 1\\). Este modelo, denominado CAR Intrínseco , é amplamente utilizado como prior para efeitos aleatórios espaciais (Besag, York, e Mollié 1991; Held e Rue 2010; Matthew J. Keefe, Ferreira, e Franck 2018b; Michael J. Keefe, Ferreira, e Franck 2019).\n\n4.8.1 Modelo Condicional Autorregressivo Intrínseco (ICAR)\nO modelo Condicional Autorregressivo Intrínseco (Intrinsic Conditional Autoregressive, (ICAR)) é um caso particular e amplamente utilizado do modelo CAR, no qual a matriz de precisão (\\(\\mathbf{Q}\\)) é singular (não é invertível), refletindo uma prior espacial que apenas penaliza diferenças entre vizinhos, sem especificar um nível absoluto para as variáveis. O ICAR pode ser entendido como o limite de um CAR próprio quando o parâmetro de dependência espacial se aproxima da fronteira do espaço paramétrico, resultando em uma matriz de precisão (\\(\\mathbf{Q}\\)) semidefinida positiva com posto \\(n-1\\) (Besag e Kooperberg 1995).\nPartindo da especificação condicional do CAR dada na Eq. 4.2, o ICAR é definido quando a dependência espacial atinge seu máximo (\\(\\rho \\approx 1\\)), com a média condicional de cada área dependendo apenas da média simples de seus vizinhos. Para pesos da matriz de vizinhança (\\(w_{ij}=1\\) se \\(i \\sim j\\), 0 caso contrário), as distribuições condicionais são:\n\\[\nY_i \\mid \\mathbf{Y}_{-i} \\sim \\mathcal{N}\\left( \\bar{Y}_{\\partial i},\\, \\frac{\\sigma^2}{m_i} \\right),\n\\qquad\n\\mathbb{E}[Y_i \\mid \\mathbf{Y}_{-i}] = \\bar{Y}_{\\partial i} = \\frac{1}{m_i}\\sum_{j \\in \\partial i} Y_j,\n\\qquad\n\\text{Var}[Y_i \\mid \\mathbf{Y}_{-i}] = \\frac{\\sigma^2}{m_i},\n\\tag{4.3}\\]\nonde \\(\\partial i\\) denota o conjunto de vizinhos da área \\(i\\), \\(m_i = |\\partial i|\\) é o número de vizinhos (cardinal de \\(\\partial i\\)), e \\(\\sigma^2 &gt; 0\\) é um parâmetro de escala que controla a suavidade espacial. Nesta formulação, o valor esperado condicional é a média aritmética dos valores nas áreas vizinhas (\\(\\mathbb{E}[Y_i \\mid \\mathbf{Y}_{-i}]\\)), e a variância condicional (\\(\\text{Var}[Y_i \\mid \\mathbf{Y}_{-i}]\\)) é inversamente proporcional ao número de vizinhos, refletindo maior incerteza em áreas com menos conexões/vizinhos (Held e Rue 2010).\nA especificação condicional implica uma matriz de precisão conjunta (\\(\\mathbf{Q}\\)) singular (sem inversa), com posto \\(n-1\\). Seja \\(\\mathbf{D} = \\text{diag}(m_1, \\dots, m_n)\\) e \\(\\mathbf{W}=[w_{ij}]_{n \\times n}\\) a matriz de de vizinhança. A matriz de precisão do ICAR é proporcional a \\(\\mathbf{Q} = \\sigma^{-2} (\\mathbf{D} - \\mathbf{W})=\\tau (\\mathbf{D} - \\mathbf{W})\\), que é semidefinida positiva, com um autovalor zero correspondente ao autovetor \\(\\mathbf{1}\\) (vetor de uns). Consequentemente, a densidade conjunta é imprópria e pode ser escrita (a menos de uma constante) como:\n\\[\np(\\mathbf{Y}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i \\sim j} (Y_i - Y_j)^2 \\right),\n\\]\nonde a soma percorre todos os pares de áreas adjacentes. Esta forma é conhecida como pairwise difference prior e destaca que a densidade só depende dos contrastes locais entre vizinhos, sendo invariante à adição de uma constante global (Besag e Kooperberg 1995; Held e Rue 2010).\nPara obter uma distribuição própria e identificável, é comum impor uma restrição de soma-zero, \\(\\sum_{i=1}^n Y_i = 0\\). Esta restrição pode ser aplicada formalmente projetando o vetor de efeitos espaciais no subespaço ortogonal a \\(\\mathbf{1}\\), resultando em uma distribuição Gaussiana singular própria com matriz de covariância proporcional à pseudoinversa de Moore-Penrose de \\(\\mathbf{Q}_{ICAR}\\) (Matthew J. Keefe, Ferreira, e Franck 2018a). Em modelos hierárquicos Bayesianos, esta restrição é frequentemente implementada durante a amostragem MCMC, mas a formalização via projeção assegura unicidade e propriedades matemáticas bem definidas.\nO ICAR é extensivamente utilizado como componente espacial estruturado em modelos hierárquicos, como no modelo Besag-York-Mollié (BYM) ( Seção 4.10) (Besag, York, e Mollié 1991), que combina um efeito ICAR para variação espacial suave e um efeito aleatório independente para variação não estruturada. Matthew J. Keefe, Ferreira, e Franck (2019) derivaram priors de referência objetivas para modelos com componentes ICAR, facilitando análise Bayesiana automática sem a necessidade de especificação subjetiva de hiperparâmetros.\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\n# CARBayes é o pacote padrão para modelagem Bayesiana de áreas no CRAN\npacman::p_load(CARBayes, spdep, sf, ggplot2, patchwork, coda, gt)\n\nif (!exists(\"mg_dados\")) {\n  mg_dados &lt;- geobr::read_municipality(code_muni = \"MG\", year = 2020, showProgress = FALSE)\n  coords &lt;- st_coordinates(st_centroid(mg_dados))\n  set.seed(123)\n  mg_dados$taxa_bruta &lt;- (-coords[,2] * 10) + rnorm(nrow(mg_dados), 0, 5)\n  mg_dados$variavel_x &lt;- rnorm(nrow(mg_dados))\n}\n\n#Matriz de Vizinhança Binária (W)\n# O pacote CARBayes exige uma matriz binária (0 e 1), não normalizada.\nnb &lt;- poly2nb(mg_dados, queen = TRUE)\nW_binaria &lt;- nb2mat(nb, style = \"B\", zero.policy = TRUE)\n\n#Ajuste do Modelo ICAR (Bayesiano)\n\nset.seed(123)\nmodelo_icar &lt;- S.CARleroux(formula = taxa_bruta ~ variavel_x, \n                           data = mg_dados, \n                           family = \"gaussian\", #terias que ver a distr dos seus dados para decidir aqui\n                           W = W_binaria, \n                           burnin = 2000,   # Descarta as primeiras 2000 iterações (aquecimento)\n                           n.sample = 10000, # Total de amostras MCMC\n                           thin = 10,        # Salva a cada 10 para reduzir autocorrelação\n                           rho = 1,          # RHO = 1 define o ICAR\n                           verbose = FALSE)\n\n\n#Extrair os resultados\n# O objeto mod_icar$summary.results contém médias e quantis\nsumm &lt;- as.data.frame(modelo_icar$summary.results)\n\n\nparams_interesse &lt;- c(\"(Intercept)\", \"variavel_x\", \"tau2\", \"nu2\")\nsumm_filt &lt;- summ[rownames(summ) %in% params_interesse, ]\n\n# Função para formatar: \"Média [IC 2.5%; IC 97.5%]\"\nformat_bayes &lt;- function(mean, lower, upper) {\n  paste0(format(round(mean, 3), nsmall=3), \" [\", \n         format(round(lower, 3), nsmall=3), \"; \", \n         format(round(upper, 3), nsmall=3), \"]\")\n}\n\ntabela_icar &lt;- data.frame(\n  Parametro = rownames(summ_filt),\n  Estimativa = mapply(format_bayes, summ_filt$Mean, summ_filt$`2.5%`, summ_filt$`97.5%`)\n)\n\n# Renomear\ntabela_icar$Parametro &lt;- recode(\n  tabela_icar$Parametro,\n  \"(Intercept)\" = \"Intercepto\",\n  \"variavel_x\"  = \"Variável X\",\n  \"tau2\"         = \"Variância Espacial $\\\\tau^2$\",\n  \"nu2\"          = \"Variância do Erro $\\\\nu^2$\"\n)\n\n\nkbl(tabela_icar, \n    format = \"latex\",\n    booktabs = TRUE, \n    align = \"lc\", \n    caption = NULL, \n    escape = FALSE) %&gt;%\n  kable_styling(latex_options = c(\"HOLD_position\", \"striped\"), \n                full_width = FALSE, \n                position = \"center\") %&gt;%\n  footnote(general = \"Estimativas: Média a posteriori [IC 95%].\") \n\n\nCódigo\n# O modelo estima um efeito aleatório (phi) para cada município.\nmg_dados$efeito_icar &lt;- apply(modelo_icar$samples$phi, 2, mean)\n\n#\nggplot(mg_dados) +\n  geom_sf(aes(fill = efeito_icar), color = NA) +\n  scale_fill_distiller(palette = \"RdBu\", name = expression(\"Efeito Espacial\" ~Phi)) +\n  labs(title = \"Componente Espacial ICAR\", \n       subtitle = \"Padrão latente recuperado (Suavizado)\") +\n  theme_void() +\n  theme(plot.title = element_text(face = \"bold\"))+\n  \n  annotation_scale(\n    location = \"bl\",           \n    width_hint = 0.3,          \n    bar_cols = c(\"black\", \"white\"), \n    text_family = \"sans\"       \n  ) +\n  \n  annotation_north_arrow(\n    location = \"tl\",           \n    which_north = \"true\",      \n    pad_x = unit(0.2, \"in\"),   \n    pad_y = unit(0.2, \"in\"),   \n    style = north_arrow_fancy_orienteering \n  )\n\n\n\n\nTabela 4.2: \n\n\n\n\n\n\nInterpretação\nO intercepto do modelo foi estimado em 8,975 e é significativo uma vez que seu intervalo de pois o intervalo credibilidade de 95% (entre 8,657 e 9,285) encontra-se inteiramente acima de zero (não inclui zero). Em contrapartida, a covariável aleatória (“Variável X”) não demonstrou qualquer influência explicativa sobre a taxa, apresentando uma estimativa pontual quase nula (-0,001) e um intervalo de credibilidade que inclui zero, o que confirma sua irrelevância para o fenômeno analisado.\nA variância do erro não estruturado (\\(\\nu^2\\)) foi estimada em 21,944, indicando uma presença de ruído nos dados brutos, enquanto a variância espacial (\\(\\tau^2\\)), estimada em 5,672 com um intervalo de credibilidade estritamente positivo (3,028 a 9,547), confirma que há um componente de vizinhança. A visualização deste efeito espacial (\\(\\phi\\)) no mapa recupera o gradiente norte-sul latente, isolando-o efetivamente da variabilidade aleatória capturada pelo termo de erro.\n\n\n\n\n\n\nNoneSaiba mais\n\n\n\nAcesse ao link 1 e link 2 e veja outras formas de de ajuste do mesmo modelo.",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#sec-SAR",
    "href": "lattice_data.html#sec-SAR",
    "title": "4  Dados de Área",
    "section": "4.9 Modelos Simultâneos Autorregressivos (SAR)",
    "text": "4.9 Modelos Simultâneos Autorregressivos (SAR)\nO modelo Autorregressivo Simultâneo (SAR - Simultaneous Autoregressive), introduzido por Whittle (1954), tem raízes na análise de séries temporais e é predominante na econometria espacial (Anselin 1988).\nAo contrário do CAR (Seção 4.7.1), o SAR modela a dependência através de um sistema de equações simultâneas onde a variável \\(Y_i\\) é função direta dos seus valores contemporâneos \\(Y_j\\) e de um termo de erro:\n\\[Y_i = \\rho \\sum_{j=1}^n w_{ij} Y_j + \\mathbf{x}_i^\\top \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\]\nAqui, \\(\\rho \\sum w_{ij} Y_j\\) é o termo de defasagem espacial (spatial lag). Existe um efeito de feedback global: um choque em \\(\\epsilon_i\\) afeta \\(Y_i\\), que afeta os vizinhos \\(Y_j\\), que por sua vez afetam de volta \\(Y_i\\) e se propagam por todo o sistema.\nEm notação matricial, o modelo é:\n\\[\n\\mathbf{Y} = \\rho \\mathbf{W}\\mathbf{Y} + \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\implies (\\mathbf{I} - \\rho \\mathbf{W})\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\: \\: \\mathbf{Y} = (\\mathbf{I} - \\rho \\mathbf{W})^{-1}\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\: \\: \\text{ se a inversa existir}\n\\]\nA forma reduzida para a média e a covariância é:\n\\[\\mathbf{Y}|\\mathbf{X} \\sim \\mathcal{N}\\left( (\\mathbf{I} - \\rho \\mathbf{W})^{-1}\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 [(\\mathbf{I} - \\rho \\mathbf{W})^\\top (\\mathbf{I} - \\rho \\mathbf{W})]^{-1} \\right)\\]\nVer Hoef, Hanks, e Hooten (2018) e Wall (2004) destacam as diferenças fundamentais na estrutura de covariância:\n\nA matriz de precisão do SAR é \\(\\mathbf{Q}_{SAR} = \\sigma^{-2}(\\mathbf{I} - \\rho \\mathbf{W})^\\top (\\mathbf{I} - \\rho \\mathbf{W})\\). Note o produto cruzado das matrizes. Isso implica que a correlação no SAR decai mais lentamente com a distância do grafo do que no CAR.\nO parâmetro \\(\\rho\\) no SAR tem uma interpretação de spillover global (efeito multiplicador), enquanto no CAR é uma medida de correlação condicional local.\n\n\n\n\n\n\n\nNota\n\n\n\n\nOutras especificações dos modelos SAR e CAR são discutidas no capítulo 4 do livro Scalon (2024).\nConsulte o capítulo 4 do livro Scalon (2024) para conhecer todos os procedimentos a seguir antes do ajuste do modelo, desde a análise exploratória até o ajuste propriamente dito.\nA descrição completa dos pacotes spdep e spatialreg encontra-se no capítulo 4 do livro Scalon (2024).",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#sec-BYM",
    "href": "lattice_data.html#sec-BYM",
    "title": "4  Dados de Área",
    "section": "4.10 O Modelo BYM (Besag-York-Mollié) e sua reparametrização (BYM2)",
    "text": "4.10 O Modelo BYM (Besag-York-Mollié) e sua reparametrização (BYM2)\nAté este ponto, vimos os modelos CAR, SAR e ICAR que consideram que a variável de interesse \\(\\mathbf{Y}\\) segue, condicional aos parâmetros, uma distribuição Normal multivariada. Contudo, em epidemiologia, demografia e ciências sociais, é comum lidar com dados de contagem \\(y_i\\) observados em uma região \\(i\\), as quais são comumente modeladas como variáveis aleatórias seguindo uma distribuição de Poisson:\n\\[y_i \\mid \\theta_i \\sim \\text{Poisson}(E_i \\theta_i), \\quad i = 1, \\dots, n\\]\nNessa formulação, \\(E_i\\) representa o número esperado de casos, ajustado por características populacionais, enquanto \\(\\theta_i\\) denota o risco relativo verdadeiro ( ver Seção 4.4). Como os dados são de contagem, é imperativo que a média (número esperado) do processo seja positiva. No entanto, se utilizássemos um modelo linear direto para o risco, poderíamos incorrer no erro de estimar valores negativos, uma vez que, na distribuição Normal, o suporte do parâmetro de média compreende todos os números reais. É importante enfatizar que o foco da modelagem estatística reside na estrutura da média, embora existam extensões importantes como os modelos lineares generalizados (GLM) duplos, que modelam a média e a variância (ver Paula 2013), ou os modelos GAMLSS, que permitem modelar simultaneamente a média, a variância, a curtose e a assimetria (ver Stasinopoulos et al. 2017, 2024; Rigby et al. 2019).\nPara resolver o problema da restrição de positividade, não se aplica uma transformação diretamente à variável resposta, como se faria em modelos de regressão clássicos para estabilização de variância, mas sim uma função de ligação ao valor esperado do processo. Essa é a essência dos Modelos Lineares Generalizados (GLM). No caso da distribuição Poisson, a função de ligação canônica é o logaritmo natural (ver outras funções em (Paula 2025)), o que nos permite definir o preditor linear \\(\\eta_i = \\log(\\theta_i)\\). Esse preditor é então modelado através de uma estrutura aditiva que incorpora o intercepto \\(\\mu\\), o efeito das covariáveis \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\) e um termo de efeito aleatório espacial \\(\\varepsilon_i\\), resultando na expressão:\n\\[\\eta_i = \\log(\\theta_i)=\\mu + \\mathbf{x}_i^\\top \\boldsymbol{\\beta} + \\varepsilon_i .\\]\n\n4.10.1 Modelo BYM\nProposto por Besag, York, e Mollié (1991), o modelo Besag-York-Mollié (BYM) decompõe o efeito aleatório espacial \\(\\varepsilon_i\\) em dois componentes aditivos (Riebler et al. 2016):\n\\[\n\\varepsilon_i = u_i + v_i\n\\]\nonde:\n\n\\(u_i\\) é componente estrutural que modela a dependência espacial. Assume-se uma prior ICAR (ver seção sobre Seção 4.8.1), onde a distribuição condicional de \\(u_i\\) depende apenas dos vizinhos \\(\\partial i\\):\n\n\\[u_i \\mid \\mathbf{u}_{-i}, \\tau_u \\sim \\mathcal{N}\\left( \\frac{1}{m_i} \\sum_{j \\in \\partial i} u_j, \\frac{1}{m_i \\tau_u} \\right), \\: \\:\\frac{1}{\\tau_u} = \\sigma_u^2\\]\nA distribuição conjunta é imprópria e dada por \\(\\mathbf{u} \\sim \\mathcal{N}(\\mathbf{0}, \\tau_u^{-1}\\mathbf{Q}^-)\\), onde \\(\\mathbf{Q}\\) é a matriz de estrutura definida pela vizinhança (\\(Q_{ii} = m_i\\) e \\(Q_{ij} = -1\\) se \\(i \\sim j\\)) e \\(\\mathbf{Q}^-\\) denota a sua inversa generalizada. \\(\\tau_u\\) é a precisão (inverso da variância) deste componente.\n\n\\(v_i\\) é componente não estrutural que modela o ruído aleatório independente (heterogeneidade pura). Assume-se normalidade i.i.d.:\n\n\\[v_i \\mid \\tau_v \\sim \\mathcal{N}(0, \\tau_v^{-1}), \\: \\: \\frac{1}{\\tau_v} = \\sigma_v^2\\]\nA variância marginal do efeito total \\(\\varepsilon_i\\) no modelo BYM original é, portanto, a soma das variâncias dos componentes:\n\\[\\text{Var}(\\varepsilon_i \\mid \\tau_u, \\tau_v) = \\text{Var}(v_i) + \\text{Var}(u_i) = \\tau_v^{-1} + (\\tau_u^{-1}\\mathbf{Q}^-)_{ii}\\]\nApesar de sua ampla utilização no mapeamento de doenças, Riebler et al. (2016) identificaram limitações nesta parametrização que afetam a inferência:\n\nIdentificabilidade: Apenas a soma \\(\\varepsilon_i = u_i + v_i\\) é identificável. A separação entre \\(u\\) e \\(v\\) depende inteiramente das distribuições a priori, e os hiperparâmetros \\(\\tau_u\\) e \\(\\tau_v\\) competem pela explicação da variância total. Lembre-se que um parâmetro \\(\\boldsymbol{\\theta}\\) é considerado identificável se valores distintos do parâmetro implicam necessariamente em distribuições de probabilidade distintas para os dados observados (formalmente, se \\(P_{\\boldsymbol{\\theta}_1} = P_{\\boldsymbol{\\theta}_2} \\implies \\boldsymbol{\\theta}_1 = \\boldsymbol{\\theta}_2\\)). No modelo BYM, infinitas combinações dos componentes \\(u_i\\) e \\(v_i\\) podem resultar no mesmo valor latente \\(\\varepsilon_i\\) e, por consequência, na mesma função de verossimilhança.\nEscalonamento (Scaling): A variância marginal do componente espacial \\((\\mathbf{Q}^-)_{ii}\\) não é constante e depende da geometria do grafo (matriz) de vizinhança. Em grafos mais conectados (regiões com muitos vizinhos), a variância marginal induzida por um mesmo \\(\\tau_u\\) é menor do que em grafos menos conectados (regiões com poucos vizinhos). Isso torna as priors para \\(\\tau_u\\) não transferíveis: uma prior que é vagamente informativa para um mapa pode ser fortemente informativa para outro, impedindo a criação de padrões de referência (Sigrunn H. Sørbye e Rue 2014).\n\nExemplo: Imagine que você ajusta um modelo espacial para os municípios de do estado de São Paulo (muitos vizinhos, malha densa) e outro para os municípios do estado do Amazonas (áreas enormes, poucos vizinhos). Se você usar a mesma prior para a precisão \\(\\tau\\) (ex: \\(\\tau=1\\)) em ambos os mapas, no mapa denso, a variância induzida será pequena enquanto no mapa esparso, a variância induzida será grande. Isso significa que a prior não é transferível, ou seja, o significado de \\(\\tau\\) muda dependendo do mapa que você está usando.\n\n\n4.10.2 Modelo BYM2\nPara solucionar os problemas de escalonamento e confundimento de parâmetros, Riebler et al. (2016) propuseram o modelo BYM2. A inovação consiste em escalonar a matriz de precisão espacial e reparametrizar o modelo em termos de variância total e proporção de variância espacial.\nPrimeiramente, define-se uma matriz de estrutura escalonada \\(\\mathbf{Q}_*\\). O escalonamento é realizado de tal forma que a média geométrica das variâncias marginais generalizadas do componente espacial seja igual a 1:\n\\[\\exp\\left( \\frac{1}{n} \\sum_{i=1}^n \\log((\\mathbf{Q}^-)_{ii}) \\right) = 1\\]\nEsta operação normaliza o tamanho médio do efeito espacial, tornando-o comparável ao efeito não estruturado (cuja variância é 1 na escala padronizada). Com a matriz \\(\\mathbf{Q}_*\\) normalizada, o vetor de efeitos aleatórios \\(\\boldsymbol{\\varepsilon} = (\\varepsilon_1, \\dots, \\varepsilon_n)^\\top\\) é reescrito como:\n\\[\\boldsymbol{\\varepsilon} = \\frac{1}{\\sqrt{\\tau}} \\left( \\sqrt{1 - \\phi}\\,\\mathbf{v} + \\sqrt{\\phi}\\,\\mathbf{u}_* \\right)\\]\nonde \\(\\mathbf{v} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\) é o componente não estruturado padronizado; \\(\\mathbf{u}_* \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{Q}_*^-)\\) é o componente estruturado escalonado; \\(\\tau &gt; 0\\) é o parâmetro de precisão marginal total e, \\(\\phi \\in [0, 1]\\) é o parâmetro de mistura.\nA matriz de covariância do vetor \\(\\boldsymbol{\\varepsilon}\\) no modelo BYM2 torna-se:\n\\[\\text{Var}(\\boldsymbol{\\varepsilon} \\mid \\tau, \\phi) = \\frac{1}{\\tau} \\left[ (1 - \\phi)\\mathbf{I} + \\phi \\mathbf{Q}_*^- \\right]\\]\nEsta formulação desacopla a magnitude da variabilidade da estrutura de dependência, facilitando a interpretação. Assim, \\(\\tau\\) controla a variância marginal total do efeito latente. \\(\\sigma = \\tau^{-1/2}\\) é o desvio padrão marginal de \\(\\varepsilon\\), sendo invariante à estrutura do grafo (região com muitos ou poucos vizinhos), enquanto \\(\\phi\\) representa a proporção da variância total que é explicada pela estrutura espacial.\nNote que se \\(\\phi=1\\), temos um modelo puramente ICAR; se \\(\\phi=0\\), um modelo puramente i.i.d.\nEsta parametrização permite o uso de PC Priors (Penalised Complexity Priors, (Sigrunn Holbek Sørbye e Rue 2017; Simpson et al. 2017)), onde o pesquisador pode expressar conhecimento a priori de forma intuitiva (ex: acredito que a probabilidade do efeito espacial ser superior a 0.5 é baixa).",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#priors-de-penalização-de-complexidade-pc-priors",
    "href": "lattice_data.html#priors-de-penalização-de-complexidade-pc-priors",
    "title": "4  Dados de Área",
    "section": "4.11 Priors de Penalização de Complexidade (PC Priors)",
    "text": "4.11 Priors de Penalização de Complexidade (PC Priors)\nA especificação de distribuições a priori para parâmetros de variância e correlação em modelos hierárquicos Bayesianos constitui, historicamente, um desafio. As escolhas tradicionais, notadamente a família de distribuições Gama-Inversa (\\(\\epsilon, \\epsilon\\)) para a variância \\(\\sigma^2\\) (ou Gama para a precisão) com parâmetros vagos (por exemplo, \\(\\epsilon = 0.001\\)), foram severamente criticadas por Gelman (2006). Gelman demonstra que, embora matematicamente convenientes devido à conjugação condicional, tais distribuições são inadequadas como referência não informativa. O autor argumenta que a família Gama-Inversa é patológica no limite \\(\\epsilon \\to 0\\), pois não converge para uma distribuição a posteriori própria em situações onde a verossimilhança permite variância nula. Consequentemente, a inferência torna-se instável e altamente sensível à escolha arbitrária do hiperparâmetro \\(\\epsilon\\), especialmente em conjuntos de dados com um número pequeno de grupos ou quando a variância dos efeitos aleatórios é pequena. Na prática, a prior Gama-Inversa introduz um viés que empurra a massa da posteriori para longe da origem, distorcendo a inferência ao sugerir uma variabilidade entre grupos maior do que a suportada pelos dados. Gelman (2006) recomenda, em vez disso, o uso de prioris Uniformes ou da família Half-t (como a Half-Cauchy) para o desvio padrão \\(\\sigma\\), que possuem melhor comportamento na origem e nas caudas.\nEm resposta a essas e várias outras limitações, Simpson et al. (2017) propuseram uma estrutura unificada para a construção de distribuições a priori denominadas Priors de Penalização de Complexidade (PC Priors).\nA construção das PC Priors fundamenta-se em quatro princípios. O primeiro princípio, o princípio da parcimônia, estabelece a existência de um modelo base (denotado por \\(g\\)) que é uma versão simplificada do modelo flexível (denotado por \\(f\\)). A prior deve ser construída de modo a favorecer o modelo base, penalizando o afastamento deste a menos que os dados forneçam evidência robusta para justificar a complexidade adicional. O segundo princípio define uma medida de complexidade baseada na Divergência de Kullback-Leibler (KLD),\n\\[\\text{KLD}(f \\| g) = \\int f(x) \\log(f(x)/g(x)) dx\\],\nque quantifica a perda de informação ao aproximar o modelo flexível pelo modelo base. Para conferir interpretabilidade, essa divergência é transformada em uma distância unidirecional definida como\n\\[d(\\xi) = \\sqrt{2 \\text{KLD}(f \\| g)},\\]\nonde \\(\\xi\\) é o parâmetro que governa a flexibilidade do modelo.\nO terceiro princípio determina a taxa de penalização. Simpson et al. (2017) argumentam que, na ausência de informações externas que sugiram o contrário, a penalização deve ocorrer a uma taxa constante ao longo da distância métrica \\(d\\). A única distribuição de probabilidade contínua definida em \\([0, \\infty)\\) que satisfaz a propriedade de falta de memória (taxa de risco constante) é a distribuição exponencial. Portanto, a prior na escala da distância deve ser\n\\[\\pi(d) = \\lambda e^{-\\lambda d},\\]\nonde \\(\\lambda\\) é a taxa de decaimento. A distribuição a priori para o parâmetro original \\(\\xi\\) é então obtida através da regra de transformação de variáveis, dada por\n\\[\\pi(\\xi) = \\lambda e^{-\\lambda d(\\xi)} \\left| \\partial d(\\xi) / \\partial \\xi \\right|.\\]\nO quarto princípio permite a calibração definida pelo usuário. Em vez de escolher o parâmetro \\(\\lambda\\) diretamente, o pesquisador especifica uma regra probabilística intuitiva sobre a cauda da distribuição, da forma\n\\[\\text{Prob}(Q(\\xi) &gt; U) = \\alpha,\\]\nonde \\(Q(\\xi)\\) é uma transformação interpretável do parâmetro, \\(U\\) é um limite superior e \\(\\alpha\\) uma probabilidade pequena.\nExemplo da Aplicação de PC Priors\nConsidere um pesquisador que analisa a incidência de uma doença infecciosa em \\(n\\) municípios. Os dados observados são contagens \\(y_i\\) de casos, modeladas como \\(y_i \\mid \\theta_i \\sim \\text{Poisson}(E_i \\theta_i)\\), onde \\(E_i\\) é o número esperado de casos e \\(\\theta_i\\) o risco relativo. O preditor linear é definido como \\(\\eta_i = \\log(\\theta_i) = \\mu + \\mathbf{x}_i^\\top \\boldsymbol{\\beta} + \\varepsilon_i\\). O efeito aleatório espacial \\(\\varepsilon_i\\) é modelado utilizando a reparametrização BYM2: \\(\\varepsilon_i = \\tau^{-1/2} \\left( \\sqrt{1-\\phi} \\, v_i + \\sqrt{\\phi} \\, u_i^* \\right)\\), onde \\(\\tau\\) é a precisão marginal total e \\(\\phi \\in [0,1]\\) é o parâmetro de mistura. A especificação de distribuições a priori para \\(\\tau\\) e \\(\\phi\\) é crucial. O uso de priors Gama para a precisão, como \\(\\tau \\sim \\Gamma(1, 0.01)\\), é comum, mas problemático, pois coloca densidade zero em \\(\\tau \\to \\infty\\) (o modelo base de variância nula), podendo levar a sobreajuste ao não permitir o encolhimento adequado.\nA aplicação das PC Priors resolve este problema. O primeiro passo é identificar os modelos base para cada parâmetro (se o objetivo é modelar dependência espacial, o modelo base é aquele que não tem dependência espacial). Para a precisão \\(\\tau\\), o modelo base é a ausência de efeitos aleatórios, correspondente a \\(\\tau \\to \\infty\\) ou, equivalentemente, ao desvio padrão marginal \\(\\sigma = \\tau^{-1/2} = 0\\). Para o parâmetro de mistura \\(\\phi\\), o modelo base é a ausência de dependência espacial, ou seja, \\(\\phi = 0\\). As PC Priors serão construídas para penalizar o afastamento destes modelos base.\nNa prática, o pesquisador traduz conhecimento epidemiológico prévio em declarações probabilísticas (isso é prior). Para a precisão \\(\\tau\\), é mais intuitivo pensar na escala do desvio padrão \\(\\sigma\\) (o quão afastados estão os casos de dengue em Lavras em relação média geral de todo estado de Minas Gerais) . Suponha que, com base na literatura, o pesquisador acredite ser improvável que a variação não explicada no log-risco seja extrema. Um desvio padrão de \\(\\sigma = 0.5\\) implica que os riscos relativos (na escala original) variam tipicamente por um fator de até \\(e^{0.5} \\approx 1.65\\) para mais ou para menos. O pesquisador pode considerar que é pouco plausível que a variação não explicada supere este patamar. Assim, formula-se a declaração: \\(\\text{Prob}(\\sigma &gt; 0.5) = 0.01\\). Isto significa que se atribui apenas 1% de probabilidade a priori a valores de \\(\\sigma\\) superiores a 0.5.\nEsta declaração define os parâmetros da PC Prior: \\(U = 0.5\\) e \\(\\alpha = 0.01\\). Conforme a derivação de Simpson et al. (2017), isso implica uma distribuição exponencial para \\(\\sigma\\) com taxa \\(\\lambda = -\\ln(\\alpha)/U = -\\ln(0.01)/0.5 \\approx 9.21\\). A prior correspondente para a precisão \\(\\tau\\) é uma distribuição tipo-2 Gumbel, mas o usuário não precisa manipulá-la diretamente.\nPara o parâmetro de mistura \\(\\phi\\), que representa a proporção da variância atribuída à dependência espacial, o raciocínio é similar. O modelo base é \\(\\phi=0\\) (ausência de estrutura espacial). O pesquisador deve refletir sobre a importância relativa esperada da dependência espacial. Em muitas aplicações epidemiológicas, é razoável assumir, na falta de informação forte, que a heterogeneidade não espacial (ruído) pode ser tão ou mais importante que a estrutura espacial. Uma declaração conservadora poderia ser: \\(\\text{Prob}(\\phi &gt; 0.5) = 0.5\\). Isto significa que se atribui igual probabilidade (50%) a \\(\\phi\\) estar acima ou abaixo de 0.5, mas ainda assim a prior é construída para encolher em direção a zero.\nNo pacote R-INLA, a especificação destas PC Priors é direta. Suponha que id_regiao é um vetor de índices que identificam cada município do estado de Minas Gerais, grafo é a matriz de vizinhança, e y é o vetor de contagens. A fórmula do modelo, incorporando as PC Priors com os parâmetros \\(U\\) e \\(\\alpha\\) definidos acima, seria:\nO argumento param = c(U, α) na prior \"pc.prec\" para a precisão codifica exatamente a declaração \\(\\text{Prob}(\\sigma &gt; U) = \\alpha\\). Para a prior \"pc\" no parâmetro phi, param = c(0.5, 0.5) codifica \\(\\text{Prob}(\\phi &gt; 0.5) = 0.5\\). A opção scale.model = TRUE garante o escalonamento automático da matriz de precisão do componente estruturado, essencial para a interpretabilidade de \\(\\phi\\).\nA prior para \\(\\sigma\\) (derivada de pc.prec) tem seu pico em zero e decai exponencialmente. Assim, se os dados forem escassos ou pouco informativos, a posteriori de \\(\\sigma\\) será puxada para valores próximos de zero, efetivamente encolhendo os efeitos aleatórios \\(\\varepsilon_i\\) e produzindo estimativas de risco mais suaves e próximas da média global. Segundo, a prior para \\(\\phi\\) também encolhe em direção a zero. Portanto, na ausência de um sinal espacial claro nos dados, o modelo tenderá a alocar mais variância ao componente não estruturado (\\(v_i\\)), evitando a imposição de uma suavização espacial injustificada.\n\nCódigo\n# Instalação do INLA \noptions(timeout = 600)\n\nif (!requireNamespace(\"INLA\", quietly = TRUE)) {\n   install.packages(\"INLA\", repos=c(getOption(\"repos\"), INLA=\"https://inla.r-inla-download.org/R/stable\"), dep=TRUE)\n}\n\n\n# Veja como instalar INLA no link:  https://www.r-inla.org/download-install\n# veja também browseVignettes(\"INLA\") ou system.file(\"doc\", package = \"INLA\") e veja a documentacao\n# Veja também https://github.com/hrue/r-inla/tree/devel/rinla/vignettes\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(INLA, spdep, sf, dplyr, knitr, kableExtra, ggplot2, patchwork)\n\n\n# Preparação dos Dados\nif (!exists(\"mg_dados\")) {\n  mg_dados &lt;- geobr::read_municipality(code_muni = \"MG\", year = 2020, showProgress = FALSE)\n  coords &lt;- st_coordinates(st_centroid(mg_dados))\n  set.seed(123)\n  mg_dados$taxa_bruta &lt;- (-coords[,2] * 10) + rnorm(nrow(mg_dados), 0, 5)\n  mg_dados$variavel_x &lt;- rnorm(nrow(mg_dados))\n}\n\n# Criar um identificador numérico sequencial para as áreas (exigência do INLA)\nmg_dados$ID_AREA &lt;- 1:nrow(mg_dados)\n\n#Matriz de Vizinhança e Grafo\nnb &lt;- poly2nb(mg_dados, queen = TRUE)\n\nplot(st_geometry(mg_dados), border = \"lightgrey\", main = \"Estrutura de Vizinhança (Grafo)\")\nplot(nb, coords, add = TRUE, col = \"red\", pch = 19, cex = 0.6, lwd = 0.5)\n\n\nCódigo\n# Converter para formato de grafo do INLA\nnb2INLA(\"mg_graph.adj\", nb)\ng &lt;- inla.read.graph(\"mg_graph.adj\")\n\n\n#Definição dos PC Priors (Penalised Complexity)\n# Prior para a Precisão (Tau): Prob(desvio padrão &gt; 1) = 0.01 , vc pode usar outros\n   #A probabilidade do Desvio Padrão (σ) ser maior que 1 é de apenas 1% (0.01).\n\n# Prior para Mistura (Phi): Prob(phi &lt; 0.5) = 0.5 (neutro), note que Phi = 1 (tudo é espacial), Phi=0 (tudo é aleatório).\n       #Você está dizendo ao modelo: \"Eu não sei se o fenômeno é mais espacial ou \n       # mais aleatório, então vou deixar 50% de chance para cada lado\"\n\nhyper_pc &lt;- list(\n  prec = list(prior = \"pc.prec\", param = c(1, 0.01)),\n  phi  = list(prior = \"pc\", param = c(0.5, 0.5))\n)\n\n#Ajuste do Modelo BYM2\n\nformula_bym2 &lt;- taxa_bruta ~ variavel_x + \n                f(ID_AREA, model = \"bym2\", graph = g, scale.model = TRUE, \n                  hyper = hyper_pc)\n\nmodelo_inla &lt;- inla(formula_bym2, \n                    data = mg_dados, \n                    family = \"gaussian\", \n                    control.predictor = list(compute = TRUE), \n                    control.compute = list(dic = TRUE, waic = TRUE))\n\n# Função auxiliar de formatação \"Média [IC 95%]\"\nfmt &lt;- function(m, l, u) {\n  paste0(format(round(m, 3), nsmall=3), \" [\", \n         format(round(l, 3), nsmall=3), \"; \", \n         format(round(u, 3), nsmall=3), \"]\")\n}\n\n# Efeitos Fixos\nfix &lt;- modelo_inla$summary.fixed[, c(\"mean\", \"0.025quant\", \"0.975quant\")]\ndf_fix &lt;- data.frame(\n  Parametro = rownames(fix),\n  Valor = mapply(fmt, fix$mean, fix$`0.025quant`, fix$`0.975quant`)\n)\n\n# renomear nomes\ndf_fix$Parametro &lt;- recode(df_fix$Parametro, \n                           \"(Intercept)\" = \"Intercepto\", \n                           \"variavel_x\" = \"Variável X\")\n\n# Hiperparâmetros\nhyp &lt;- modelo_inla$summary.hyperpar[, c(\"mean\", \"0.025quant\", \"0.975quant\")]\ndf_hyp &lt;- data.frame(\n  Parametro = rownames(hyp),\n  Valor = mapply(fmt, hyp$mean, hyp$`0.025quant`, hyp$`0.975quant`)\n)\n\ndf_hyp$Parametro &lt;- recode(df_hyp$Parametro,\n                           \"Precision for the Gaussian observations\" = \"Precisão (Likelihood)\",\n                           \"Precision for ID_AREA\" = \"Precisão Marginal $\\\\tau$\",\n                           \"Phi for ID_AREA\" = \"Dependência Espacial $\\\\phi$\")\n\n#Métricas de Ajuste\ndf_met &lt;- data.frame(\n  Parametro = c(\"DIC\", \"WAIC\"),\n  Valor = c(format(round(modelo_inla$dic$dic, 2), nsmall=2),\n            format(round(modelo_inla$waic$waic, 2), nsmall=2))\n)\n\n# Unir tudo\ndf_final &lt;- rbind(df_fix, df_hyp, df_met)\n\n# Gerar Tabela\nkbl(df_final, \n    format = \"latex\",\n    booktabs = TRUE, \n    align = \"lr\", \n    caption = NULL) %&gt;% \n  kable_styling(latex_options = c(\"HOLD_position\"), \n                full_width = FALSE, \n                position = \"center\") %&gt;%\n  pack_rows(\"Efeitos Fixos (Média [IC 95%])\", 1, nrow(df_fix)) %&gt;%\n  pack_rows(\"Hiperparâmetros (Média [IC 95%])\", nrow(df_fix) + 1, nrow(df_fix) + nrow(df_hyp)) %&gt;%\n  pack_rows(\"Qualidade do Ajuste\", nrow(df_final) - 1, nrow(df_final)) %&gt;%\n  row_spec((nrow(df_final)-1):nrow(df_final), bold = TRUE) \n\n\n\n\nTabela 4.3: Resultados do Modelo BYM2 com PC Priors.\n\n\n\n\n\n\nInterpretação\nA análise dos efeitos fixos (Tabela 4.3) mostra que o intercepto do modelo foi estimado com uma média de 8,974, situando-se num intervalo de credibilidade de 95% entre 8,664 e 9,283 (não inclui zero). Por outro lado, a covariável “Variável X” apresentou uma estimativa média muito próxima de zero (-0,001) e, mais importante, o seu intervalo de credibilidade varia de -0,331 a 0,328. Como este intervalo inclui o valor zero, conclui-se que não existe evidência estatística de uma associação significativa entre a Variável X e a variável resposta; ou seja, a variável não contribui para explicar o fenômeno estudado neste modelo.\nNo que nos hiperparâmetros e à estrutura aleatória, a precisão da verossimilhança (Likelihood) é extremamente alta (média de 21117,082), o que indica que a variância do erro residual dos dados em torno da média predita é muito pequena. O parâmetro de dependência espacial (\\(\\phi\\)) foi estimado em 0,124 (com intervalo de 0,068 a 0,200), o que revela que apenas cerca de 12,4% da variabilidade capturada pelo efeito aleatório (campo latente) se deve à estrutura espacial de vizinhança, enquanto a maior parte (os restantes 87,6%) é atribuída a ruído não estruturado (efeito iid). Por fim, os critérios de informação DIC (-4822,46) e WAIC (-5056,00) indicam a qualidade do ajuste, servindo como base de comparação para modelos alternativos, onde valores menores indicariam um melhor ajuste.\n\n\nCódigo\n# DIAGNÓSTICO\n\nmg_dados$efeito_bym &lt;- modelo_inla$summary.random$ID_AREA$mean[1:nrow(mg_dados)]\nmg_dados$ajustado   &lt;- modelo_inla$summary.fitted.values$mean\nmg_dados$residuos   &lt;- mg_dados$taxa_bruta - mg_dados$ajustado\n\n#Mapa do Efeito Espacial (Risco/Nível estimado)\np1 &lt;- ggplot(mg_dados) +\n  geom_sf(aes(fill = efeito_bym), color = NA) +\n  scale_fill_distiller(palette = \"RdBu\", name = \"Efeito\\nEspacial\") +\n  labs(title = \"A. Componente Espacial (BYM2)\") +\n  theme_minimal()+\n  \n  annotation_scale(\n    location = \"bl\",           \n    width_hint = 0.3,          \n    bar_cols = c(\"black\", \"white\"), \n    text_family = \"sans\"       \n  ) +\n  \n  annotation_north_arrow(\n    location = \"tl\",           \n    which_north = \"true\",      \n    pad_x = unit(0.2, \"in\"),   \n    pad_y = unit(0.2, \"in\"),   \n    style = north_arrow_fancy_orienteering \n  )\n\n# Mapa dos Resíduos \np2 &lt;- ggplot(mg_dados) +\n  geom_sf(aes(fill = residuos), color = NA) +\n  scale_fill_distiller(palette = \"PuOr\", name = \"Resíduos\") +\n  labs(title = \"B. Mapa de Resíduos\") +\n   theme_minimal()+\n  \n  annotation_scale(\n    location = \"bl\",           \n    width_hint = 0.3,          \n    bar_cols = c(\"black\", \"white\"), \n    text_family = \"sans\"       \n  ) +\n  \n  annotation_north_arrow(\n    location = \"tl\",           \n    which_north = \"true\",      \n    pad_x = unit(0.2, \"in\"),   \n    pad_y = unit(0.2, \"in\"),   \n    style = north_arrow_fancy_orienteering \n  )\n\n#Gráfico Observado vs Esperado\ncor_p &lt;- round(cor(mg_dados$taxa_bruta, mg_dados$ajustado), 3)\np3 &lt;- ggplot(mg_dados, aes(x = ajustado, y = taxa_bruta)) +\n  geom_point(alpha = 0.3, color = \"darkblue\") +\n  geom_abline(col = \"red\", linetype = \"dashed\") +\n  labs(title = \"C. Ajuste do Modelo\", \n       subtitle = paste0(\"Correlação: \", cor_p),\n       x = \"Predito (INLA)\", y = \"Observado\") +\n  theme_minimal()\n\n\n(p1 + p2 + p3)\n\n# TESTE DE MORAN NOS RESÍDUOS\n\nlw &lt;- nb2listw(nb, style = \"W\")\nmoran_test &lt;- moran.test(mg_dados$residuos, lw)\n\nprint(moran_test)\n\n\n\n    Moran I test under randomisation\n\ndata:  mg_dados$residuos  \nweights: lw    \n\nMoran I statistic standard deviate = -1.3919, p-value = 0.918\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n    -0.0307562833     -0.0011737089      0.0004517036 \n\n\nCódigo\nif(moran_test$p.value &gt; 0.05) {\n  message(\"SUCESSO: Resíduos são aleatórios (p &gt; 0.05). O modelo removeu a autocorrelação.\")\n} else {\n  message(\"ATENÇÃO: Ainda existe dependência espacial nos resíduos.\")\n}\n\n\n\n\n\n\n\n\nFigura 4.11: Diagnóstico do modelo BYM2 ajustado\n\n\n\n\n\nInterpretação\nO componente espacial mapeado em Figura 4.11 (A) delineia a variabilidade latente, capturando a heterogeneidade geográfica do fenômeno. O ajuste é evidenciado na Figura 4.11 (C), onde a relação entre os valores observados e os preditos pelo INLA exibe uma correlação unitária, com os pontos assentando-se perfeitamente sobre a linha de identidade, indicando ausência de viés sistemático nas predições.\nA validade do modelo é confirmada pela análise dos resíduos. O mapa em Figura 4.11 (B) sugere visualmente uma distribuição espacial aleatória dos erros, sem padrões de aglomeração (clustering) remanescentes. Esta inspeção visual é confirmada pelo Teste de I de Moran realizado nos resíduos (\\(I = -0.0311\\); \\(p = 0.9203\\)). O valor-p elevado não permite rejeitar a hipótese nula de aleatoriedade espacial, o que confirma que o modelo BYM2 foi eficaz em incorporar toda a autocorrelação espacial significativa presente nos dados, restando nos resíduos apenas ruído branco estocástico.",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#estatística-espacial-vs.-econometria-espacial",
    "href": "lattice_data.html#estatística-espacial-vs.-econometria-espacial",
    "title": "4  Dados de Área",
    "section": "4.12 Estatística Espacial vs. Econometria Espacial",
    "text": "4.12 Estatística Espacial vs. Econometria Espacial\nA escolha entre as famílias de modelos condicionais (CAR/ICAR/BYM) e simultâneos (SAR) é fundamentalmente guiada pelo objetivo, pela natureza teórica da dependência espacial e pelo contexto disciplinar. A Estatística Espacial, com seus modelos CAR e ICAR, adota uma abordagem condicional ou markoviana (Besag 1974), cujo objetivo primário é a predição, suavização e mapeamento de superfícies latentes, como o risco de doença ou a concentração de poluentes (Cressie 1993). Neste paradigma, o foco reside em modelar com precisão a estrutura de covariância para capturar padrões espaciais, utilizando uma formulação baseada em distribuições condicionais locais. Esta especificação resulta em uma matriz de precisão esparsa, o que confere eficiência computacional a métodos bayesianos e hierárquicos, como MCMC e INLA (Håvard Rue, Martino, e Chopin 2009; Havard Rue e Held 2005). Esta abordagem é predominante em campos como a epidemiologia, ecologia e ciências ambientais, onde a interpretação do processo espacial latente e a quantificação da incerteza de predição são objetivos centrais (Riebler et al. 2016; Banerjee, Carlin, e Gelfand 2003).\nEm contraste, a Econometria Espacial, fundamentada no modelo SAR, adota uma abordagem simultânea ou estrutural (Anselin 1988). Seu objetivo primário é a inferência causal e a estimação não enviesada de parâmetros que capturem efeitos de interação e transbordamento (spillovers) entre unidades (J. LeSage e Pace 2009). Sua formulação constitui um sistema de equações com feedback, onde o valor em uma unidade depende simultaneamente dos valores em outras, gerando uma estrutura de precisão geralmente densa (Ver Hoef, Hanks, e Hooten 2018). Esta característica associa-a mais fortemente a métodos de estimação frequentistas, como a máxima verossimilhança ou o método dos momentos generalizado (GMM) (Anselin 2010). Esta abordagem é predominante em campos como economia regional, ciências políticas e estudos urbanos, onde a questão central é testar teorias sobre interdependência estratégica, externalidades espaciais ou difusão de políticas.\nA seleção do modelo deve ser uma função direta da pergunta de pesquisa. Deve-se optar por modelos CAR, ICAR ou BYM quando o interesse principal for a predição, suavização ou recuperação de um campo aleatório espacial subjacente (Wall 2004). Estes são a escolha natural para a criação de mapas de risco, interpolação de superfícies ou para modelagem bayesiana hierárquica que “empreste força” (borrowing strength) entre áreas vizinhas (Riebler et al. 2016). Por outro lado, modelos SAR (ou modelos de erro espacial - SEM) são mais adequados quando o interesse é a inferência causal sobre um parâmetro de interação espacial (\\(\\rho\\)) ou quando a teoria subjacente postula um mecanismo de dependência simultânea e feedback entre unidades observadas, como em modelos de competição ou difusão (Ver Hoef, Hanks, e Hooten 2018). Tais modelos são essenciais para corrigir viés de variável omitida espacial e para estimar efeitos diretos e indiretos de políticas. Ambas as abordagens são complementares, e a fronteira entre elas tem se tornado mais permeável, com avanços metodológicos permitindo, por exemplo, a interpretação de priors CAR em termos de equilíbrio de sistemas dinâmicos ou o uso de técnicas de álgebra esparsa para a estimação eficiente de certas especificações de modelos SAR (Ver Hoef, Hanks, e Hooten 2018; J. LeSage e Pace 2009).",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#modelos-espaciais-econométricos-para-dados-de-área",
    "href": "lattice_data.html#modelos-espaciais-econométricos-para-dados-de-área",
    "title": "4  Dados de Área",
    "section": "4.13 Modelos Espaciais Econométricos para Dados de Área",
    "text": "4.13 Modelos Espaciais Econométricos para Dados de Área\nA econometria espacial distingue-se da estatística espacial clássica (como os modelos CAR, SAR Seção 4.7.1) por sua ênfase na identificação de relações causais, na fundamentação teórica dos processos de dependência (como interações estratégicas, efeitos de contágio ou externalidades) e na garantia de propriedades assintóticas dos estimadores, especialmente a consistência sob condições de endogeneidade. Enquanto os modelos CAR são frequentemente usados para suavização e predição em contextos Bayesianos hierárquicos, os modelos econométricos (SAR, SEM, SDM, etc.) são desenhados para testar hipóteses sobre mecanismos de interação e estimar o impacto de políticas ou choques exógenos em um sistema interconectado.\nA literatura econométrica, consolidada por Anselin (1988) e expandida expressivamente por J. LeSage e Pace (2009), Elhorst et al. (2014) e H. Kelejian e Piras (2017), organiza os modelos espaciais com base na inclusão de três tipos fundamentais de interação entre unidades espaciais:\n\nDefasagem espacial na variável dependente (\\(\\mathbf{W}\\mathbf{y}\\)): Refere-se ao fenômeno em que o resultado observado em uma região ou agente é diretamente influenciado pelos resultados observados em regiões ou agentes vizinhos. Isto é, o valor da variável de interesse na unidade \\(i\\) (\\(y_i\\)) é determinado simultaneamente pelos valores de \\(y\\) nas unidades vizinhas (\\(\\mathbf{W}\\mathbf{y}\\), ver Elhorst (2022)).\n\nExemplo: Competição fiscal entre municípios (a alíquota de imposto de um município depende da alíquota dos vizinhos para não perder base tributária) ou efeitos de pares em educação (o desempenho de um aluno depende do desempenho dos colegas).\n\nDefasagem espacial nas variáveis explicativas (\\(\\mathbf{W}\\mathbf{X}\\)): Refere-se à influência que as características ou políticas de regiões vizinhas exercem sobre os resultados de uma região. É um efeito de transbordamento direto ou externalidade, onde o que acontece ao lado importa tanto quanto o que acontece aqui (ver Elhorst (2022)). Note que aqui \\(\\mathbf{X}\\) é covariável.\n\nExemplo: A criminalidade em um bairro pode ser afetada não apenas pelo policiamento local, mas também pelo policiamento nos bairros vizinhos (deslocamento do crime).\n\nDependência espacial no termo de erro (\\(\\mathbf{W}\\mathbf{u}\\)): Refere-se à dependência residual. A correlação espacial surge devido a variáveis omitidas que são espacialmente correlacionadas ou a erros de medição com padrão espacial. Isto é, mesmo após controlar pelas variáveis explicativas observadas (\\(\\mathbf{X}\\)), os resíduos de unidades geograficamente próximas podem não ser independentes, podendo ser correlacionados. Isso indica que os fatores não observados que influenciam a variável resposta \\(\\mathbf{y}\\) possuem, eles próprios, uma estrutura espacial.\n\nExemplo: Considere um modelo de preços de imóveis que inclui variáveis como tamanho, idade, número de quartos e proximidade do centro. Se o modelo omitir a poluição sonora (ruído de tráfego) um fator que varia gradualmente no espaço (ruas adjacentes têm níveis similares), os imóveis em ruas mais barulhentas terão sistematicamente preços abaixo do previsto pelo modelo, enquanto aqueles em ruas mais silenciosas terão preços acima do previsto. Como a poluição sonora é espacialmente correlacionada, os erros do modelo (resíduos) também o serão: resíduos negativos se agruparão em regiões barulhentas e positivos em regiões quietas. Essa estrutura espacial nos resíduos é exatamente a dependência espacial no termo de erro (\\(W u\\)).\nO modelo mais geral que engloba essas três formas de interação é o modelo geral de aninhamento espacial (GNS - General Nesting Spatial Model, Elhorst (2022)), por vezes referido como Spatial Autoregressive model with Spatial Autoregressive errors (SARAR) ou Spatial Autoregressive Combined (SAC) generalizado:\n\\[\n\\begin{aligned}\n\\mathbf{y} &= \\rho \\mathbf{W}\\mathbf{y} + \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{W}\\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{u} \\\\\n\\mathbf{u} &= \\lambda \\mathbf{W}\\mathbf{u} + \\boldsymbol{\\epsilon}\n\\end{aligned}\n\\tag{4.4}\\]\nOnde:\n\n\\(\\mathbf{y}\\) é um vetor \\(N \\times 1\\) da variável dependente.\n\\(\\mathbf{X}\\) é uma matriz \\(N \\times K\\) de variáveis explicativas (covaráveis).\n\\(\\mathbf{W}\\) é a matriz de pesos espaciais (ou matriz de vizinhança) \\(N \\times N\\). Pode-se usar matrizes diferentes para cada termo (\\(\\mathbf{W}_1, \\mathbf{W}_2\\)), mas frequentemente assume-se a mesma estrutura por parcimônia.\n\\(\\rho\\) é o coeficiente autorregressivo espacial (intensidade da dependência substantiva).\n\\(\\boldsymbol{\\beta}\\) é o vetor \\(K \\times 1\\) de parâmetros associados às covariáveis.\n\\(\\boldsymbol{\\theta}\\) é o vetor \\(K \\times 1\\) de parâmetros associados à defasagem espacial.\n\\(\\lambda\\) é o coeficiente de erro autorregressivo espacial.\n\\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)\\) é o vetor de erros, tipicamente assumido i.i.d. com média zero e variância constante \\(\\sigma^2\\) (embora métodos modernos relaxem a homocedasticidade).\n\nA partir desta estrutura geral, impõem-se restrições aos parâmetros (\\(\\rho=0\\), \\(\\lambda=0\\), ou \\(\\boldsymbol{\\theta}=\\mathbf{0}\\)) para derivar os modelos específicos mais utilizados na prática.\n\n4.13.1 Modelo de Defasagem Espacial (SAR – Spatial Autoregressive Model)\nO Modelo de Defasagem Espacial (SAR, o mesmo descrito na Seção 4.9), também conhecido como Spatial Lag Model, constitui um caso particular do Modelo Geral de Aninhamento Espacial (GNS, Eq. 4.4) obtido ao impor as restrições paramétricas \\(\\lambda = 0\\) e \\(\\boldsymbol{\\theta} = \\mathbf{0}\\).\nEsta especificação pressupõe que a dependência espacial é um fenômeno substantivo, resultante da interação direta e simultânea entre as unidades de observação, onde o resultado de uma localidade é condicionado pelos resultados de suas vizinhas. Formalmente, o modelo é definido pela seguinte equação estrutural (Anselin 1988):\n\\[\n\\mathbf{y} = \\rho \\mathbf{W}\\mathbf{y} + \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},\n\\tag{4.5}\\]\ncom erros i.i.d. \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)\\). Na sua forma escalar, para uma unidade \\(i\\), o modelo é expresso como:\n\\[\ny_i = \\rho \\sum_{j=1}^{n} w_{ij} y_j + \\sum_{k=1}^{K} x_{ik}\\beta_k + \\epsilon_i,\n\\]\nem que \\(\\rho\\) é o coeficiente de autocorrelação espacial, que quantifica a intensidade da dependência espacial.\nA partir da forma reduzida do modelo, obtém-se a distribuição condicional completa de \\(\\mathbf{y}\\). Assumindo que \\((\\mathbf{I}_n - \\rho \\mathbf{W})\\) é não singular (tem inversa), temos:\n\\[\n\\mathbf{y} \\, | \\, \\mathbf{X}, \\mathbf{W} \\sim \\mathcal{N}\\left( (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\mathbf{X}\\boldsymbol{\\beta}, \\quad \\sigma^2 (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1} (\\mathbf{I}_n - \\rho \\mathbf{W})^{-\\top} \\right).\n\\]\nEsta expressão evidencia que tanto a média condicional quanto a estrutura de covariância de \\(\\mathbf{y}\\) são globalmente afetadas pela configuração espacial através da matriz de pesos \\(\\mathbf{W}\\) e do parâmetro \\(\\rho\\).\nA especificação é motivada por processos de difusão, contágio, competição estratégica, externalidades ou efeitos de aprendizagem entre agentes ou regiões geograficamente próximas (J. LeSage e Pace 2009).\nA natureza simultânea do modelo é explicitada por sua forma reduzida (ou Data Generating Process – DGP), obtida ao resolver a equação para \\(\\mathbf{y}\\):\n\\[\n\\begin{aligned}\n(\\mathbf{I}_n - \\rho \\mathbf{W})\\mathbf{y} &= \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\\\\n\\mathbf{y} &= (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\boldsymbol{\\epsilon}.\n\\end{aligned}\n\\]\nA matriz \\(\\mathbf{S}(\\mathbf{W}) = (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\) é denominada multiplicador espacial. Sua expansão em série de potências,\n\\[\n(\\mathbf{I}_n - \\rho \\mathbf{W})^{-1} = \\mathbf{I}_n + \\rho \\mathbf{W} + \\rho^2 \\mathbf{W}^2 + \\rho^3 \\mathbf{W}^3 + \\cdots,\n\\]\nrevela que um choque exógeno (seja via \\(\\mathbf{X}\\) ou \\(\\boldsymbol{\\epsilon}\\)) em uma unidade \\(i\\) não afeta apenas \\(y_i\\) ou seus vizinhos diretos, mas propaga-se por toda a rede através de efeitos de feedback de ordem superior, envolvendo caminhos de comprimento crescente na estrutura de vizinhança (Elhorst et al. 2014).\nConsequentemente, a esperança condicional da variável dependente é espacialmente estruturada:\n\\[\n\\mathbb{E}[\\mathbf{y} \\, | \\, \\mathbf{X}, \\mathbf{W}] = (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\mathbf{X}\\boldsymbol{\\beta}.\n\\] A presença do multiplicador espacial implica que a interpretação dos parâmetros \\(\\boldsymbol{\\beta}\\) no modelo SAR difere fundamentalmente da regressão linear clássica. Estes coeficientes não representam os efeitos marginais diretos de uma mudança nas variáveis explicativas. Como qualquer alteração em uma covariável para a unidade \\(i\\) afeta sua própria variável dependente e, via interdependência, as das demais unidades, os efeitos são globais.\nJ. LeSage e Pace (2009) propõe uma decomposição da matriz de efeitos totais das variáveis explicativas, derivada da forma reduzida do modelo. Para uma variável explicativa \\(k\\), o efeito total de uma mudança unitária em todo o sistema é dado pela matriz:\n\\[\n\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}_k'} = \\beta_k (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}.\n\\]\nOs efeitos são então decompostos em:\n\nEfeito direto médio que captura o impacto médio de uma mudança em \\(x_{ik}\\) sobre o próprio \\(y_i\\). Este efeito inclui tanto o impacto inicial quanto os feedbacks espaciais que retornam à unidade de origem.\nEfeito indireto médio (ou Spillover) que captura o impacto médio da mudança em \\(x_{ik}\\) sobre todas as outras unidades \\(y_j\\) (\\(j \\neq i\\)).\n\nA presença da variável dependente defasada espacialmente (\\(\\mathbf{W}\\mathbf{y}\\)) no lado direito da equação gera um problema de simultaneidade: como \\(y_i\\) depende de \\(y_j\\) e vice-versa, o termo \\(\\mathbf{W}\\mathbf{y}\\) está correlacionado com o termo de erro \\(\\boldsymbol{\\epsilon}\\). Isto implica que o estimador de Mínimos Quadrados Ordinários (MQO) é viesado e inconsistente (\\(plim \\, \\hat{\\rho}_{MQO} \\neq \\rho\\)), tendendo a superestimar a dependência espacial e enviesar os coeficientes \\(\\boldsymbol{\\beta}\\) (Anselin 1988).\nAs abordagens mais comuns para a estimação consistente e eficiente são:\n\nmáxima verossimilhança: Maximiza a função de verossimilhança que incorpora o log-determinante do Jacobiano da transformação, \\(\\ln|\\mathbf{I}_n - \\rho \\mathbf{W}|\\), para corrigir a simultaneidade.\nMétodo dos Momentos Generalizados (GMM) / Variáveis Instrumentais (VI): Utiliza defasagens espaciais das covariáveis (\\(\\mathbf{W}\\mathbf{X}, \\mathbf{W}^2\\mathbf{X}, \\dots\\)) como instrumentos válidos para \\(\\mathbf{W}\\mathbf{y}\\), uma estratégia robusta proposta por (H. H. Kelejian e Prucha 1998) .\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialreg, spdep, sf, texreg, knitr, kableExtra, dplyr)\n\n# Preparação dos Dados\nif (!exists(\"mg_dados\")) {\n  mg_dados &lt;- geobr::read_municipality(code_muni = \"MG\", year = 2020, showProgress = FALSE)\n  coords &lt;- st_coordinates(st_centroid(mg_dados))\n  set.seed(123)\n  mg_dados$taxa_bruta &lt;- (-coords[,2] * 10) + rnorm(nrow(mg_dados), 0, 5)\n  mg_dados$variavel_x &lt;- rnorm(nrow(mg_dados))\n}\n\n# Matriz de Pesos Espaciais (Padronizada por linha 'W' é o padrão para SAR)\nnb &lt;- poly2nb(mg_dados, queen = TRUE)\nlw &lt;- nb2listw(nb, style = \"W\", zero.policy = TRUE)\n\n# Ajuste do Modelo\n# A) OLS (Referência)\nmod_ols &lt;- lm(taxa_bruta ~ variavel_x, data = mg_dados)\n\n# SAR (Spatial Autoregressive Model) - Máxima Verossimilhança\n# A função lagsarlm ajusta do modelo SAR via ML\nmod_sar &lt;- lagsarlm(taxa_bruta ~ variavel_x, \n                    data = mg_dados, \n                    listw = lw)\n\n#\nmapa_vars &lt;- c(\n  \"(Intercept)\" = \"Intercepto\",\n  \"variavel_x\"  = \"Variável X\",\n  \"rho\"         = \"$\\\\rho$ (Dependência Espacial)\"\n)\n\n#\nmapa_gof &lt;- list(\n  list(\"raw\" = \"nobs\", \"clean\" = \"N\", \"fmt\" = 0),\n  list(\"raw\" = \"r.squared\", \"clean\" = \"$\\\\ R^2$\", \"fmt\" = 3),\n  list(\"raw\" = \"aic\", \"clean\" = \"AIC\", \"fmt\" = 1),\n  list(\"raw\" = \"logLik\", \"clean\" = \"Log Likelihood\", \"fmt\" = 1)\n)\n# Gerar a Tabela Principal\nmodelsummary(\n  list(\"OLS (Clássico)\" = mod_ols, \"SAR (Lag Espacial)\" = mod_sar),\n  coef_map = mapa_vars,      \n  gof_map = mapa_gof,      \n  estimate=\"{estimate} [{conf.low}, {conf.high}]\",\n  stars = c('*' = .05, '**' = .01, '***' = .001),\n  title = NULL, \n  output = \"kableExtra\", \n  escape = FALSE\n) %&gt;%\n  kable_styling(latex_options = c(\"HOLD_position\"), \n                full_width = FALSE, \n                position = \"center\") %&gt;%\n  row_spec(5, bold = TRUE) %&gt;% \n  as.character() %&gt;%\n  cat()\n\n\n\n\nTabela 4.4: Resultados da Estimação do Modelo SAR e Decomposição dos Efeitos. Estimativas [intervalo de confiança] e abaixo o respetivo erro padrão.\n\n\n\n\n\n\n\n\nOLS (Clássico)\n\n\nSAR (Lag Espacial)\n\n\n\n\n\n\nIntercepto\n\n\n8.973 [8.616, 9.330]\n\n\n5.751 [4.876, 6.626]\n\n\n\n\n\n\n(0.182)\n\n\n(0.446)\n\n\n\n\nVariável X\n\n\n0.058 [−0.306, 0.422]\n\n\n0.029 [−0.316, 0.374]\n\n\n\n\n\n\n(0.185)\n\n\n(0.176)\n\n\n\n\n\\(\\rho\\) (Dependência Espacial)\n\n\n\n\n0.359 [0.269, 0.449]\n\n\n\n\n\n\n\n\n(0.046)\n\n\n\n\nN\n\n\n853\n\n\n\n\n\n\n\\(\\ R^2\\)\n\n\n0.000\n\n\n\n\n\n\nAIC\n\n\n5275.0\n\n\n5212.9\n\n\n\n\nLog Likelihood\n\n\n−2634.5\n\n\n\n\n\n\n\n\n * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\nInterpretação\nA Tabela 4.4 para a comparação de modelos evidencia a superioridade de ajuste do modelo de SAR em detrimento do linear (OLS), como observa-se pela redução substancial no Critério de Informação de Akaike (AIC), que decresceu de \\(5275.0\\) no modelo clássico para \\(5212.9\\) na especificação espacial. Este ganho de ajuste é atribuído à significância do parâmetro de autoregressão espacial \\(\\rho\\) (\\(0.359\\); \\(IC_{95\\%} [0.269, 0.449]\\)), que confirma a existência de dependência espacial positiva na variável resposta.\nNão obstante a melhor adequação da estrutura espacial, a Tabela 4.4 de decomposição dos impactos demonstra que a covariável \\(X\\) não possui significância estatística. A análise dos efeitos marginais revela que a variável explicativa não exerce influência sobre o desfecho, apresentando valores-p não significativos tanto para o efeito direto (\\(0.030\\); \\(p=0.829\\)), que mensura o impacto local, quanto para o efeito indireto (\\(0.015\\); \\(p=0.837\\)), que mensura o transbordamento espacial. Conclui-se, portanto, que embora a modelagem da dependência espacial seja necessária para a correção do viés de especificação, a variável \\(X\\) não é um determinante estatisticamente relevante do fenômeno estudado, apresentando um efeito total nulo (\\(0.045\\); \\(p=0.831\\)).\nVocê poderia gerar formatação em latex e colar no seu overleaf ou latex.\n\nCódigo\ntexreg(\n  list(mod_ols, mod_sar),\n  custom.model.names = c(\"OLS (Clássico)\", \"SAR (Lag Espacial)\"),\n  custom.coef.names = c(\"Intercepto\", \"Variável X\", \"$\\\\rho$ (Autocorrelação)\"),\n  caption = \"Comparação de Modelos: OLS vs SAR\",\n  digits = 3,\n  booktabs = TRUE, \n  dcolumn = TRUE\n)\n\n\n\n\nTabela 4.5\n\n\n\n\n\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialreg, spdep, sf, ggplot2, dplyr, tidyr, patchwork, viridis, Matrix)\n\n# \nif (!exists(\"mod_sar\") || !exists(\"mg_dados\")) {\n  mg_dados &lt;- geobr::read_municipality(code_muni = \"MG\", year = 2020, showProgress = FALSE)\n  coords &lt;- st_coordinates(st_centroid(mg_dados))\n  set.seed(123)\n  mg_dados$taxa_bruta &lt;- (-coords[,2] * 10) + rnorm(nrow(mg_dados), 0, 5)\n  mg_dados$variavel_x &lt;- rnorm(nrow(mg_dados))\n  \n  nb &lt;- poly2nb(mg_dados, queen = TRUE)\n  lw &lt;- nb2listw(nb, style = \"W\", zero.policy = TRUE)\n  mod_sar &lt;- lagsarlm(taxa_bruta ~ variavel_x, data = mg_dados, listw = lw)\n}\n\n#Gráfico de Impactos (Direto vs Indireto Unificado)\n\nset.seed(123)\nimp_mc_plot &lt;- impacts(mod_sar, listw = lw, R = 1000)\n\nif (is.null(imp_mc_plot$smat) && is.null(imp_mc_plot$res)) {\n   imp_mc_plot &lt;- impacts(mod_sar, listw = lw, R = 1000, zstats = TRUE)\n}\n\n# Dataframe Direto e Indireto\ndf_impactos &lt;- data.frame(\n  direct = imp_mc_plot$res$direct,\n  indirect = imp_mc_plot$res$indirect\n) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Tipo\", values_to = \"Valor\") %&gt;%\n  mutate(Tipo = factor(Tipo, levels = c(\"direct\", \"indirect\"),\n                       labels = c(\"Direto\", \"Indireto (Spillover)\")))\n\n# \ng_impactos &lt;- ggplot(df_impactos, aes(x = Tipo, fill = Tipo, y=Valor)) +\n  geom_col(width = 0.2, color = \"gray30\") +\n  scale_fill_manual(values = c(\"Direto\" = \"#1b9e77\", \"Indireto (Spillover)\" = \"#d95f02\")) +\n  labs(title = \"A. Impactos: Direto vs. Indireto\", \n       y = \"Magnitude do efeito\", x = NULL) +\n  theme_minimal() + \n  theme(legend.position = \"none\", \n        legend.title = element_blank())\n\n# Mapa dos Valores Ajustados\n\nmg_dados$fitted_sar &lt;- fitted(mod_sar)\n\ng_fit &lt;- ggplot(mg_dados) +\n  geom_sf(aes(fill = fitted_sar), color = NA) +\n  scale_fill_viridis_c(option = \"turbo\", name = \"Predito\") +\n  labs(title = \"B. Valores Preditos (SAR)\", \n       subtitle = \"Padrão espacial recuperado\")+\n  theme_minimal() + \n  \n  annotation_scale(\n    location = \"bl\",           \n    width_hint = 0.3,          \n    bar_cols = c(\"black\", \"white\"), \n    text_family = \"sans\"       \n  ) +\n  \n  annotation_north_arrow(\n    location = \"tl\",           \n    which_north = \"true\",      \n    pad_x = unit(0.2, \"in\"),   \n    pad_y = unit(0.2, \"in\"),   \n    style = north_arrow_fancy_orienteering \n  )\n\n\n# Diagnóstico dos Resíduos\nmg_dados$resid_sar &lt;- residuals(mod_sar)\nmoran_res &lt;- moran.test(mg_dados$resid_sar, lw)\nmg_dados$resid_lag &lt;- lag.listw(lw, mg_dados$resid_sar)\n\ng_resid_scatter &lt;- ggplot(mg_dados, aes(x = resid_sar, y = resid_lag)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", size = 0.8) +\n  labs(title = \"C. Scatter de Moran (Resíduos)\", \n       subtitle = paste0(\"I de Moran: \", round(moran_res$estimate[1], 3), \n                         \" (p-valor: \", round(moran_res$p.value, 3), \")\"),\n       x = \"Resíduos\", y = \"Lag Espacial\") +\n  theme_minimal()\n\n\n# \n(g_impactos | g_fit | g_resid_scatter)\n\n\n\n\n\n\n\n\nFigura 4.12: Diagnóstico SAR: (A) Comparação Direto vs Indireto, (B) Ajuste do Modelo e (C) Análise de Resíduos.\n\n\n\n\n\nInterpretação\nO mapa de valores preditos em Figura 4.12 (B) mostra que o modelo foi capaz de recuperar a estrutura espacial do processo, reproduzindo a heterogeneidade regional e os padrões de aglomeração observados na variável dependente, uma contribuição atribuível quase exclusivamente ao termo de autoregressão espacial (\\(\\rho\\)) dada a nulidade da covariável \\(X\\). A validade dessa especificação é confirmada na Figura 4.12 (C), onde o grafico de dispersão para os resíduos de Moran não mostra nenhuma tendência. A estatística de Moran (\\(I = -0.015\\)) com valor-p não significativo (\\(0.735\\)) atesta a independência espacial dos resíduos.\n\n\n4.13.2 Modelo de Erro Espacial (SEM - Spatial Error Model)\nO Modelo de Erro Espacial (SEM) é um caso particular do Modelo Espacial Geral (GNS, Eq. 4.4) obtido ao se impor as restrições paramétricas \\(\\rho = 0\\) e \\(\\boldsymbol{\\theta} = \\mathbf{0}\\). Esta especificação pressupõe a ausência de interações espaciais diretas tanto na variável dependente quanto nas variáveis explicativas, concentrando toda a estrutura de dependência espacial no termo de erro estocástico. Formalmente, o modelo é definido pelo sistema de equações (Anselin 1988):\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{u}, \\qquad\n\\mathbf{u} = \\lambda \\mathbf{W}\\mathbf{u} + \\boldsymbol{\\epsilon},\n\\tag{4.6}\\]\ncom inovações i.i.d. \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)\\). A distribuição condicional de \\(\\mathbf{y}\\) é, portanto, uma normal multivariada:\n\\[\n\\mathbf{y} \\, | \\, \\mathbf{X}, \\mathbf{W} \\sim \\mathcal{N}\\left( \\mathbf{X}\\boldsymbol{\\beta}, \\;\n\\sigma^2 (\\mathbf{I}_n - \\lambda \\mathbf{W})^{-1} (\\mathbf{I}_n - \\lambda \\mathbf{W})^{-\\top} \\right).\n\\]\nNa forma escalar, para uma unidade de observação \\(i\\), o modelo se expressa como:\n\\[\ny_i = \\sum_{k=1}^K x_{ik}\\beta_k + u_i, \\qquad\nu_i = \\lambda \\sum_{j=1}^N w_{ij} u_j + \\epsilon_i, \\qquad\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2).\n\\]\nConforme destacado por J. LeSage e Pace (2009), a utilidade do SEM reside no tratamento da dependência espacial como um fenômeno residual, frequentemente resultante de variáveis omitidas com padrão espacial ou de erros de medição decorrentes de delimitações administrativas arbitrárias que não coincidem com os verdadeiros limites econômicos. Diferentemente dos modelos SAR (Eq. 4.5) e SDM (Eq. 4.8), que capturam interações espaciais substantivas, a estrutura do SEM implica que a dependência espacial é um ruído a ser controlado para inferência válida.\nA forma reduzida do processo gerador de dados é obtida substituindo a equação do erro na equação principal:\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{I}_n - \\lambda \\mathbf{W})^{-1}\\boldsymbol{\\epsilon},\n\\]\no que revela que erros (choques) aleatórios \\(\\boldsymbol{\\epsilon}\\) propagam-se globalmente através do multiplicador espacial inverso \\((\\mathbf{I}_n - \\lambda \\mathbf{W})^{-1}\\) (desde que este exista). Contudo, a esperança condicional da variável dependente mantém-se não espacial:\n\\[\n\\mathbb{E}[\\mathbf{y} \\, | \\, \\mathbf{X}, \\mathbf{W}] = \\mathbf{X}\\boldsymbol{\\beta}.\n\\]\nNo SEM, os coeficientes \\(\\boldsymbol{\\beta}\\) representam derivadas parciais verdadeiras, isto é, os efeitos marginais diretos das variáveis explicativas. Uma variação em \\(x_{ik}\\) afeta apenas \\(y_i\\), sem desencadear feedback ou spillovers espaciais na média das unidades vizinhas (Elhorst et al. 2014).\nDo ponto de vista da estimação, a imposição incorreta de \\(\\lambda = 0\\) e o uso de Mínimos Quadrados Ordinários (MQO) não comprometem a consistência dos estimadores pontuais de \\(\\boldsymbol{\\beta}\\), uma vez que a média dos erros permanece zero e ortogonal aos regressores. No entanto, essa estratégia produz estimadores ineficientes e, mais criticamente, enviesa a estimação da matriz de covariância dos parâmetros, invalidando a inferência baseada em testes \\(t\\) e \\(F\\) padrão (Anselin 1988).\nA estimação eficiente exige métodos que incorporem explicitamente a estrutura não esférica da matriz de covariância dos erros:\n\\[\n\\operatorname{Cov}[\\mathbf{y} \\, | \\, \\mathbf{X}, \\mathbf{W}] = \\boldsymbol{\\Omega} =\n\\sigma^2 \\left[ (\\mathbf{I}_n - \\lambda \\mathbf{W})^{\\top} (\\mathbf{I}_n - \\lambda \\mathbf{W}) \\right]^{-1}.\n\\]\nAs abordagens usuais para obter estimadores consistentes e eficientes são a máxima verossimilhança e o Método dos Momentos Generalizados (GMM), que permitem a recuperação adequada dos erros-padrão e a realização de inferência válida (H. H. Kelejian e Prucha 2010).\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialreg, spdep, sf, modelsummary, kableExtra, dplyr)\n\n# SEM (Spatial Error Model)\n# A função errorsarlm ajusta o SEM via Máxima Verossimilhança\nmod_sem &lt;- errorsarlm(taxa_bruta ~ variavel_x, data = mg_dados, listw = lw)\n\n#Tabela \nmapa_vars &lt;- c(\n  \"(Intercept)\" = \"Intercepto\",\n  \"variavel_x\"  = \"Variável X\",\n  \"rho\"         = \"$\\\\rho$ (Lag Espacial)\",  # Parâmetro do SAR\n  \"lambda\"      = \"$\\\\lambda$ (Erro Espacial)\" # Parâmetro do SEM\n)\n\nmapa_gof &lt;- list(\n  list(\"raw\" = \"nobs\", \"clean\" = \"N\", \"fmt\" = 0),\n  list(\"raw\" = \"r.squared\", \"clean\" = \"$R^2$\", \"fmt\" = 3),\n  list(\"raw\" = \"aic\", \"clean\" = \"AIC\", \"fmt\" = 1),\n  list(\"raw\" = \"logLik\", \"clean\" = \"Log Likelihood\", \"fmt\" = 1)\n)\n\n#Gerar a Tabela Comparativa (OLS, SAR, SEM)\nmodelsummary(\n  list(\n    \"OLS (Clássico)\" = mod_ols, \n    \"SAR\" = mod_sar, \n    \"SEM\" = mod_sem\n  ),\n  coef_map = mapa_vars,      \n  gof_map = mapa_gof,      \n  estimate = \"{estimate} [{conf.low}, {conf.high}]\",\n  statistic = NULL, \n  stars = c('*' = .05, '**' = .01, '***' = .001),\n  title = NULL,     \n  output = \"kableExtra\", \n  escape = FALSE  \n) %&gt;%\n  kable_styling(latex_options = c(\"HOLD_position\"), \n                full_width = FALSE, \n                position = \"center\") %&gt;%\n  row_spec(c(5, 7), bold = TRUE) %&gt;% \n  as.character() %&gt;%\n  cat()\n\n\n\n\nTabela 4.6: Resultados da Estimação: Comparação OLS, SAR e SEM. Estimativas [intervalo de confiança 95%].\n\n\n\n\n\n\n\n\nOLS (Clássico)\n\n\nSAR\n\n\nSEM\n\n\n\n\n\n\nIntercepto\n\n\n8.973 [8.616, 9.330]\n\n\n5.751 [4.876, 6.626]\n\n\n8.978 [8.449, 9.507]\n\n\n\n\nVariável X\n\n\n0.058 [−0.306, 0.422]\n\n\n0.029 [−0.316, 0.374]\n\n\n−0.001 [−0.342, 0.340]\n\n\n\n\n\\(\\rho\\) (Lag Espacial)\n\n\n\n\n0.359 [0.269, 0.449]\n\n\n\n\n\n\n\\(\\lambda\\) (Erro Espacial)\n\n\n\n\n\n\n0.359 [0.270, 0.449]\n\n\n\n\nN\n\n\n853\n\n\n\n\n\n\n\n\n\\(R^2\\)\n\n\n0.000\n\n\n\n\n\n\n\n\nAIC\n\n\n5275.0\n\n\n5212.9\n\n\n5213.0\n\n\n\n\nLog Likelihood\n\n\n−2634.5\n\n\n\n\n\n\n\n\n\n\n * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\nInterprete\nA Tabela 4.6 apresenta a análise comparativa entre os modelos de regressão linear (OLS), de defasagem espacial (SAR) e de erro espacial (SEM), evidenciando a superioridade de ajuste das especificações espaciais sobre a abordagem clássica. Observa-se uma melhoria substancial na qualidade do ajuste ao incorporar a dependência espacial, demonstrada pela redução do Critério de Informação de Akaike (AIC), que declinou de \\(5275.0\\) no OLS para valores idênticos de \\(5212.9\\) no SAR e \\(5213.0\\) no SEM. Essa equivalência entre os modelos espaciais é reforçada pela magnitude dos parâmetros estimados: tanto o coeficiente de defasagem espacial \\(\\rho\\) (\\(0.359\\); \\(IC_{95\\%} [0.269, 0.449]\\)) quanto o coeficiente de erro espacial \\(\\lambda\\) (\\(0.359\\); \\(IC_{95\\%} [0.270, 0.449]\\)) são estatisticamente significativos e apresentam estimativas pontuais coincidentes, indicando que a estrutura de autocorrelação independentemente da especificação funcional adotada. Adicionalmente, no modelo SEM o coeficiente estimado permaneceu estatisticamente nulo (\\(-0.001\\); \\(IC_{95\\%} [-0.342, 0.340]\\)), confirmando que a variabilidade do fenômeno é explicada predominantemente pela estrutura de dependência espacial e não pela variável preditora \\(X\\).\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialreg, spdep, sf, ggplot2, dplyr, tidyr, patchwork, viridis, Matrix, ggspatial)\n\n\nif (!exists(\"mod_sem\") || !exists(\"mg_dados\")) {\n  mg_dados &lt;- geobr::read_municipality(code_muni = \"MG\", year = 2020, showProgress = FALSE)\n  coords &lt;- st_coordinates(st_centroid(mg_dados))\n  set.seed(123)\n  mg_dados$taxa_bruta &lt;- (-coords[,2] * 10) + rnorm(nrow(mg_dados), 0, 5)\n  mg_dados$variavel_x &lt;- rnorm(nrow(mg_dados))\n  \n  nb &lt;- poly2nb(mg_dados, queen = TRUE)\n  lw &lt;- nb2listw(nb, style = \"W\", zero.policy = TRUE)\n\n  mod_sem &lt;- errorsarlm(taxa_bruta ~ variavel_x, data = mg_dados, listw = lw)\n}\n\n\n\n# Mapa dos Valores Ajustados \nmg_dados$fitted_sem &lt;- fitted(mod_sem)\n\ng_fit &lt;- ggplot(mg_dados) +\n  geom_sf(aes(fill = fitted_sem), color = NA) +\n  scale_fill_viridis_c(option = \"turbo\", name = \"Predito\") +\n  labs(title = \"A. Valores Preditos (SEM)\", \n       subtitle = \"Padrão recuperado (Erro Corrigido)\") +\n  theme_minimal() + \n  annotation_scale(\n    location = \"bl\",            \n    width_hint = 0.3,           \n    bar_cols = c(\"black\", \"white\"), \n    text_family = \"sans\"        \n  ) +\n  annotation_north_arrow(\n    location = \"tl\",            \n    which_north = \"true\",       \n    pad_x = unit(0.2, \"in\"),    \n    pad_y = unit(0.2, \"in\"),    \n    style = north_arrow_fancy_orienteering \n  )\n\n\n# Diagnóstico dos Resíduos\nmg_dados$resid_sem &lt;- residuals(mod_sem)\nmoran_res &lt;- moran.test(mg_dados$resid_sem, lw)\nmg_dados$resid_lag_sem &lt;- lag.listw(lw, mg_dados$resid_sem)\n\ng_resid_scatter &lt;- ggplot(mg_dados, aes(x = resid_sem, y = resid_lag_sem)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", size = 0.8) +\n  labs(title = \"B. Scatter de Moran (Resíduos SEM)\", \n       subtitle = paste0(\"I de Moran: \", round(moran_res$estimate[1], 3), \n                         \" (p-valor: \", round(moran_res$p.value, 3), \")\"),\n       x = \"Resíduos\", y = \"Lag Espacial\") +\n  theme_minimal()\n\n(g_fit | g_resid_scatter)\n\n\n\n\n\n\n\n\nFigura 4.13: Diagnóstico SEM: (A) Ajuste do Modelo e (B) Análise de Resíduos.\n\n\n\n\n\nInterpretação\nO mapeamento dos valores preditos em Figura 4.13 (A) demonstra a capacidade do modelo em capturar a variabilidade espacial latente, reproduzindo os gradientes regionais distintivos do fenômeno através da correção via termo \\(\\lambda\\). A robustez desta especificação é atestada em Figura 4.13 (B), onde a distribuição dos resíduos de Moran mostra ausência de associação espacial, evidenciada pela inclinação nula da reta de regressão e dispersão isotrópica. A estatística de I de Moran (\\(-0.014\\)) associada a um valor-p não significativo (\\(0.734\\)) confirma que a autocorrelação espacial foi integralmente absorvida pela estrutura de erro especificada, garantindo que os resíduos remanescentes se comportem como ruído branco estocástico.\n\n\n4.13.3 Modelo de Defasagem Espacial de X (SLX – Spatial Lag of X)\nO Modelo de Defasagem Espacial de X (SLX) representa a especificação mais parcimoniosa para incorporação de efeitos de interação espacial. Diferentemente dos modelos SAR (Eq. 4.5) e SEM (Eq. 4.6), que modelam a dependência através da variável resposta ou do termo de erro, o SLX postula que o resultado de uma unidade é influenciado tanto por suas características próprias quanto pelas características observáveis de suas unidades vizinhas.\nNa taxonomia do Modelo Geral Espacial (GNS, Eq. 4.4), o SLX é obtido ao impor as restrições paramétricas \\(\\rho = 0\\) e \\(\\lambda = 0\\). O modelo é especificado pela equação estrutural:\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{W}\\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon},\n\\tag{4.7}\\]\ncom erros independentes e identicamente distribuídos (i.i.d.) \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)\\).\nPara uma unidade de observação \\(i\\), o modelo expressa-se como:\n\\[\ny_i = \\alpha + \\sum_{k=1}^K x_{ik}\\beta_k + \\sum_{k=1}^K \\theta_k \\left( \\sum_{j=1}^n w_{ij} x_{jk} \\right) + \\epsilon_i.\n\\]\nonde: - \\(\\mathbf{W}\\mathbf{X}\\) denota a matriz de defasagens espaciais das variáveis explicativas, capturando o contexto espacial das covariáveis.\n\n\\(\\boldsymbol{\\beta} \\in \\mathbb{R}^{K}\\) é o vetor de parâmetros associado aos efeitos diretos das características da própria unidade \\(i\\).\n\\(\\boldsymbol{\\theta} \\in \\mathbb{R}^{K}\\) é o vetor de parâmetros associado aos efeitos de transbordamento exógenos (spillovers), capturando a influência das características das unidades vizinhas.\n\nHalleck Vega e Elhorst (2015), posiciona o SLX como um modelo de referência inicial para modelagem de dados espaciais (o primeiro a ser testado antes de ir aos mais complexos). Sua justificativa teórica reside na modelagem de transbordamentos locais (ou de primeira ordem), em contraste com a dependência global induzida pelo multiplicador espacial \\((\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\) do modelo SAR (Eq. 4.5) (J. LeSage e Pace 2009).\nA derivada parcial da esperança condicional \\(\\mathbb{E}[\\mathbf{y} \\, | \\, \\mathbf{X}, \\mathbf{W}]\\) em relação à \\(k\\)-ésima variável explicativa revela uma separação clara dos efeitos:\n\\[\n\\frac{\\partial \\, \\mathbb{E}[\\mathbf{y} \\, | \\, \\mathbf{X}, \\mathbf{W}]}{\\partial \\, \\mathbf{x}_k'} = \\beta_k \\mathbf{I}_n + \\theta_k \\mathbf{W}.\n\\]\nEsta expressão implica:\n\nEfeito Direto: \\(\\frac{\\partial y_i}{\\partial x_{ik}} = \\beta_k\\). Uma mudança em \\(x_{ik}\\) afeta \\(y_i\\) diretamente, sem mecanismos de retroalimentação (feedback).\nEfeito Indireto (Spillover Local): \\(\\frac{\\partial y_i}{\\partial x_{jk}} = \\theta_k w_{ij}\\). O impacto de uma mudança na unidade vizinha \\(j\\) sobre \\(y_i\\) é proporcional ao peso espacial \\(w_{ij}\\) e ao parâmetro \\(\\theta_k\\). Se \\(w_{ij} = 0\\), o efeito é nulo, caracterizando a localidade do transbordamento.\n\nUma limitação crítica dos modelos de dependência global, como o SAR, é que a razão entre efeitos indiretos e diretos é determinada unicamente pelo parâmetro \\(\\rho\\), sendo idêntica para todas as variáveis explicativas. O modelo SLX supera esta restrição, permitindo que a magnitude e até o sinal dos efeitos de transbordamento (\\(\\theta_k\\)) sejam estimados livremente para cada covariável \\(k\\). Esta flexibilidade permite testar hipóteses substantivas complexas, como a coexistência de externalidades positivas para uma variável e negativas para outra.\nSob os pressupostos clássicos de linearidade, exogeneidade de \\(\\mathbf{X}\\) e \\(\\mathbf{W}\\mathbf{X}\\), e erros esféricos, o modelo SLX apresenta propriedades estatísticas desejáveis:\n\nDado que todas as variáveis do lado direito da equação (\\(\\mathbf{X}\\) e \\(\\mathbf{W}\\mathbf{X}\\)) são exógenas, o estimador MQO de \\(\\boldsymbol{\\beta}\\) e \\(\\boldsymbol{\\theta}\\) é não viesado, consistente e eficiente (sob homocedasticidade). Esta é uma vantagem operacional, dispensando métodos computacionalmente intensivos como máxima verossimilhança (ML) (Anselin 1988).\nO modelo SLX minimiza problemas de multicolinearidade severa que podem surgir em modelos mais gerais (como o SDM ou GNS) devido à alta correlação entre \\(\\mathbf{W}\\mathbf{y}\\) e \\(\\mathbf{W}\\mathbf{X}\\).\nConforme descrito por Halleck Vega e Elhorst (2015), a estrutura do SLX simplifica o tratamento estatístico da possível endogeneidade das variáveis explicativas \\(\\mathbf{X}\\). Métodos padrão de Variáveis Instrumentais (VI) ou Mínimos Quadrados em Dois Estágios (MQ2E) podem ser aplicados diretamente para instrumentar \\(\\mathbf{X}\\) e \\(\\mathbf{W}\\mathbf{X}\\), sem a complexidade adicional introduzida pela endogeneidade de \\(\\mathbf{W}\\mathbf{y}\\) em modelos SAR.\nA simplicidade da função objetivo (soma de quadrados dos resíduos) no SLX permite a estimação conjunta dos parâmetros do modelo (\\(\\boldsymbol{\\beta}, \\boldsymbol{\\theta}\\)) e de parâmetros que definem a matriz de pesos espaciais \\(\\mathbf{W}\\). Por exemplo, pode-se especificar \\(w_{ij} = d_{ij}^{-\\gamma}\\) e estimar o parâmetro de decaimento \\(\\gamma\\) via Mínimos Quadrados Não Lineares (NLS), permitindo que a estrutura de interação espacial seja inferida diretamente dos dados.\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialreg, spdep, sf, modelsummary, kableExtra, dplyr, ggplot2, patchwork, viridis)\n\n\nif (!exists(\"mg_dados\")) {\n  mg_dados &lt;- geobr::read_municipality(code_muni = \"MG\", year = 2020, showProgress = FALSE)\n  coords &lt;- st_coordinates(st_centroid(mg_dados))\n  set.seed(123)\n  mg_dados$taxa_bruta &lt;- (-coords[,2] * 10) + rnorm(nrow(mg_dados), 0, 5)\n  mg_dados$variavel_x &lt;- rnorm(nrow(mg_dados))\n}\n\n\n# SLX (Spatial Lag of X)\n# lmSLX cria automaticamente as defasagens (lag.variavel_x)\nmod_slx &lt;- lmSLX(taxa_bruta ~ variavel_x, data = mg_dados, listw = lw)\n\n\nmapa_vars &lt;- c(\n  \"(Intercept)\"    = \"Intercepto\",\n  \"variavel_x\"     = \"Variável X (Direto)\",\n  \"lag.variavel_x\" = \"WX $\\\\theta$\", # SLX\n  \"rho\"            = \"$\\\\rho$ (Lag Espacial)\",       # SAR\n  \"lambda\"         = \"$\\\\lambda$ (Erro Espacial)\"    # SEM\n)\n\nmapa_gof &lt;- list(\n  list(\"raw\" = \"nobs\", \"clean\" = \"N\", \"fmt\" = 0),\n  list(\"raw\" = \"r.squared\", \"clean\" = \"$\\\\ R^2$\", \"fmt\" = 3),\n  list(\"raw\" = \"aic\", \"clean\" = \"AIC\", \"fmt\" = 1),\n  list(\"raw\" = \"logLik\", \"clean\" = \"Log Likelihood\", \"fmt\" = 1)\n)\n\n# \nmodelsummary(\n  list(\n    \"OLS\" = mod_ols, \n    \"SLX\" = mod_slx,\n    \"SAR\" = mod_sar, \n    \"SEM\" = mod_sem\n  ),\n  coef_map = mapa_vars,      \n  gof_map = mapa_gof,      \n  estimate = \"{estimate} [{conf.low}, {conf.high}]\",\n  statistic = NULL, \n  stars = c('*' = .05, '**' = .01, '***' = .001),\n  title = NULL,     \n  output = \"kableExtra\",\n  escape = FALSE \n) %&gt;%\n  kable_styling(latex_options = c(\"HOLD_position\"), \n                full_width = FALSE, \n                position = \"center\") %&gt;%\n  row_spec(c(5, 7, 9), bold = TRUE) %&gt;% \n  as.character() %&gt;%\n  cat()\n\n\n\n\nTabela 4.7: Resultados da Estimação: Comparação OLS, SLX, SAR e SEM. Estimativas [intervalo de confiança 95%].\n\n\n\n\n\n\n\n\nOLS\n\n\n SLX\n\n\nSAR\n\n\nSEM\n\n\n\n\n\n\nIntercepto\n\n\n8.973 [8.616, 9.330]\n\n\n8.970 [8.613, 9.327]\n\n\n5.751 [4.876, 6.626]\n\n\n8.978 [8.449, 9.507]\n\n\n\n\nVariável X (Direto)\n\n\n0.058 [−0.306, 0.422]\n\n\n0.056 [−0.308, 0.420]\n\n\n0.029 [−0.316, 0.374]\n\n\n−0.001 [−0.342, 0.340]\n\n\n\n\nWX \\(\\theta\\)\n\n\n\n\n0.397 [−0.396, 1.190]\n\n\n\n\n\n\n\n\n\\(\\rho\\) (Lag Espacial)\n\n\n\n\n\n\n0.359 [0.269, 0.449]\n\n\n\n\n\n\n\\(\\lambda\\) (Erro Espacial)\n\n\n\n\n\n\n\n\n0.359 [0.270, 0.449]\n\n\n\n\nN\n\n\n853\n\n\n853\n\n\n\n\n\n\n\n\n\\(\\ R^2\\)\n\n\n0.000\n\n\n0.001\n\n\n\n\n\n\n\n\nAIC\n\n\n5275.0\n\n\n5276.1\n\n\n5212.9\n\n\n5213.0\n\n\n\n\nLog Likelihood\n\n\n−2634.5\n\n\n−2634.0\n\n\n\n\n\n\n\n\n\n\n * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nCódigo\n# Extraindo coeficientes e intervalos de confiança do SLX\ncoefs &lt;- coef(mod_slx)\ncis   &lt;- confint(mod_slx)\n\n\n# Mapa dos Valores Ajustados\nmg_dados$fitted_slx &lt;- fitted(mod_slx)\n\ng_fit &lt;- ggplot(mg_dados) +\n  geom_sf(aes(fill = fitted_slx), color = NA) +\n  scale_fill_viridis_c(option = \"turbo\", name = \"Predito\") +\n  labs(title = \"A. Valores Preditos (SLX)\", \n       subtitle = \"Ajuste com defasagens de X\") +\n  theme_minimal() + \n  annotation_scale(location = \"bl\", width_hint = 0.3) +\n  annotation_north_arrow(location = \"tl\", style = north_arrow_fancy_orienteering,\n                         pad_x = unit(0.1, \"in\"), pad_y = unit(0.1, \"in\"))\n\n# Diagnóstico dos Resíduos (Scatter de Moran)\nmg_dados$resid_slx &lt;- residuals(mod_slx)\nmoran_slx &lt;- moran.test(mg_dados$resid_slx, lw)\nmg_dados$resid_lag_slx &lt;- lag.listw(lw, mg_dados$resid_slx)\n\ng_resid_scatter &lt;- ggplot(mg_dados, aes(x = resid_slx, y = resid_lag_slx)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", size = 0.8) +\n  labs(title = \"B. Scatter de Moran (Resíduos SLX)\", \n       subtitle = paste0(\"I de Moran: \", round(moran_slx$estimate[1], 3), \n                         \" (p-valor: \", round(moran_slx$p.value, 3), \")\"),\n       x = \"Resíduos\", y = \"Lag Espacial\") +\n  theme_minimal()\n\n\n( g_fit | g_resid_scatter)\n\n\n\n\n\n\n\n\nFigura 4.14: Diagnóstico SLX: (A) Ajuste do Modelo e (B) Análise de Resíduos.\n\n\n\n\n\nInterpretação\nAo contrário dos modelos globais (SAR e SEM), o mapa de valores preditos em Figura 4.14 (A) exibe uma superfície de predição com variabilidade atenuada, falhando em reproduzir a heterogeneidade e os clusters de valores altos observados nos dados originais. A inadequação do ajuste é demonstrada definitivamente em Figura 4.14 (C), onde o Scatter de Moran dos resíduos mostra tendência linear positiva, indicando que o modelo não foi capaz de capturar a estrutura espacial dos dados. A estatística de I de Moran (\\(I = 0.198\\)) com valor-p significativo (\\(p \\approx 0\\)) confirma a persistência de autocorrelação espacial positiva nos erros, violando o pressuposto de independência estocástica e evidenciando que a simples inclusão de defasagens exógenas de \\(X\\) é ineficaz para capturar a dependência espacial presente no fenômeno.\n\n\n4.13.4 Modelo Espacial de Durbin (SDM – Spatial Durbin Model)\nO Modelo Espacial de Durbin (SDM) constitui uma especificação geral que unifica os mecanismos de dependência espacial endógena e efeitos contextuais exógenos. Dentro da família do Modelo Geral Espacial (GNS, Eq. 4.4), o SDM é obtido ao impor a restrição \\(\\lambda = 0\\), mantendo os parâmetros \\(\\rho\\) e \\(\\boldsymbol{\\theta}\\) livres. Esta estrutura postula que o valor observado para uma unidade \\(i\\) é uma função simultânea: (i) dos valores da variável resposta em suas unidades vizinhas, (ii) de suas próprias características observadas, e (iii) das características observadas de suas vizinhas.\nFormalmente, o modelo é definido pela equação estrutural (Anselin 1988; J. LeSage e Pace 2009):\n\\[\n\\mathbf{y} = \\rho \\mathbf{W}\\mathbf{y} + \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{W}\\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon},\n\\tag{4.8}\\]\ncom o vetor de erros assumido como \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)\\). Na sua forma escalar para uma unidade \\(i\\):\n\\[\ny_i = \\rho \\sum_{j=1}^n w_{ij} y_j + \\sum_{k=1}^K x_{ik}\\beta_k + \\sum_{k=1}^K \\theta_k \\left( \\sum_{j=1}^n w_{ij} x_{jk} \\right) + \\epsilon_i.\n\\]\nO SDM é frequentemente defendido como uma especificação de partida robusta em modelagem espacial (J. LeSage e Pace 2009). Sua principal justificativa reside em sua capacidade de mitigar potenciais tendências de especificação (omitted variable bias). Se o processo gerador de dados subjacente envolver fatores não observados que são espacialmente correlacionados e relacionados a \\(\\mathbf{X}\\), a omissão desses fatores induzirá dependência espacial nos resíduos. A inclusão do termo \\(\\mathbf{W}\\mathbf{X}\\) no SDM pode absorver parte desta correlação espacial omitida, produzindo estimativas mais estáveis para \\(\\boldsymbol{\\beta}\\).\nUma ligação fundamental na teoria dos modelos espaciais é que o modelo SEM (Eq. 4.6) é um caso restrito do SDM. Esta relação é estabelecida pela hipótese de fator comom (Common Factor Hypothesis) (Burridge 1981; Anselin 1988). A restrição não-linear \\(\\boldsymbol{\\theta} = -\\rho \\boldsymbol{\\beta}\\), quando válida, reduz a forma reduzida do SDM à do SEM. Consequentemente, estimar um SDM permite testar estatisticamente se a dependência espacial é de natureza substancial (capturada por \\(\\rho\\) e \\(\\boldsymbol{\\theta}\\)) ou meramente residual (um artefato de erros correlacionados, como no SEM).\nA presença simultânea do termo autorregressivo \\(\\mathbf{W}\\mathbf{y}\\) confere ao SDM uma estrutura de dependência global. A forma reduzida do modelo, obtida isolando \\(\\mathbf{y}\\), é:\n\\[\n\\mathbf{y} = (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1} (\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{W}\\mathbf{X}\\boldsymbol{\\theta}) + (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1} \\boldsymbol{\\epsilon}.\n\\]\nA matriz \\((\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\) atua como um multiplicador espacial global, propagando choques e efeitos através de toda a rede de interconexões (matriz de pesos / de vizinhança).\nA interpretação direta dos coeficientes \\(\\boldsymbol{\\beta}\\) e \\(\\boldsymbol{\\theta}\\) é limitada, pois não representam efeitos marginais simples. Seguindo J. LeSage e Pace (2009), a matriz de derivadas parciais da esperança condicional \\(\\mathbb{E}[\\mathbf{y} \\, | \\, \\mathbf{X}, \\mathbf{W}]\\) em relação a uma variável explicativa \\(k\\) fornece a decomposição completa dos efeitos:\n\\[\n\\frac{\\partial \\, \\mathbb{E}[\\mathbf{y} \\, | \\, \\mathbf{X}, \\mathbf{W}]}{\\partial \\, \\mathbf{x}_k'} = \\mathbf{S}_k(\\mathbf{W}) = (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1} (\\beta_k \\mathbf{I}_n + \\theta_k \\mathbf{W}).\n\\]\nA partir desta matriz \\(\\mathbf{S}_k(\\mathbf{W})\\) de dimensão \\(n \\times n\\), calculam-se os seguintes impactos médios:\n\nEfeito direto médio, que é a média dos elementos da diagonal de \\(\\mathbf{S}_k(\\mathbf{W})\\). Mede o impacto esperado de uma mudança em \\(x_{ik}\\) sobre o próprio \\(y_i\\), incluindo todos os feedbacks espaciais que retornam à unidade \\(i\\).\nEfeito indireto médio (ou de transbordamento), que é média da soma dos elementos fora da diagonal para cada linha (ou coluna) de \\(\\mathbf{S}_k(\\mathbf{W})\\). Mede o impacto esperado de uma mudança em \\(x_{ik}\\) sobre todas as outras unidades \\(y_j\\) (\\(j \\neq i\\)).\nEfeito total médio que é a soma do efeito direto e do efeito indireto, equivalente ao impacto médio de uma mudança simultânea em \\(x_{k}\\) para todas as unidades.\n\nAo contrário do modelo SAR, onde a razão entre efeitos indiretos e diretos é idêntica para todas as covariáveis, no SDM esta razão é única para cada variável \\(k\\), determinada pela combinação específica de \\(\\rho\\), \\(\\beta_k\\) e \\(\\theta_k\\) (Halleck Vega e Elhorst 2015).\nA presença da variável dependente defasada espacialmente (\\(\\mathbf{W}\\mathbf{y}\\)) no lado direito da equação introduz simultaneidade, tornando o estimador de Mínimos Quadrados Ordinários (MQO) viesado e inconsistente. Métodos de estimação consistentes são necessários:\n\nmáxima verossimilhança, que maximiza a função de log-verossimilhança condicional, incorporando o termo \\(\\ln |\\mathbf{I}_n - \\rho \\mathbf{W}|\\) para corrigir a simultaneidade (Anselin 1988).\nAbordagem Bayesiana (MCMC), na qual J. P. LeSage (1997) desenvolveu um procedimento de inferência via Monte Carlo via Cadeias de Markov (MCMC). Esta abordagem é particularmente útil para realizar inferência sobre os impactos médios (diretos, indiretos e totais), que são funções não-lineares dos parâmetros, permitindo a construção direta de intervalos de credibilidade.\n\nAssim, o SDM serve como um modelo geral que aninha especificações mais simples: o SAR (\\(\\boldsymbol{\\theta} = \\mathbf{0}\\)), o SLX (\\(\\rho = 0\\)), e o SEM (sob a restrição \\(\\boldsymbol{\\theta} = -\\rho \\boldsymbol{\\beta}\\)). Por esta razão, uma estratégia de modelagem do geral para o específico, começando pelo SDM, é frequentemente recomendada para a análise de dados espaciais (Elhorst et al. 2014).\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialreg, spdep, sf, modelsummary, kableExtra, dplyr, ggplot2, patchwork, viridis, ggspatial, tidyr, Matrix)\n\n# 1. Preparação dos Dados\nif (!exists(\"mg_dados\")) {\n  mg_dados &lt;- geobr::read_municipality(code_muni = \"MG\", year = 2020, showProgress = FALSE)\n  coords &lt;- st_coordinates(st_centroid(mg_dados))\n  set.seed(123)\n  mg_dados$taxa_bruta &lt;- (-coords[,2] * 10) + rnorm(nrow(mg_dados), 0, 5)\n  mg_dados$variavel_x &lt;- rnorm(nrow(mg_dados))\n}\n\n\n# SDM (Spatial Durbin Model)\n# type = \"mixed\" inclui lag de Y (rho) e lag de X (theta)\nmod_sdm &lt;- lagsarlm(taxa_bruta ~ variavel_x, data = mg_dados, listw = lw, type = \"mixed\")\n\n#Tabela\nmapa_vars &lt;- c(\n  \"(Intercept)\"    = \"Intercepto\",\n  \"variavel_x\"     = \"$\\\\beta$\",\n  \"lag.variavel_x\" = \"WX $\\\\theta$\", # Compartilhado por SLX e SDM\n  \"rho\"            = \"$\\\\rho$\",       # Compartilhado por SAR e SDM\n  \"lambda\"         = \"$\\\\lambda$\"    # Exclusivo do SEM\n)\n\nmapa_gof &lt;- list(\n  list(\"raw\" = \"nobs\", \"clean\" = \"N\", \"fmt\" = 0),\n  list(\"raw\" = \"r.squared\", \"clean\" = \"$R^2$\", \"fmt\" = 3),\n  list(\"raw\" = \"aic\", \"clean\" = \"AIC\", \"fmt\" = 1),\n  list(\"raw\" = \"logLik\", \"clean\" = \"Log Likelihood\", \"fmt\" = 1)\n)\n\n# Tabela Unificada\nmodelsummary(\n  list(\n    \"OLS\" = mod_ols, \n    \"SLX\" = mod_slx,\n    \"SAR\" = mod_sar, \n    \"SEM\" = mod_sem,\n    \"SDM\" = mod_sdm\n  ),\n  coef_map = mapa_vars,      \n  gof_map = mapa_gof,      \n  estimate = \"{estimate} [{conf.low}, {conf.high}]\",\n  statistic = NULL, \n  stars = c('*' = .05, '**' = .01, '***' = .001),\n  title = NULL,     \n  output = \"kableExtra\",\n  escape = FALSE\n) %&gt;%\n  kable_styling(latex_options = c(\"HOLD_position\"), \n                full_width = FALSE, \n                position = \"center\") %&gt;%\n  row_spec(c(5, 7, 9), bold = TRUE) %&gt;% \n  as.character() %&gt;%\n  cat()\n\n\n\n\nTabela 4.8: Resultados da Estimação: Comparação Geral (OLS, SLX, SAR, SEM, SDM). Estimativas [IC 95%].\n\n\n\n\n\n\n\n\nOLS\n\n\n SLX\n\n\nSAR\n\n\nSEM\n\n\nSDM\n\n\n\n\n\n\nIntercepto\n\n\n8.973 [8.616, 9.330]\n\n\n8.970 [8.613, 9.327]\n\n\n5.751 [4.876, 6.626]\n\n\n8.978 [8.449, 9.507]\n\n\n5.748 [4.873, 6.622]\n\n\n\n\n\\(\\beta\\)\n\n\n0.058 [−0.306, 0.422]\n\n\n0.056 [−0.308, 0.420]\n\n\n0.029 [−0.316, 0.374]\n\n\n−0.001 [−0.342, 0.340]\n\n\n0.027 [−0.318, 0.372]\n\n\n\n\nWX \\(\\theta\\)\n\n\n\n\n0.397 [−0.396, 1.190]\n\n\n\n\n\n\n0.391 [−0.361, 1.143]\n\n\n\n\n\\(\\rho\\)\n\n\n\n\n\n\n0.359 [0.269, 0.449]\n\n\n\n\n0.359 [0.270, 0.449]\n\n\n\n\n\\(\\lambda\\)\n\n\n\n\n\n\n\n\n0.359 [0.270, 0.449]\n\n\n\n\n\n\nN\n\n\n853\n\n\n853\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\)\n\n\n0.000\n\n\n0.001\n\n\n\n\n\n\n\n\n\n\nAIC\n\n\n5275.0\n\n\n5276.1\n\n\n5212.9\n\n\n5213.0\n\n\n5213.9\n\n\n\n\nLog Likelihood\n\n\n−2634.5\n\n\n−2634.0\n\n\n\n\n\n\n\n\n\n\n\n\n * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nCódigo\n# Mapa dos Valores Ajustados\nmg_dados$fitted_sdm &lt;- fitted(mod_sdm)\n\ng_fit &lt;- ggplot(mg_dados) +\n  geom_sf(aes(fill = fitted_sdm), color = NA) +\n  scale_fill_viridis_c(option = \"turbo\", name = \"Predito\") +\n  labs(title = \"A. Valores Preditos (SDM)\", \n       subtitle = \"Ajuste considerando Wy e WX\") +\n  theme_minimal() + \n  annotation_scale(location = \"bl\", width_hint = 0.3) +\n  annotation_north_arrow(location = \"tl\", style = north_arrow_fancy_orienteering,\n                         pad_x = unit(0.1, \"in\"), pad_y = unit(0.1, \"in\"))\n\n#Diagnóstico dos Resíduos\nmg_dados$resid_sdm &lt;- residuals(mod_sdm)\nmoran_sdm &lt;- moran.test(mg_dados$resid_sdm, lw)\nmg_dados$resid_lag_sdm &lt;- lag.listw(lw, mg_dados$resid_sdm)\n\ng_resid_scatter &lt;- ggplot(mg_dados, aes(x = resid_sdm, y = resid_lag_sdm)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", size = 0.8) +\n  labs(title = \"C. Scatter de Moran (Resíduos SDM)\", \n       subtitle = paste0(\"I de Moran: \", round(moran_sdm$estimate[1], 3), \n                         \" (p-valor: \", round(moran_sdm$p.value, 3), \")\"),\n       x = \"Resíduos\", y = \"Lag Espacial\") +\n  theme_minimal()\n\ng_fit | g_resid_scatter\n\n\n\n\n\n\n\n\nFigura 4.15: Diagnóstico SDM: (A) Densidade dos Impactos Globais, (B) Ajuste e (C) Resíduos.\n\n\n\n\n\n\n\n4.13.5 Modelo Espacial de Durbin com Erro (SDEM – Spatial Durbin Error Model)\nO Modelo Espacial de Durbin com Erro (SDEM) é uma especificação híbrida que combina a modelagem de efeitos contextuais exógenos locais com a correção para dependência espacial residual de natureza global. No contexto do Modelo Geral Espacial (GNS, Eq. 4.4), o SDEM é obtido ao impor a restrição paramétrica \\(\\rho = 0\\), mantendo livres os parâmetros \\(\\boldsymbol{\\theta}\\) e \\(\\lambda\\).\nEsta estrutura pressupõe que o valor da variável dependente em uma unidade \\(i\\) é explicado por: (1) suas próprias características observadas, (2) as características observadas de suas unidades vizinhas, e (3) um termo de erro que exibe autocorrelação espacial. O modelo é definido pelo seguinte sistema de equações (Anselin 1988; Elhorst et al. 2014):\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{W}\\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{u}, \\quad \\mathbf{u} = \\lambda \\mathbf{W}\\mathbf{u} + \\boldsymbol{\\epsilon},\n\\tag{4.9}\\] com erros independentes e identicamente distribuídos \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)\\).\nNa sua forma escalar, para uma unidade \\(i\\):\n\\[\ny_i = \\sum_{k=1}^K x_{ik}\\beta_k + \\sum_{k=1}^K \\theta_k \\left( \\sum_{j=1}^n w_{ij} x_{jk} \\right) + u_i, \\quad u_i = \\lambda \\sum_{j=1}^n w_{ij} u_j + \\epsilon_i.\n\\]\nO SDEM tem sido destacado na literatura como uma alternativa robusta ao Modelo de Durbin Espacial (SDM), especialmente quando a teoria subjacente sugere que os mecanismos de interação substantiva têm alcance geográfico limitado (Halleck Vega e Elhorst 2015). A especificação separa duas fontes de dependência espacial:\n\nA ausência do termo autorregressivo \\(\\rho \\mathbf{W}\\mathbf{y}\\) implica que os spillovers substantivos são estritamente locais. Uma mudança nas características \\(\\mathbf{x}_j\\) de uma unidade \\(j\\) afeta a unidade \\(i\\) apenas se \\(w_{ij} \\neq 0\\), ou seja, se as unidades forem vizinhas diretas na estrutura definida por \\(\\mathbf{W}\\). Não há mecanismos de feedback ou propagação global destes efeitos através da rede (matriz de pesos / de vizinhança).\nO parâmetro \\(\\lambda\\) captura a autocorrelação espacial nos resíduos, tipicamente atribuída a variáveis omitidas com padrão espacial ou a choques não observados que se difundem regionalmente. Diferentemente dos efeitos na média, estes choques propagam-se globalmente através do multiplicador espacial \\((\\mathbf{I}_n - \\lambda \\mathbf{W})^{-1}\\) no termo de erro.\n\nEsta separação é teoricamente atraente em aplicações onde se espera que externalidades de política ou características de vizinhança (\\(\\mathbf{W}\\mathbf{X}\\)) tenham alcance limitado (ex.: poluição sonora, sombreamento), enquanto fatores de confusão não observados (ex.:, normas culturais, clima) exibam um padrão de correlação espacial de longo alcance (J. LeSage e Pace 2009).\nA forma reduzida do modelo é obtida ao substituir a estrutura do erro na Eq. 4.9:\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{W}\\mathbf{X}\\boldsymbol{\\theta} + (\\mathbf{I}_n - \\lambda \\mathbf{W})^{-1} \\boldsymbol{\\epsilon}.\n\\]\nA esperança condicional da variável dependente, \\(\\mathbb{E}[\\mathbf{y} \\, | \\, \\mathbf{X}, \\mathbf{W}]\\), é dada por:\n\\[\n\\mathbb{E}[\\mathbf{y} \\, | \\, \\mathbf{X}, \\mathbf{W}] = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{W}\\mathbf{X}\\boldsymbol{\\theta}.\n\\]\nCrucialmente, a ausência do multiplicador global \\((\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\) na parte determinística simplifica radicalmente a interpretação dos coeficientes. A matriz de derivadas parciais é linear e diretamente obtida dos parâmetros estimados:\n\\[\n\\frac{\\partial \\, \\mathbb{E}[\\mathbf{y} \\, | \\, \\mathbf{X}, \\mathbf{W}]}{\\partial \\, \\mathbf{x}_k'} = \\beta_k \\mathbf{I}_n + \\theta_k \\mathbf{W}.\n\\]\nDisto decorre uma decomposição dos efeitos:\n\nEfeito direto, onde \\(\\frac{\\partial y_i}{\\partial x_{ik}} = \\beta_k\\) representa o impacto de uma mudança na característica própria \\(x_{ik}\\) sobre o resultado \\(y_i\\). Este efeito é idêntico ao de uma regressão clássica, sem feedback espacial.\nEfeito indireto (ou de Spillover local), onde \\(\\frac{\\partial y_i}{\\partial x_{jk}} = \\theta_k w_{ij}\\) representa o impacto de uma mudança na característica de uma unidade vizinha \\(j\\) sobre o resultado da unidade \\(i\\). Se \\(w_{ij} = 0\\) (unidades não conectadas), o efeito é nulo.\n\nEmbora a equação da média contenha apenas regressores exógenos (\\(\\mathbf{X}\\) e \\(\\mathbf{W}\\mathbf{X}\\)), a presença de autocorrelação espacial no erro (\\(\\lambda \\neq 0\\)) viola o pressuposto de esfericidade. Consequentemente, o estimador de Mínimos Quadrados Ordinários (MQO) permanece não viesado e consistente para \\(\\boldsymbol{\\beta}\\) e \\(\\boldsymbol{\\theta}\\), mas se torna ineficiente. Mais criticamente, a estimativa da matriz de covariância dos parâmetros e, portanto, os erros-padrão e testes de hipótese padrão (testes \\(t\\), \\(F\\)) tornam-se inválidos (Anselin 1988).\nA estimação eficiente e a inferência válida exigem métodos que incorporem explicitamente a estrutura de covariância não esférica dos erros, \\(\\operatorname{Cov}(\\mathbf{u}) = \\sigma^2 [(\\mathbf{I}_n - \\lambda \\mathbf{W})^{\\top}(\\mathbf{I}_n - \\lambda \\mathbf{W})]^{-1}\\). As abordagens padrão são:\n\nmáxima verossimilhança, que maximiza a função de log-verossimilhança que inclui o termo \\(\\ln |\\mathbf{I}_n - \\lambda \\mathbf{W}|\\) associado à transformação do erro.\nMétodo dos Momentos Generalizados (GMM) que utiliza os momentos dos resíduos para estimar \\(\\lambda\\) de forma consistente, conforme proposto por H. H. Kelejian e Prucha (1999) para modelos com erros espaciais.\n\nO SDEM é, portanto, uma ferramenta valiosa quando a teoria apoia a existência de spillovers contextuais locais, mas é necessário controlar rigorosamente a heterogeneidade espacial não observada de longo alcance. Conforme recomendado por Elhorst et al. (2014), este modelo é preferível ao SDM quando testes de especificação rejeitam a dependência espacial na variável dependente (rejeitam \\(\\rho \\neq 0\\) em favor de \\(\\rho = 0\\)), mas indicam a presença simultânea de efeitos contextuais (\\(\\boldsymbol{\\theta} \\neq \\mathbf{0}\\)) e dependência residual nos erros (\\(\\lambda \\neq 0\\)).\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialreg, spdep, sf, modelsummary, kableExtra, dplyr, ggplot2, patchwork, viridis, ggspatial, tidyr, Matrix)\n\n\nif (!exists(\"mg_dados\")) {\n  mg_dados &lt;- geobr::read_municipality(code_muni = \"MG\", year = 2020, showProgress = FALSE)\n  coords &lt;- st_coordinates(st_centroid(mg_dados))\n  set.seed(123)\n  mg_dados$taxa_bruta &lt;- (-coords[,2] * 10) + rnorm(nrow(mg_dados), 0, 5)\n  mg_dados$variavel_x &lt;- rnorm(nrow(mg_dados))\n}\n\n\n# SDEM (Spatial Durbin Error Model)\n# errorsarlm com Durbin=TRUE inclui WX (theta) e lambda (erro)\nmod_sdem &lt;- errorsarlm(taxa_bruta ~ variavel_x, data = mg_dados, listw = lw, Durbin = TRUE)\n\n#Configuração da Tabela\nmapa_vars &lt;- c(\n  \"(Intercept)\"    = \"Intercepto\",\n  \"variavel_x\"     = \"$\\\\beta$\",\n  \"lag.variavel_x\" = \"WX $\\\\theta$\",      # Theta (SLX, SDM, SDEM)\n  \"rho\"            = \"$\\\\rho$\",      # Rho (SAR, SDM)\n  \"lambda\"         = \"$\\\\lambda$\"     # Lambda (SEM, SDEM)\n)\n\nmapa_gof &lt;- list(\n  list(\"raw\" = \"nobs\", \"clean\" = \"N\", \"fmt\" = 0),\n  list(\"raw\" = \"r.squared\", \"clean\" = \"$R^2$\", \"fmt\" = 3),\n  list(\"raw\" = \"aic\", \"clean\" = \"AIC\", \"fmt\" = 1),\n  list(\"raw\" = \"logLik\", \"clean\" = \"Log Likelihood\", \"fmt\" = 1)\n)\n\n# Tabela Unificada (6 Modelos)\nmodelsummary(\n  list(\n    \"OLS\"  = mod_ols, \n    \"SLX\"  = mod_slx,\n    \"SAR\"  = mod_sar, \n    \"SEM\"  = mod_sem,\n    \"SDM\"  = mod_sdm,\n    \"SDEM\" = mod_sdem\n  ),\n  coef_map = mapa_vars,      \n  gof_map = mapa_gof,      \n  estimate = \"{estimate} [{conf.low}, {conf.high}]\",\n  statistic = NULL, \n  stars = c('*' = .05, '**' = .01, '***' = .001),\n  title = NULL,    \n  output = \"kableExtra\", \n  escape = FALSE\n) %&gt;%\n  kable_styling(latex_options = c(\"HOLD_position\"), \n                full_width = FALSE, \n                position = \"center\") %&gt;%\n  row_spec(c(5, 7, 9), bold = TRUE) %&gt;% \n  as.character() %&gt;%\n  cat()\n\n\n\n\nTabela 4.9: Resultados da Estimação: Comparação Geral (OLS, SLX, SAR, SEM, SDM, SDEM). Estimativas [IC 95%].\n\n\n\n\n\n\n\n\nOLS\n\n\n SLX\n\n\nSAR\n\n\nSEM\n\n\nSDM\n\n\nSDEM\n\n\n\n\n\n\nIntercepto\n\n\n8.973 [8.616, 9.330]\n\n\n8.970 [8.613, 9.327]\n\n\n5.751 [4.876, 6.626]\n\n\n8.978 [8.449, 9.507]\n\n\n5.748 [4.873, 6.622]\n\n\n8.974 [8.444, 9.503]\n\n\n\n\n\\(\\beta\\)\n\n\n0.058 [−0.306, 0.422]\n\n\n0.056 [−0.308, 0.420]\n\n\n0.029 [−0.316, 0.374]\n\n\n−0.001 [−0.342, 0.340]\n\n\n0.027 [−0.318, 0.372]\n\n\n0.060 [−0.297, 0.417]\n\n\n\n\nWX \\(\\theta\\)\n\n\n\n\n0.397 [−0.396, 1.190]\n\n\n\n\n\n\n0.391 [−0.361, 1.143]\n\n\n0.496 [−0.366, 1.359]\n\n\n\n\n\\(\\rho\\)\n\n\n\n\n\n\n0.359 [0.269, 0.449]\n\n\n\n\n0.359 [0.270, 0.449]\n\n\n\n\n\n\n\\(\\lambda\\)\n\n\n\n\n\n\n\n\n0.359 [0.270, 0.449]\n\n\n\n\n0.360 [0.270, 0.450]\n\n\n\n\nN\n\n\n853\n\n\n853\n\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\)\n\n\n0.000\n\n\n0.001\n\n\n\n\n\n\n\n\n\n\n\n\nAIC\n\n\n5275.0\n\n\n5276.1\n\n\n5212.9\n\n\n5213.0\n\n\n5213.9\n\n\n5213.7\n\n\n\n\nLog Likelihood\n\n\n−2634.5\n\n\n−2634.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nCódigo\n# Mapa dos Valores Ajustados\nmg_dados$fitted_sdem &lt;- fitted(mod_sdem)\n\ng_fit &lt;- ggplot(mg_dados) +\n  geom_sf(aes(fill = fitted_sdem), color = NA) +\n  scale_fill_viridis_c(option = \"turbo\", name = \"Predito\") +\n  labs(title = \"A. Valores Preditos (SDEM)\", \n       subtitle = \"Padrão recuperado (WX + Erro)\") +\n  theme_minimal() + \n  annotation_scale(location = \"bl\", width_hint = 0.3, bar_cols = c(\"black\", \"white\")) +\n  annotation_north_arrow(location = \"tl\", which_north = \"true\", \n                         pad_x = unit(0.2, \"in\"), pad_y = unit(0.2, \"in\"),\n                         style = north_arrow_fancy_orienteering)\n\n# Diagnóstico dos Resíduos\nmg_dados$resid_sdem &lt;- residuals(mod_sdem)\nmoran_sdem &lt;- moran.test(mg_dados$resid_sdem, lw)\nmg_dados$resid_lag_sdem &lt;- lag.listw(lw, mg_dados$resid_sdem)\n\ng_resid_scatter &lt;- ggplot(mg_dados, aes(x = resid_sdem, y = resid_lag_sdem)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", size = 0.8) +\n  labs(title = \"B. Scatter de Moran (Resíduos SDEM)\", \n       subtitle = paste0(\"I de Moran: \", round(moran_sdem$estimate[1], 3), \n                         \" (p-valor: \", round(moran_sdem$p.value, 3), \")\"),\n       x = \"Resíduos\", y = \"Lag Espacial\") +\n  theme_minimal()\n\n\n(g_fit | g_resid_scatter)\n\n\n\n\n\n\n\n\nFigura 4.16: Diagnóstico SDEM: (A) Impactos Locais (Sem Feedback Global), (B) Ajuste e (C) Resíduos.\n\n\n\n\n\n\n\n4.13.6 Modelo Autorregressivo Espacial com Erros Autorregressivos (SARAR-Spatial Autoregressive with Autoregressive Disturbances)\nO Modelo Autorregressivo Espacial com Erros Autorregressivos (SARAR), também referido como modelo SAC (Spatial Autoregressive Combined) ou modelo generalizado de Cliff-Ord, é uma especificação abrangente que modela simultaneamente a dependência espacial na variável resposta e a autocorrelação espacial no termo de erro.\nNo contexto do Modelo Geral Espacial (GNS, Eq. 4.4), o SARAR é obtido ao impor a restrição \\(\\boldsymbol{\\theta} = \\mathbf{0}\\), mantendo os parâmetros \\(\\rho \\neq 0\\) e \\(\\lambda \\neq 0\\). Formalmente, o modelo é definido pelo sistema de equações (Anselin 1988; H. H. Kelejian e Prucha 1998):\n\\[\n\\begin{aligned}\n\\mathbf{y} &= \\rho \\mathbf{W}_1 \\mathbf{y} + \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{u}, \\quad\n\\mathbf{u} = \\lambda \\mathbf{W}_2 \\mathbf{u} + \\boldsymbol{\\epsilon},\n\\end{aligned}\n\\]\ncom erros independentes e identicamente distribuídos \\(\\boldsymbol{\\epsilon} \\sim (\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)\\). As matrizes de pesos espaciais \\(\\mathbf{W}_1\\) e \\(\\mathbf{W}_2\\) podem ser distintas, refletindo diferentes estruturas de interação para a variável dependente e para os erros, embora a especificação com \\(\\mathbf{W}_1 = \\mathbf{W}_2 = \\mathbf{W}\\) seja comum por parcimônia.\nNa forma escalar, para uma unidade \\(i\\):\n\\[\ny_i = \\rho \\sum_{j=1}^n w_{1,ij} y_j + \\sum_{k=1}^K x_{ik}\\beta_k + u_i, \\quad u_i = \\lambda \\sum_{j=1}^n w_{2,ij} u_j + \\epsilon_i.\n\\]\nA especificação SARAR é motivada pela necessidade de distinguir e controlar dois processos espaciais operando conjuntamente: 1) um mecanismo de interação substantiva ou de feedback, onde o resultado de uma unidade é diretamente influenciado pelos resultados de suas vizinhas (capturado por \\(\\rho\\)); e 2) um mecanismo de dependência residual, onde fatores não observados ou choques exógenos exibem padrão espacial (capturado por \\(\\lambda\\)).\nA identificação conjunta dos parâmetros \\(\\rho\\) e \\(\\lambda\\) é assegurada pela não-linearidade da função de verossimilhança, embora o uso de matrizes de pesos distintas (\\(\\mathbf{W}_1 \\neq \\mathbf{W}_2\\)) possa melhorar as propriedades de identificação ao introduzir variação exógena adicional (Anselin 1988).\nAssumindo que \\((\\mathbf{I}_n - \\rho \\mathbf{W}_1)\\) e \\((\\mathbf{I}_n - \\lambda \\mathbf{W}_2)\\) são não singulares, a forma reduzida do modelo é obtida por substituição:\n\\[\n\\mathbf{y} = (\\mathbf{I}_n - \\rho \\mathbf{W}_1)^{-1}\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{I}_n - \\rho \\mathbf{W}_1)^{-1}(\\mathbf{I}_n - \\lambda \\mathbf{W}_2)^{-1}\\boldsymbol{\\epsilon}.\n\\]\nEsta expressão revela a estrutura do processo gerador de dados:\n\n\\(\\mathbb{E}[\\mathbf{y} \\, | \\, \\mathbf{X}] = (\\mathbf{I}_n - \\rho \\mathbf{W}_1)^{-1}\\mathbf{X}\\boldsymbol{\\beta}\\). A interpretação dos efeitos marginais das variáveis explicativas segue a mesma lógica do modelo SAR, com a decomposição em efeitos diretos, indiretos e totais via o multiplicador espacial \\((\\mathbf{I}_n - \\rho \\mathbf{W}_1)^{-1}\\) (J. LeSage e Pace 2009).\n\\(\\operatorname{Cov}(\\mathbf{y} \\, | \\, \\mathbf{X}) = \\sigma^2 [(\\mathbf{I}_n - \\rho \\mathbf{W}_1)^{\\top}(\\mathbf{I}_n - \\lambda \\mathbf{W}_2)^{\\top}(\\mathbf{I}_n - \\lambda \\mathbf{W}_2)(\\mathbf{I}_n - \\rho \\mathbf{W}_1)]^{-1}\\). A estrutura de dependência estocástica resulta de uma dupla filtragem espacial: os choques \\(\\boldsymbol{\\epsilon}\\) são primeiro filtrados pelo processo de erro (\\(\\lambda\\)) e depois propagados pelo mecanismo autorregressivo da variável dependente (\\(\\rho\\)).\n\nA presença simultânea da variável dependente defasada (\\(\\mathbf{W}_1\\mathbf{y}\\)) e da autocorrelação nos erros torna o estimador de Mínimos Quadrados Ordinários (MQO) inconsistente e ineficiente. Os métodos de estimação padrão são:\n\nmáxima verossimilhança, onde Sob a suposição de normalidade dos erros, a função de log-verossimilhança é:\n\n\\[\n\\begin{aligned}\n\\ln L(\\boldsymbol{\\beta}, \\rho, \\lambda, \\sigma^2) &= C + \\ln|\\mathbf{I}_n - \\rho \\mathbf{W}_1| + \\ln|\\mathbf{I}_n - \\lambda \\mathbf{W}_2| - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\boldsymbol{\\epsilon}(\\rho, \\lambda)^{\\top} \\boldsymbol{\\epsilon}(\\rho, \\lambda),\n\\end{aligned}\n\\] onde \\(\\boldsymbol{\\epsilon}(\\rho, \\lambda) = (\\mathbf{I}_n - \\lambda \\mathbf{W}_2)[(\\mathbf{I}_n - \\rho \\mathbf{W}_1)\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}]\\). A maximização requer o cálculo de dois determinantes Jacobianos, o que pode ser computacionalmente intensivo para grandes amostras.\n\nMétodo Generalizado dos Momentos (GMM) / Mínimos Quadrados em Dois Estágios Espaciais Generalizados (GS2SLS), desenvolvida por H. H. Kelejian e Prucha (1998) e H. H. Kelejian e Prucha (1999), não exige suposições distribucionais fortes e evita o cálculo de determinantes. O procedimento é iterativo:\n\nNo primeiro estágio (2SLS) estima-se a equação estrutural ignorando inicialmente a autocorrelação do erro. Utilizam-se como instrumentos para \\(\\mathbf{W}_1\\mathbf{y}\\) as variáveis \\(\\mathbf{H} = [\\mathbf{X}, \\mathbf{W}_1\\mathbf{X}, \\mathbf{W}_1^2\\mathbf{X}, \\dots]\\), obtendo estimativas consistentes de \\(\\rho\\) e \\(\\boldsymbol{\\beta}\\) e os resíduos \\(\\hat{\\mathbf{u}}\\).\nEstimação de \\(\\lambda\\) utilizando um estimador GMM baseado nos momentos dos resíduos \\(\\hat{\\mathbf{u}}\\) para obter uma estimativa consistente de \\(\\lambda\\).\nTransformação e estimação, na qual aplica-se a transformação de Cochrane-Orcutt espacial aos dados para eliminar a autocorrelação, usando \\(\\hat{\\lambda}\\). A equação transformada é então reestimada via 2SLS, produzindo estimativas finais eficientes.\n\n\nO modelo SARAR é a especificação preferencial quando evidências empíricas (testes de Multiplicador de Lagrange robustos) indicam a presença conjunta de dependência espacial substantiva e dependência residual. Ele oferece um controle robusto para a heterogeneidade espacial não observada enquanto captura os mecanismos de interação de interesse.\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialreg, spdep, sf, modelsummary, kableExtra, dplyr, ggplot2, patchwork, viridis, ggspatial, tidyr, Matrix)\n\n# Ajuste do SARAR (SAC)\n# Estima rho e lambda simultaneamente\nmod_sarar &lt;- sacsarlm(taxa_bruta ~ variavel_x, data = mg_dados, listw = lw)\n\nmapa_vars &lt;- c(\n  \"(Intercept)\"    = \"Intercepto\",\n  \"variavel_x\"     = \"$\\\\beta$\",\n  \"lag.variavel_x\" = \"WX $\\\\theta$\",      \n  \"rho\"            = \"$\\\\rho$\",      \n  \"lambda\"         = \"$\\\\lambda$\"     \n)\n\nmapa_gof &lt;- list(\n  list(\"raw\" = \"nobs\", \"clean\" = \"N\", \"fmt\" = 0),\n  list(\"raw\" = \"r.squared\", \"clean\" = \"$R^2$\", \"fmt\" = 3),\n  list(\"raw\" = \"aic\", \"clean\" = \"AIC\", \"fmt\" = 1),\n  list(\"raw\" = \"logLik\", \"clean\" = \"Log Likelihood\", \"fmt\" = 1)\n)\n\nmodelsummary(\n  list(\n    \"OLS\"   = mod_ols, \n    \"SLX\"   = mod_slx,\n    \"SAR\"   = mod_sar, \n    \"SEM\"   = mod_sem,\n    \"SDM\"   = mod_sdm,\n    \"SDEM\"  = mod_sdem,\n    \"SARAR\" = mod_sarar\n  ),\n  coef_map = mapa_vars,      \n  gof_map = mapa_gof,      \n  estimate = \"{estimate} [{conf.low}, {conf.high}]\",\n  statistic = NULL, \n  stars = c('*' = .05, '**' = .01, '***' = .001),\n  title = NULL,     \n  output = \"kableExtra\", \n  escape = FALSE\n) %&gt;%\n  kable_styling(latex_options = c(\"HOLD_position\"), \n                full_width = FALSE, \n                position = \"center\") %&gt;%\n  row_spec(c(5, 7, 9), bold = TRUE) %&gt;% \n  as.character() %&gt;%\n  cat()\n\n\n\n\nTabela 4.10: Comparação Completa (OLS, SLX, SAR, SEM, SDM, SDEM, SARAR). Estimativas [IC 95%].\n\n\n\n\n\n\n\n\nOLS\n\n\n SLX\n\n\nSAR\n\n\nSEM\n\n\nSDM\n\n\nSDEM\n\n\nSARAR\n\n\n\n\n\n\nIntercepto\n\n\n8.973 [8.616, 9.330]\n\n\n8.970 [8.613, 9.327]\n\n\n5.751 [4.876, 6.626]\n\n\n8.978 [8.449, 9.507]\n\n\n5.748 [4.873, 6.622]\n\n\n8.974 [8.444, 9.503]\n\n\n2.715 [1.695, 3.735]\n\n\n\n\n\\(\\beta\\)\n\n\n0.058 [−0.306, 0.422]\n\n\n0.056 [−0.308, 0.420]\n\n\n0.029 [−0.316, 0.374]\n\n\n−0.001 [−0.342, 0.340]\n\n\n0.027 [−0.318, 0.372]\n\n\n0.060 [−0.297, 0.417]\n\n\n0.065 [−0.243, 0.373]\n\n\n\n\nWX \\(\\theta\\)\n\n\n\n\n0.397 [−0.396, 1.190]\n\n\n\n\n\n\n0.391 [−0.361, 1.143]\n\n\n0.496 [−0.366, 1.359]\n\n\n\n\n\n\n\\(\\rho\\)\n\n\n\n\n\n\n0.359 [0.269, 0.449]\n\n\n\n\n0.359 [0.270, 0.449]\n\n\n\n\n0.698 [0.587, 0.809]\n\n\n\n\n\\(\\lambda\\)\n\n\n\n\n\n\n\n\n0.359 [0.270, 0.449]\n\n\n\n\n0.360 [0.270, 0.450]\n\n\n−0.558 [−0.767, −0.349]\n\n\n\n\nN\n\n\n853\n\n\n853\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\)\n\n\n0.000\n\n\n0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAIC\n\n\n5275.0\n\n\n5276.1\n\n\n5212.9\n\n\n5213.0\n\n\n5213.9\n\n\n5213.7\n\n\n5200.4\n\n\n\n\nLog Likelihood\n\n\n−2634.5\n\n\n−2634.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\nEnquanto as especificações de dependência única (SAR, SEM) e suas generalizações com defasagens exógenas (SDM, SDEM) atingiram um patamar de ajuste estacionário, com AICs oscilando em torno de \\(5213\\) (Tabela 4.10), o modelo SARAR (Tabela 4.10 ) reduziu o critério de informação para \\(5200.4\\). Esta melhoria no ajuste é suportada pela significância estatística simultânea de ambos os parâmetros de dependência: uma forte autoregressão espacial positiva capturada por \\(\\rho\\) (\\(0.698\\); \\(IC_{95\\%} [0.587, 0.809]\\)) e um ajuste corretivo nos resíduos evidenciado por um \\(\\lambda\\) negativo significante (\\(-0.558\\); \\(IC_{95\\%} [-0.767, -0.349]\\)). Adicionalmente, a tabela reforça a robustez da inferência sobre a covariável \\(X\\): em todas as sete especificações testadas desde o OLS básico até o complexo SARAR o coeficiente \\(\\beta\\) e seus transbordamentos locais (\\(\\theta\\)) permaneceram estatisticamente nulos, consolidando a evidência de que a estrutura dos dados é dominada por dinâmicas espaciais endógenas e estocásticas, independentes da variável explicativa proposta.\n\n\nCódigo\nset.seed(123)\nimp_sarar &lt;- impacts(mod_sarar, listw = lw, R = 1000)\n\nif (is.null(imp_sarar$res)) {\n  imp_sarar &lt;- impacts(mod_sarar, listw = lw, R = 1000, zstats = TRUE)\n}\n\ndf_impactos &lt;- data.frame(\n  direct = imp_sarar$res$direct,\n  indirect = imp_sarar$res$indirect\n) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Tipo\", values_to = \"Valor\") %&gt;%\n  mutate(Tipo = factor(Tipo, levels = c(\"direct\", \"indirect\"),\n                       labels = c(\"Direto\", \"Indireto (Spillover Global)\")))\n\n\ng_impactos &lt;- ggplot(df_impactos, aes(x = Tipo, fill = Tipo, y=Valor)) +\n  geom_col(width = 0.2, color = \"gray30\") +\n  scale_fill_manual(values = c(\"Direto\" = \"#1b9e77\", \"Indireto (Spillover)\" = \"#d95f02\")) +\n  labs(title = \"A. Impactos: Direto vs. Indireto\", \n       y = \"Magnitude do efeito\", x = NULL) +\n  theme_minimal() + \n  theme(legend.position = \"none\", \n        legend.title = element_blank())\n\n# Mapa dos Valores Ajustados\nmg_dados$fitted_sarar &lt;- fitted(mod_sarar)\n\ng_fit &lt;- ggplot(mg_dados) +\n  geom_sf(aes(fill = fitted_sarar), color = NA) +\n  scale_fill_viridis_c(option = \"turbo\", name = \"Predito\") +\n labs(\n  title = \"B. Valores Preditos (SARAR)\", \n  subtitle = expression(\"Ajuste simultâneo\" ~ (rho + lambda))\n) +\ntheme_minimal() + \n  annotation_scale(location = \"bl\", width_hint = 0.3, bar_cols = c(\"black\", \"white\")) +\n  annotation_north_arrow(location = \"tl\", style = north_arrow_fancy_orienteering,\n                         pad_x = unit(0.1, \"in\"), pad_y = unit(0.1, \"in\"))\n\n# Diagnóstico dos Resíduos\nmg_dados$resid_sarar &lt;- residuals(mod_sarar)\nmoran_sarar &lt;- moran.test(mg_dados$resid_sarar, lw)\nmg_dados$resid_lag_sarar &lt;- lag.listw(lw, mg_dados$resid_sarar)\n\ng_resid_scatter &lt;- ggplot(mg_dados, aes(x = resid_sarar, y = resid_lag_sarar)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", size = 0.8) +\n  labs(title = \"C. Scatter de Moran (Resíduos SARAR)\", \n       subtitle = paste0(\"I de Moran: \", round(moran_sarar$estimate[1], 3), \n                         \" (p-valor: \", round(moran_sarar$p.value, 3), \")\"),\n       x = \"Resíduos\", y = \"Lag Espacial\") +\n  theme_minimal()\n\n\n(g_impactos | g_fit | g_resid_scatter)\n\n\n\n\n\n\n\n\nFigura 4.17: Diagnóstico SARAR: (A) Impactos Globais, (B) Ajuste e (C) Resíduos.\n\n\n\n\n\n\n\n4.13.7 Modelo de Média Móvel Espacial (SMA – Spatial Moving Average)\nO modelo de Média Móvel Espacial (SMA) oferece uma abordagem alternativa para a modelagem da dependência espacial no termo de erro. Enquanto o modelo SEM especifica um processo autorregressivo que induz uma dependência de longo alcance, o SMA postula que a dependência espacial é um fenômeno estritamente local e de curto alcance.\nEmbora menos comum na literatura aplicada que os modelos SAR ou SEM, o SMA é relevante para testar hipóteses sobre a extensão geográfica das interdependências. O modelo é especificado pela seguinte estrutura (Anselin 1988; R. P. Haining 2003):\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{u}, \\quad \\mathbf{u} = \\boldsymbol{\\epsilon} + \\lambda \\mathbf{W}\\boldsymbol{\\epsilon},\n\\]\ncom erros independentes e identicamente distribuídos \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)\\).\nNa forma escalar para uma unidade \\(i\\), o processo para o termo de erro é dado por:\n\\[\nu_i = \\epsilon_i + \\lambda \\sum_{j=1}^n w_{ij} \\epsilon_j.\n\\]\nOnde \\(\\lambda\\) é o coeficiente de média móvel espacial.\nA distinção fundamental entre o SMA e os modelos autorregressivos (SAR, SEM) reside na estrutura de propagação dos choques estocásticos.\n\nNos modelos autorregressivos, a dependência é modelada através da inversa de uma matriz de filtro espacial, por exemplo, \\((\\mathbf{I}_n - \\lambda \\mathbf{W})^{-1}\\). A expansão em série de Neumann desta inversa, \\((\\mathbf{I}_n - \\lambda \\mathbf{W})^{-1} = \\mathbf{I}_n + \\lambda \\mathbf{W} + \\lambda^2 \\mathbf{W}^2 + \\cdots\\), implica que um choque em uma unidade \\(i\\) afeta seus vizinhos diretos (\\(\\lambda \\mathbf{W}\\)), os vizinhos dos vizinhos (\\(\\lambda^2 \\mathbf{W}^2\\)), e assim sucessivamente, propagando-se por todo o sistema, ainda que com intensidade decrescente (Elhorst et al. 2014). Isso caracteriza uma dependência de longo alcance.\nNo modelo SMA, não há inversão de matriz na definição do erro \\(\\mathbf{u}\\). Um choque \\(\\epsilon_i\\) afeta diretamente a unidade \\(i\\) e, através do termo \\(\\lambda \\mathbf{W}\\boldsymbol{\\epsilon}\\), afeta apenas os vizinhos imediatos de \\(i\\) (aqueles para os quais \\(w_{ij} \\neq 0\\)). O efeito é contido na primeira ordem de vizinhança; não há mecanismo de retransmissão para unidades mais distantes.\n\nA natureza local do SMA é explicitamente descrita por sua matriz de covariância. Dado que \\(\\mathbf{u} = (\\mathbf{I}_n + \\lambda \\mathbf{W}) \\boldsymbol{\\epsilon}\\), a matriz de covariância de \\(\\mathbf{u}\\) (condicional a \\(\\mathbf{X}\\)) é:\n\\[\n\\operatorname{Cov}(\\mathbf{u}) = \\sigma^2 (\\mathbf{I}_n + \\lambda \\mathbf{W})(\\mathbf{I}_n + \\lambda \\mathbf{W})^{\\top} = \\sigma^2 (\\mathbf{I}_n + \\lambda \\mathbf{W} + \\lambda \\mathbf{W}^{\\top} + \\lambda^2 \\mathbf{W}\\mathbf{W}^{\\top}).\n\\]\nNote que:\n\nA covariância entre \\(u_i\\) e \\(u_j\\) é não nula se \\(w_{ij} \\neq 0\\) (são vizinhos diretos) ou se \\(w_{ji} \\neq 0\\) (para \\(\\mathbf{W}\\) não simétrica).\nO elemento \\((i, j)\\) da matriz \\(\\mathbf{W}\\mathbf{W}^{\\top}\\) é não nulo se existe pelo menos uma unidade \\(k\\) tal que \\(w_{ik} \\neq 0\\) e \\(w_{jk} \\neq 0\\). Ou seja, se as unidades \\(i\\) e \\(j\\) compartilham pelo menos um vizinho comum.\n\nPortanto, no modelo SMA, a correlação espacial é efetivamente nula para qualquer par de unidades que não sejam vizinhas diretas nem compartilhem um vizinho comum (vizinhos de segunda ordem). Isto contrasta com o SEM, onde a matriz de covariância é teoricamente densa, implicando correlação não nula (embora pequena) entre todos os pares de unidades no sistema.\nAssim como no SEM, o estimador de Mínimos Quadrados Ordinários (MQO) para \\(\\boldsymbol{\\beta}\\) no modelo SMA é não viesado e consistente sob a exogeneidade de \\(\\mathbf{X}\\). No entanto, é ineficiente na presença de autocorrelação espacial (\\(\\lambda \\neq 0\\)), e as estimativas dos erros-padrão obtidas pelo procedimento MQO padrão são viesadas, invalidando a inferência estatística habitual (Anselin 1988).\nA estimação eficiente tipicamente requer o método de máxima verossimilhança. A função de log-verossimilhança assume a forma:\n\\[\n\\ln L(\\boldsymbol{\\beta}, \\lambda, \\sigma^2) = C - \\frac{1}{2} \\ln |\\sigma^2 \\boldsymbol{\\Omega}(\\lambda)| - \\frac{1}{2\\sigma^2} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^{\\top} \\boldsymbol{\\Omega}(\\lambda)^{-1} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}),\n\\]\nonde \\(\\boldsymbol{\\Omega}(\\lambda) = (\\mathbf{I}_n + \\lambda \\mathbf{W})(\\mathbf{I}_n + \\lambda \\mathbf{W})^{\\top}\\). A complexidade computacional reside no cálculo do determinante e da inversa de \\(\\boldsymbol{\\Omega}(\\lambda)\\). Diferentemente dos modelos autorregressivos, que envolvem o determinante de \\((\\mathbf{I}_n - \\lambda \\mathbf{W})\\), o SMA requer o cálculo do determinante de \\((\\mathbf{I}_n + \\lambda \\mathbf{W})\\).\nPara garantir que o processo seja invertível e a matriz de covariância seja positiva definida, são impostas restrições no espaço do parâmetro \\(\\lambda\\). Geralmente, exige-se que \\(|\\lambda| &lt; 1 / |\\omega_{max}|\\), onde \\(\\omega_{max}\\) é o maior autovalor de \\(\\mathbf{W}\\) em módulo (Hepple 1979; Anselin 1988).\nO uso do SMA é recomendado quando a análise exploratória dos dados (por exemplo, correlogramas espaciais ou testes de dependência para diferentes ordens de vizinhança) indica um decaimento abrupto da correlação espacial após a primeira defasagem, em contraste com o decaimento suave e prolongado característico dos processos autorregressivos.\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialreg, spdep, sf, modelsummary, kableExtra, dplyr, ggplot2, patchwork, viridis, ggspatial, tidyr, Matrix)\n\n\nif (!exists(\"mg_dados\")) {\n  mg_dados &lt;- geobr::read_municipality(code_muni = \"MG\", year = 2020, showProgress = FALSE)\n  coords &lt;- st_coordinates(st_centroid(mg_dados))\n  set.seed(123)\n  mg_dados$taxa_bruta &lt;- (-coords[,2] * 10) + rnorm(nrow(mg_dados), 0, 5)\n  mg_dados$variavel_x &lt;- rnorm(nrow(mg_dados))\n}\n\nif (!exists(\"lw\")) {\n   nb &lt;- poly2nb(mg_dados, queen = TRUE)\n   lw &lt;- nb2listw(nb, style = \"W\", zero.policy = TRUE)\n}\n\n\nmod_sma &lt;- spautolm(taxa_bruta ~ variavel_x, data = mg_dados, listw = lw, family = \"SMA\")\n\nsum_sma &lt;- summary(mod_sma)\n\n#Extrair Betas da matriz de coeficientes \ncoefs_mat &lt;- sum_sma$Coef # Matriz com Estimate, Std. Error, etc.\ndf_tidy_betas &lt;- data.frame(\n  term = rownames(coefs_mat),\n  estimate = coefs_mat[, \"Estimate\"],\n  std.error = coefs_mat[, \"Std. Error\"]\n)\n\n# Extrair Lambda\ndf_tidy_lambda &lt;- data.frame(\n  term = \"lambda\",\n  estimate = mod_sma$lambda,\n  std.error = mod_sma$lambda.se\n)\n\n#Unir tudo\ndf_tidy_sma &lt;- rbind(df_tidy_betas, df_tidy_lambda)\n\n#Calcular estatísticas finais\ndf_tidy_sma$statistic &lt;- df_tidy_sma$estimate / df_tidy_sma$std.error\ndf_tidy_sma$p.value &lt;- 2 * (1 - pnorm(abs(df_tidy_sma$statistic)))\ndf_tidy_sma$conf.low &lt;- df_tidy_sma$estimate - (1.96 * df_tidy_sma$std.error)\ndf_tidy_sma$conf.high &lt;- df_tidy_sma$estimate + (1.96 * df_tidy_sma$std.error)\n\n#\ndf_glance_sma &lt;- data.frame(\n  nobs = length(residuals(mod_sma)),\n  logLik = as.numeric(logLik(mod_sma)),\n  aic = AIC(mod_sma),\n  r.squared = NA\n)\n\n#\nmod_sma_custom &lt;- list(tidy = df_tidy_sma, glance = df_glance_sma)\nclass(mod_sma_custom) &lt;- \"modelsummary_list\"\n\n#\nmapa_vars &lt;- c(\n  \"(Intercept)\"    = \"Intercepto\",\n  \"variavel_x\"     = \"$\\\\beta$\",\n  \"lag.variavel_x\" = \"WX $\\\\theta$\",      \n  \"rho\"            = \"$\\\\rho$\",      \n  \"lambda\"         = \"$\\\\lambda$\"     \n)\n\nmapa_gof &lt;- list(\n  list(\"raw\" = \"nobs\", \"clean\" = \"N\", \"fmt\" = 0),\n  list(\"raw\" = \"r.squared\", \"clean\" = \"$R^2$\", \"fmt\" = 3),\n  list(\"raw\" = \"aic\", \"clean\" = \"AIC\", \"fmt\" = 1),\n  list(\"raw\" = \"logLik\", \"clean\" = \"Log Likelihood\", \"fmt\" = 1)\n)\n\n# Tabela Final\nmodelsummary(\n  list(\n    \"OLS\"   = mod_ols, \n    \"SLX\"   = mod_slx,\n    \"SAR\"   = mod_sar, \n    \"SEM\"   = mod_sem,\n    \"SDM\"   = mod_sdm,\n    \"SDEM\"  = mod_sdem,\n    \"SARAR\" = mod_sarar,\n    \"SMA\"   = mod_sma_custom\n  ),\n  coef_map = mapa_vars,      \n  gof_map = mapa_gof,      \n  estimate = \"{estimate} [{conf.low}, {conf.high}]\",\n  statistic = NULL, \n  stars = c('*' = .05, '**' = .01, '***' = .001),\n  title = NULL,     \n  output = \"kableExtra\", \n  escape = FALSE\n) %&gt;%\n  kable_styling(latex_options = c(\"HOLD_position\", \"scale_down\"),\n                full_width = FALSE, \n                position = \"center\") %&gt;%\n  row_spec(c(5, 7, 9), bold = TRUE) %&gt;% \n  as.character() %&gt;%\n  cat()\n\n\n\n\nTabela 4.11: Comparação entre ajuste dos modelos OLS, SLX, SAR, SEM, SDM, SDEM, SARAR e SMA. Estimativas [IC 95%].\n\n\n\n\n\n\n\n\nOLS\n\n\n SLX\n\n\nSAR\n\n\nSEM\n\n\nSDM\n\n\nSDEM\n\n\nSARAR\n\n\nSMA\n\n\n\n\n\n\nIntercepto\n\n\n8.973 [8.616, 9.330]\n\n\n8.970 [8.613, 9.327]\n\n\n5.751 [4.876, 6.626]\n\n\n8.978 [8.449, 9.507]\n\n\n5.748 [4.873, 6.622]\n\n\n8.974 [8.444, 9.503]\n\n\n2.715 [1.695, 3.735]\n\n\n8.975 [8.499, 9.450]\n\n\n\n\n\\(\\beta\\)\n\n\n0.058 [−0.306, 0.422]\n\n\n0.056 [−0.308, 0.420]\n\n\n0.029 [−0.316, 0.374]\n\n\n−0.001 [−0.342, 0.340]\n\n\n0.027 [−0.318, 0.372]\n\n\n0.060 [−0.297, 0.417]\n\n\n0.065 [−0.243, 0.373]\n\n\n0.001 [−0.343, 0.345]\n\n\n\n\nWX \\(\\theta\\)\n\n\n\n\n0.397 [−0.396, 1.190]\n\n\n\n\n\n\n0.391 [−0.361, 1.143]\n\n\n0.496 [−0.366, 1.359]\n\n\n\n\n\n\n\n\n\\(\\rho\\)\n\n\n\n\n\n\n0.359 [0.269, 0.449]\n\n\n\n\n0.359 [0.270, 0.449]\n\n\n\n\n0.698 [0.587, 0.809]\n\n\n\n\n\n\n\\(\\lambda\\)\n\n\n\n\n\n\n\n\n0.359 [0.270, 0.449]\n\n\n\n\n0.360 [0.270, 0.450]\n\n\n−0.558 [−0.767, −0.349]\n\n\n0.363\n\n\n\n\nN\n\n\n853\n\n\n853\n\n\n\n\n\n\n\n\n\n\n\n\n853\n\n\n\n\n\\(R^2\\)\n\n\n0.000\n\n\n0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAIC\n\n\n5275.0\n\n\n5276.1\n\n\n5212.9\n\n\n5213.0\n\n\n5213.9\n\n\n5213.7\n\n\n5200.4\n\n\n5221.0\n\n\n\n\nLog Likelihood\n\n\n−2634.5\n\n\n−2634.0\n\n\n\n\n\n\n\n\n\n\n\n\n−2606.5\n\n\n\n\n\n\n * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nCódigo\n# Mapa\nmg_dados$fitted_sma &lt;- fitted(mod_sma)\ng_fit &lt;- ggplot(mg_dados) +\n  geom_sf(aes(fill = fitted_sma), color = NA) +\n  scale_fill_viridis_c(option = \"turbo\", name = \"Predito\") +\n  labs(title = \"A. Valores Preditos (SMA)\") +\n  theme_minimal() + \n  annotation_scale(location = \"bl\", width_hint = 0.3) +\n  annotation_north_arrow(location = \"tl\", style = north_arrow_fancy_orienteering)\n\n#Resíduos\nmg_dados$resid_sma &lt;- residuals(mod_sma)\nmoran_sma &lt;- moran.test(mg_dados$resid_sma, lw)\nmg_dados$resid_lag_sma &lt;- lag.listw(lw, mg_dados$resid_sma)\n\ng_resid_scatter &lt;- ggplot(mg_dados, aes(x = resid_sma, y = resid_lag_sma)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", size = 0.8) +\n  labs(title = \"B. Scatter de Moran (Resíduos SMA)\", \n       subtitle = paste0(\"I de Moran: \", round(moran_sma$estimate[1], 3), \n                         \" (p-valor: \", round(moran_sma$p.value, 3), \")\"),\n       x = \"Resíduos\", y = \"Lag Espacial\") +\n  theme_minimal()\n\n( g_fit | g_resid_scatter)\n\n\n\n\n\n\n\n\nFigura 4.18: Diagnóstico SMA: (A) Ajuste e (B) Resíduos.",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#modelos-espaciais-para-respostas-limitadas-e-discretas",
    "href": "lattice_data.html#modelos-espaciais-para-respostas-limitadas-e-discretas",
    "title": "4  Dados de Área",
    "section": "4.14 Modelos espaciais para respostas limitadas e discretas",
    "text": "4.14 Modelos espaciais para respostas limitadas e discretas\n\n4.14.1 Modelo Probit Espacial\nO Modelo Probit Espacial é aplicado quando a variável dependente observada \\(y_i\\) é binária (\\(y_i \\in \\{0, 1\\}\\)) e existe interdependência espacial entre as unidades de observação.\nA formulação comum para o modelo Probit Espacial Autorregressivo (SAR Probit) baseia-se numa variável latente contínua não observada, \\(y_i^*\\). Assume-se que esta variável latente segue um processo autorregressivo espacial:\n\\[\ny_i^* = \\rho \\sum_{j=1}^n w_{ij} y_j^* + \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, 1).\n\\]\nA variável binária observada \\(y_i\\) é então determinada por um mecanismo de limiar:\n\\[\ny_i =\n\\begin{cases}\n1, & \\text{se } y_i^* &gt; 0,\\\\\n0, & \\text{se } y_i^* \\le 0.\n\\end{cases}\n\\]\nAqui, \\(\\mathbf{W}\\) é a matriz de pesos espaciais, \\(\\rho\\) é o parâmetro de autocorrelação espacial, \\(\\mathbf{X}\\) é a matriz de covariáveis e \\(\\boldsymbol{\\beta}\\) o vetor de coeficientes. A variância do erro \\(\\epsilon_i\\) é fixada em 1 para identificação do modelo Probit (J. LeSage e Pace 2009).\nResolvendo a equação estrutural para o vetor latente \\(\\mathbf{y}^*\\), obtém-se sua forma reduzida:\n\\[\n\\mathbf{y}^* = (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\boldsymbol{\\epsilon}.\n\\]\nDefine-se \\(\\tilde{\\boldsymbol{\\epsilon}} = (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\boldsymbol{\\epsilon}\\). A estrutura de covariância deste termo de erro composto é:\n\\[\n\\operatorname{Cov}(\\tilde{\\boldsymbol{\\epsilon}}) = [(\\mathbf{I}_n - \\rho \\mathbf{W})(\\mathbf{I}_n - \\rho \\mathbf{W})^{\\top}]^{-1}.\n\\]\nA diagonal desta matriz de covariância não é constante. Cada elemento diagonal, \\(\\tilde{\\sigma}_i^2\\), varia em função da posição da unidade \\(i\\) na rede espacial, conforme definida por \\(\\mathbf{W}\\). Consequentemente, o modelo de variável latente exibe heterocedasticidade induzida espacialmente. A aplicação de um estimador Probit padrão, que assume homocedasticidade (\\(\\tilde{\\sigma}_i^2 = 1\\) para todo \\(i\\)), a dados gerados por este processo, produz estimativas inconsistentes dos parâmetros \\(\\rho\\) e \\(\\boldsymbol{\\beta}\\) (McMillen 1992; Calabrese e Elkink 2014).\nA função de verossimilhança para o modelo Probit Espacial é a probabilidade conjunta de observar a amostra \\(\\mathbf{y} = (y_1, \\ldots, y_n)^{\\top}\\). Esta probabilidade requer o cálculo de uma integral \\(n\\)-dimensional sobre uma distribuição normal multivariada restrita a ortantes definidos pelos valores observados \\(y_i\\):\n\\[\nL(\\boldsymbol{\\beta}, \\rho | \\mathbf{y}) = \\int_{R_1} \\cdots \\int_{R_n} \\phi_n(\\mathbf{y}^* | \\boldsymbol{\\mu}, \\boldsymbol{\\Omega}) \\, d\\mathbf{y}^*,\n\\]\nonde \\(\\boldsymbol{\\mu} = (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\mathbf{X}\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\Omega} = [(\\mathbf{I}_n - \\rho \\mathbf{W})(\\mathbf{I}_n - \\rho \\mathbf{W})^{\\top}]^{-1}\\), e \\(R_i = (-\\infty, 0]\\) se \\(y_i = 0\\) e \\(R_i = (0, \\infty)\\) se \\(y_i = 1\\). Para amostras de tamanho moderado ou grande, o cálculo numérico direto desta integral é computacionalmente proibitivo (Fleming 2004).\nA literatura desenvolveu várias estratégias para superar esta intratabilidade:\n\nAlgoritmo EM (Expectation-Maximization): Proposto por McMillen (1992), este método trata o vetor latente \\(\\mathbf{y}^*\\) como dados faltantes. O algoritmo itera entre um passo E, que calcula a esperança de \\(\\mathbf{y}^*\\) condicional nos parâmetros atuais e em \\(\\mathbf{y}\\), e um passo M, que maximiza a verossimilhança de um modelo espacial linear contínuo. Apesar de fornecer estimativas consistentes, a obtenção de erros-padrão válidos é complexa.\nMétodo Generalizado dos Momentos (GMM): Pinkse e Slade (1998) desenvolveram um estimador GMM baseado nos resíduos generalizados do modelo Probit. Klier e McMillen (2008) propuseram uma versão linearizada (LGMM), computacionalmente eficiente e adequada para grandes conjuntos de dados, embora potencialmente sujeita a viés quando a dependência espacial \\(\\rho\\) é forte (Calabrese e Elkink 2014).\nSimulação recursiva de importância (GHK): Beron e Vijverberg (2004) implementaram a estimação de máxima verossimilhança simulada usando o simulador GHK (Geweke-Hajivassiliou-Keane) para aproximar a integral \\(n\\)-dimensional. Este método é preciso, mas seu custo computacional cresce consideravelmente com o tamanho da amostra.\nAbordagem Bayesiana (MCMC): Consolidada por J. P. LeSage (2000), esta abordagem emprega métodos de Monte Carlo via Cadeias de Markov (MCMC). Utiliza-se uma estratégia de aumento de dados (data augmentation), na qual o vetor latente \\(\\mathbf{y}^*\\) é tratado como um parâmetro a ser amostrado sequencialmente de uma distribuição normal multivariada truncada, condicional aos valores observados \\(\\mathbf{y}\\) e aos parâmetros do modelo. Esta é frequentemente considerada a abordagem mais precisa para amostras finitas, fornecendo a distribuição completa a posteriori dos parâmetros (Calabrese e Elkink 2014; Wilhelm e Godinho de Matos 2013).\nAproximações e Verossimilhança Parcial: Para grandes conjuntos de dados, métodos aproximados ganham relevância. Destacam-se a aproximação de verossimilhança via método de Mendell-Elston (Martinetti e Geniaux 2017), a verossimilhança parcial por pares (Billé e Leorato 2020), e técnicas de amostragem por importância eficiente (EIS) que exploram a esparsidade da matriz de precisão (Liesenfeld, Richard, e Vogler 2013).\n\nEm modelos Probit Espaciais, os coeficientes \\(\\boldsymbol{\\beta}\\) não possuem uma interpretação direta como efeitos marginais sobre a probabilidade \\(P(y_i = 1)\\), devido à não linearidade da função de ligação Probit e à presença do multiplicador espacial global \\((\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\).\nA análise deve concentrar-se nos efeitos médios diretos, indiretos (de transbordamento) e totais (J. LeSage e Pace 2009). Um cambio em uma covariável \\(x_{ik}\\) não afeta apenas a probabilidade na unidade \\(i\\), mas também se propaga através da rede, afetando as probabilidades em todas as outras unidades \\(j \\neq i\\). Estes efeitos são não lineares e variam entre observações. Prática comum resume-os através das médias amostrais dos efeitos calculados para cada unidade, fornecendo uma medida sumária do impacto global de cada variável explicativa.\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialprobit, spdep, sf, geobr, ggplot2, viridis, ggspatial, kableExtra, dplyr, Matrix, patchwork, scales)\n\n# Preparação e Simulação (IGNORE ISTO)\n\nif (!exists(\"sp_dados\") || !(\"y_bin\" %in% names(sp_dados))) {\n  sp_dados &lt;- geobr::read_municipality(code_muni = \"SP\", year = 2020, showProgress = FALSE)\n  coords &lt;- st_coordinates(st_centroid(sp_dados))\n  \n  knn &lt;- knearneigh(coords, k = 6)\n  nb_sp &lt;- knn2nb(knn)\n  lw_sp &lt;- nb2listw(nb_sp, style = \"W\")\n  W_mat &lt;- as(lw_sp, \"CsparseMatrix\")\n  \n  set.seed(999)\n  n &lt;- nrow(sp_dados)\n  rho_true &lt;- 0.65\n  beta_0 &lt;- -1.0\n  beta_1 &lt;- 1.5\n  \n  sp_dados$x_var &lt;- rnorm(n, 0, 1)\n  \n  I_n &lt;- Matrix::Diagonal(n)\n  inv_spatial &lt;- solve(I_n - rho_true * W_mat)\n  epsilon &lt;- rnorm(n, 0, 1)\n  \n  y_latente &lt;- as.vector(inv_spatial %*% (beta_0 + beta_1 * sp_dados$x_var + epsilon))\n  \n  # Y binário: 0 ou 1 (Corte em 0)\n  sp_dados$y_bin &lt;- ifelse(y_latente &gt; 0, 1, 0)\n  \n} else {\n    if (!exists(\"W_mat\")) {\n      knn &lt;- knearneigh(st_coordinates(st_centroid(sp_dados)), k = 6)\n      nb_sp &lt;- knn2nb(knn)\n      lw_sp &lt;- nb2listw(nb_sp, style = \"W\")\n      W_mat &lt;- as(lw_sp, \"CsparseMatrix\")\n  }\n}\n\n# Ajuste do Modelo PROBIT (Binário)\n\nmod_sar_probit &lt;- sarprobit(y_bin ~ x_var, \n                             W = W_mat, \n                             data = sp_dados, \n                             ndraw = 2000, \n                             burn.in = 500, \n                             showProgress = FALSE)\n\n# Tabela de Resultados\n\nall_draws1 &lt;- as.data.frame(mod_sar_probit$B)\nif (!is.null(mod_sar_probit$names)) colnames(all_draws1) &lt;- mod_sar_probit$names\nif (!any(grepl(\"rho\", colnames(all_draws1), ignore.case = TRUE))) all_draws1$rho &lt;- as.vector(mod_sar_probit$rho)\n\nresumo_bayesiano1 &lt;- data.frame(\n  Parametro  = names(all_draws1),\n  Estimativa = colMeans(all_draws1),\n  IC_Inf     = apply(all_draws1, 2, quantile, probs = 0.025),\n  IC_Sup     = apply(all_draws1, 2, quantile, probs = 0.975)\n)\n\n# Formatação da Tabela\nresumo_bayesiano1$Resultado &lt;- sprintf(\"%.3f [%.3f, %.3f]\", \n                                      resumo_bayesiano1$Estimativa, \n                                      resumo_bayesiano1$IC_Inf, \n                                      resumo_bayesiano1$IC_Sup)\n\nresumo_bayesiano1$Parametro &lt;- dplyr::case_when(\n  resumo_bayesiano1$Parametro %in% c(\"(Intercept)\", \"beta_1\") ~ \"Intercepto\",\n  resumo_bayesiano1$Parametro %in% c(\"x_var\", \"beta_2\") ~ \"Variável X\",\n  grepl(\"rho\", resumo_bayesiano1$Parametro, ignore.case = TRUE) ~ \"$\\\\rho$ (Dependência)\",\n  TRUE ~ resumo_bayesiano1$Parametro\n)\n\ntabela_final1 &lt;- resumo_bayesiano1 %&gt;%\n  filter(!duplicated(Parametro)) %&gt;%\n  dplyr::select(Parametro, Resultado)\nrownames(tabela_final1) &lt;- NULL\n\nkbl(tabela_final1, \n    format = \"latex\",\n    booktabs = TRUE, \n    align = \"lc\",  \n    caption = \"\", \n    escape = FALSE) %&gt;%\n  kable_styling(latex_options = c(\"HOLD_position\", \"striped\"), \n                full_width = FALSE, \n                position = \"center\") %&gt;%\n  row_spec(0, bold = TRUE) \n\n\n\n\nTabela 4.12: Modelo Probit Binário (Bayesiano) - SP.\n\n\n\n\n\n\nInterpretação\nA Tabela Tabela 4.12 apresenta as estimativas estruturais modelo Probit ajustado. Observa-se que a Variável \\(X\\) exerce um efeito positivo (\\(2.274\\); \\(IC_{95\\%} [1.677, 2.909]\\)), indicando que elevações nesta covariável aumentam significativamente a probabilidade de classificação nos estratos superiores da resposta ordinal. A dependência espacial, mensurada pelo parâmetro \\(\\rho\\) (\\(0.707\\); \\(IC_{95\\%} [0.649, 0.761]\\)), demonstra que a categoria observada em uma unidade espacial é fortemente condicionada pelo perfil de sua vizinhança, validando a presença de feedback espacial no processo gerador dos dados.\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialprobit, spdep, sf, geobr, ggplot2, viridis, ggspatial, kableExtra, dplyr, Matrix, patchwork, scales)\n\n\n#\nX_mat &lt;- model.matrix(~ x_var, data = sp_dados)\n\n#\nall_params &lt;- colMeans(mod_sar_probit$B)\nbeta_vec &lt;- all_params[1:ncol(X_mat)] \nrho_hat  &lt;- mean(mod_sar_probit$rho)\n\n#Predição Linear (X * Beta)\nxb &lt;- X_mat %*% beta_vec\n\n#(I - rho*W)^-1\nI_n &lt;- Matrix::Diagonal(nrow(sp_dados))\nS_inv &lt;- solve(I_n - rho_hat * W_mat)\n\n#y*_pred = S_inv * Xb\ny_star_pred &lt;- as.vector(S_inv %*% xb)\n\n#\ny_obs &lt;- sp_dados$y_bin\nlo &lt;- ifelse(y_obs == 0, -Inf, 0)\nhi &lt;- ifelse(y_obs == 0, 0, Inf)\n\n#\nz_lo &lt;- lo - y_star_pred\nz_hi &lt;- hi - y_star_pred\n\nsafe_pnorm &lt;- function(q) pnorm(q)\nsafe_dnorm &lt;- function(x) dnorm(x)\n\n#\ndiff_cdf &lt;- safe_pnorm(z_hi) - safe_pnorm(z_lo)\ndiff_cdf[diff_cdf &lt; 1e-10] &lt;- 1e-10 \n\n#\ndiff_pdf &lt;- safe_dnorm(z_lo) - safe_dnorm(z_hi) # pdf(lo) - pdf(hi)\n\n#\n# E[y* | y] = mu + sigma * (pdf_lo - pdf_hi) / (cdf_hi - cdf_lo)\nsp_dados$y_latente_esperada &lt;- y_star_pred + (diff_pdf / diff_cdf)\n\n# u = (I - rho*W) * E[y*|y] - X*beta\nA_mat &lt;- (I_n - rho_hat * W_mat)\nterm_spatial_removed &lt;- as.vector(A_mat %*% sp_dados$y_latente_esperada)\n\nsp_dados$resid_generalized &lt;- term_spatial_removed - as.vector(xb)\n\n# Teste de Moran nos Resíduos Generalizados\nmoran_resid &lt;- moran.test(sp_dados$resid_generalized, lw_sp)\nlabel_moran &lt;- paste0(\"Moran (Gen. Resid): \", round(moran_resid$estimate[1], 3), \n                      \" (p: \", round(moran_resid$p.value, 3), \")\")\n\n# Impactos (Efeitos Marginais)\nimp_probit &lt;- impacts(mod_sar_probit)\n\n——–Marginal Effects——–\n\nDirect effects lower_005 posterior_mean upper_095 x_var 0.04418 0.07616 0.111\nIndirect effects lower_005 posterior_mean upper_095 x_var 0.1064 0.1909 0.283\nTotal effects lower_005 posterior_mean upper_095 x_var 0.1499 0.2671 0.394\n\n\nCódigo\ndf_imp &lt;- data.frame(\n  Tipo = factor(c(\"direct\", \"indirect\", \"total\"), \n                levels = c(\"direct\", \"indirect\", \"total\"),\n                labels = c(\"Direto\", \"Indireto\", \"Total\")),\n  Valor = c(imp_probit$direct[, \"posterior_mean\"], \n            imp_probit$indirect[, \"posterior_mean\"], \n            imp_probit$total[, \"posterior_mean\"])\n)\n\n# PREDIÇÃO E CLASSIFICAÇÃO \nsp_dados$latente_predita &lt;- as.vector(fitted(mod_sar_probit))\n\n#Definição dos Cortes (Breaks): O vetor 'phi' contém os limites: 0 (fixo)\nbreaks_finais &lt;- c(-Inf, 0, Inf)\n\n#Classificação\nsp_dados$cat_predita &lt;- cut(sp_dados$latente_predita, \n                                  breaks = breaks_finais, \n                                  labels = FALSE)\nsp_dados$cat_predita &lt;- sp_dados$cat_predita-1\n\n#\nif(any(is.na(sp_dados$cat_predita))) {\n  sp_dados$cat_predita_class[is.na(sp_dados$cat_predita)] &lt;- 1\n}\n\n\n# Tema Customizado (Do seu exemplo)\ntheme_map_custom &lt;- function() {\n  list(\n    theme_void(),\n    theme(\n      legend.position = \"bottom\", \n      legend.box.spacing = unit(5, \"pt\"),\n      legend.title = element_text(size=9, face=\"bold\"),\n      plot.title = element_text(face=\"bold\", size=12, hjust = 0),\n      plot.subtitle = element_text(size=9, color=\"grey30\")\n    ),\n    annotation_scale(location = \"br\", width_hint = 0.3),\n    annotation_north_arrow(location = \"tr\", height = unit(1, \"cm\"), width = unit(1, \"cm\"),\n                           style = north_arrow_fancy_orienteering)\n  )\n}\n\n# CORES PADRONIZADAS\ncor_zero &lt;- \"white\"\ncor_um   &lt;- \"#FDE725\"\n\n# Mapa Observado\ng_obs &lt;- ggplot(sp_dados) +\n  geom_sf(aes(fill = as.factor(y_bin)), color = \"black\", lwd = 0.05) +\n  scale_fill_manual(values = c(\"0\" = cor_zero, \"1\" = cor_um), name = \"Observado\") +\n  labs(title = \"C. Observado (Binário)\", subtitle = \"Valor Real\") +\n  theme_void() + \n  theme(legend.position = \"bottom\", legend.box.spacing = unit(0, \"pt\")) +\n  annotation_scale(location = \"br\", width_hint = 0.3) +\n  annotation_north_arrow(location = \"tr\", height = unit(0.8, \"cm\"), width = unit(0.8, \"cm\"), \n                         style = north_arrow_fancy_orienteering)\n\n#Mapa Predito\ng_pred &lt;- ggplot(sp_dados) +\n  geom_sf(aes(fill = as.factor(cat_predita)), color = \"black\", lwd = 0.05) +\n  scale_fill_manual(values = c(\"0\" = cor_zero, \"1\" = cor_um), name = \"Predito\") +\n  labs(title = \"C. Predito pelo Modelo\") +\n  theme_void() + \n  theme(legend.position = \"bottom\", legend.box.spacing = unit(0, \"pt\")) +\n  annotation_scale(location = \"br\", width_hint = 0.3) +\n  annotation_north_arrow(location = \"tr\", height = unit(0.8, \"cm\"), width = unit(0.8, \"cm\"), \n                         style = north_arrow_fancy_orienteering)\n\n#Gráfico de Impactos\ng_imp &lt;- ggplot(df_imp, aes(x = Tipo, y = Valor, fill = Tipo)) +\n  geom_col(width = 0.5, color = \"black\", alpha = 0.9) +\n  geom_text(aes(label = round(Valor, 3)), vjust = -0.5, fontface = \"bold\") +\n  scale_fill_viridis_d(option = \"viridis\", begin = 0.3, end = 0.9) +\n  labs(title = \"A. Efeitos Marginais Médios\", \n       y = \"Mudança na Probabilidade\", x = NULL) +\n  theme_minimal() + theme(legend.position = \"none\")\n\n#Mapa de Resíduos\ng_resid &lt;- ggplot(sp_dados) +\n  geom_sf(aes(fill = resid_generalized), color = \"black\", lwd = 0.02) +\n  scale_fill_distiller(palette = \"RdBu\", direction = -1,\n                       name = \"Resíduo\") +\n  labs(title = \"D. Resíduos\", \n       subtitle = paste0(\"Autocorrelação:\\n\", label_moran)) +\n  theme_void()+\n  theme(legend.box.spacing = unit(0, \"pt\")) +\n  annotation_scale(location = \"br\", width_hint = 0.3) +\n  annotation_north_arrow(location = \"tr\", height = unit(0.8, \"cm\"), width = unit(0.8, \"cm\"),\n                         style = north_arrow_fancy_orienteering)\n\n(g_obs | g_pred) / (g_imp | g_resid) + plot_layout(heights = c(1.2, 1))\n\n\n\n\n\n\n\nFigura 4.19: (A) Efeitos Marginais, (B) Resíduos Generalizados, (C) Observado e (D) Probabilidade Predita.\n\n\n\nInterpretação\nA análise comparativa entre a distribuição dos eventos observados (Figura 4.19 C) e as probabilidades preditas (Figura 4.19 D) indica que o modelo probit identificou as zonas de maior suscetibilidade, recuperando o padrão geográfico do fenômeno. O efeito indireto (\\(0.192\\); Figura 4.19 A) supera o efeito direto (\\(0.073\\); Figura 4.19 A), sugerindo que a alteração na probabilidade do evento local depende preponderantemente das características das vizinhanças. Entretanto, o diagnóstico dos resíduos apresentado em Figura 4.19 (B) evidencia uma violação do pressuposto de independência condicional. A estatística de I de Moran global aplicada aos resíduos (\\(I = 0.076\\)), estatisticamente significativa (\\(p \\approx 0\\)), aponta para a persistência de autocorrelação espacial positiva não capturada pelo termo de defasagem, indicando que a especificação atual é insuficiente para filtrar a totalidade da dependência espacial latente.\n\n\n4.14.2 Modelo Probit Ordenado Espacial\nO Modelo Probit Ordenado Espacial é uma extensão para dados categóricos ordenados em contextos espaciais. Ele é adequado quando a variável dependente observada, \\(y_i\\), assume valores em categorias ordenadas, como classificações de intensidade, níveis de satisfação ou escalas de severidade, e existe interdependência espacial entre as unidades de observação.\nA premissa central do modelo é que a variável categórica ordenada observada é uma manifestação censurada de uma variável latente contínua subjacente, \\(y_i^*\\), que representa uma propensão ou utilidade não observada. A dependência espacial é modelada diretamente no processo gerador desta variável latente.\nSeguindo a estrutura autorregressiva espacial, a equação estrutural para a variável latente é especificada como (J. LeSage e Pace 2009; Wang e Kockelman 2009):\n\\[\n\\mathbf{y}^* = \\rho \\mathbf{W}\\mathbf{y}^* + \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_n).\n\\]\nA variável observada \\(y_i\\) está relacionada com sua contraparte latente através de um conjunto de limiares ordenados, \\(\\gamma = (\\gamma_0, \\gamma_1, \\dots, \\gamma_J, \\gamma_{J+1})'\\). Para uma observação \\(i\\) com \\(J+1\\) categorias possíveis (\\(j = 0, 1, \\dots, J\\)):\n\\[\ny_i = j \\quad \\text{se, e somente se,} \\quad \\gamma_j &lt; y_i^* \\le \\gamma_{j+1}.\n\\]\nPara identificação do modelo, fixam-se os valores \\(\\gamma_0 = -\\infty\\), \\(\\gamma_1 = 0\\) e \\(\\gamma_{J+1} = \\infty\\). Os limiares restantes, \\(\\gamma_2, \\dots, \\gamma_J\\), são parâmetros desconhecidos a serem estimados, sujeitos à restrição de ordenamento \\(\\gamma_2 &lt; \\gamma_3 &lt; \\dots &lt; \\gamma_J\\). A variância do erro \\(\\epsilon_i\\) é fixada em 1, conforme convenção padrão em modelos Probit.\nResolvendo a equação estrutural para o vetor latente \\(\\mathbf{y}^*\\), obtém-se sua forma reduzida:\n\\[\n\\mathbf{y}^* = (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{u}, \\quad \\text{onde} \\quad \\mathbf{u} = (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\boldsymbol{\\epsilon}.\n\\]\nA matriz de covariância do termo de erro composto \\(\\mathbf{u}\\) é:\n\\[\n\\operatorname{Cov}(\\mathbf{u}) = \\boldsymbol{\\Omega} = [(\\mathbf{I}_n - \\rho \\mathbf{W})'(\\mathbf{I}_n - \\rho \\mathbf{W})]^{-1}.\n\\]\nEsta matriz é não diagonal e seus elementos diagonais, \\(\\sigma_i^2 = \\boldsymbol{\\Omega}_{ii}\\), não são constantes. Eles variam em função da posição de cada unidade \\(i\\) na estrutura de vizinhança definida por \\(\\mathbf{W}\\). Consequentemente, o modelo exibe heterocedasticidade induzida espacialmente. A aplicação de um estimador Probit Ordenado padrão, que assume independência e homocedasticidade, a dados gerados por este processo resulta em estimadores inconsistentes para \\(\\boldsymbol{\\beta}\\), \\(\\rho\\) e \\(\\boldsymbol{\\gamma}\\), comprometendo a inferência (Wang e Kockelman 2009).\nA função de verossimilhança do modelo envolve o cálculo de probabilidades de uma distribuição normal multivariada restrita a ortantes definidos pelos limiares, uma tarefa computacionalmente intratável para amostras de tamanho moderado. A abordagem Bayesiana, utilizando métodos de Monte Carlo via Cadeias de Markov (MCMC) com aumento de dados (data augmentation), supera esta dificuldade e tornou-se o método padrão (J. LeSage e Pace 2009).\nO algoritmo de amostragem de Gibbs itera entre os seguintes passos, amostrando de cada distribuição condicional completa:\n\nVariável Latente (\\(\\mathbf{y}^*\\)): Condicional aos parâmetros \\((\\boldsymbol{\\beta}, \\rho, \\boldsymbol{\\gamma})\\) e aos dados observados \\(\\mathbf{y}\\), cada \\(y_i^*\\) é amostrado de uma distribuição normal univariada truncada. O suporte de truncagem é o intervalo \\((\\gamma_{y_i}, \\gamma_{y_i+1}]\\), determinado pela categoria observada \\(y_i\\). A média e variância condicionais de \\(y_i^*\\) dependem dos valores latentes atuais das unidades vizinhas, requerendo algoritmos eficientes como o de Geweke para a amostragem.\nParâmetros de regressão e espacial (\\(\\boldsymbol{\\beta}, \\rho\\)): Condicional ao vetor latente completo \\(\\mathbf{y}^*\\), o modelo reduz-se a um modelo autorregressivo espacial (SAR) linear contínuo. Os parâmetros \\(\\boldsymbol{\\beta}\\) são amostrados de uma distribuição normal multivariada, e \\(\\rho\\) é amostrado via um passo de Metropolis-Hastings, utilizando suas distribuições condicionais completas padrão.\nParâmetros de limiar (\\(\\boldsymbol{\\gamma}\\)): Condicional a \\(\\mathbf{y}^*\\), os limiares \\(\\gamma_j\\) (para \\(j=2, \\dots, J\\)) são amostrados de distribuições uniformes em intervalos restritos. O intervalo para \\(\\gamma_j\\) é determinado pelos valores latentes das observações nas categorias adjacentes: \\[\n\\gamma_j \\mid \\mathbf{y}^*, \\mathbf{y} \\sim \\mathcal{U}\\left( \\max_{\\{i: y_i = j-1\\}} y_i^*, \\min_{\\{i: y_i = j\\}} y_i^* \\right).\n\\] Para garantir uma boa mistura da cadeia de Markov, emprega-se frequentemente o algoritmo de Cowles (1996) (Tobias 2024).\n\nA interpretação dos coeficientes \\(\\boldsymbol{\\beta}\\) no modelo Probit Ordenado Espacial não é direta. O sinal de \\(\\beta_k\\) indica a direção do efeito da variável \\(x_k\\) sobre a variável latente \\(y_i^*\\), mas o efeito sobre as probabilidades das categorias observadas \\(P(y_i = j)\\) é ambíguo e depende dos limiares estimados \\(\\boldsymbol{\\gamma}\\) (Greene 2003). Além disso, a presença do multiplicador espacial \\((\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\) implica que uma mudança em uma covariável para a unidade \\(i\\) gera efeitos de transbordamento (spillovers) sobre as probabilidades de todas as outras unidades.\nPortanto, a análise deve concentrar-se nos efeitos marginais espaciais para cada categoria \\(j\\). O efeito marginal de uma mudança na covariável \\(x_{rk}\\) (da unidade \\(r\\)) sobre a probabilidade da unidade \\(i\\) pertencer à categoria \\(j\\) é dado por:\n\\[\n\\frac{\\partial P(y_i = j \\mid \\mathbf{X})}{\\partial x_{rk}} = \\left[ \\phi(\\gamma_j - \\mu_i) - \\phi(\\gamma_{j+1} - \\mu_i) \\right] \\cdot \\frac{ \\left[ (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1} \\beta_k \\right]_{ir} }{\\sigma_i},\n\\]\nonde \\(\\mu_i\\) é o i-ésimo elemento do vetor \\((\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\mathbf{X}\\boldsymbol{\\beta}\\), \\(\\sigma_i = \\sqrt{\\boldsymbol{\\Omega}_{ii}}\\), e \\(\\phi(\\cdot)\\) é a função de densidade da normal padrão. O termo \\(\\left[ (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1} \\beta_k \\right]_{ir}\\) representa o elemento \\((i, r)\\) da matriz de multiplicador espacial ponderada por \\(\\beta_k\\).\nEstes efeitos são decompostos em:\n\nEfeito direto médio: O impacto médio de uma mudança em \\(x_{ik}\\) sobre a probabilidade da própria unidade \\(i\\) estar na categoria \\(j\\).\nEfeito indireto (ou de Transbordamento) médio: O impacto médio cumulativo de uma mudança em \\(x_{ik}\\) sobre as probabilidades de todas as outras unidades \\(r \\neq i\\) estarem na categoria \\(j\\).\nEfeito total médio: A soma dos efeitos direto e indireto.\n\nDada a não linearidade do modelo, estes efeitos variam para cada observação. A prática padrão é reportar as médias amostrais dos efeitos diretos, indiretos e totais para cada categoria \\(j\\), juntamente com medidas de incerteza (como intervalos de credibilidade) obtidas a partir das amostras da cadeia MCMC.\n\nCódigo\n#\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialprobit, spdep, sf, geobr, ggplot2, viridis, \n               kableExtra, dplyr, Matrix, patchwork, ggspatial, scales, truncnorm)\n\n#Ignore esta parte\n\nif (!exists(\"sp_dados\") || !(\"y_ordered\" %in% names(sp_dados))) {\n  message(\"Baixando shapefile e simulando dados...\")\n  sp_dados &lt;- geobr::read_municipality(code_muni = \"SP\", year = 2020, showProgress = FALSE)\n\n  sp_dados &lt;- sp_dados[!is.na(st_dimension(sp_dados)), ]\n  coords &lt;- st_coordinates(st_centroid(sp_dados))\n  \n  # Matriz de Vizinhança (k=6)\n  knn &lt;- knearneigh(coords, k = 6)\n  nb_sp &lt;- knn2nb(knn)\n  lw_sp &lt;- nb2listw(nb_sp, style = \"W\")\n  W_mat &lt;- as(lw_sp, \"CsparseMatrix\")\n  \n  set.seed(123) \n  n &lt;- nrow(sp_dados)\n  rho_true &lt;- 0.60\n  beta_x &lt;- 2.0\n  intercept_true &lt;- -0.5\n  \n  sp_dados$x_var &lt;- rnorm(n, 0, 1)\n  \n  # SAR: y* = (I - rho W)^-1 (Xb + e)\n  I_n &lt;- Matrix::Diagonal(n)\n  inv_spatial &lt;- solve(I_n - rho_true * W_mat)\n  epsilon &lt;- rnorm(n, 0, 1)\n  \n  xb &lt;- intercept_true + (beta_x * sp_dados$x_var)\n  y_latente &lt;- as.vector(inv_spatial %*% (xb + epsilon))\n  \n  cortes_sim &lt;- quantile(y_latente, probs = c(0.33, 0.66))\n  sp_dados$y_ordered &lt;- cut(y_latente, \n                            breaks = c(-Inf, cortes_sim, Inf), \n                            labels = FALSE)\n} else {\n\n    if (!exists(\"W_mat\")) {\n    knn &lt;- knearneigh(st_coordinates(st_centroid(sp_dados)), k = 6)\n    nb_sp &lt;- knn2nb(knn)\n    lw_sp &lt;- nb2listw(nb_sp, style = \"W\")\n    W_mat &lt;- as(lw_sp, \"CsparseMatrix\")\n  }\n}\n\n# AJUSTE DO MODELO (ORDERED PROBIT)\nmod_sar_ordered &lt;- sarorderedprobit(y_ordered ~ x_var, \n                                    W = W_mat, \n                                    data = sp_dados, \n                                    ndraw = 2000, \n                                    burn.in = 500, \n                                    showProgress = FALSE)\n\n#TABELA DE RESULTADOS E IMPACTOS\n\nall_draws &lt;- as.data.frame(mod_sar_ordered$B)\nif (!is.null(mod_sar_ordered$names)) colnames(all_draws) &lt;- mod_sar_ordered$names\nif (!any(grepl(\"rho\", colnames(all_draws), ignore.case = TRUE))) all_draws$rho &lt;- as.vector(mod_sar_ordered$rho)\n\nresumo_bayesiano &lt;- data.frame(\n  Parametro  = names(all_draws),\n  Estimativa = colMeans(all_draws),\n  IC_Inf     = apply(all_draws, 2, quantile, probs = 0.025),\n  IC_Sup     = apply(all_draws, 2, quantile, probs = 0.975)\n)\n\n# Formatação da Tabela\nresumo_bayesiano$Resultado &lt;- sprintf(\"%.3f [%.3f, %.3f]\", \n                                      resumo_bayesiano$Estimativa, \n                                      resumo_bayesiano$IC_Inf, \n                                      resumo_bayesiano$IC_Sup)\n\nresumo_bayesiano$Parametro &lt;- dplyr::case_when(\n  resumo_bayesiano$Parametro %in% c(\"(Intercept)\", \"beta_1\") ~ \"Intercepto\",\n  resumo_bayesiano$Parametro %in% c(\"x_var\", \"beta_2\") ~ \"Variável X\",\n  grepl(\"rho\", resumo_bayesiano$Parametro, ignore.case = TRUE) ~ \"$\\\\rho$ (Dependência)\",\n  TRUE ~ resumo_bayesiano$Parametro\n)\n\ntabela_final &lt;- resumo_bayesiano %&gt;%\n  filter(!duplicated(Parametro)) %&gt;%\n  dplyr::select(Parametro, Resultado)\nrownames(tabela_final) &lt;- NULL\n\nkbl(tabela_final, \n    format = \"latex\", \n    booktabs = TRUE, \n    caption = \"\", \n    escape = FALSE) %&gt;%\n  kable_styling(latex_options = c(\"HOLD_position\", \"striped\"), \n                full_width = FALSE, \n                position = \"center\") %&gt;%\n  row_spec(0, bold = TRUE) \n\n\n\n\nTabela 4.13: Estimativas do modelo Probit Ordenado\n\n\n\n\n\n\nInterpretação\nA Tabela Tabela 4.13 apresenta os parâmetros estruturais do modelo. A covariável \\(X\\) exerce uma influência positiva sobre a variável latente (\\(0.863\\); \\(IC_{95\\%} [0.762, 0.963]\\)), indicando que elevações nesta preditora aumentam a probabilidade de classificação nos estratos ordinais superiores. Simultaneamente, a dependência espacial é confirmada pela estimativa de \\(\\rho\\) (\\(0.437\\); \\(IC_{95\\%} [0.331, 0.537]\\)), demonstrando que o nível ordenado de uma observação é positivamente correlacionado com o status de sua vizinhança geográfica. A calibração dos limiares (thresholds), especificamente o parâmetro de corte \\(y \\ge 3\\) (\\(0.865\\); \\(IC_{95\\%} [0.802, 0.941]\\)), define as fronteiras probabilísticas que segregam as categorias mais elevadas da distribuição.\n\nCódigo\n#\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialprobit, spdep, sf, geobr, ggplot2, viridis, \n               kableExtra, dplyr, Matrix, patchwork, ggspatial, scales, truncnorm)\n\n\n# Cálculo dos Impactos (Médios)\nrho_medio &lt;- mean(mod_sar_ordered$rho)\nbeta_val &lt;- resumo_bayesiano$Estimativa[resumo_bayesiano$Parametro == \"Variável X\"]\n\nimpacto_total  &lt;- beta_val / (1 - rho_medio)\nimpacto_direto &lt;- beta_val \nimpacto_indireto &lt;- impacto_total - impacto_direto\n\ndf_imp &lt;- data.frame(\n  Tipo = factor(c(\"direct\", \"indirect\", \"total\"), \n                levels = c(\"direct\", \"indirect\", \"total\"),\n                labels = c(\"Direto\", \"Indireto\", \"Total\")),\n  Valor = c(impacto_direto, impacto_indireto, impacto_total)\n)\n\n\n\n# PREDIÇÃO E CLASSIFICAÇÃO \nsp_dados$latente_predita &lt;- as.vector(fitted(mod_sar_ordered))\n\n#Definição dos Cortes (Breaks): O vetor 'phi' contém os limites: 0 (fixo) e o valor estimado\nbreaks_finais &lt;- c(-Inf, mod_sar_ordered$phi, Inf)\n\n#Classificação\nsp_dados$cat_predita_class &lt;- cut(sp_dados$latente_predita, \n                                  breaks = breaks_finais, \n                                  labels = FALSE)\n\n#\nif(any(is.na(sp_dados$cat_predita_class))) {\n  sp_dados$cat_predita_class[is.na(sp_dados$cat_predita_class)] &lt;- 1\n}\n\n\n#CÁLCULO DOS RESÍDUOS GENERALIZADOS\n\n#Chesher, A. and Irish, M., 1987. Residual analysis in the grouped and censored normal linear model. Journal of Econometrics, 34(1-2), pp.33-61.\n\n#Gourieroux, C., Monfort, A., Renault, E. and Trognon, A., 1987. Generalised residuals. Journal of econometrics, 34(1-2), pp.5-32.\n\n\nbeta_hat &lt;- resumo_bayesiano$Estimativa[resumo_bayesiano$Parametro == \"Variável X\"] \nintercepto &lt;- resumo_bayesiano$Estimativa[resumo_bayesiano$Parametro == \"Intercepto\"]\nrho_hat &lt;- mean(mod_sar_ordered$rho)\ncuts &lt;- c(-Inf, 0, mod_sar_ordered$phi, Inf) # Cuts: 0 é fixo no spatialprobit\n\n#y* = (I - rho*W)^-1 * (X*beta)\n\nX_mat &lt;- model.matrix(~ x_var, data = sp_dados) \nbetas_vec &lt;- c(intercepto, beta_hat) # Ordem deve bater com X_mat\n\n#X * Beta\nxb &lt;- X_mat %*% betas_vec\n\n#(I - rho * W)^-1\n\nI_n &lt;- Matrix::Diagonal(nrow(sp_dados)) #I\nS_inv &lt;- solve(I_n - rho_hat * W_mat) #(I - rho * W)^-1\ny_star_pred &lt;- as.vector(S_inv %*% xb)  #(I - rho * W)^-1 *xb\n\n#E[y* | y_obs]: mu + sigma * (pdf(a) - pdf(b)) / (cdf(b) - cdf(a))\n\ny_obs &lt;- as.numeric(sp_dados$y_ordered)\nlo &lt;- cuts[y_obs]     # Limite inferior da categoria observada\nhi &lt;- cuts[y_obs + 1] # Limite superior da categoria observada\n\nz_lo &lt;- lo - y_star_pred\nz_hi &lt;- hi - y_star_pred\n\nsafe_pnorm &lt;- function(q) pnorm(q)\nsafe_dnorm &lt;- function(x) dnorm(x)\n\ndiff_cdf &lt;- safe_pnorm(z_hi) - safe_pnorm(z_lo)\ndiff_cdf[diff_cdf &lt; 1e-10] &lt;- 1e-10 \n\ndiff_pdf &lt;- safe_dnorm(z_lo) - safe_dnorm(z_hi) # Note a ordem: pdf(lo) - pdf(hi)\n\n# E[y* | y] = mu + (phi(lo) - phi(hi)) / (Phi(hi) - Phi(lo))\nsp_dados$y_latente_esperada &lt;- y_star_pred + (diff_pdf / diff_cdf)\n\n# u = (I - rho*W) * E[y*|y] - X*beta\nA_mat &lt;- (I_n - rho_hat * W_mat)\nterm_spatial_removed &lt;- as.vector(A_mat %*% sp_dados$y_latente_esperada)\n\nsp_dados$resid_generalized &lt;- term_spatial_removed - as.vector(xb)\n\n#Teste de Moran\nmoran_resid &lt;- moran.test(sp_dados$resid_generalized, lw_sp)\n\nlabel_moran &lt;- paste0(\"Moran (Gen. Resid): \", round(moran_resid$estimate[1], 3), \n                      \" (p: \", round(moran_resid$p.value, 3), \")\")\n\nmax_res &lt;- max(abs(sp_dados$resid_generalized), na.rm=TRUE)\n\n# Graficos\ntheme_map_custom &lt;- function() {\n  list(\n    theme_void(),\n    theme(\n      legend.position = \"bottom\", \n      legend.box.spacing = unit(5, \"pt\"),\n      legend.title = element_text(size=9, face=\"bold\"),\n      plot.title = element_text(face=\"bold\", size=12, hjust = 0),\n      plot.subtitle = element_text(size=9, color=\"grey30\")\n    ),\n    annotation_scale(location = \"br\", width_hint = 0.3),\n    annotation_north_arrow(location = \"tr\", height = unit(1, \"cm\"), width = unit(1, \"cm\"),\n                           style = north_arrow_fancy_orienteering)\n  )\n}\n\n#Observado\ng_obs &lt;- ggplot(sp_dados) +\n  geom_sf(aes(fill = factor(y_ordered, levels = 1:3)), color = \"white\", lwd = 0.02) +\n  scale_fill_viridis_d(option = \"viridis\", name = \"Observed\", drop = FALSE) +\n  labs(title = \"A. Dados Observados\", subtitle = \"Variável Dependente Real\") +\n  theme_map_custom()\n\n#Predito\ng_pred &lt;- ggplot(sp_dados) +\n  geom_sf(aes(fill = factor(cat_predita_class, levels = 1:3)), color = \"white\", lwd = 0.02) +\n  scale_fill_viridis_d(option = \"viridis\", name = \"Predicted\", drop = FALSE) +\n  labs(title = \"B. Predição do Modelo\") +\n  theme_map_custom()\n\n#Impactos\ng_imp &lt;- ggplot(df_imp, aes(x = Tipo, y = Valor, fill = Tipo)) +\n  geom_col(width = 0.6, color = \"black\", alpha = 0.8) +\n  geom_text(aes(label = round(Valor, 2)), vjust = -0.5, size=4, fontface = \"bold\") +\n  scale_fill_viridis_d(option = \"cividis\", begin = 0.2, end = 0.8) +\n  labs(title = \"C. Decomposição de Impactos\", y = \"Magnitude\", x = NULL) +\n  theme_minimal() + \n  theme(legend.position = \"none\", panel.grid.minor = element_blank())\n\n#Resíduos\nmax_res &lt;- max(abs(sp_dados$resid_generalized), na.rm=TRUE)\ng_resid &lt;- ggplot(sp_dados) +\n  geom_sf(aes(fill = resid_generalized), color = \"white\", lwd = 0.02) +\n  scale_fill_distiller(palette = \"RdBu\", direction = -1, \n                       limits = c(-max_res, max_res),\n                       name = \"Resíduo\") +\n  labs(title = \"D. Resíduos\", \n       subtitle = paste0(\"Autocorrelação:\\n\", label_moran)) +\n  theme_map_custom()\n\n\ng_obs+g_pred+g_imp+g_resid\n\n\n\n\n\n\n\nFigura 4.20: (A) Observado, (B) Predito, (C) Impactos e (D) Resíduos Generalizados (Estimativa do erro estrutural).\n\n\n\nInterpretação\nOs padrões observados em Figura 4.20 (A) e as estimativas do modelo em Figura 4.20 (B), demonstram a eficácia da especificação modelo probit ordinal na recuperação da estrutura espacial das categorias, replicando com fidelidade a heterogeneidade e as aglomerações regionais. A decomposição dos impactos ilustrada em Figura 4.20 (C) revela que a dinâmica do fenômeno é governada preponderantemente por fatores intra-regionais, uma vez que a magnitude do efeito direto (\\(0.86\\)) supera a do efeito indireto (\\(0.67\\)), embora a influência do transbordamento espacial permaneça substantiva. A validade estatística das inferências é assegurada em Figura 4.20 (D), onde a análise dos resíduos generalizados atesta a ausência de padrões espaciais sistemáticos; a estatística de I de Moran (\\(-0.077\\)) associada ao p-valor unitário (\\(p=1\\)) confirma que o modelo filtrou adequadamente a autocorrelação espacial, garantindo a independência estocástica dos erros.\n\n\n\n\n\n\nNotaEstimativas do Modelo SAR Probit Ordenado\n\n\n\nPara ajustar o modelo certifique-se de que \\(\\mathbf{y}^*\\) (seja discreta), \\(\\mathbf{y}^* \\in \\{1, \\dots, J\\}\\). Após ajustar o modelo, os resultados não serão categóricos, e sim quantitativos contínuos. A transição da escala contínua para as categorias discretas observadas \\(\\mathbf{y}^* \\in \\{1, \\dots, J\\}\\) é determinada pelo vetor de limiares \\(\\boldsymbol{\\gamma}\\) (referenciado internamente no software como phi), onde o primeiro corte \\(\\gamma_1\\) é restrito a zero por definição. Consequentemente, os valores ajustados para beta, rho e phi representam as médias a posteriori das distribuições dos parâmetros estimados via MCMC. Para fins de diagnóstico e inferência estatística, o objeto (modelo ajustado) armazena não apenas essas médias pontuais e os valores ajustados, mas também as cadeias completas de simulação (bdraw, pdraw, phidraw) e os metadados da matriz de pesos \\(\\mathbf{W}\\), permitindo ao pesquisador avaliar a convergência e a incerteza associada aos parâmetros espaciais e aos cortes.\n\n\n\n\n4.14.3 Modelo Tobit Espacial (Spatial Tobit Model)\nO Modelo Tobit Espacial é a extensão para processos espaciais onde a variável dependente observada é contínua, mas sujeita a censura. Esta situação ocorre frequentemente quando a variável de interesse assume um valor limite (comumente zero) para uma proporção substantiva das observações, enquanto apresenta variação contínua acima desse limite. Exemplos incluem dados de despesas, fluxos comerciais ou níveis de poluição, onde muitas unidades registram zero, mas os valores positivos são contínuos (J. LeSage e Pace 2009).\nA aplicação de um modelo Tobit padrão, que assume independência entre as observações, ou de um modelo linear espacial, que ignora a censura, a dados com tais características produz estimadores viesados e inconsistentes dos parâmetros de interesse.\nO modelo é formulado utilizando uma variável latente contínua não observada, \\(y_i^*\\), que segue um processo autorregressivo espacial. Para o Modelo Tobit Espacial Autorregressivo (SAR Tobit), a equação estrutural é (J. LeSage e Pace 2009):\n\\[\ny_i^* = \\rho \\sum_{j=1}^n w_{ij} y_j^* + \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2).\n\\]\nA variável observada \\(y_i\\) relaciona-se com sua contraparte latente através da seguinte regra de censura à esquerda no limiar zero:\n\\[\ny_i = \\max(0, y_i^*).\n\\]\nAqui, \\(\\mathbf{W}\\) é a matriz de pesos espaciais, \\(\\rho\\) o parâmetro de autocorrelação espacial, \\(\\mathbf{X}\\) a matriz de covariáveis e \\(\\boldsymbol{\\beta}\\) o vetor de coeficientes.\nA introdução da dependência espacial na variável latente tem implicações importantes na estrutura do erro na forma reduzida. Resolvendo a equação estrutural para o vetor latente \\(\\mathbf{y}^*\\), obtém-se:\n\\[\n\\mathbf{y}^* = (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\boldsymbol{\\epsilon}.\n\\]\nDefinindo o termo de erro composto como \\(\\tilde{\\boldsymbol{\\epsilon}} = (\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\boldsymbol{\\epsilon}\\), sua matriz de covariância é:\n\\[\n\\operatorname{Cov}(\\tilde{\\boldsymbol{\\epsilon}}) = \\sigma^2 [(\\mathbf{I}_n - \\rho \\mathbf{W})(\\mathbf{I}_n - \\rho \\mathbf{W})^{\\top}]^{-1}.\n\\]\nConforme demonstrado por McMillen (1992), os elementos da diagonal principal desta matriz não são constantes. Cada variância \\(\\tilde{\\sigma}_i^2\\) é uma função da localização da unidade \\(i\\) na rede definida por \\(\\mathbf{W}\\). Esta heterocedasticidade induzida espacialmente na forma reduzida do modelo é uma consequência direta da dependência espacial. Em modelos não lineares como o Tobit, a violação do pressuposto de homocedasticidade leva à inconsistência do estimador de máxima verossimilhança padrão, não apenas a uma perda de eficiência (Billé e Arbia 2019; H. Kelejian e Piras 2017).\nA função de verossimilhança para o modelo Tobit Espacial envolve a probabilidade conjunta de observar os valores \\(\\mathbf{y}\\), o que requer a integração sobre uma distribuição normal multivariada truncada, com a dimensão da integral igual ao número de observações censuradas. Este cálculo é computacionalmente intratável para amostras de tamanho moderado ou grande.\nA literatura propõe várias estratégias para superar este obstáculo:\n\nAlgoritmo EM (Expectation-Maximization): McMillen (1992) adaptou o algoritmo EM para este contexto. O método itera entre um passo E, que imputa os valores esperados da variável latente \\(y_i^*\\) para as observações censuradas (condicional nos parâmetros atuais), e um passo M, que maximiza a verossimilhança de um modelo espacial linear contínuo utilizando os dados latentes completos. Embora produza estimativas consistentes, a obtenção de erros-padrão válidos é não trivial.\nAbordagem Bayesiana (MCMC com Aumento de Dados): Esta é uma abordagem robusta e amplamente utilizada, detalhada por J. P. LeSage (2000) e J. LeSage e Pace (2009). O método emprega amostragem de Gibbs e uma estratégia de aumento de dados (data augmentation), tratando os valores latentes das observações censuradas como parâmetros a serem estimados. O algoritmo amostra sequencialmente das seguintes distribuições condicionais completas:\n\nOs parâmetros \\((\\boldsymbol{\\beta}, \\rho, \\sigma^2)\\) condicionais ao vetor latente completo \\(\\mathbf{y}^*\\). Dado \\(\\mathbf{y}^*\\), o problema reduz-se à estimação de um modelo SAR Bayesiano padrão.\nA variável latente \\(\\mathbf{y}^*\\) condicional aos parâmetros e aos dados observados \\(\\mathbf{y}\\). Para uma observação não censurada (\\(y_i &gt; 0\\)), temos \\(y_i^* = y_i\\).\n\n\nPara uma observação censurada (\\(y_i = 0\\)), amostra-se \\(y_i^*\\) de uma distribuição normal truncada à esquerda em zero, \\(y_i^* | \\cdot \\sim \\mathcal{N}_{(-\\infty, 0]}(E[y_i^* | \\cdot], \\operatorname{Var}(y_i^* | \\cdot))\\), onde a média e a variância condicionais incorporam a dependência espacial dos vizinhos.\nEsta abordagem fornece a distribuição a posteriori completa dos parâmetros, tratando adequadamente a incerteza associada aos valores censurados e à estrutura de dependência.\n\nMétodos de Simulação (GHK): Técnicas de simulação, como o simulador GHK, podem ser empregadas para aproximar a integral da verossimilhança (Fleming 2004). No entanto, o custo computacional pode tornar-se proibitivo para amostras grandes com alta proporção de censura.\nAntes de uma estimação de um modelo complexo, é recomendável testar a presença de dependência espacial nos resíduos de um modelo Tobit padrão (teste de especificação). H. Kelejian e Piras (2017) propõem versões generalizadas do teste I de Moran adaptadas para resíduos de modelos Tobit.\n\nNo modelo Tobit Espacial, a interpretação dos coeficientes \\(\\boldsymbol{\\beta}\\) não é direta. O efeito de uma mudança em uma covariável \\(x_{ik}\\) sobre o valor esperado da variável observada \\(E[y_i | \\mathbf{X}]\\) é não linear e depende do multiplicador espacial global \\((\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\) e da probabilidade de a observação não ser censurada.\nSeguindo a decomposição de J. LeSage e Pace (2009), é necessário calcular os efeitos médios diretos, indiretos (de transbordamento) e totais.\n\nO efeito direto médio captura o impacto esperado de uma mudança em \\(x_{ik}\\) sobre \\(y_i\\), incluindo os feedbacks espaciais que retornam à unidade \\(i\\).\nO efeito indireto médio captura o impacto esperado da mudança em \\(x_{ik}\\) sobre todos os outros \\(y_j\\) (\\(j \\neq i\\)), ou seja, os spillovers espaciais.\nO efeito total médio é a soma dos dois.\n\nEstes efeitos são calculados a partir das derivadas parciais de \\(E[\\mathbf{y} | \\mathbf{X}]\\) em relação a \\(\\mathbf{x}_k\\), que envolvem a matriz \\((\\mathbf{I}_n - \\rho \\mathbf{W})^{-1}\\) e os termos da função de distribuição normal associados à probabilidade de censura. Como variam entre observações, a prática padrão é reportar suas médias amostrais.\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialprobit, spdep, sf, geobr, ggplot2, viridis, kableExtra, dplyr, Matrix, patchwork, ggspatial, scales)\n\n# Preparação e Simulação (DADOS TOBIT - CENSURA EM 0)\n\nif (!exists(\"sp_dados\") || !(\"y_tobit\" %in% names(sp_dados))) {\n  sp_dados &lt;- geobr::read_municipality(code_muni = \"SP\", year = 2020, showProgress = FALSE)\n  coords &lt;- st_coordinates(st_centroid(sp_dados))\n  \n  knn &lt;- knearneigh(coords, k = 6)\n  nb_sp &lt;- knn2nb(knn)\n  lw_sp &lt;- nb2listw(nb_sp, style = \"W\")\n  W_mat &lt;- as(lw_sp, \"CsparseMatrix\")\n  \n  set.seed(123)\n  n &lt;- nrow(sp_dados)\n  rho_true &lt;- 0.60\n  beta_x &lt;- 2.0\n  sigma_true &lt;- 1.5 \n  \n  sp_dados$x_var &lt;- rnorm(n, 0, 1)\n  \n  I_n &lt;- Matrix::Diagonal(n)\n  inv_spatial &lt;- solve(I_n - rho_true * W_mat)\n  epsilon &lt;- rnorm(n, 0, sigma_true)\n  \n  y_latente &lt;- as.vector(inv_spatial %*% (-1 + beta_x * sp_dados$x_var + epsilon))\n  \n  sp_dados$y_tobit &lt;- pmax(0, y_latente)\n  \n} else {\n    if (!exists(\"W_mat\")) {\n      knn &lt;- knearneigh(st_coordinates(st_centroid(sp_dados)), k = 6)\n      nb_sp &lt;- knn2nb(knn)\n      lw_sp &lt;- nb2listw(nb_sp, style = \"W\")\n      W_mat &lt;- as(lw_sp, \"CsparseMatrix\")\n  }\n}\n\n# Ajuste do Modelo (SAR TOBIT)\nmod_sar_tobit &lt;- sartobit(y_tobit ~ x_var, \n                          W = W_mat, \n                          data = sp_dados, \n                          ndraw = 1000, \n                          burn.in = 200, \n                          showProgress = FALSE)\n\n# Tabela de Resultados \n\ndraws_beta &lt;- as.data.frame(mod_sar_tobit$B)\nif (!is.null(mod_sar_tobit$names) && length(mod_sar_tobit$names) == ncol(draws_beta)) {\n  colnames(draws_beta) &lt;- mod_sar_tobit$names\n}\n\n#\nif (!is.null(mod_sar_tobit$pdraw)) {\n  draws_rho &lt;- data.frame(rho = as.vector(mod_sar_tobit$pdraw))\n} else {\n  draws_rho &lt;- data.frame(rho = as.vector(mod_sar_tobit$rho))\n}\n\n#\ndraws_sigma &lt;- data.frame(sigma2 = as.vector(mod_sar_tobit$sdraw))\n\n#\nif (\"rho\" %in% colnames(draws_beta)) {\n  draws_beta &lt;- draws_beta[, !colnames(draws_beta) %in% \"rho\"]\n}\n\nall_draws &lt;- cbind(draws_beta, draws_rho, draws_sigma)\n\n# Estatísticas\nresumo_bayesiano &lt;- data.frame(\n  Parametro  = names(all_draws),\n  Estimativa = colMeans(all_draws),\n  IC_Inf     = apply(all_draws, 2, quantile, probs = 0.025),\n  IC_Sup     = apply(all_draws, 2, quantile, probs = 0.975)\n)\n\nresumo_bayesiano$Resultado &lt;- sprintf(\"%.3f [%.3f, %.3f]\", \n                                      resumo_bayesiano$Estimativa, \n                                      resumo_bayesiano$IC_Inf, \n                                      resumo_bayesiano$IC_Sup)\n\n# Renomear\nresumo_bayesiano$Parametro &lt;- dplyr::case_when(\n  resumo_bayesiano$Parametro %in% c(\"(Intercept)\", \"beta_1\") ~ \"Intercepto\",\n  resumo_bayesiano$Parametro %in% c(\"x_var\", \"beta_2\") ~ \"Variável X\",\n  grepl(\"rho\", resumo_bayesiano$Parametro, ignore.case = TRUE) ~ \"$\\\\rho$ (Dependência)\",\n  grepl(\"sigma\", resumo_bayesiano$Parametro, ignore.case = TRUE) ~ \"$\\\\sigma^2$ (Variância)\",\n  TRUE ~ resumo_bayesiano$Parametro\n)\n\ntabela_final &lt;- resumo_bayesiano %&gt;% dplyr::select(Parametro, Resultado)\nrownames(tabela_final) &lt;- NULL\n\nkbl(tabela_final, \n    format = \"latex\", \n    booktabs = TRUE, \n    caption = \"\", \n    escape = FALSE) %&gt;%\n  kable_styling(latex_options = c(\"HOLD_position\", \"striped\"), \n                full_width = FALSE, \n                position = \"center\") %&gt;%\n  row_spec(0, bold = TRUE) %&gt;%\n  footnote(general = \"Estimativas: Média a Posteriori [Intervalo de Credibilidade 95%].\") \n\n\n\n\nTabela 4.14: Resultados da Estimação: Modelo SAR Tobit (Bayesiano) - SP.\n\n\n\n\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatialprobit, spdep, sf, geobr, ggplot2, viridis, kableExtra, dplyr, Matrix, patchwork, ggspatial, scales)\n\n\n# A. Impactos (Marginais na Latente)\nbeta_val &lt;- resumo_bayesiano$Estimativa[resumo_bayesiano$Parametro == \"Variável X\"]\nrho_val  &lt;- mean(draws_rho$rho, na.rm = TRUE)\n\n#Cálculo\nimpacto_total_latente  &lt;- beta_val / (1 - rho_val)\nimpacto_direto_latente &lt;- beta_val\nimpacto_indireto_latente &lt;- impacto_total_latente - impacto_direto_latente\n\n#\ndf_imp &lt;- data.frame(\n  Tipo = factor(c(\"direct\", \"indirect\", \"total\"), \n                levels = c(\"direct\", \"indirect\", \"total\"),\n                labels = c(\"Direto\", \"Indireto\", \"Total\")),\n  Valor = c(impacto_direto_latente, \n            impacto_indireto_latente, \n            impacto_total_latente)\n)\n\n\n# Resíduos Generalizados (Chesher & Irish, 1987)\nbeta_hat &lt;- colMeans(draws_beta)\nrho_hat  &lt;- mean(draws_rho$rho)\nsigma_hat &lt;- sqrt(mean(draws_sigma$sigma2)) \n\n# Matrizes\nX_mat &lt;- model.matrix(~ x_var, data = sp_dados)\n\nbeta_hat &lt;- beta_hat[colnames(draws_beta) %in% colnames(X_mat) | colnames(draws_beta) == \"(Intercept)\"]\n\nxb &lt;- X_mat %*% beta_hat\n\nI_n &lt;- Matrix::Diagonal(nrow(sp_dados))\nS_inv &lt;- solve(I_n - rho_hat * W_mat)\n\n# Média Latente (Sem censura): mu = (I - rho W)^-1 Xb\ny_star_mu &lt;- as.vector(S_inv %*% xb)\n\n# E[y* | y]: \n# Se y &gt; 0: E = y_obs\n# Se y = 0: E = mu - sigma * (pdf(z)/cdf(z)), onde z = (0 - mu)/sigma \n\ny_obs &lt;- sp_dados$y_tobit\nz_score &lt;- (0 - y_star_mu) / sigma_hat\nmills_ratio &lt;- dnorm(z_score) / pnorm(z_score)\n\ny_latente_generalized &lt;- y_obs\ncensurados &lt;- (y_obs == 0)\n\n# E[y* | y* &lt; 0] é mu - sigma * lambda(-z_score)\ny_latente_generalized[censurados] &lt;- y_star_mu[censurados] - (sigma_hat * mills_ratio[censurados])\n\n# u = (I - rho W) * y_generalized - Xb\nterm_spatial_removed &lt;- as.vector((I_n - rho_hat * W_mat) %*% y_latente_generalized)\nsp_dados$resid_generalized &lt;- term_spatial_removed - as.vector(xb)\n\n# Moran\nmoran_resid &lt;- moran.test(sp_dados$resid_generalized, lw_sp)\nlabel_moran &lt;- paste0(\"I de Moran: \", round(moran_resid$estimate[1], 3), \n                      \" (p: \", round(moran_resid$p.value, 3), \")\")\n\n# Cores\ncor_zero &lt;- \"white\"\ncor_um   &lt;- \"#FDE725\"\n\n# PLOTS\nsp_dados$y_pred_censurado &lt;- pmax(0, y_star_mu) \n\n# C. Mapa Observado \ng_obs &lt;- ggplot(sp_dados) +\n  geom_sf(aes(fill = y_tobit), color = \"black\", lwd = 0.05) +\n  scale_fill_viridis_c(option = \"magma\", direction = -1, name = \"Real\") +\n  labs(title = \"C. Observado (y)\", subtitle = \"Valores Reais (Censurados em 0)\") +\n  theme_void() + \n  theme(legend.position = \"bottom\", legend.box.spacing = unit(0, \"pt\")) +\n  annotation_scale(location = \"br\", width_hint = 0.3) +\n  annotation_north_arrow(location = \"tr\", height = unit(0.8, \"cm\"), width = unit(0.8, \"cm\"), \n                         style = north_arrow_fancy_orienteering)\n\n# Mapa Predito\ng_pred &lt;- ggplot(sp_dados) +\n  geom_sf(aes(fill = y_pred_censurado), color = \"black\", lwd = 0.05) +\n  scale_fill_viridis_c(option = \"magma\", direction = -1, name = \"Predito\",\n                       limits = c(0, max(sp_dados$y_tobit))) +\n  labs(title = \"D. Predição de Valores (y)\", subtitle = \"Expectativa dos Valores Observáveis\") +\n  theme_void() + \n  theme(legend.position = \"bottom\", legend.box.spacing = unit(0, \"pt\")) +\n  annotation_scale(location = \"br\", width_hint = 0.3) +\n  annotation_north_arrow(location = \"tr\", height = unit(0.8, \"cm\"), width = unit(0.8, \"cm\"),\n                         style = north_arrow_fancy_orienteering)\n\n#Impactos\ng_imp &lt;- ggplot(df_imp, aes(x = Tipo, y = Valor, fill = Tipo)) +\n  geom_col(width = 0.5, color = \"black\", alpha = 0.9) +\n  geom_text(aes(label = round(Valor, 3)), vjust = -0.5, fontface = \"bold\") +\n  scale_fill_viridis_d(option = \"viridis\", begin = 0.3, end = 0.9) +\n  labs(title = \"A. Impactos na Latente (y*)\", y = \"Mudança\", x = NULL) +\n  theme_minimal() + theme(legend.position = \"none\")\n\n# Mapa de Resíduos\ng_resid &lt;- ggplot(sp_dados) +\n  geom_sf(aes(fill = resid_generalized), color = \"black\", lwd = 0.05) +\n  scale_fill_gradient2(low = \"#440154\", mid = \"white\", high = \"#FDE725\", midpoint = 0,\n                       limits = c(-max_res, max_res), name = \"Resíduo\") +\n  labs(title = \"B. Resíduos Generalizados\", subtitle = label_moran) +\n  theme_void() + \n  theme(legend.position = \"bottom\", legend.box.spacing = unit(0, \"pt\")) +\n  annotation_scale(location = \"br\", width_hint = 0.3) +\n  annotation_north_arrow(location = \"tr\", style = north_arrow_fancy_orienteering)\n\n(g_obs | g_pred) / (g_imp | g_resid) + plot_layout(heights = c(1.2, 1))\n\n\n\n\n\n\n\nFigura 4.21: Diagnóstico Tobit: (A) Observado (Censurado), (B) Predito (Latente Esperado), (C) Impactos e (D) Resíduos Generalizados.",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#modelos-locais-e-não-estacionários",
    "href": "lattice_data.html#modelos-locais-e-não-estacionários",
    "title": "4  Dados de Área",
    "section": "4.15 Modelos locais e não estacionários",
    "text": "4.15 Modelos locais e não estacionários\nOs modelos vistos até aqui (CAR, SAR, GNS e seus casos particulares) fundamentam-se na premissa de estacionariedade espacial, assumindo que a relação funcional entre a variável dependente e as variáveis explicativas permanece invariante em todo o domínio geográfico. Por exemplo, ao modelar o preço de imóveis no município de São Paulo, esses modelos pressupõem que o impacto de uma variável (como a área construída) é idêntico em todas as regiões. Contudo, em fenômenos geográficos, essa premissa é frequentemente violada, caracterizando a não estacionariedade ou heterogeneidade espacial (Brunsdon, Fotheringham, e Charlton 1996). Na prática, bairros periféricos, como os da Zona Leste (por exemplo, Guaianases, Jardim Ângela e São Mateus), podem apresentar uma valorização marginal menor por metro quadrado, enquanto bairros nobres, como Vila Madalena, Pinheiros, Vila Mariana, Higienópolis, Jardim Paulista e Moema, podem apresentar uma valorização superior devido a fatores de localização e infraestrutura.\nSachdeva, Fotheringham, e Li (2022) demonstram, utilizando modelagem local, que é possível decompor o preço de um imóvel em componentes estruturais e um valor intrínseco da localização (capturado pelo intercepto local), permitindo quantificar quanto se paga apenas por estar em um determinado lugar, ceteris paribus. A aplicação de modelos globais a processos espacialmente heterogêneos tende a produzir estimativas de parâmetros que representam médias espaciais enganosas, mascarando variações locais relevantes e induzindo a erros de especificação (Binbin Lu et al. 2014; Fotheringham, Yang, e Kang 2017).\nPara abordar esta limitação, desenvolveram-se abordagens de modelagem local, entre as quais a Regressão Geograficamente Ponderada (GWR) e a Regressão Geograficamente Ponderada Multiescalar (MGWR).\n\n4.15.1 Regressão Geograficamente Ponderada (GWR)\nA Regressão Geograficamente Ponderada (GWR) é uma técnica de análise espacial local que estende o modelo de regressão linear clássico, permitindo que os coeficientes variem continuamente no espaço. Em vez de estimar um único conjunto de parâmetros globais \\(\\boldsymbol{\\beta}\\), a GWR estima um conjunto distinto de parâmetros \\(\\boldsymbol{\\beta}(u_i, v_i)\\) para cada localização \\(i\\) da amostra (Binbin Lu et al. 2014).\nO modelo GWR para uma observação na localização \\(i\\), com coordenadas \\((u_i, v_i)\\), é expresso por:\n\\[y_i = \\beta_0(u_i, v_i) + \\sum_{k=1}^{p} \\beta_k(u_i, v_i) x_{ik} + \\epsilon_i\\]\nonde:\n\n\\(y_i\\) é o valor da variável dependente na localização \\(i\\);\n\\(x_{ik}\\) é o valor da \\(k\\)-ésima variável independente na localização \\(i\\);\n\\(\\beta_k(u_i, v_i)\\) é o coeficiente de regressão local para a \\(k\\)-ésima variável independente na localização \\(i\\);\n\\(\\epsilon_i\\) é o termo de erro estocástico, tipicamente assumido como \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\).\n\nA estimação dos parâmetros locais \\(\\hat{\\boldsymbol{\\beta}}(u_i, v_i)\\) é realizada através do método de Mínimos Quadrados Ponderados (WLS), onde a ponderação é função da proximidade espacial. O estimador para a localização \\(i\\) é:\n\\[\\hat{\\boldsymbol{\\beta}}(u_i, v_i) = \\left( \\mathbf{X}^\\top \\mathbf{W}(i) \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\top \\mathbf{W}(i) \\mathbf{y}.\\]\nA matriz \\(\\mathbf{W}(i)\\) é uma matriz diagonal \\(n \\times n\\) de pesos espaciais específica para a localização de calibração \\(i\\):\n\\[\n\\mathbf{W}(i) = \\text{diag}\\left(w_{i1}, w_{i2}, \\dots, w_{in}\\right) =\n\\begin{bmatrix}\nw_{i1} & 0 & \\cdots & 0 \\\\\n0 & w_{i2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & w_{in}\n\\end{bmatrix},\n\\]\nCada elemento diagonal \\(w_{ij}\\) representa o peso atribuído à observação \\(j\\) quando o modelo é calibrado para a localização \\(i\\). Estes pesos são determinados por uma função kernel. Embora o kernel Gaussiano seja comum, Gollini et al. (2015) destacam a importância de experimentar diferentes funções, como o kernel Bi-quadrado ou Box-car. O kernel Bi-quadrado, por exemplo, oferece eficiência computacional e um corte claro de influência, sendo definido como:\n\\[\nw_{ij} = \\begin{cases}\n\\left[1 - (d_{ij}/b)^2\\right]^2 & \\text{se } d_{ij} &lt; b \\\\\n0 & \\text{caso contrário}\n\\end{cases}\n\\] onde \\(b &gt; 0\\) é o parâmetro de largura de banda (ver outras em (Gollini et al. 2015)), que controla o decaimento espacial da influência. Para \\(i = j\\), \\(d_{ii} = 0\\), resultando no peso máximo \\(w_{ii} = 1\\). Conforme \\(d_{ij}\\) aumenta, \\(w_{ij}\\) tende assintoticamente a zero, implementando a primeira lei da Geografia de Tobler.\nA seleção da largura de banda ótima \\(b\\) é fundamental, representando um compromisso entre viés e variância. Valores pequenos de \\(b\\) produzem estimativas locais de alta variância (overfitting), enquanto valores grandes introduzem viés, aproximando o modelo de uma regressão global.\nA seleção da largura de banda ótima \\(b\\) pode ser definida de duas formas principais: Fixa (uma distância constante para todas as unidades) ou Adaptativa (um número fixo de \\(N\\) vizinhos mais próximos). Guo, Ma, e Zhang (2008) demonstram empiricamente que, em dados com agrupamento espacial, kernels adaptativos tendem a capturar melhor a heterogeneidade local do que kernels fixos, que podem suavizar excessivamente os padrões em áreas densas e sofrer com escassez de dados em áreas dispersas.\nMais do que um parâmetro técnico de ajuste, Fotheringham et al. (2022) argumentam que a largura de banda deve ser interpretada como um indicador da escala do processo espacial. Segundo os autores, o valor ótimo de \\(b\\) é determinado por três características do processo:\n\nVariabilidade do parâmetro: Processos altamente heterogêneos exigem larguras de banda pequenas.\nDependência espacial: Processos com forte autocorrelação espacial tendem a resultar em larguras de banda menores.\nForça do processo (ruído): Relações fracas ou com alto nível de erro (\\(\\sigma^2\\)) tendem a resultar em larguras de banda maiores, pois o modelo precisa “emprestar” mais dados para reduzir a incerteza da estimativa.\n\nA otimização de \\(b\\) geralmente busca minimizar critérios como a Validação Cruzada (CV) ou o Critério de Informação de Akaike corrigido (AICc) (Hurvich, Simonoff, e Tsai 1998; Binbin Lu et al. 2014):\n\\[\n\\text{AICc} = 2n \\ln(\\hat{\\sigma}) + n \\ln(2\\pi) + n \\frac{n + \\text{tr}(\\mathbf{S})}{n - 2 - \\text{tr}(\\mathbf{S})}.\n\\]\nNesta expressão, \\(\\text{tr}(\\mathbf{S})\\) é o traço da matriz chapéu (hat matrix) \\(\\mathbf{S}\\), que mapeia os valores observados \\(\\mathbf{y}\\) para os preditos \\(\\hat{\\mathbf{y}}\\), representando o número efetivo de parâmetros.\nNa GWR a matriz \\(\\mathbf{S}\\) é construída linha por linha, pois cada observação \\(i\\) possui sua própria matriz de pesos \\(\\mathbf{W}(i)\\).\nConforme derivado por Hanchen Yu et al. (2020), o valor predito \\(\\hat{y}_i\\) é obtido multiplicando-se o vetor de covariáveis da observação \\(i\\), denotado por \\(\\mathbf{x}_i\\) (um vetor linha), pelos parâmetros estimados localmente:\n\\[\\hat{y}_i = \\mathbf{x}_i \\hat{\\boldsymbol{\\beta}}(u_i, v_i) = \\mathbf{x}_i \\left( \\mathbf{X}^\\top \\mathbf{W}(i) \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\top \\mathbf{W}(i) \\mathbf{y}.\\]\nDefinindo o vetor linha \\(\\mathbf{r}_i = \\mathbf{x}_i (\\mathbf{X}^\\top \\mathbf{W}(i) \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{W}(i)\\), a matriz completa \\(\\mathbf{S}\\) de dimensão \\(n \\times n\\) é formada pelo empilhamento destes vetores:\n\\[\\mathbf{S} = \\begin{bmatrix} \\mathbf{r}_1 \\\\ \\mathbf{r}_2 \\\\ \\vdots \\\\ \\mathbf{r}_n \\end{bmatrix}.\\]\nAssim, temos a relação matricial \\(\\hat{\\mathbf{y}} = \\mathbf{S}\\mathbf{y}\\). O traço desta matriz, \\(\\text{tr}(\\mathbf{S})\\), utilizado no denominador do AICc, representa a soma das influências de cada observação sobre o seu próprio valor predito, fornecendo uma medida da complexidade do modelo equivalente aos graus de liberdade em modelos lineares generalizados (Binbin Lu et al. 2014).\nEmbora a minimização do AICc ou a Validação Cruzada (CV) sejam procedimentos padrão, a literatura recente aponta limitações nestes métodos automáticos. Guo, Ma, e Zhang (2008) alertam que o critério AICc tende a ser conservador, selecionando frequentemente larguras de banda maiores que suavizam excessivamente os padrões espaciais, obscurecendo heterogeneidades locais biologicamente ou socialmente relevantes em favor de um ajuste global mais estável.\nAdicionalmente, da Silva e Mendes (2018) demonstram que a função objetivo de Validação Cruzada em modelos GWR com kernels adaptativos frequentemente não é estritamente convexa, apresentando múltiplos mínimos locais. O uso de algoritmos de otimização padrão, como a Busca de Seção Áurea (Golden Section Search), pode convergir para soluções sub-ótimas, sugerindo a necessidade de algoritmos mais robustos como o Lightning Search Algorithm (ver Shareef, Ibrahim, e Mutlag (2015)) ou abordagens híbridas de divisão de intervalo para garantir a identificação do mínimo global.\nAlternativamente, Koç (2022) propõe a substituição dos critérios clássicos por critérios de Complexidade de Informação (ICOMP). Ao penalizar não apenas o número de parâmetros, mas também a interdependência (estrutura de covariância) entre as estimativas dos parâmetros, o ICOMP demonstrou, em estudos de simulação e aplicações reais, selecionar larguras de banda que produzem modelos com maior precisão preditiva e melhor equilíbrio entre viés e variância do que o AICc ou CV.\nA inferência estatística na GWR, como o cálculo de intervalos de confiança e testes de hipóteses para os parâmetros locais, requer a estimação da variância dos coeficientes locais. Reformulando a GWR como um modelo aditivo generalizado (GAM), Hanchen Yu et al. (2020) fornecem a expressão para a matriz de covariância dos estimadores:\n\\[\\text{Var}(\\hat{\\boldsymbol{\\beta}}_j) = \\text{diag}(\\mathbf{C}\\mathbf{C}^\\top \\hat{\\sigma}^2)\\]\nonde \\(\\mathbf{C} = [\\text{diag}(\\mathbf{X}_j)]^{-1}\\mathbf{R}_j\\) e \\(\\mathbf{R}_j\\) é a matriz de projeção específica da covariável. Isso permite o cálculo de erros-padrão locais precisos e o ajuste dos valores críticos da distribuição \\(t\\) para evitar falsos positivos, garantindo que a heterogeneidade espacial detectada seja estatisticamente significante e não apenas ruído aleatório.\nNa reformulação do modelo GWR em modelo GAM, consistiu em não olhar o GWR como uma soma de produtos entre coeficientes variáveis e covariáveis. O vetor de resposta \\(\\mathbf{y}\\) é modelado como uma soma de funções suaves (smooth functions) mais um termo de erro:\n\\[\\mathbf{y} = \\sum_{j=0}^{p} \\mathbf{f}_j + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)\\]\nNo contexto da GWR, cada termo aditivo \\(\\mathbf{f}_j\\) representa o componente espacial da \\(j\\)-ésima variável explicativa. Este termo é definido como o produto elemento a elemento entre a covariável e seu coeficiente espacialmente variável:\n\\[\\mathbf{f}_j = \\text{diag}(\\mathbf{X}_j) \\boldsymbol{\\beta}_j\\]\nonde \\(\\mathbf{X}_j\\) é o vetor \\(n \\times 1\\) contendo as observações da \\(j\\)-ésima variável independente, \\(\\text{diag}(\\mathbf{X}_j)\\) é uma matriz diagonal com esses valores, e \\(\\boldsymbol{\\beta}_j\\) é o vetor \\(n \\times 1\\) dos parâmetros locais para a variável \\(j\\) em todas as localizações.\nA calibração do modelo gera uma matriz de projeção específica \\(\\mathbf{R}_j\\) para cada covariável, tal que o valor ajustado para o componente \\(j\\) é:\n\\[\\hat{\\mathbf{f}}_j = \\mathbf{R}_j \\mathbf{y}\\]\nCombinando as definições, temos que \\(\\text{diag}(\\mathbf{X}_j) \\hat{\\boldsymbol{\\beta}}_j = \\mathbf{R}_j \\mathbf{y}\\). Isolando o vetor de parâmetros estimados \\(\\hat{\\boldsymbol{\\beta}}_j\\), obtemos uma expressão linear em relação a \\(\\mathbf{y}\\):\n\\[\\hat{\\boldsymbol{\\beta}}_j = [\\text{diag}(\\mathbf{X}_j)]^{-1} \\mathbf{R}_j \\mathbf{y} = \\mathbf{C}_j \\mathbf{y}\\]\nonde definimos a matriz linear transformadora como \\(\\mathbf{C}_j = [\\text{diag}(\\mathbf{X}_j)]^{-1} \\mathbf{R}_j\\).\nCom o estimador \\(\\hat{\\boldsymbol{\\beta}}_j\\) expresso como uma combinação linear da variável resposta (\\(\\mathbf{C}_j \\mathbf{y}\\)), a derivação de sua variância torna-se direta, aplicando as propriedades de variância de operadores lineares (\\(\\text{Var}(\\mathbf{A}\\mathbf{y}) = \\mathbf{A}\\text{Var}(\\mathbf{y})\\mathbf{A}^\\top\\)):\n\\[\\text{Var}(\\hat{\\boldsymbol{\\beta}}_j) = \\text{Var}(\\mathbf{C}_j \\mathbf{y}) = \\mathbf{C}_j \\text{Var}(\\boldsymbol{\\epsilon}) \\mathbf{C}_j^\\top = \\mathbf{C}_j (\\sigma^2 \\mathbf{I}) \\mathbf{C}_j^\\top\\]\nPortanto, a matriz de covariância para as estimativas dos parâmetros locais da \\(j\\)-ésima variável é dada por (Hanchen Yu et al. 2020):\n\\[\\text{Var}(\\hat{\\boldsymbol{\\beta}}_j) = \\sigma^2 \\mathbf{C}_j \\mathbf{C}_j^\\top\\]\nOs erros-padrão locais para a localização \\(i\\) e variável \\(j\\) são obtidos simplesmente pela raiz quadrada dos elementos diagonais desta matriz:\n\\[SE(\\hat{\\beta}_{ij}) = \\sqrt{\\left( \\text{Var}(\\hat{\\boldsymbol{\\beta}}_j) \\right)_{ii}}\\]\nEste formalismo permite a construção de estatísticas \\(t\\) locais (\\(t_{ij} = \\hat{\\beta}_{ij} / SE(\\hat{\\beta}_{ij})\\)). Contudo, a realização de testes individuais para cada localização incorre no problema de comparações múltiplas. Para mitigar o aumento da taxa de erro tipo I sem a severidade excessiva da correção de Bonferroni, da Silva e Fotheringham (2016) propõem o ajuste do nível de significância \\(\\alpha\\) baseando-se no número efetivo de parâmetros (\\(\\text{ENP}_j = \\text{tr}(\\mathbf{R}_j)\\)) específico de cada covariável, garantindo que a heterogeneidade espacial detectada seja estatisticamente significativa.\nUma limitação inerente do modelo GWR é a suposição de uma única escala espacial para todos os processos, uma vez que utiliza uma largura de banda \\(b\\) comum a todas as variáveis independentes. Como notado por (Wenbai Yang, Fotheringham, e Harris 2011; Wenbo Yang 2014) e formalizado por Fotheringham, Yang, e Kang (2017), diferentes processos (ex: renda, clima, topografia) operam em escalas distintas. Esta restrição motivou o desenvolvimento da Regressão Geograficamente Ponderada Multiescalar (MGWR).\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(GWmodel, sf, sp, spdep, ggplot2, viridis, gridExtra, dplyr, geobr, ggspatial)\n\nif (!exists(\"mg_dados\")) {\n  mg_dados &lt;- geobr::read_municipality(code_muni = \"MG\", year = 2020, showProgress = FALSE)\n}\n\n#\nmg_proj &lt;- st_transform(mg_dados, 5880)\n\nset.seed(999) \ncoords &lt;- st_coordinates(st_centroid(mg_proj))\nn &lt;- nrow(mg_proj)\n\n# Variável X1\nmg_proj$X1 &lt;- rnorm(n, 10, 2)\n\n# Variável X2\nmg_proj$X2 &lt;- 0.3 * mg_proj$X1 + rnorm(n, 5, 1)\n\nlat_norm &lt;- (coords[,2] - min(coords[,2])) / (max(coords[,2]) - min(coords[,2]))\nbeta1_local &lt;- 0.5 + (2.0 * lat_norm) \n\nlon_norm &lt;- (coords[,1] - min(coords[,1])) / (max(coords[,1]) - min(coords[,1]))\nbeta2_local &lt;- 3.0 - (1.5 * lon_norm)\n\n# Variável Dependente Y = Intercepto + Beta1*X1 + Beta2*X2 + Erro\nmg_proj$Y &lt;- 10 + (beta1_local * mg_proj$X1) + (beta2_local * mg_proj$X2) + rnorm(n, 0, 1)\n\n\n\n\n\n#Conversão para Objeto Spatial (Requisito do GWmodel)\nmg_sp &lt;- as(mg_proj, \"Spatial\")\n\n\n#DIAGNÓSTICO DE COLINEARIDADE LOCAL\n\n# Seleção da largura da banda (bandwidth)\nbw_diag &lt;- bw.gwr(Y ~ X1 + X2, data = mg_sp, approach = \"AICc\", \n                  kernel = \"bisquare\", adaptive = TRUE)\n\n\nAdaptive bandwidth (number of nearest neighbours): 534 AICc value: 3863.265 \nAdaptive bandwidth (number of nearest neighbours): 338 AICc value: 3401.226 \nAdaptive bandwidth (number of nearest neighbours): 215 AICc value: 3093.095 \nAdaptive bandwidth (number of nearest neighbours): 141 AICc value: 2892.588 \nAdaptive bandwidth (number of nearest neighbours): 93 AICc value: 2771.422 \nAdaptive bandwidth (number of nearest neighbours): 65 AICc value: 2728.761 \nAdaptive bandwidth (number of nearest neighbours): 46 AICc value: 2717.541 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 2740.905 \nAdaptive bandwidth (number of nearest neighbours): 54 AICc value: 2720.107 \nAdaptive bandwidth (number of nearest neighbours): 43 AICc value: 2721.94 \nAdaptive bandwidth (number of nearest neighbours): 50 AICc value: 2718.747 \nAdaptive bandwidth (number of nearest neighbours): 45 AICc value: 2718.291 \nAdaptive bandwidth (number of nearest neighbours): 48 AICc value: 2718.798 \nAdaptive bandwidth (number of nearest neighbours): 46 AICc value: 2717.541 \n\n\nCódigo\npaste(\"Bandwidth Ótimo (k vizinhos):\", bw_diag)\n\n\n[1] \"Bandwidth Ótimo (k vizinhos): 46\"\n\n\nCódigo\n#\ncollin_diag &lt;- gwr.collin.diagno(Y ~ X1 + X2, data = mg_sp, bw = bw_diag, \n                                 kernel = \"bisquare\", adaptive = TRUE)\n\nprint(summary(collin_diag$SDF$local_CN))\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.11   17.13   19.03   19.53   21.58   33.93 \n\n\nCódigo\n#Ajuste do Modelo GWR\ngwr_model &lt;- gwr.basic(Y ~ X1 + X2, \n                       data = mg_sp, \n                       bw = bw_diag, \n                       kernel = \"bisquare\", \n                       adaptive = TRUE, \n                       F123.test = TRUE)\n\n#print(gwr_model)\n\n\n#INFERÊNCIA  \nmc_test &lt;- gwr.montecarlo(Y ~ X1 + X2, \n                          data = mg_sp, \n                          nsims = 99, \n                          kernel = \"bisquare\", \n                          adaptive = TRUE, \n                          bw = bw_diag)\n\n\n\nTests based on the Monte Carlo significance test\n\n            p-value\n(Intercept)    0.99\nX1             0.97\nX2             0.99\n\n\nCódigo\n#print(mc_test)\n\ngwr_adj &lt;- gwr.t.adjust(gwr_model)\n\n\n#Nota, precisamos extrair os resultados para fazer os mapas\n\n# Extrair resultados para SF\nresults_sf &lt;- st_as_sf(gwr_adj$SDF)\n\n#Estimativa do Coeficiente Local (Beta X1)\np_beta &lt;- ggplot(results_sf) +\n  geom_sf(aes(fill = X1_t), color = \"white\") +\n  scale_fill_viridis_c(option = \"turbo\", name = expression(hat(beta)[1])) +\n  labs(title = \"A)\") +\n  theme_void()+\n  annotation_scale(location = \"br\", width_hint = 0.3)+\n    annotation_north_arrow(location = \"tr\", height = unit(1, \"cm\"), width = unit(1, \"cm\"),\n                           style = north_arrow_fancy_orienteering)\n\np_beta2 &lt;- ggplot(results_sf) +\n  geom_sf(aes(fill = X2_t), color = \"white\") +\n  scale_fill_viridis_c(option = \"turbo\", name = expression(hat(beta)[2])) +\n  labs(title = \"B)\") +\n  theme_void()+\n  annotation_scale(location = \"br\", width_hint = 0.3)+\n    annotation_north_arrow(location = \"tr\", height = unit(1, \"cm\"), width = unit(1, \"cm\"),\n                           style = north_arrow_fancy_orienteering)\n\n# valor-p ajustado\np_sig &lt;- ggplot(results_sf) +\n  geom_sf(aes(fill = X2_p_by &lt; 0.05), color = \"white\", size = 0.05) +\n  scale_fill_manual(values = c(\"TRUE\" = \"#377eb8\", \"FALSE\" = \"gray95\"), \n                    name = \"Significativo\\n(p-adj &lt; 0.05)\") +\n  labs(title = \"C)\") +\n  theme_void() +\n  theme(legend.position = \"bottom\")+\n  annotation_scale(location = \"br\", width_hint = 0.3)+\n    annotation_north_arrow(location = \"tr\", height = unit(1, \"cm\"), width = unit(1, \"cm\"),\n                           style = north_arrow_fancy_orienteering)\n\n# Resíduos\nnb &lt;- poly2nb(results_sf, queen = TRUE)\nlw &lt;- nb2listw(nb, style = \"W\", zero.policy = TRUE)\n\nresults_sf$residual &lt;- gwr_model$SDF$residual\nmoran_gwr &lt;- moran.test(results_sf$residual, lw, zero.policy = TRUE)\nlabel_moran &lt;- paste0(\"I de Moran: \", round(moran_gwr$estimate[1], 3), \n                      \" (p = \", round(moran_gwr$p.value, 3), \")\")\n\np_resid &lt;- ggplot(results_sf) +\n  geom_sf(aes(fill = residual), color = \"white\", size = 0.05) +\n  scale_fill_gradient2(low = \"#d73027\", mid = \"white\", high = \"#4575b4\", \n                       midpoint = 0, name = \"Resíduos\") +\n  labs(title = \"D)\", \n       subtitle = label_moran) +\n  theme_void() +\n  annotation_scale(location = \"br\", width_hint = 0.3)+\n    annotation_north_arrow(location = \"tr\", height = unit(1, \"cm\"), width = unit(1, \"cm\"),\n                           style = north_arrow_fancy_orienteering)\n\n\n(p_beta |p_beta2 )/(p_sig |p_resid)\n\n\n\n\n\n\n\n\n\n\n\n4.15.2 Regressão Geograficamente Ponderada Multiescalar (MGWR)\nA aplicação da GWR impõe uma restrição: a suposição de que todos os processos modelados operam na mesma escala espacial. Isto decorre da estimação de uma única largura de banda ótima \\(b\\) para todas as covariáveis simultaneamente. Contudo, em sistemas geográficos complexos, é intuitivo e frequentemente observado que diferentes preditores influenciem a variável resposta em escalas distintas (Fotheringham, Yang, e Kang 2017; Wenbo Yang 2014).\nPor exemplo, em um modelo de preços imobiliários, a influência da proximidade a uma centralidade metropolitana pode variar suavemente em uma escala regional (processo de larga escala), enquanto o efeito da qualidade da infraestrutura local (como pavimentação) pode mudar abruptamente entre quarteirões adjacentes (processo de escala local). A imposição de uma largura de banda única na GWR introduz viés nas estimativas de processos localmente heterogêneos (ao suavizá-los excessivamente) e aumenta a variância das estimativas de processos globalmente estáveis (ao torná-las desnecessariamente ruidosas) (Binbin Lu et al. 2014).\nAdicionalmente, estudos demonstram que o uso de uma largura de banda única pode exacerbar problemas de multicolinearidade local e concurvidade (colinearidade funcional em modelos não paramétricos), levando a coeficientes instáveis e de difícil interpretação (Wheeler e Tiefelsdorf 2005). Oshan, Smith, e Fotheringham (2020) evidenciam, em um estudo sobre determinantes espaciais da obesidade, que a GWR consome excessivos graus de liberdade ao forçar a modelagem local de processos que são, na verdade, globais. Isto resulta em sobreajuste e perda de parcimônia. A abordagem multiescalar, ao permitir que processos globais sejam modelados como tal (com larguras de banda tendendo ao infinito), produz modelos mais parcimoniosos, com menor AICc e diagnósticos de colinearidade mais robustos.\nA MGWR relaxa a suposição de homogeneidade de escala da GWR, permitindo que cada covariável possua sua própria largura de banda ótima \\(bw_j\\). Isto transforma a largura de banda de um parâmetro de suavização em um indicador empírico das propriedades espaciais intrínsecas de cada relação causal investigada (Fotheringham, Yang, e Kang 2017).\nO modelo MGWR para uma observação na localização \\(i\\) é formalmente especificado como:\n\\[\ny_i = \\beta_0(u_i, v_i; bw_0) + \\sum_{j=1}^{p} \\beta_j(u_i, v_i; bw_j) \\, x_{ij} + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2),\n\\]\nonde a notação \\(\\beta_j(\\cdot; bw_j)\\) enfatiza que a superfície do coeficiente associado à \\(j\\)-ésima covariável é estimada utilizando uma função kernel com largura de banda específica \\(bw_j\\). O intercepto \\(\\beta_0\\) também possui sua própria escala, \\(bw_0\\).\nDiferentemente da GWR, que possui uma solução em forma fechada via Mínimos Quadrados Ponderados para cada localização, a MGWR não tem uma solução analítica direta devido à dependência mútua das superfícies de coeficientes estimadas com diferentes larguras de banda. A estratégia padrão de estimação reformula o problema dentro da estrutura dos Modelos Aditivos Generalizados (GAMs) (Hanchen Yu et al. 2020).\nO modelo é reescrito como uma soma de funções suaves:\n\\[\n\\mathbf{y} = \\sum_{j=0}^{p} \\mathbf{f}_j + \\boldsymbol{\\epsilon},\n\\]\nonde cada termo aditivo \\(\\mathbf{f}_j\\) é um vetor \\(n \\times 1\\) que representa a contribuição espacialmente suave da \\(j\\)-ésima covariável. No contexto da MGWR, \\(\\mathbf{f}_j = \\text{diag}(\\mathbf{x}_j) \\, \\boldsymbol{\\beta}_j\\), com \\(\\boldsymbol{\\beta}_j\\) sendo o vetor de coeficientes locais para a covariável \\(j\\).\nA estimação é realizada através de um algoritmo iterativo de back-fitting, inspirado na estimação de GAMs (Hastie e Tibshirani 1990). O procedimento, detalhado por Fotheringham, Yang, e Kang (2017) e Hanchen Yu et al. (2020), segue os seguintes passos:\n\nInicialização: As funções \\(\\hat{\\mathbf{f}}_j^{(0)}\\) são inicializadas, por exemplo, com as estimativas de um modelo de regressão linear ou de uma GWR com banda única.\nIteração (Back-fitting): Para cada covariável \\(j = 0, 1, \\dots, p\\) na iteração \\([k+1]\\):\n\nCalcula-se o resíduo parcial removendo a contribuição atual de todas as outras covariáveis: \\[\n\\mathbf{r}_j^{[k]} = \\mathbf{y} - \\sum_{l &lt; j} \\hat{\\mathbf{f}}_l^{[k+1]} - \\sum_{l &gt; j} \\hat{\\mathbf{f}}_l^{[k]}.\n\\]\nAjusta-se um modelo GWR univariado do resíduo parcial \\(\\mathbf{r}_j^{[k]}\\) contra a covariável \\(\\mathbf{x}_j\\). Nesta etapa, otimiza-se a largura de banda específica \\(bw_j\\) que minimiza um critério de informação (usualmente o AICc) para este submodelo.\nAtualiza-se a estimativa da função: \\(\\hat{\\mathbf{f}}_j^{[k+1]} = \\text{diag}(\\mathbf{x}_j) \\, \\hat{\\boldsymbol{\\beta}}_j^{[k+1]}\\), onde \\(\\hat{\\boldsymbol{\\beta}}_j^{[k+1]}\\) é o vetor de coeficientes obtido do modelo GWR univariado calibrado com \\(bw_j^{[k+1]}\\).\n\nConvergência: O algoritmo itera até que a mudança nas estimativas dos coeficientes ou na soma dos quadrados dos resíduos entre iterações sucessivas seja inferior a um limiar pré-definido (ex: \\(10^{-5}\\)). Wenbo Yang (2014) introduz o conceito de Score of Change (SOC) para monitorar esta convergência.\n\nA estabilidade deste processo depende do algoritmo de otimização utilizado para encontrar cada \\(bw_j\\). Conforme destacado por da Silva e Mendes (2018), a superfície do critério de seleção (AICc ou CV) para larguras de banda adaptativos é frequentemente não convexa e multimodal. Portanto, métodos robustos como a busca exaustiva em grade (grid search) ou algoritmos heurísticos são preferíveis ao método padrão de Busca da Seção Áurea para evitar mínimos locais subóptimos.\nA principal inovação da MGWR é a capacidade de interpretar as larguras de banda \\(bw_j\\) como métricas da escala espacial de cada processo. Baseando-se na teoria desenvolvida por Fotheringham et al. (2022), pode-se inferir que:\n\n\\(bw_j\\) pequeno: Indica um processo localmente heterogêneo, onde a relação entre \\(x_j\\) e \\(y\\) muda rapidamente no espaço. Isto pode ser causado por alta variabilidade do parâmetro verdadeiro ou por forte dependência espacial de curto alcance.\n\\(bw_j\\) grande (próximo de \\(n\\)): Indica um processo regional ou globalmente estável, onde a relação é aproximadamente constante no espaço. Quando \\(bw_j \\to \\infty\\), o coeficiente \\(\\beta_j\\) converge para uma constante, recuperando a estacionariedade do modelo OLS.\n\nA análise dessas escalas não deve ser estática. Bo Lu et al. (2023) demonstram, em um estudo longitudinal do mercado imobiliário, que as larguras de banda ótimas podem evoluir temporalmente, refletindo mudanças na estrutura subjacente do fenômeno (ex.: a influência de áreas verdes tornar-se mais localizada após a implementação de políticas urbanas específicas).\nPara realizar inferência estatística na MGWR, é necessário estimar a variância das superfícies de coeficientes. Hanchen Yu et al. (2020) estendem o arcabouço de inferência da GWR para o caso multiescalar.\nO modelo MGWR completo pode ser representado por uma única matriz de projeção (ou hat matrix) \\(\\mathbf{S}_{MGWR}\\), tal que \\(\\hat{\\mathbf{y}} = \\mathbf{S}_{MGWR} \\, \\mathbf{y}\\). Esta matriz é complexa de derivar analiticamente, mas pode ser entendida como o resultado da composição das operações de back-fitting. A contribuição de cada covariável é associada a uma matriz de suavização \\(\\mathbf{S}_j(bw_j)\\).\nA variância do estimador para o vetor de coeficientes da covariável \\(j\\), \\(\\hat{\\boldsymbol{\\beta}}_j\\), é dada por:\n\\[\n\\text{Var}(\\hat{\\boldsymbol{\\beta}}_j) = \\sigma^2 \\, \\mathbf{C}_j \\mathbf{C}_j^\\top, \\quad \\text{onde} \\quad \\mathbf{C}_j = [\\text{diag}(\\mathbf{x}_j)]^{-1} \\mathbf{S}_j(bw_j).\n\\]\nOs erros-padrão locais para \\(\\hat{\\beta}_j(u_i, v_i)\\) são as raízes quadradas dos elementos diagonais correspondentes de \\(\\text{Var}(\\hat{\\boldsymbol{\\beta}}_j)\\). Estatísticas \\(t\\) locais podem então ser construídas para testar hipóteses pontuais (ex.: \\(\\beta_j(u_i, v_i) = 0\\)).\nContudo, a realização de testes simultâneos em centenas ou milhares de localizações incorre no problema de comparações múltiplas. da Silva e Fotheringham (2016) propõem uma correção baseada no Número Efetivo de Parâmetros (ENP) específico de cada superfície. O nível de significância \\(\\alpha\\) é ajustado utilizando \\(ENP_j = \\text{tr}(\\mathbf{S}_j(bw_j))\\) como uma medida dos graus de liberdade consumidos pela covariável \\(j\\), oferecendo um controle mais adequado da taxa de erro tipo I do que a correção de Bonferroni excessivamente conservadora.\nA lógica multiescalar foi estendida para incorporar explicitamente a dimensão temporal, resultando no modelo MGWR Espaço-Temporal (MGTWR) (Wu et al. 2019; Huili Yu et al. 2020). Nesta formulação, cada covariável possui uma largura de banda espacial (\\(h_S\\)) e uma largura de banda temporal (\\(h_T\\)) específicas, definidas por um kernel espaço-temporal (ex.: produto de kernels separáveis).\nO modelo MGTWR permite discriminar processos que são (1) espacialmente locais mas temporalmente estáveis (ex.: efeito de uma escola no preço da habitação); (2) espacialmente globais mas temporalmente voláteis (ex.: impacto de uma política monetária nacional).\nA calibração envolve a otimização conjunta de \\(h_S\\) e \\(h_T\\) para cada termo, aumentando a complexidade computacional, mas oferecendo uma representação mais rica da dinâmica espaço-temporal.\nA flexibilidade da MGWR tem facilitado sua integração com fontes de dados não tradicionais e de alta dimensão. Exemplos incluem:\n\nImagens de Street View e Visão Computacional: He et al. (2022) utilizaram a MGWR para modelar a relação entre características visuais extraídas do Google Street View (vegetação, manutenção de edifícios) e taxas de criminalidade em Nova York, revelando que a influência de certas características varia espacialmente em escalas diferentes.\nDados de mobilidade e redes sociais: Liu, Chau, e Bao (2023) aplicaram a MGWR para analisar como diferentes fatores socioeconômicos, em diferentes escalas espaciais, explicam padrões de uso do transporte público derivados de dados de cartões inteligentes.\n\nNestes contextos, a MGWR frequentemente supera modelos tradicionais (OLS, SLM, SEM, GWR padrão) em medidas de ajuste (\\(R^2\\) ajustado, AICc) e capacidade explicativa, por sua aptidão em capturar a multiescalaridade inerente a processos urbanos e sociais complexos.\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(GWmodel, sf, sp, ggplot2, viridis, gridExtra, kableExtra, dplyr, ggspatial)\n\n#CALIBRAÇÃO DO MODELO MGWR \n\nmgwr_model &lt;- gwr.multiscale(\n  formula = Y ~ X1 + X2,\n  data = mg_sp,\n  kernel = \"bisquare\",\n  adaptive = TRUE,         \n  criterion = \"dCVR\",      # Critério de convergência robusto\n  threshold = 1e-5,        # Tolerância para convergência\n  max.iterations = 100,   \n  predictor.centered = c(TRUE, TRUE), # Centralizar X1 e X2\nverbose = FALSE\n  )\n\n\n------ Calculate the initial bandwidths for each independent variable ------\nNow select an optimum bandwidth for the model:  Y~1 \n[1] \"bws0[i]&lt;-bw.gwr(Y~1,data=regression.points,kernel=kernel,approach=approach,adaptive=adaptive,dMat=dMats[[var.dMat.indx[i]]], parallel.method=parallel.method,parallel.arg=parallel.arg)\"\nAdaptive bandwidth (number of nearest neighbours): 534 AICc value: 5108.313 \nAdaptive bandwidth (number of nearest neighbours): 338 AICc value: 5024.466 \nAdaptive bandwidth (number of nearest neighbours): 215 AICc value: 4988.935 \nAdaptive bandwidth (number of nearest neighbours): 141 AICc value: 4974.096 \nAdaptive bandwidth (number of nearest neighbours): 93 AICc value: 4967.439 \nAdaptive bandwidth (number of nearest neighbours): 65 AICc value: 4969.396 \nAdaptive bandwidth (number of nearest neighbours): 111 AICc value: 4969.312 \nAdaptive bandwidth (number of nearest neighbours): 82 AICc value: 4967.451 \nAdaptive bandwidth (number of nearest neighbours): 100 AICc value: 4967.842 \nAdaptive bandwidth (number of nearest neighbours): 89 AICc value: 4967.256 \nAdaptive bandwidth (number of nearest neighbours): 86 AICc value: 4967.559 \nAdaptive bandwidth (number of nearest neighbours): 90 AICc value: 4967.335 \nAdaptive bandwidth (number of nearest neighbours): 87 AICc value: 4967.546 \nAdaptive bandwidth (number of nearest neighbours): 89 AICc value: 4967.256 \nNow select an optimum bandwidth for the model:  Y~X1 \n[1] \"bws0[i]&lt;-bw.gwr(Y~X1,data=regression.points,kernel=kernel,approach=approach,adaptive=adaptive,dMat=dMats[[var.dMat.indx[i]]], parallel.method=parallel.method,parallel.arg=parallel.arg)\"\nAdaptive bandwidth (number of nearest neighbours): 534 AICc value: 4405.382 \nAdaptive bandwidth (number of nearest neighbours): 338 AICc value: 4196.427 \nAdaptive bandwidth (number of nearest neighbours): 215 AICc value: 4092.447 \nAdaptive bandwidth (number of nearest neighbours): 141 AICc value: 4040.078 \nAdaptive bandwidth (number of nearest neighbours): 93 AICc value: 4016.505 \nAdaptive bandwidth (number of nearest neighbours): 65 AICc value: 4021.724 \nAdaptive bandwidth (number of nearest neighbours): 111 AICc value: 4022.917 \nAdaptive bandwidth (number of nearest neighbours): 82 AICc value: 4015.8 \nAdaptive bandwidth (number of nearest neighbours): 75 AICc value: 4016.762 \nAdaptive bandwidth (number of nearest neighbours): 86 AICc value: 4015.667 \nAdaptive bandwidth (number of nearest neighbours): 89 AICc value: 4015.827 \nAdaptive bandwidth (number of nearest neighbours): 84 AICc value: 4016.029 \nAdaptive bandwidth (number of nearest neighbours): 87 AICc value: 4015.536 \nAdaptive bandwidth (number of nearest neighbours): 88 AICc value: 4015.645 \nAdaptive bandwidth (number of nearest neighbours): 87 AICc value: 4015.536 \nNow select an optimum bandwidth for the model:  Y~X2 \n[1] \"bws0[i]&lt;-bw.gwr(Y~X2,data=regression.points,kernel=kernel,approach=approach,adaptive=adaptive,dMat=dMats[[var.dMat.indx[i]]], parallel.method=parallel.method,parallel.arg=parallel.arg)\"\nAdaptive bandwidth (number of nearest neighbours): 534 AICc value: 4416.663 \nAdaptive bandwidth (number of nearest neighbours): 338 AICc value: 4203.26 \nAdaptive bandwidth (number of nearest neighbours): 215 AICc value: 4094.814 \nAdaptive bandwidth (number of nearest neighbours): 141 AICc value: 4045.604 \nAdaptive bandwidth (number of nearest neighbours): 93 AICc value: 4030.648 \nAdaptive bandwidth (number of nearest neighbours): 65 AICc value: 4034.646 \nAdaptive bandwidth (number of nearest neighbours): 111 AICc value: 4033.74 \nAdaptive bandwidth (number of nearest neighbours): 82 AICc value: 4029.413 \nAdaptive bandwidth (number of nearest neighbours): 75 AICc value: 4029.958 \nAdaptive bandwidth (number of nearest neighbours): 86 AICc value: 4029.663 \nAdaptive bandwidth (number of nearest neighbours): 79 AICc value: 4029.528 \nAdaptive bandwidth (number of nearest neighbours): 83 AICc value: 4029.424 \nAdaptive bandwidth (number of nearest neighbours): 80 AICc value: 4029.383 \nAdaptive bandwidth (number of nearest neighbours): 80 AICc value: 4029.383 \n------            The end for the initial selections              ------\nCalculate the initial beta0 from the above bandwidths.\nEnd of calculating the inital beta0.\nFind the optimum bandwidths for each independent variable.\n    Iteration 1\nNow select an optimum bandwidth for the variable: Intercept\nThe newly selected bandwidth for variable: Intercept is 32\nThe bandwidth used in the last iteration is: 89 and the difference in bandwidths is: 57\nThe bandwidth for variable Intercept will be continually selected in the next iteration.\nNow select an optimum bandwidth for the variable: X1\nThe newly selected bandwidth for variable: X1 is 97\nThe bandwidth used in the last iteration is: 87 and the difference in bandwidths is: 10\nThe bandwidth for variable X1 will be continually selected in the next iteration.\nNow select an optimum bandwidth for the variable: X2\nThe newly selected bandwidth for variable: X2 is 238\nThe bandwidth used in the last iteration is: 80 and the difference in bandwidths is: 158\nThe bandwidth for variable X2 will be continually selected in the next iteration.\nIteration 1 change of RSS (dCVR) is 0.0572369.\n    Iteration 2\nNow select an optimum bandwidth for the variable: Intercept\nThe newly selected bandwidth for variable: Intercept is 32\nThe bandwidth used in the last iteration is: 32 and the difference in bandwidths is: 0\nThe bandwidth for variable Intercept seems to be converged for 1 times. It will be continually optimized in the next 4 times\nNow select an optimum bandwidth for the variable: X1\nThe newly selected bandwidth for variable: X1 is 130\nThe bandwidth used in the last iteration is: 97 and the difference in bandwidths is: 33\nThe bandwidth for variable X1 will be continually selected in the next iteration.\nNow select an optimum bandwidth for the variable: X2\nThe newly selected bandwidth for variable: X2 is 267\nThe bandwidth used in the last iteration is: 238 and the difference in bandwidths is: 29\nThe bandwidth for variable X2 will be continually selected in the next iteration.\nIteration 2 change of RSS (dCVR) is 0.0819936.\n    Iteration 3\nNow select an optimum bandwidth for the variable: Intercept\nThe newly selected bandwidth for variable: Intercept is 32\nThe bandwidth used in the last iteration is: 32 and the difference in bandwidths is: 0\nThe bandwidth for variable Intercept seems to be converged for 2 times. It will be continually optimized in the next 3 times\nNow select an optimum bandwidth for the variable: X1\nThe newly selected bandwidth for variable: X1 is 130\nThe bandwidth used in the last iteration is: 130 and the difference in bandwidths is: 0\nThe bandwidth for variable X1 seems to be converged for 1 times. It will be continually optimized in the next 4 times\nNow select an optimum bandwidth for the variable: X2\nThe newly selected bandwidth for variable: X2 is 267\nThe bandwidth used in the last iteration is: 267 and the difference in bandwidths is: 0\nThe bandwidth for variable X2 seems to be converged for 1 times. It will be continually optimized in the next 4 times\nIteration 3 change of RSS (dCVR) is 0.0417213.\n    Iteration 4\nNow select an optimum bandwidth for the variable: Intercept\nThe newly selected bandwidth for variable: Intercept is 32\nThe bandwidth used in the last iteration is: 32 and the difference in bandwidths is: 0\nThe bandwidth for variable Intercept seems to be converged for 3 times. It will be continually optimized in the next 2 times\nNow select an optimum bandwidth for the variable: X1\nThe newly selected bandwidth for variable: X1 is 130\nThe bandwidth used in the last iteration is: 130 and the difference in bandwidths is: 0\nThe bandwidth for variable X1 seems to be converged for 2 times. It will be continually optimized in the next 3 times\nNow select an optimum bandwidth for the variable: X2\nThe newly selected bandwidth for variable: X2 is 267\nThe bandwidth used in the last iteration is: 267 and the difference in bandwidths is: 0\nThe bandwidth for variable X2 seems to be converged for 2 times. It will be continually optimized in the next 3 times\nIteration 4 change of RSS (dCVR) is 0.0116581.\n    Iteration 5\nNow select an optimum bandwidth for the variable: Intercept\nThe newly selected bandwidth for variable: Intercept is 32\nThe bandwidth used in the last iteration is: 32 and the difference in bandwidths is: 0\nThe bandwidth for variable Intercept seems to be converged for 4 times. It will be continually optimized in the next 1 times\nNow select an optimum bandwidth for the variable: X1\nThe newly selected bandwidth for variable: X1 is 130\nThe bandwidth used in the last iteration is: 130 and the difference in bandwidths is: 0\nThe bandwidth for variable X1 seems to be converged for 3 times. It will be continually optimized in the next 2 times\nNow select an optimum bandwidth for the variable: X2\nThe newly selected bandwidth for variable: X2 is 267\nThe bandwidth used in the last iteration is: 267 and the difference in bandwidths is: 0\nThe bandwidth for variable X2 seems to be converged for 3 times. It will be continually optimized in the next 2 times\nIteration 5 change of RSS (dCVR) is 0.00850599.\n    Iteration 6\nNow select an optimum bandwidth for the variable: Intercept\nThe newly selected bandwidth for variable: Intercept is 32\nThe bandwidth used in the last iteration is: 32 and the difference in bandwidths is: 0\nThe bandwidth for variable Intercept seems to be converged and will be kept the same in the following iterations.\nNow select an optimum bandwidth for the variable: X1\nThe newly selected bandwidth for variable: X1 is 130\nThe bandwidth used in the last iteration is: 130 and the difference in bandwidths is: 0\nThe bandwidth for variable X1 seems to be converged for 4 times. It will be continually optimized in the next 1 times\nNow select an optimum bandwidth for the variable: X2\nThe newly selected bandwidth for variable: X2 is 267\nThe bandwidth used in the last iteration is: 267 and the difference in bandwidths is: 0\nThe bandwidth for variable X2 seems to be converged for 4 times. It will be continually optimized in the next 1 times\nIteration 6 change of RSS (dCVR) is 0.00497904.\n    Iteration 7\nNow select an optimum bandwidth for the variable: X1\nThe newly selected bandwidth for variable: X1 is 130\nThe bandwidth used in the last iteration is: 130 and the difference in bandwidths is: 0\nThe bandwidth for variable X1 seems to be converged and will be kept the same in the following iterations.\nNow select an optimum bandwidth for the variable: X2\nThe newly selected bandwidth for variable: X2 is 267\nThe bandwidth used in the last iteration is: 267 and the difference in bandwidths is: 0\nThe bandwidth for variable X2 seems to be converged and will be kept the same in the following iterations.\nIteration 7 change of RSS (dCVR) is 0.00280838.\n    Iteration 8\nIteration 8 change of RSS (dCVR) is 0.00156492.\n    Iteration 9\nIteration 9 change of RSS (dCVR) is 0.000868011.\n    Iteration 10\nIteration 10 change of RSS (dCVR) is 0.000480783.\n    Iteration 11\nIteration 11 change of RSS (dCVR) is 0.000266314.\n    Iteration 12\nIteration 12 change of RSS (dCVR) is 0.000147614.\n    Iteration 13\nIteration 13 change of RSS (dCVR) is 8.18955e-05.\n    Iteration 14\nIteration 14 change of RSS (dCVR) is 4.54831e-05.\n    Iteration 15\nIteration 15 change of RSS (dCVR) is 2.52891e-05.\n    Iteration 16\nIteration 16 change of RSS (dCVR) is 1.4078e-05.\n    Iteration 17\nIteration 17 change of RSS (dCVR) is 7.84706e-06.\n\n\nCódigo\n#ANÁLISE DAS ESCALAS ESPACIAIS (BANDWIDTHS) \n\n# Extrair bandwidths do objeto\nbws_mgwr &lt;- mgwr_model$GW.arguments$bws\nnames(bws_mgwr) &lt;- c(\"Intercepto\", \"X1\", \"X2\") \n\ndf_bws &lt;- data.frame(\n  Variavel = names(bws_mgwr),\n  Bandwidth_Otimizado = as.numeric(bws_mgwr),\n  N_Total = nrow(mg_sp)\n) %&gt;%\n  mutate(\n    Proporcao_N = round((Bandwidth_Otimizado / N_Total) * 100, 1),\n    Escala_Interpretada = case_when(\n      Bandwidth_Otimizado &lt; N_Total * 0.2 ~ \"Local (Heterogêneo)\",\n      Bandwidth_Otimizado &gt; N_Total * 0.8 ~ \"Global (Estacionário)\",\n      TRUE ~ \"Regional (Intermediária)\"\n    )\n  )\n\nkbl(df_bws, \n    caption = \"Escalas Espaciais Identificadas pelo MGWR\",\n    col.names = c(\"Variável\", \"Vizinhos (k)\", \"N Total\", \"% da Amostra\", \"Interpretação\"),\n    booktabs = TRUE) %&gt;%\n  kable_styling(latex_options = \"HOLD_position\", full_width = FALSE) %&gt;%\n  row_spec(0, bold = TRUE)\n\n\n\nEscalas Espaciais Identificadas pelo MGWR\n\n\nVariável\nVizinhos (k)\nN Total\n% da Amostra\nInterpretação\n\n\n\n\nIntercepto\n32\n853\n3.8\nLocal (Heterogêneo)\n\n\nX1\n130\n853\n15.2\nLocal (Heterogêneo)\n\n\nX2\n267\n853\n31.3\nRegional (Intermediária)\n\n\n\n\n\nCódigo\n#COMPARAÇÃO DE AJUSTE: GWR vs MGWR\nif (exists(\"gwr_model\")) {\n  aic_gwr &lt;- gwr_model$GW.diagnostic$AICc\n  aic_mgwr &lt;- mgwr_model$GW.diagnostic$AICc \n  \n  df_comp &lt;- data.frame(\n    Modelo = c(\"GWR\", \"MGWR\"),\n    AICc = c(aic_gwr, aic_mgwr),\n    Melhoria = c(\"-\", sprintf(\"%.2f\", aic_gwr - aic_mgwr))\n  )\n  \n  print(kbl(df_comp, caption = \"Comparação de Ajuste (AICc)\", booktabs = TRUE))\n}\n\n\n&lt;table&gt;\n&lt;caption&gt;Comparação de Ajuste (AICc)&lt;/caption&gt;\n &lt;thead&gt;\n  &lt;tr&gt;\n   &lt;th style=\"text-align:left;\"&gt; Modelo &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; AICc &lt;/th&gt;\n   &lt;th style=\"text-align:left;\"&gt; Melhoria &lt;/th&gt;\n  &lt;/tr&gt;\n &lt;/thead&gt;\n&lt;tbody&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;\"&gt; GWR &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 2717.541 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; - &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;\"&gt; MGWR &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 2613.408 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; 104.13 &lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\n\nCódigo\n#VISUALIZAÇÃO DOS RESULTADOS\nmgwr_sf &lt;- st_as_sf(mgwr_model$SDF)\n\n#Coeficiente Local X1\np_beta_mgwr &lt;- ggplot(mgwr_sf) +\n  geom_sf(aes(fill = X1), color = \"white\") +\n  scale_fill_viridis_c(option = \"turbo\", name = expression(hat(beta)[X1] ~ \"(MGWR)\")) +\n  labs(title = \"A)\") +\n  theme_void() +\n  annotation_scale(location = \"br\", width_hint = 0.3)+\n    annotation_north_arrow(location = \"tr\", height = unit(1, \"cm\"), width = unit(1, \"cm\"),\n                           style = north_arrow_fancy_orienteering)\n\n#Coeficiente Local X2\np_beta2_mgwr &lt;- ggplot(mgwr_sf) +\n  geom_sf(aes(fill = X2), color = \"white\") +\n  scale_fill_viridis_c(option = \"turbo\", name = expression(hat(beta)[X2] ~ \"(MGWR)\")) +\n  labs(title = \"B)\") +\n  theme_void() +\n  annotation_scale(location = \"br\", width_hint = 0.3)+\n    annotation_north_arrow(location = \"tr\", height = unit(1, \"cm\"), width = unit(1, \"cm\"),\n                           style = north_arrow_fancy_orienteering)\n\n#Significância (t-value &gt; 1.96) para X1\np_sig_mgwr &lt;- ggplot(mgwr_sf) +\n  geom_sf(aes(fill = abs(X1_TV) &gt; 1.96), color = \"white\", size = 0.05) +\n  scale_fill_manual(values = c(\"TRUE\" = \"#377eb8\", \"FALSE\" = \"gray95\"), \n                    name = \"Signif. (t &gt; 1.96)\") +\n  labs(title = \"C. Significância X1\", \n       subtitle = \"Inferência Local\") +\n  theme_void() +\n  annotation_scale(location = \"br\", width_hint = 0.3)+\n    annotation_north_arrow(location = \"tr\", height = unit(1, \"cm\"), width = unit(1, \"cm\"),\n                           style = north_arrow_fancy_orienteering)\n\n#I de Moran\nnb &lt;- spdep::poly2nb(mgwr_sf, queen = TRUE)\nlw &lt;- spdep::nb2listw(nb, style = \"W\", zero.policy = TRUE)\n\nresid_col &lt;- grep(\"resid\", names(mgwr_sf), ignore.case = TRUE, value = TRUE)[1]\nmoran_mgwr &lt;- spdep::moran.test(mgwr_sf[[resid_col]], lw, zero.policy = TRUE)\n\nlabel_moran_m &lt;- paste0(\"I de Moran: \", round(moran_mgwr$estimate[1], 3), \n                        \" (p = \", round(moran_mgwr$p.value, 3), \")\")\n\np_resid_mgwr &lt;- ggplot(mgwr_sf) +\n  geom_sf(aes(fill = .data[[resid_col]]), color = \"white\", size = 0.05) +\n  scale_fill_gradient2(low = \"#d73027\", mid = \"white\", high = \"#4575b4\", \n                       midpoint = 0, name = \"Resíduos\") +\n  labs(title = \"D. Resíduos MGWR\", \n       subtitle = label_moran_m) +\n  theme_void() +\n  annotation_scale(location = \"br\", width_hint = 0.3)+\n    annotation_north_arrow(location = \"tr\", height = unit(1, \"cm\"), width = unit(1, \"cm\"),\n                           style = north_arrow_fancy_orienteering)\n\n\ngrid.arrange(p_beta_mgwr, p_beta2_mgwr, p_sig_mgwr, p_resid_mgwr, ncol = 2)",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "lattice_data.html#pacote-gwmodel",
    "href": "lattice_data.html#pacote-gwmodel",
    "title": "4  Dados de Área",
    "section": "4.16 Pacote GWmodel",
    "text": "4.16 Pacote GWmodel\nO pacote GWmodel (Gollini et al. 2015) é um pacote mais abrangente e usado para modelo GWR. Desenvolvido por Binbin Lu, Paul Harris, Martin Charlton, Chris Brunsdon e colaboradores, o pacote implementa tanto GWR assim como MGWR.\nPrincipais funcionalidades\nA escolha da largura de banda é crucial em modelos GW, controlando o grau de suavização espacial. O pacote oferece funções especializadas:\n\nbw.gwr(): Seleciona a largura de banda ótima para GWR\nbw.ggwr()**: Para modelos GWR generalizados (Poisson ou Binomial)\nbw.gwr.lcr(): Para modelos GWR com compensação local de ridge\nbw.gwpca(): Para Análise de Componentes Principais Geograficamente Ponderada\nbw.gwda(): Para Análise Discriminante Geograficamente Ponderada\nbw.gtwr(): Para Regressão Geográfica e Temporalmente Ponderada\nbw.gwss.average(): Para estatísticas descritivas geograficamente ponderadas\n\nAjuste do modelo\n\ngwr.basic(): Ajusta o modelo GWR básico, produzindo estimativas locais de coeficientes\ngwr.robust(): Versões robustas resistentes a outliers\ngwr.hetero(): Modelos GWR heterocedásticos\ngwr.mixed()**: GWR mista (semiparamétrica)\ngwr.lcr(): GWR com compensação local de ridge para colinearidade\ngwr.predict(): Previsão espacial usando modelos GWR\ngwr.scalable(): Implementação escalável para grandes conjuntos de dados\ngwr.multiscale(): Ajusta GWR multiescala (MGWR), onde cada variável pode ter sua própria largura de banda\ngwr.mink.approach(): Seleciona métricas de distância ótimas além da Euclidiana\ngwss(): Calcula médias, medianas, desvios-padrão, covariâncias e correlações locais\ngwss.montecarlo(): Testes de significância para estatísticas locais\n\nDiagnóstico\n\ngw.dist(): Cálculo de matrizes de distância com várias métricas\ngw.weight(): Cálculo de matrizes de pesos espaciais\ngwr.collin.diagno(): Diagnósticos de colinearidade local\ngwr.model.selection(): Seleção de modelos GWR\ngwr.bootstrap(): Métodos de bootstrap para inferência\ngwr.t.adjust(): Ajuste de valores-p para testes múltiplos\n\n\n\n\n\nAnselin, Luc. 1988. “Spatial econometrics: methods and models”. Kluwer Academic Publishers google schola 2: 283–91.\n\n\n———. 1995. “Local indicators of spatial association—LISA”. Geographical analysis 27 (2): 93–115.\n\n\n———. 2001. “Spatial Econometrics”. Em A Companion to Theoretical Econometrics, editado por Badi H. Baltagi, 310–30. Oxford: Blackwell Publishing.\n\n\n———. 2002. “Under the hood issues in the specification and interpretation of spatial regression models”. Agricultural economics 27 (3): 247–67.\n\n\n———. 2010. “Thirty years of spatial econometrics”. Papers in regional science 89 (1): 3–26.\n\n\nBanerjee, Sudipto. 2016. “Spatial data analysis”. Annual review of public health 37 (1): 47–60.\n\n\nBanerjee, Sudipto, Bradley P Carlin, e Alan E Gelfand. 2003. Hierarchical modeling and analysis for spatial data. Chapman; Hall/CRC.\n\n\nBeron, Kurt J, e Wim PM Vijverberg. 2004. “Probit in a spatial context: a Monte Carlo analysis”. Em Advances in spatial econometrics: methodology, tools and applications, 169–95. Springer.\n\n\nBesag, Julian. 1974. “Spatial interaction and the statistical analysis of lattice systems”. Journal of the Royal Statistical Society: Series B (Methodological) 36 (2): 192–225.\n\n\n———. 1975. “Statistical analysis of non-lattice data”. Journal of the Royal Statistical Society Series D: The Statistician 24 (3): 179–95.\n\n\nBesag, Julian, e Peter J Green. 1993. “Spatial statistics and Bayesian computation”. Journal of the Royal Statistical Society Series B: Statistical Methodology 55 (1): 25–37.\n\n\nBesag, Julian, e Charles Kooperberg. 1995. “On Conditional and Intrinsic Autoregression”. Biometrika 82 (4): 733–46. https://doi.org/10.2307/2337341.\n\n\nBesag, Julian, Jeremy York, e Annie Mollié. 1991. “Bayesian image restoration, with two applications in spatial statistics”. Annals of the institute of statistical mathematics 43 (1): 1–20.\n\n\nBillé, Anna Gloria, e Giuseppe Arbia. 2019. “Spatial limited dependent variable models: A review focused on specification, estimation, and health economics applications”. Journal of Economic Surveys 33 (5): 1531–54.\n\n\nBillé, Anna Gloria, e Samantha Leorato. 2020. “Partial ML estimation for spatial autoregressive nonlinear probit models with autoregressive disturbances”. Econometric Reviews 39 (5): 437–75.\n\n\nBrunsdon, Chris, A Stewart Fotheringham, e Martin E Charlton. 1996. “Geographically weighted regression: a method for exploring spatial nonstationarity”. Geographical analysis 28 (4): 281–98.\n\n\nBurridge, Peter. 1980. “On the Cliff-Ord test for spatial correlation”. Journal of the Royal Statistical Society: Series B (Methodological) 42 (1): 107–8.\n\n\n———. 1981. “Testing for a common factor in a spatial autoregression model”. Environment and planning A 13 (7): 795–800.\n\n\nCalabrese, Raffaella, e Johan A Elkink. 2014. “Estimators of binary spatial autoregressive models: A Monte Carlo study”. Journal of Regional Science 54 (4): 664–87.\n\n\nCliff, Andrew David, e J Keith Ord. 1981. “Spatial processes: models & applications”. (No Title). https://doi.org/https://doi.org/10.2307/2530324.\n\n\nCressie, Noel. 1993. Statistics for spatial data. John Wiley & Sons.\n\n\nCressie, Noel, e Ngai H Chan. 1989. “Spatial modeling of regional variables”. Journal of the American Statistical Association 84 (406): 393–401.\n\n\nCressie, Noel, e Matthew T Moores. 2022. “Spatial statistics”. Em Encyclopedia of mathematical geosciences, 1–11. Springer.\n\n\nda Silva, Anderson R., e A. Stewart Fotheringham. 2016. “The multiple testing issue in geographically weighted regression”. Geographical Analysis 48 (3): 233–47.\n\n\nda Silva, Anderson R., e Flávio F. Mendes. 2018. “On comparing some algorithms for finding the optimal bandwidth in geographically weighted regression”. Applied Soft Computing 73: 943–57.\n\n\nElhorst, J Paul et al. 2014. Spatial econometrics: from cross-sectional data to spatial panels. Vol. 479. Springer.\n\n\nElhorst, J Paul. 2022. “The dynamic general nesting spatial econometric model for spatial panels with common factors: Further raising the bar”. Review of Regional Research 42 (3): 249–67.\n\n\nFleming, Mark M. 2004. “Techniques for estimating spatially dependent discrete choice models”. Em Advances in spatial econometrics: methodology, tools and applications, 145–68. Springer.\n\n\nFotheringham, A. Stewart, Wenbai Yang, e Wei Kang. 2017. “Multiscale Geographically Weighted Regression (MGWR)”. Annals of the American Association of Geographers 107 (6): 1247–65. https://doi.org/10.1080/24694452.2017.1352480.\n\n\nFotheringham, A. Stewart, Hanchen Yu, Levi John Wolf, Taylor M. Oshan, e Ziqi Li. 2022. “On the notion of ‘bandwidth’ in geographically weighted regression models of spatially varying processes”. International Journal of Geographical Information Science 36 (8): 1485–1502. https://doi.org/10.1080/13658816.2022.2034829.\n\n\nGelman, Andrew. 2006. “Prior Distributions for Variance Parameters in Hierarchical Models”. Bayesian Analysis 1 (3): 515–34.\n\n\nGetis, Arthur. 1995. “Cliff, ad and ord, jk 1973: Spatial autocorrelation. london: Pion”. Progress in Human Geography 19 (2): 245–49.\n\n\n———. 2010. “Spatial Autocorrelation”. Em Handbook of Applied Spatial Analysis: Software Tools, Methods and Applications, editado por Manfred M. Fischer e Arthur Getis, 255–78. Berlin, Heidelberg: Springer.\n\n\nGetis, Arthur, e J Keith Ord. 1992. “The analysis of spatial association by use of distance statistics”. Geographical analysis 24 (3): 189–206.\n\n\nGollini, Isabella, Binbin Lu, Martin Charlton, Christopher Brunsdon, e Paul Harris. 2015. “GWmodel: an R package for exploring spatial heterogeneity using geographically weighted models”. Journal of statistical software 63: 1–50.\n\n\nGreene, William H. 2003. Econometric Analysis. 5º ed. Prentice Hall.\n\n\nGuo, L., Z. Ma, e L. Zhang. 2008. “Comparison of bandwidth selection in application of geographically weighted regression: a case study”. Canadian Journal of Forest Research 38 (9): 2526–34.\n\n\nHaining, Robert Haining, Stephen Wise, e Jingsheng Ma. 1998. “Exploratory spatial data analysis in a geographic information system environment”. Journal of the Royal Statistical Society Series D: The Statistician 47 (3): 457–69.\n\n\nHaining, Robert P. 2003. Spatial data analysis: theory and practice. Cambridge university press.\n\n\nHalleck Vega, Solmaria, e J Paul Elhorst. 2015. “The SLX model”. Journal of Regional Science 55 (3): 339–63.\n\n\nHarris, Richard, John Moffat, e Victoria Kravtsova. 2011. “In search of ‘W’”. Spatial Economic Analysis 6 (3): 249–70.\n\n\nHastie, Trevor, e Robert Tibshirani. 1990. Generalized Additive Models. Monographs on Statistics e Applied Probability. London: Chapman; Hall.\n\n\nHe, Zhanjun, Zhe Wang, Zhiqiang Xie, Lin Wu, e Zhen Chen. 2022. “Multiscale analysis of the influence of street built environment on crime occurrence using street-view images”. Computers, Environment and Urban Systems 97: 101865. https://doi.org/10.1016/j.compenvurbsys.2022.101865.\n\n\nHeld, Leonhard, e Havard Rue. 2010. “Conditional and intrinsic autoregressions”. Handbook of spatial statistics, 201–16.\n\n\nHepple, Leslie W. 1979. “Bayesian analysis of the linear model with spatial dependence”. Em Exploratory and explanatory statistical analysis of spatial data, 179–99. Springer.\n\n\nHodges, James S, e Brian J Reich. 2010. “Adding spatially-correlated errors can mess up the fixed effect you love”. The American Statistician 64 (4): 325–34.\n\n\nHurvich, Clifford M., Jeffrey S. Simonoff, e Chih-Ling Tsai. 1998. “Smoothing Parameter Selection in Nonparametric Regression Using an Improved Akaike Information Criterion”. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 60 (2): 271–93.\n\n\nJong, Peter de, C Sprenger, e Frans van Veen. 1984. “On extreme values of Moran’s I and Geary’s c”. Geographical Analysis 16 (1): 17–24.\n\n\nKeefe, Matthew J., Marcelo A. Ferreira, e Christopher T. Franck. 2018a. “On the formal specification of sum-zero constrained intrinsic conditional autoregressive models”. Spatial Statistics 24: 54–65. https://doi.org/10.1016/j.spasta.2018.02.005.\n\n\nKeefe, Matthew J., Marco A. R. Ferreira, e Christopher T. Franck. 2019. “Objective Bayesian Analysis for Gaussian Hierarchical Models with Intrinsic Conditional Autoregressive Priors”. Bayesian Analysis 14 (1): 181–209. https://doi.org/10.1214/18-BA1107.\n\n\nKeefe, Matthew J, Marco AR Ferreira, e Christopher T Franck. 2018b. “On the formal specification of sum-zero constrained intrinsic conditional autoregressive models”. Spatial statistics 24: 54–65.\n\n\nKeefe, Michael J., Marco A. R. Ferreira, e Christopher T. Franck. 2019. “Objective Bayesian analysis for Gaussian hierarchical models with intrinsic conditional autoregressive priors”. Bayesian Analysis 14 (1): 181–209. https://doi.org/10.1214/18-BA1107.\n\n\nKelejian, Harry H, e Ingmar R Prucha. 1998. “A generalized spatial two-stage least squares procedure for estimating a spatial autoregressive model with autoregressive disturbances”. The journal of real estate finance and economics 17 (1): 99–121.\n\n\n———. 1999. “A generalized moments estimator for the autoregressive parameter in a spatial model”. International economic review 40 (2): 509–33.\n\n\n———. 2010. “Specification and estimation of spatial autoregressive models with autoregressive and heteroskedastic disturbances”. Journal of econometrics 157 (1): 53–67.\n\n\nKelejian, Harry, e Gianfranco Piras. 2017. Spatial econometrics. Academic Press.\n\n\nKlier, Thomas, e Daniel P McMillen. 2008. “Clustering of auto supplier plants in the United States: generalized method of moments spatial logit for large samples”. Journal of Business & Economic Statistics 26 (4): 460–71.\n\n\nKoç, Tufan. 2022. “Bandwidth selection in geographically weighted regression models via information complexity criteria”. Journal of Mathematics.\n\n\nLeSage, James P. 1997. “Bayesian estimation of spatial autoregressive models”. International regional science review 20 (1-2): 113–29.\n\n\n———. 2000. “Bayesian estimation of limited dependent variable spatial autoregressive models”. Geographical Analysis 32 (1): 19–35.\n\n\nLeSage, James, e Robert Kelley Pace. 2009. Introduction to spatial econometrics. Chapman; Hall/CRC.\n\n\nLiesenfeld, Roman, Jean-François Richard, e Jan Vogler. 2013. “Analysis of discrete dependent variable models with spatial correlation”. Economics Working Paper.\n\n\nLiu, Jun, K. W. Chau, e Zhen Bao. 2023. “Multiscale spatial analysis of metro usage and its determinants for sustainable urban development in Shenzhen, China”. Tunnelling and Underground Space Technology. https://doi.org/10.1016/j.tust.2022.104912.\n\n\nLu, Binbin, Martin Charlton, Paul Harris, e A Stewart Fotheringham. 2014. “Geographically weighted regression with a non-Euclidean distance metric: a case study using hedonic house price data”. International Journal of Geographical Information Science 28 (4): 660–81.\n\n\nLu, Bo, Yong Ge, Yeqiao Shi, Jing Zheng, e Paul Harris. 2023. “Uncovering drivers of community-level house price dynamics through multiscale geographically weighted regression: A case study of Wuhan, China”. Spatial Statistics 53: 100723. https://doi.org/10.1016/j.spasta.2022.100723.\n\n\nMallows, Cohn L. 1995. “More comments on Cp”. Technometrics 37 (4): 362–72.\n\n\nMallows, Colin L. 1973. “Some Comments on \\(C_p\\)”. Technometrics 15 (4): 661–75. https://doi.org/10.1080/00401706.1973.10489103.\n\n\nMartinetti, Davide, e Ghislain Geniaux. 2017. “Approximate likelihood estimation of spatial probit models”. Regional Science and Urban Economics 64: 30–45.\n\n\nMcMillen, Daniel P. 1992. “Probit with spatial autocorrelation”. Journal of Regional Science 32 (3): 335–48.\n\n\nMiao, Xin, Fang Fang, Xuening Zhu, e Hansheng Wang. 2025. “Spatial weights matrix selection and model averaging for multivariate spatial autoregressive models”. Econometric Reviews, 1–31.\n\n\nMoran, Patrick AP. 1950. “Notes on continuous stochastic phenomena”. Biometrika 37 (1/2): 17–23.\n\n\nOpenshaw, Stan. 1984. “The modifiable areal unit problem”. Concepts and techniques in modern geography.\n\n\nOshan, Taylor M., James P. Smith, e A. Stewart Fotheringham. 2020. “Targeting the spatial context of obesity determinants via multiscale geographically weighted regression”. International Journal of Health Geographics 19 (1): 11. https://doi.org/10.1186/s12942-020-00204-8.\n\n\nPaula, Gilberto A. 2013. “On diagnostics in double generalized linear models”. Computational Statistics & Data Analysis 68: 44–51. https://doi.org/10.1016/j.csda.2013.07.003.\n\n\n———. 2025. Modelos de Regressão e Aplicações. São Paulo: Instituto de Matemática e Estatı́stica, Universidade de São Paulo.\n\n\nPinkse, Joris, e Margaret E Slade. 1998. “Contracting in space: An application of spatial statistics to discrete-choice models”. Journal of Econometrics 85 (1): 125–54.\n\n\nReich, Brian J, James S Hodges, e Vesna Zadnik. 2006. “Effects of residual smoothing on the posterior of the fixed effects in disease-mapping models”. Biometrics 62 (4): 1197–1206.\n\n\nRiebler, Andrea, Sigrunn H Sørbye, Daniel Simpson, e Håvard Rue. 2016. “An intuitive Bayesian spatial model for disease mapping that accounts for scaling”. Statistical methods in medical research 25 (4): 1145–65.\n\n\nRigby, Robert A., Mikis D. Stasinopoulos, Gillian Z. Heller, e Frosso De Bastiani. 2019. Distributions for Modeling Location, Scale, and Shape: Using GAMLSS in R. Boca Raton: Chapman; Hall/CRC.\n\n\nRue, Havard, e Leonhard Held. 2005. Gaussian Markov random fields: theory and applications. Chapman; Hall/CRC.\n\n\nRue, Håvard, e Leonhard Held. 2005. Gaussian Markov Random Fields: Theory and Applications. Chapman & Hall/CRC Monographs on Statistics & Applied Probability. Boca Raton: Chapman; Hall/CRC.\n\n\nRue, Håvard, Sara Martino, e Nicolas Chopin. 2009. “Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations”. Journal of the Royal Statistical Society Series B: Statistical Methodology 71 (2): 319–92.\n\n\nSachdeva, Manish, A. Stewart Fotheringham, e Zhen Li. 2022. “Do places have value? Quantifying the intrinsic value of housing neighborhoods using MGWR”. Journal of Housing Research 31 (1): 24–52.\n\n\nScalon, João Domingos. 2024. Análise de Dados Espaciais com Aplicações em R. Lavras: Ed. UFLA.\n\n\nShareef, Hitham, A. A. Ibrahim, e A. H. Mutlag. 2015. “Lightning search algorithm”. Applied Soft Computing 36: 315–33.\n\n\nSimpson, Daniel, Håvard Rue, Andrea Riebler, Thiago G Martins, e Sigrunn H Sørbye. 2017. “Penalising model component complexity: A principled, practical approach to constructing priors”.\n\n\nSørbye, Sigrunn Holbek, e Håvard Rue. 2017. “Penalised complexity priors for stationary autoregressive processes”. Journal of Time Series Analysis 38 (6): 923–35.\n\n\nSørbye, Sigrunn H., e Håvard Rue. 2014. “Scaling intrinsic Gaussian Markov random field priors in spatial modelling”. Spatial Statistics 8: 39–51. https://doi.org/10.1016/j.spasta.2013.06.004.\n\n\nStasinopoulos, Mikis D., Thomas Kneib, Nadja Klein, Andreas Mayr, e Gillian Z. Heller. 2024. Generalized Additive Models for Location, Scale and Shape: A Distributional Regression Approach, with Applications. Vol. 56. Cambridge: Cambridge University Press.\n\n\nStasinopoulos, Mikis D., Robert A. Rigby, Gillian Z. Heller, Vlasios Voudouris, e Frosso De Bastiani. 2017. Flexible Regression and Smoothing: Using GAMLSS in R. Chapman & Hall/CRC Texts em Statistical Science. Boca Raton: CRC Press.\n\n\nTiefelsdorf, Michael. 2000. Modelling spatial processes: the identification and analysis of spatial relationships in regression residuals by means of Moran’s I. Springer.\n\n\nTobias, Justin L. 2024. “A Note on the Cowles-EM Algorithm for Bayesian Ordinal Probit Models”. Research in Statistics 3 (1): 2476409.\n\n\nVer Hoef, Jay M, Ephraim M Hanks, e Mevin B Hooten. 2018. “On the relationship between conditional (CAR) and simultaneous (SAR) autoregressive models”. Spatial statistics 25: 68–85.\n\n\nWall, Melanie M. 2004. “A close look at the spatial structure implied by the CAR and SAR models”. Journal of statistical planning and inference 121 (2): 311–24.\n\n\nWang, Xiaokun, e Kara M Kockelman. 2009. “Baysian inference for ordered response data with a dynamic spatial-ordered probit model”. Journal of Regional Science 49 (5): 877–913.\n\n\nWheeler, David, e Michael Tiefelsdorf. 2005. “Multicollinearity and correlation among local regression coefficients in geographically weighted regression”. Journal of Geographical Systems 7 (2): 161–87. https://doi.org/10.1007/s10109-005-0155-6.\n\n\nWhittle, Peter. 1954. “On stationary processes in the plane”. Biometrika, 434–49.\n\n\nWilhelm, Stefan, e Miguel Godinho de Matos. 2013. “Estimating spatial probit models in R”.\n\n\nWu, Chengdong, Feng Ren, Wei Hu, e Qingyun Du. 2019. “Multiscale geographically and temporally weighted regression: Exploring the spatiotemporal determinants of housing prices”. International Journal of Geographical Information Science 33 (3): 489–511. https://doi.org/10.1080/13658816.2018.1528246.\n\n\nYang, Wenbai, A. Stewart Fotheringham, e Paul Harris. 2011. “An Extension of Geographically Weighted Regression with Flexible Bandwidths”. Em Proceedings of an International Conference on Spatial Analysis and Modelling. St Andrews, Scotland, UK.\n\n\nYang, Wenbo. 2014. “An extension of geographically weighted regression with flexible bandwidths”. Tese de doutorado, University of St Andrews.\n\n\nYu, Hanchen, A. Stewart Fotheringham, Zhen Li, Taylor M. Oshan, Wei Kang, e Levi J. Wolf. 2020. “Inference in multiscale geographically weighted regression”. Geographical Analysis 52 (1): 87–106.\n\n\nYu, Huili, A. Stewart Fotheringham, Zhenlong Li, Taylor M. Oshan, e Levi J. Wolf. 2020. “On the measurement of bias in geographically weighted regression models”. Spatial Statistics 38: 100453. https://doi.org/10.1016/j.spasta.2020.100453.\n\n\nZhang, Xinyu, e Jihai Yu. 2018. “Spatial weights matrix selection and model averaging for spatial autoregressive models”. Journal of Econometrics 203 (1): 1–18.",
    "crumbs": [
      "Dados de Área (Lattice)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dados de Área</span>"
    ]
  },
  {
    "objectID": "point_process.html",
    "href": "point_process.html",
    "title": "5  Processos Pontuais",
    "section": "",
    "text": "5.1 Elementos de processos pontuais\nConstituem conjunto de dados que fornecem as localizações espaciais de fenômenos ou eventos observados (Peter J. Diggle 2013; Adrian Baddeley, Rubak, e Turner 2015; Scalon 2024). São modelos matemáticos que descrevem a disposição de eventos que estão irregular ou aleatoriamente distribuídos no plano, ou no espaço (J. Illian et al. 2008). São conjuntos contáveis de eventos espaciais que surgem como realizações de processos estocásticos (aleatórios ou probabilísticos) de eventos espaciais tomando valores em uma região plana \\(B\\subset \\mathbb{R}^{2}\\) (Moller e Waagepetersen 2003; Mateus 2013; Leininger 2014; Moraga 2023).\nSeja \\(S \\subseteq \\mathbb{R}^{d}\\) um espaço métrico, onde \\(\\mathbb{R}^{d}\\) denota um espaço euclidiano \\(d\\)-dimensional, onde os eventos ocorrem. Um processo pontual espacial em \\(B\\subseteq \\mathbb{R}^{d}\\), é uma coleção de variáveis aleatórias {\\(n(S): S \\subseteq B\\)}, onde \\(n(S)\\) representa o número de eventos ocorridos em um conjunto mensurável \\(s\\) de \\(B\\).\nProcessos pontuais referem-se a uma área da estatística espacial que estuda a distribuição de eventos em uma área geográfica específica delimitada, onde as coordenadas geográficas são a própria informação. Isto é, os eventos são geralmente representados por pontos e descritos utilizando coordenadas geográficas. São exemplos de processos pontuais, coordenadas geográficas da localização de: árvores, estabelecimentos comerciais, acidentes rodoviários, epicentros de terremotos, crimes/raptos, ninhos de pássaros, Universidades, etc.\nDependendo do número de tipos diferentes de eventos considerados, um processo pontual pode ser classificado como univariado, quando envolve um único tipo de evento ou mais de um tipo de evento sendo estudado cada um separadamente, ou seja, sem investigar a relação entre diferentes eventos; e multivariado, quando envolve mais de um tipo de evento e o objetivo é investigar a relação entre diferentes eventos (dependência interespecífica). Esta pesquisa se concentra exclusivamente em processos pontuais univariados.\nNesta seção, apresentamos os principais elementos que compõem os processos pontuais, incluindo as marcas e as covariáveis.\nMarcas\nUm processo pontual espacial pode envolver não apenas a localização dos eventos no espaço, mas também informações qualitativas ou quantitativas adicionais associadas a cada evento, ou localização \\(s_{i}=(x, y)\\), em que \\(x\\) e \\(y\\) são as respectivas coordenadas. Essas informações adicionais são denominadas marcas, e são representadas por \\(m(s_{i})\\) (J. B. Illian 2019; Adrian Baddeley, Rubak, e Turner 2015; Scalon 2024). Assim, considerando \\(B \\subseteq \\mathbb{R}^{d}\\) e \\(C \\subseteq \\mathbb{R}\\), em que \\(B\\) representa a região em que ocorrem os eventos ou fenômenos \\(s_{i}=(x, y)\\) de interesse e \\(C\\) as informações adicionais para cada evento ou fenômeno \\(s_{i}=(x, y)\\), um processo pontual marcado \\(M(B \\times C)\\), pode ser representado:\n\\[\nM(B \\times C) = \\{[s_{i}, m(s_{i})]\\}= \\{[s_{1}, m(s_{1})],[s_{2}, m(s_{2})], \\ldots, [s_{n}, m(s_{n})]\\},\\,s_{i} \\in B,\\,m(s_{i}) \\in C.\n\\tag{5.1}\\]\nEm um processo pontual onde os eventos são localizações de árvores em uma floresta, exemplos de marcas incluem a altura das árvores, tipo de espécie florestal, diâmetro à altura do peito (DAP), entre outros. Em um processo pontual com eventos sendo acidentes de trânsito em uma cidade, as marcas podem representar a gravidade do acidente (ferimentos leves, graves ou fatalidades). Em um processo pontual onde os eventos são estabelecimentos comerciais em uma cidade, as marcas podem representar o tipo de estabelecimento (restaurantes, lojas de roupas, supermercados).\nCovariáveis\nAlém das marcas, um processo pontual pode incluir variáveis explicativas, também conhecidas como covariáveis. Essas covariáveis são variáveis adicionais que auxiliam na compreensão dos motivos pelos quais os eventos ocorrem em determinados locais ou por que ocorrem de maneira desigual.\nAdrian Baddeley, Rubak, e Turner (2015) definem covariáveis como quaisquer dados tratados como explicativos e não como resposta, tomando como exemplo, uma função espacial \\(z(s_{i})\\) que descreve a altitude no evento \\(s_{i}\\), em uma floresta. Em um processo pontual onde os eventos são crimes em uma cidade, as covariáveis podem incluir densidade populacional, níveis de desemprego e acesso a serviços policiais.",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#sec-2.3.2",
    "href": "point_process.html#sec-2.3.2",
    "title": "5  Processos Pontuais",
    "section": "5.2 Propriedades fundamentais de processos pontuais",
    "text": "5.2 Propriedades fundamentais de processos pontuais\nEstacionaridade e Isotropia\nUm processo pontual é estacionário se a distribuição espacial dos eventos é invariante sob condição de translação (J. Illian et al. 2008; Adrian Baddeley, Rubak, e Turner 2015). Seja \\(N=\\{s_{1}, s_{2}, \\ldots , s_{n}\\}\\) um processo pontual e \\(N_{t} = \\{s_{1} + t, s_{2}+t, \\ldots , s_{n} +t\\}\\) a sua translação, este será estacionário se para qualquer \\(t\\in \\mathbb{R}^{d}\\), \\(N\\) e \\(N_{t}\\) apresentarem a mesma distribuição espacial dos eventos, ou seja,\n\\[\n\\forall t \\in \\mathbb{R}^{d}, N \\stackrel{D}{=} N_{t} \\Leftrightarrow \\{s_{1}, s_{2}, \\ldots , s_{n}\\} \\stackrel{D}{=} \\{s_{1} + t, s_{2}+t, \\ldots, s_{n} +t\\},\n\\tag{5.2}\\]\nem que \\(t\\) indica as unidades transladadas. No caso de um processo pontual marcado, onde \\(m(s_{i})\\) é a marca associada ao evento, a expressão Eq. 5.2 fica representada por:\n\\[\n\\forall t \\in \\mathbb{R}^{d}, N_{[s_{i}, m(s_{i})]} \\stackrel{D}{=} N_{\\{[s_{i}+t, m(s_{i})]\\}}\n\\Leftrightarrow \\{[s_{1}, m(s_{i})], \\ldots , [s_{n},m(s_{n})]\\} \\stackrel{D}{=} \\{[s_{1}+t, m(s_{i})], \\ldots , [s_{n}+t,m(s_{n})]\\} .\n\\tag{5.3}\\]\nUm processo pontual é isotrópico se a distribuição espacial dos eventos é invariante sob rotação em torno da origem (Moller e Waagepetersen 2003; J. Illian et al. 2008; Wiegand e Moloney 2014; Scalon 2024).\nSeja \\(N=\\{s_{1}, s_{2}, \\ldots , s_{n}\\}\\) um processo pontual e \\(R_{\\alpha} N = \\{R_{\\alpha}s_{1}, R_{\\alpha}s_{2}, \\ldots , R_{\\alpha}s_{n}\\}\\) a sua rotação, onde \\(\\alpha \\in [0^{o}, 360^{o}]\\). Este processo será isotrópico se a distribuição espacial dos eventos \\(N\\) e \\(R_{\\alpha} N\\) for a mesma, ou seja,\n\\[\n\\forall \\alpha \\in [0^{o}, 360^{o}], N \\stackrel{D}{=} R_{\\alpha} N \\Leftrightarrow \\{s_{1}, s_{2}, \\ldots , s_{n}\\} \\stackrel{D}{=} \\{R_{\\alpha}s_{1}, R_{\\alpha}s_{2}, \\ldots , R_{\\alpha}s_{n}\\}.\n\\tag{5.4}\\]\nNo caso do processo pontual ser marcado, onde \\(m(s_{i})\\) é a marca associada a cada evento, a expressão Eq. 5.4 fica representada por:\n\\[\n\\forall \\alpha \\in [0^{o}, 360^{o}], N_{[s_{i}, m(s_{i})]} \\stackrel{D}{=} R_{\\alpha}N_{\\{[s_{i}, m(s_{i})]\\}}\n\\Leftrightarrow \\{[s_{1}, m(s_{1})], \\ldots , [s_{n},m(s_{n})]\\} \\stackrel{D}{=} \\{R_{\\alpha}[s_{1}, m(s_{1})], \\ldots , R_{\\alpha}[s_{n},m(s_{n})]\\}.\n\\tag{5.5}\\]\nUm processo pontual pode ser isotrópico sem ser estacionário, mas o contrário não é verdadeiro (Adrian Baddeley, Rubak, e Turner 2015).\nHomogeneidade, Independência e Tendência Espacial\nUm processo pontual é considerado homogêneo quando os eventos ocorrem em proporções iguais na área de estudo. Isso significa que a probabilidade de encontrar um evento em qualquer sub-região da área de estudo onde ocorrem os eventos, é constante e igual. Em contraste, se essa probabilidade não for constante e variar, indicando que certas áreas são mais propensas a ter um maior ou menor número de eventos, ou se os eventos ocorrerem apenas em algumas sub-regiões específicas da área de estudo, diz-se que o processo pontual apresenta tendência espacial.\nPela teoria de probabilidade sabe-se que dois eventos \\(s\\) e \\(Z\\) são independentes se e somente se \\(P(S\\cap Z) = P(S)P(Z)\\), ou seja, eles serão independentes se e somente se sua probabilidade conjunta (ocorrência simultânea) for igual ao produto de suas probabilidades marginais (ocorrências individuais), como evidenciado por (Ferreira 2020).\nDa mesma forma, pode-se aplicar essa lógica ao conceito de independência de dois ou mais eventos em processos pontuais. Em um processo pontual, dois ou mais eventos são considerados independentes se a ocorrência de um evento em um determinado local ou sub-região da área de estudo não influencia a ocorrência de outros eventos.",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#principais-distribuições-de-probabilidade-em-processos-pontuais",
    "href": "point_process.html#principais-distribuições-de-probabilidade-em-processos-pontuais",
    "title": "5  Processos Pontuais",
    "section": "5.3 Principais distribuições de probabilidade em processos pontuais",
    "text": "5.3 Principais distribuições de probabilidade em processos pontuais\nNesta seção, apresentamos as principais distribuições de probabilidade, incluindo distribuição uniforme, binomial e Poisson.\nDistribuição Uniforme\nSeja \\(B\\) a região de estudo e \\(|B|\\) a sua área ou volume correspondente. Diz-se que os eventos de um processo pontual qualquer estão uniformemente distribuídos em \\(B\\), se a probabilidade de ocorrência for constante na região \\(B\\) e zero fora dela, ou seja,\n\\[\nf(s_{i}) = \\left\\{\\begin{array}{rcl}\n\\frac{1}{|B|} & \\text {se} & (x,y) \\in B \\\\\n0 & \\text{ se } & (x,y) \\notin B .\n\\end{array}\\right.\n\\tag{5.6}\\]\nConsiderando \\(S \\subseteq B\\), como uma sub-região de \\(B\\), onde \\(|S|\\) e \\(|B|\\) são as áreas ou volumes correspondentes, a probabilidade dos eventos \\(s_{i} \\in s\\) ocorrerem na sub-região \\(s\\) é,\n\\[\nP(s \\in S) = \\int f(x,y) dx dy = \\frac{1}{|B|} \\int_{S} 1 dx dy = \\frac{|S|}{|B|} ,\n\\] em que \\(s_{i}=(x, y)\\).\nDistribuição Binomial\nSeja \\(S \\subseteq B\\) e \\(n(S \\cap B) \\in B\\) representando os eventos no processo pontual \\(s\\) na região de estudo \\(B\\), onde \\(|S|\\) e \\(|B|\\) são as áreas ou volumes correspondentes. Diz-se que os eventos aleatórios \\(n(S \\cap B)\\) do processo pontual \\(s\\) seguem distribuição binomial, se resultam de ensaios independentes e idênticos, com apenas dois resultados possíveis, sucesso (\\(p\\)) ou fracasso (\\(1-p\\)), constantes em cada ensaio, ou seja,\n\\[\nP(N(S)=\\kappa) = \\binom{n}{\\kappa} p^{\\kappa} (1-p)^{n-\\kappa}; \\quad \\kappa= 0,1, \\ldots, n; \\quad p = \\frac{|S|}{|B|} .\n\\tag{5.7}\\]\nSabe-se da teoria de probabilidade que o valor esperado (média) de uma distribuição binomial \\(X\\) é dado por \\(\\mu = \\mathbb{E}(X) = n\\times p\\), onde \\(n\\) é o número de ensaios realizados. Substituindo \\(p = \\frac{|S|}{|B|}\\) e \\(X\\) por \\(n(S\\cap B)\\), obtém-se o número médio de eventos de um processo pontual dado por,\n\\[\n\\mu = \\mathbb{E}[n(S\\cap B)] = n\\times \\frac{|S|}{|B|} \\Leftrightarrow \\frac{ \\mathbb{E}[n(S\\cap B)]}{|S|} =\\frac{n}{|B|} \\Rightarrow \\hat{\\lambda} (s) =\\frac{n}{|B|},\n\\tag{5.8}\\]\nonde \\(\\hat{\\lambda}(s)\\) é denominado estimador da intensidade \\(\\lambda (s)\\), e corresponde ao número médio de eventos por unidade de área ou volume sob condição de homogeneidade.\nDistribuição Poisson\nA distribuição Poisson é comumente empregada para modelar a ocorrência de eventos raros e independentes em uma área de estudo. Assim, um processo pontual que segue a distribuição Poisson descreve o número de eventos raros que ocorrem em uma determinada área de estudo, e sua distribuição de probabilidade é dada por,\n\\[\nP(n(S)=K) =\\frac{\\mu^{k} \\times e^{- \\mu }}{k!} = \\frac{\\left(\\lambda (s)|S| \\right)^{k} \\times e^{ - \\lambda (s)|S|} }{k!} .\n\\tag{5.9}\\]\nUm processo pontual que segue uma distribuição Poisson e é homogêneo, é denominado processo pontual Poisson homogêneo. Este processo é caracterizado por ser estacionário e isotrópico, razão pela qual é também denominado processo de aleatoriedade espacial completa (não exibe dependência espacial), e se a região \\(B\\) de estudo é dividida em \\(s\\) sub-regiões diferentes, a intensidade \\(\\lambda (s)\\) nessas \\(s\\) sub-regiões, estimada por \\(\\hat{\\lambda} (s)\\) será constante (não variará), ou seja, \\(\\forall s_{i}, s_{i^{'}} \\in B: \\lambda (s_{i}) \\equiv \\lambda (s_{i^{'}})\\). Isso decorre do fato de que a distribuição Poisson possui média e variância iguais, e sua relação com a distribuição Binomial é demonstrada em Lima (2005) e Mateus (2013). Caso o processo pontual seja Poisson, mas apresente tendência, ele é denominado processo pontual Poisson não homogêneo. Nesse contexto, embora o processo não exiba dependência espacial, a intensidade \\(\\lambda (s)\\) nas \\(s\\) sub-regiões não é constante, ou seja, \\(\\exists \\, s_{i} \\in B: \\lambda (s_{i}) \\neq \\lambda (s_{i^{'}})\\) e, o número médio de eventos nessa situação é dado por, \\(\\mu = \\int_{B} \\lambda (s)\\).",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#sec-2.2.4",
    "href": "point_process.html#sec-2.2.4",
    "title": "5  Processos Pontuais",
    "section": "5.4 Propriedade de primeira e segunda ordem",
    "text": "5.4 Propriedade de primeira e segunda ordem\nPropriedade de primeira ordem\nSejam \\(S_{i}\\) e \\(S_{i^{'}}\\) sub-regiões de \\(B\\) contendo os eventos \\(s_{i}=(x,y)\\) e \\(s_{i^{'}}=(x^{'},y^{'})\\), onde o número de eventos é dado por \\(n(s_{i})\\) e \\(n(s_{i^{'}})\\) e sua área dada por \\(|S_{i}|\\) e \\(|S_{i^{'}}|\\) respectivamente. A propriedade de primeira ordem, também designada intensidade \\(\\lambda(s_{i})\\), corresponde o número de eventos esperados por unidade de área, ou seja,\n\\(\\lambda (s_{i}) = \\lim_{|S_{i}| \\to 0} \\left\\{\\frac{ \\mathbb{E}[n(s_{i})]}{|S_{i}|}\\right\\}\\), conforme descrito por Mateus (2013), Pebesma e Bivand (2023) e Moraga (2023).\nSe o processo pontual for homogêneo, a intensidade \\(\\lambda (s_{i})\\) é constante, e dado por, \\(\\lambda (s_{i}) = \\lambda =\\frac{ \\mathbb{E}[n(s_{i})]}{|S_{i}|}, \\,\\hat{\\lambda}= \\frac{n(s_{i})}{|S_{i}|}\\).\nPara ilustrar estes conceitos, vamos simular um Processo Pontual de Poisson Não-Homogêneo, onde a intensidade dos eventos aumenta conforme a coordenada \\(X\\) aumenta (um gradiente).\n\n\nCódigo\npacman::p_load(spatstat,ggplot2,patchwork)\n\njanela &lt;- owin(c(0, 10), c(0, 10))\n\nfuncao_intensidade &lt;- function(x, y) { 5 * x }\n\nset.seed(123)\npp_simulado &lt;- rpoispp(funcao_intensidade, win = janela)\n\nplot(pp_simulado, main = \"Tendência em X\", \n     pch = 19, cex = 0.4)\n\n\n\n\n\n\n\n\n\nUma maneira de verificar se a intensidade é constante consiste em dividir a área de estudo em sub-regiões ou quadrats do mesmo tamanho (área), contar os eventos em cada sub-região ou quadrats e dividir pela respectiva área da sub-região ou quadrats. No entanto, este procedimento resulta em uma intensidade não suavizada ( Figura 5.1 (a)).\nOutra alternativa é usar uma janela móvel de tamanho fixo, centrada em vários locais da área de estudo. Ao mover essa janela pela região, os eventos são contados e divididos pela área da janela, proporcionando uma estimativa mais suave e interpretável da variação da intensidade \\(\\lambda (s_{i})\\) ( Figura 5.1 (b)). No entanto, em cada uma das estimativas de intensidade (com janela móvel ou sub-regiões do mesmo tamanho), não se considera a localização relativa dos eventos dentro janela/sub-região (se os eventos estão próximos ou afastados) e a escolha de um tamanho de janela adequado não é clara (Gatrell et al. 1996).\nVale ressaltar que a variação na intensidade é mais significativa em processos pontuais não homogêneos. Assim, no caso do processo pontual não homogêneo, uma maneira de estimar a função intensidade \\(\\lambda (s_{i})\\), é usando os estimadores não paramétricos de kernel, onde a janela ou quadrats outrora descrita (o), é substituída por uma função (Kernel) que descreve um objeto tridimensional móvel, e que pondera os eventos dentro de sua esfera de influência consoante a distância do ponto onde a intensidade está sendo estimada (Gatrell et al. 1996) ( Figura 5.1 (c)). Essencialmente, neste método estima-se a função de densidade de probabilidade \\(f(z)\\) e não função intensidade \\(\\lambda(s_{i})\\).\nA função de densidade de probabilidade descreve a probabilidade de observar um evento em uma determinada região de estudo e esta função é não-negativa e integra para 1, enquanto a função intensidade \\(\\lambda (s_{i})\\) não é uma medida de probabilidade, mas sim uma medida da taxa de ocorrência de eventos em uma determinada área, que embora seja não-negativa, não precisa integrar para 1, pois ela não descreve uma distribuição de probabilidade (Moraga 2023).\nEmbora a função de densidade de probabilidade \\(f(z)\\) e a função intensidade \\(\\lambda(s_{i})\\) não sejam iguais, estas são proporcionais, o que significa que locais com maior taxa de ocorrências dos eventos (intensidade), terão também maior densidade de probabilidade \\(f(z)\\). Assim, a função densidade de probabilidade e função intensidade são dadas por,\n\\[\n\\lambda (z) = f(z) \\int_{B} \\lambda (s_{i})ds_{i}, \\: \\hat{f}(z)=\\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{h^{2}}K\\left(\\frac{z-s_{i}}{h}\\right) \\: \\text{ e } \\: \\hat{\\lambda}(z)=\\sum_{i=1}^{n} \\frac{1}{h^{2}}K\\left(\\frac{z-s_{i}}{h}\\right),\n\\tag{5.10}\\]\nonde \\(\\int_{B} \\lambda (s_{i})ds_{i}\\) é o número esperado de eventos na região \\(S_{i}\\),\\(\\hat{f}(z)\\) e \\(\\hat{\\lambda}(z)\\) são respectivamente estimadores Kernel não paramétricos da função densidade e intensidade, no local \\(z\\) da região \\(B\\) em estudo, com base nos eventos \\(s_{i}\\).\\(K(s)\\) é uma função densidade de probabilidade simétrica tal que \\(\\forall s, \\, K(s)\\geq 0\\), com \\(\\int_{B} K(s)=1\\), designada função Kernel e \\(h\\) é um parâmetro de suavização ou largura da banda ( Figura 5.1 (c)).\n\n\n\n\n\n\n\n\nFigura 5.1: Comparação de Estimadores de Intensidade: (a) Quadrats (Não suavizada), (b) Janela Móvel (Disco Uniforme) e (c) Estimativa de Kernel (Gaussiano).\n\n\n\n\n\nConforme descrito por Gatrell et al. (1996) e Gelfand et al. (2010), o estimador de Kernel \\(K(s)\\), é sensível à escolha da largura de banda \\(h\\), de tal modo que, um valor maior de \\(h\\) resulta em maior suavização na variação espacial da intensidade (viés aumenta e a variância diminui), enquanto menor valor de \\(h\\) resulta em menor suavização (viés diminui e a variância aumenta). Portanto, testar várias larguras de banda é uma opção viável para selecionar o valor ideal do parâmetro de suavização, mas também existem métodos computacionais que permitem uma escolha mais eficiente, como a validação cruzada por verossimilhança ou outros descritos por Cronie e Van Lieshout (2016).\nAs escolhas comuns para o Kernel \\(k(s)\\) incluem \\(K(s)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{s^{2}}{2}\\right)\\),\\(K(s)=\\frac{3}{4} (1-s^{2}) \\mathbb{I} (|s|&lt;1)\\),\\(K(s)=\\frac{15}{16} (1-s^{2})^{2}  \\mathbb{I} (|s|&lt;1)\\),\\(K(s)=\\frac{1}{2} \\mathbb{I} (|s|&lt;1)\\), que correspondem a kernel Gaussiano, Kernel Epanechnikov, Kernel Quadrático e Kernel Uniforme, respectivamente. Além destas, existem outras na literatura e algumas são apresentadas na Figura 5.2.\n\n\nCódigo\npacman::p_load(ggplot2, dplyr,tidyr,spatstat, patchwork)\n\n#PARTE 1: Curvas Matemáticas 1D\ns &lt;- seq(-1.5, 1.5, length.out = 500)\n\ndf_curves &lt;- data.frame(s = s) %&gt;%\n  mutate(\n    Gaussiano = dnorm(s), # Aproximação padrão\n    Epanechnikov = ifelse(abs(s) &lt; 1, 0.75 * (1 - s^2), 0),\n    Quadratico = ifelse(abs(s) &lt; 1, (15/16) * (1 - s^2)^2, 0),\n    Uniforme = ifelse(abs(s) &lt; 1, 0.5, 0),\n    # Adicionando um extra comum para comparação\n    Triangular = ifelse(abs(s) &lt; 1, 1 - abs(s), 0)\n  ) %&gt;%\n  pivot_longer(cols = -s, names_to = \"Kernel\", values_to = \"Peso\")\n\n# Gráfico das Curvas\np_curves &lt;- ggplot(df_curves, aes(x = s, y = Peso, color = Kernel)) +\n  geom_line(size = .5) +\n  facet_wrap(~Kernel, nrow = 1) +\n  theme_minimal() +\n  labs(title = \"\", x = \"Distância (s)\", y = \"Peso K(s)\") +\n  theme(legend.position = \"none\", \n        plot.title = element_text(hjust = 0.5))\n\n\n#PARTE 2: Visualização Espacial 2D \npp_unico &lt;- ppp(x = 5, y = 5, window = owin(c(0, 10), c(0, 10)))\n\n# Função auxiliar para gerar o raster do spatstat e converter para data.frame\nget_kernel_raster &lt;- function(pp, kernel_name, sigma=1.5) {\n  k_spatstat &lt;- switch(kernel_name,\n                       \"Gaussiano\" = \"gaussian\",\n                       \"Epanechnikov\" = \"epanechnikov\",\n                       \"Quadratico\" = \"quartic\", \n                       \"Uniforme\" = \"disc\")     \n  \n  if(is.null(k_spatstat)) return(NULL)\n  dens &lt;- density(pp, sigma = sigma, kernel = k_spatstat)\n  df &lt;- as.data.frame(dens)\n  df$Kernel &lt;- kernel_name\n  return(df)\n}\n\n#\nlista_dfs &lt;- lapply(c(\"Gaussiano\", \"Epanechnikov\", \"Quadratico\", \"Uniforme\"), \n                    function(k) get_kernel_raster(pp_unico, k))\ndf_raster &lt;- do.call(rbind, lista_dfs)\n\n\np_raster &lt;- ggplot(df_raster, aes(x, y, fill = value)) +\n  geom_raster() +\n  facet_wrap(~Kernel, nrow = 1) +\n  scale_fill_viridis_c(option = \"mako\") +\n  coord_fixed() +\n  theme_void() +\n  labs(title = \"Efeito no Espaço (2D)\" ) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 11, margin = margin(b=5)),\n        plot.title = element_text(hjust = 0.5, margin = margin(t=15, b=5)))\n\n\np_curves / p_raster\n\n\n\n\n\n\n\n\nFigura 5.2: Funções de Kernel\n\n\n\n\n\nA escolha do Kernel dependerá da pesquisa e do pesquisador, tomando em consideração que o Kernel Uniforme atribui peso igual a todos os eventos em uma distância fixa do evento de referência, sendo útil quando a distribuição dos eventos é uniforme, embora seja menos sensível a variações na densidade dos dados. O Kernel Gaussiano atribui pesos baseados na distribuição gaussiana (normal), capturando padrões suaves e contínuos nos dados e sendo adequado para distribuições não uniformes, embora mais sensível a variações locais. O Kernel Epanechnikov, com sua forma parabólica e suporte compacto, é mais eficiente que o Gaussiano, equilibrando características locais e evitando sensibilidade excessiva ao ruído. O Kernel Quadrático, também parabólico, oferece um compromisso entre os Kernels Uniforme e Gaussiano, sendo menos sensível a outliers que o Gaussiano, mas mais sensível que o Uniforme, ideal para equilibrar robustez e suavidade (García-Portugués 2024; Scalon 2024).\nConforme descrito por Moraga (2023) e Scalon (2024), os efeitos de borda tendem a distorcer as estimativas do Kernel, próximo à fronteira (margens) da região de estudo, uma vez que os eventos próximos à fronteira (margens) têm menos vizinhos locais do que os eventos no interior. Uma maneira de lidar com esse problema é modificar a estimativa do Kernel dividindo-a pelo seguinte termo de correção de borda \\(e_{h} (s)=\\int_{B} h^{-2}K(\\frac{z-s}{h})dx\\), que representa o volume sob o Kernel centrado em \\(z\\) que está na região de estudo \\(B\\) (Gatrell et al. 1996).\nConforme descrito por Adrian Baddeley, Rubak, e Turner (2015), dentre os estimadores de Kernel usuais da função de intensidade destacam-se,\\(\\hat{\\lambda}^{0}(z)=\\sum_{i=1}^{n}K(z-s_{i})\\),\\(\\hat{\\lambda}^{U}(z)=\\frac{1}{e(z)}\\sum_{i=1}^{n}K(z-s_{i})\\) e \\(\\hat{\\lambda}^{D}(z)=\\sum_{i=1}^{n} \\frac{1}{e(s_{i})} K(z-s_{i})\\), e correspondem a estimador não corrigido, estimador uniformemente corrigido e estimador com correção de Diggle, respectivamente. Nestes estimadores,\\(K(\\cdot)\\) é função densidade de probabilidade Kernel e \\(e(s_{i})\\) a correção da borda, dada por \\(e(s_{i})=\\int_{B} K(z-s)dx\\).\nAlém do estimador não paramétricos de kernel, outra maneira de estimar a função de intensidade \\(\\lambda (s_{i})\\), é usar estimadores paramétricos (Seção 5.9).",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#propriedade-de-segunda-ordem",
    "href": "point_process.html#propriedade-de-segunda-ordem",
    "title": "5  Processos Pontuais",
    "section": "5.5 Propriedade de segunda ordem",
    "text": "5.5 Propriedade de segunda ordem\nSegundo Mateus (2013) e Pebesma e Bivand (2023), a propriedade de segunda ordem, também conhecida como intensidade de segunda ordem ou densidade, descreve a forma como os eventos estão distribuídos no espaço, podendo indicar se estão distribuídos independentemente uns dos outros (aleatoriedade espacial completa), se tendem a se agrupar (agrupamento) ou se repelem mutuamente (distribuição mais regular do que sob aleatoriedade espacial completa) e, é representada por\n\\[\n\\lambda (s_{i},s_{i^{'}})= \\lim_{\\substack{|S_{i}|\\to 0 \\\\ |S_{i^{'}}|\\to 0}} \\left\\{\\frac{ \\mathbb{E}[n(s_{i}) n(s_{i^{'}})]}{|S_{i}||S_{i^{'}}|}\\right\\}\n\\]\ne estimada usando as funções correlação par \\(g(r)\\), função \\(L(r)\\) e função \\(K (r)\\) ou variantes não homogêneas dessas funções.\nSegundo Gatrell et al. (1996) e Peter J. Diggle (2013), se o processo pontual for estacionário, a propriedade de segunda ordem dependerá apenas da diferença vetorial \\(d\\) (direção e distância), entre \\(s_{i} \\, \\text{ e } \\, s_{i^{'}}\\), e não de suas localizações absolutas, ou seja, \\(\\lambda (s_{i},s_{i^{'}})\\equiv \\lambda (||s_{i}-s_{i^{'}}||)\\). Todavia, se for estacionário e isotrópico, a intensidade de segunda ordem dependerá apenas da distância \\(d(s_{i},s_{i^{'}})\\) e não da sua orientação ou direção.\nAlém da intensidade de primeira e segunda ordem, existe a chamada intensidade condicional.\nSegundo Peter J. Diggle (2013), a intensidade condicional corresponde à intensidade do evento \\(s_{i}\\) condicionada à (dada) informação do evento \\(s_{i^{'}}\\), ou seja,\n\\(\\lambda (s_{i}|s_{i^{'}}) = \\frac{\\lambda (s_{i},s_{i^{'}})}{\\lambda(s_{i^{'}})}\\). Pela propriedade de independência entre os eventos, descrita na seção Seção 5.2, se os eventos \\(s_{i}\\) e \\(s_{i^{'}}\\) forem independentes, a intensidade condicional \\(\\lambda (s_{i}|s_{i^{'}})\\) será dada por \\(\\lambda (s_{i}|s_{i^{'}}) =\\frac{ \\lambda (s_{i},s_{i^{'}})}{\\lambda(s_{i^{'}})} =\\lambda (s_{i})\\).",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#sec-2.6.1.3",
    "href": "point_process.html#sec-2.6.1.3",
    "title": "5  Processos Pontuais",
    "section": "5.6 Padrões de distribuição espacial e sua identificação",
    "text": "5.6 Padrões de distribuição espacial e sua identificação\nUm processo pontual pode exibir três padrões espaciais distintos ( Figura 5.3), que são, o aleatório, onde os eventos (representados por “o”) estão distribuídos de forma aleatória, com média e variância iguais; regular, quando os eventos estão espaçados de maneira uniforme em comparação com um padrão aleatório, resultando em uma variância menor que a média (subdispersão); e agrupado, quando os eventos estão mais próximos uns dos outros do que seria esperado em um padrão aleatório, resultando em uma variância maior que a média (superdispersão) (Lima 2005; Scalon 2024).\n\n\nCódigo\nset.seed(42) # Para reprodutibilidade\njanela &lt;- owin(c(0, 1), c(0, 1))\nn_pontos &lt;- 60 # Número alvo aproximado de pontos\n\n#Padrão Aleatório (Poisson Homogêneo - CSR)\npp_aleatorio &lt;- rpoispp(lambda = n_pontos, win = janela)\ndf_rand &lt;- as.data.frame(pp_aleatorio)\ndf_rand$Tipo &lt;- \"(a) Aleatório\"\n\n#Padrão Regular (Processo de Inibição - Hard Core)\npp_regular &lt;- rSSI(r = 0.09, n = n_pontos, win = janela)\ndf_reg &lt;- as.data.frame(pp_regular)\ndf_reg$Tipo &lt;- \"(b) Regular\"\n\n#Padrão Agrupado (Processo de Thomas - Cluster)\npp_agrupado &lt;- rThomas(kappa = 5, scale = 0.04, mu = 12, win = janela)\ndf_clus &lt;- as.data.frame(pp_agrupado)\ndf_clus$Tipo &lt;- \"(c) Agrupado\"\n\n#\nplot_padrao &lt;- function(df, titulo, cor) {\n  ggplot(df, aes(x, y)) +\n    geom_point(shape = 1, size = 2, stroke = 1, color = cor) +\n    coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +\n    theme_bw() +\n    labs(title = titulo, x = \"\", y = \"\") +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      axis.text = element_blank(),\n      axis.ticks = element_blank(),\n      panel.grid = element_blank()\n    )\n}\n\n\ng1 &lt;- plot_padrao(df_rand, \"(a) Aleatório\", \"black\")\ng2 &lt;- plot_padrao(df_reg,  \"(b) Regular\",   \"blue\")\ng3 &lt;- plot_padrao(df_clus, \"(c) Agrupado\",  \"red\")\n\n\ng1 + g2 + g3\n\n\n\n\n\n\n\n\nFigura 5.3: Padrões de Distribuição Espacial: (a) Aleatório (Poisson), (b) Regular (Inibição) e (c) Agrupado (Thomas).\n\n\n\n\n\nPara identificar os padrões apresentados na Figura 5.3, há várias abordagens, que vão desde estatísticas simples como o índice de Morisita (1959), o índice de Clark e Evans (1954), o índice de Hopkins e Skellam (1954), até o uso de funções (Scalon 2024).\nNeste estudo, prioriza-se o uso de funções, pois estas permitem analisar os padrões em diferentes distâncias (r) e podem ser visualizadas graficamente, o que não é possível com simples estatísticas. Entre as funções, destacam-se as funções sumárias homogêneas de distância,\\(F(r)\\),\\(G(r)\\),\\(J(r)\\), bem como as funções sumárias homogêneas de segunda ordem \\(K(r)\\),\\(L(r)\\) e \\(g(r)\\), ou suas extensões não homogêneas. Essas funções são comparadas com um processo pontual Poisson homogêneo \\(F_{pois}(r)= G_{pois}(r) = 1- e^{-\\pi \\lambda r^{2}}\\) e \\(J_{pois}(r) = 1\\) ou \\(K_{pois}(r)= \\pi r^{2}\\),\\(L_{pois}(r)= r\\) e \\(g_{pois}(r)= 1\\). Se forem equivalentes, diz-se que o padrão de distribuição espacial é de aleatoriedade espacial completa (AEC), caso contrário, pode ser regular (Inibição) ou agregado (Atração) conforme descrito no Quadro Tabela 5.1.",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#sec-2.6.1",
    "href": "point_process.html#sec-2.6.1",
    "title": "5  Processos Pontuais",
    "section": "5.7 Funções \\(F(r),G(r),J(r),K(r),L(r)\\, e \\,g(r)\\)",
    "text": "5.7 Funções \\(F(r),G(r),J(r),K(r),L(r)\\, e \\,g(r)\\)\nNesta seção, apresentamos as funções \\(F(r),G(r),J(r),K(r),L(r)\\, e \\,g(r)\\) homogêneas, úteis para identificação do padrão de distribuição (regular, aleatório ou agrupado) dos processos pontuais.\nFunção F (r)\nEm um processo pontual \\(s\\), a distância \\(d(z, s_{i}) = min{||z-s_{i}|| : s_{i} \\in S}\\) representa a menor distância de quaisquer pontos fixos imaginários \\(z \\in \\mathbb{R}^{2}\\) para os \\(i\\)-ésimos eventos \\(s_{i}\\) de estudo mais próximos, conforme ilustrado na Figura 5.6 (b). Esta distância é comumente referida como a distância do espaço vazio e a função que representa todas as possíveis distâncias (função cumulativa) é denominada função \\(F(r)\\) e é definida como:\n\\[\nF(r)= P\\{d(z, s_{i})\\leq r\\} \\text{ e } \\hat{F}(r) = \\frac{1}{n(z)} \\sum_{k=1}^{K} \\mathbb{I}\\left\\{d(z_{k}, s_{i})\\leq r\\right\\}, \\forall r \\geq 0,\n\\tag{5.11}\\]\nonde \\(\\hat{F}(r)\\) é seu estimador, \\(\\mathbb{I}\\) é função indicadora e \\(d(z_{k}, s_{i})\\) a distância real entre \\(k\\)-ésimos pontos \\(z_{k}\\) (não eventos) e \\(i\\)-ésimos eventos \\(s_{i}\\) mais próximos (Gelfand et al. 2010; Peter John Diggle 2003; Adrian Baddeley, Rubak, e Turner 2015; Scalon 2024).\nSe a curva \\(\\hat{F}(r)\\) estiver acima da curva \\(F_{pois}(r)\\), ou seja, \\(\\hat{F}(r)&gt;F_{pois}(r)\\), indica que a distância entre quaisquer pontos \\(z\\) até o evento mais próximo \\(s_{i}\\) é menor do que seria sob aleatoriedade espacial completa. Isto sugere que existe menor espaço vazio (não ocupado pelos eventos) e, consequentemente, o padrão espacial dos eventos é regular ( Figura 5.4 (a)). Se \\(\\hat{F}(r)\\) é equivalente a \\(F_{pois}(r)\\)\\((\\hat{F}(r) \\equiv F_{pois}(r))\\), o padrão é considerado aleatório, também designado aleatoriedade espacial completa (AEC) ( Figura 5.4 (b)). Se a curva \\(\\hat{F}(r)\\) estiver abaixo da curva \\(F_{pois}(r)\\)\\((\\hat{F}(r) &lt; F_{pois}(r))\\), significa que a distância entre quaisquer pontos \\(z\\) até o evento mais próximo \\(s_{i}\\) é maior do que seria sob aleatoriedade espacial completa. Nesse caso, há mais espaço vazio e, consequentemente, o padrão espacial dos eventos é agrupado ( Figura 5.4 (c)).\n\n\nCódigo\nset.seed(100) \njanela &lt;- owin(c(0, 1), c(0, 1))\n\n# (b) Aleatório (Referência)\npp_aleatorio &lt;- rpoispp(lambda = 50, win = janela)\n\n# (a) Regular\npp_regular &lt;- rSSI(r = 0.1, n = 50, win = janela)\n\n# (c) Agrupado\npp_agrupado &lt;- rThomas(kappa = 5, scale = 0.05, mu = 10, win = janela)\n\n# Função auxiliar para calcular F(r) e plotar\nplot_f_function &lt;- function(pp, titulo, tipo_padrao) {\n  #correção de borda 'km' (Kaplan-Meier)\n  F_calc &lt;- Fest(pp, correction = \"km\")\n  df_F &lt;- as.data.frame(F_calc)\n  df_F &lt;- df_F[df_F$r &lt;= 0.25, ]\n  \n  ggplot(df_F, aes(x = r)) +\n    geom_line(aes(y = theo, linetype = \"Teórico (Poisson)\"), color = \"red\", size = 0.8) +\n    geom_line(aes(y = km, linetype = \"Observado\"), color = \"black\", size = 1) +\n    scale_linetype_manual(name = \"\", values = c(\"Observado\" = \"solid\", \"Teórico (Poisson)\" = \"dashed\")) +\n    labs(title = titulo, \n         subtitle = tipo_padrao,\n         x = \"Distância (r)\", y = \"F(r)\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\",\n          plot.title = element_text(size = 11))\n}\n\n\np1 &lt;- plot_f_function(pp_regular, \"(a) Regular\", \"Obs &gt; Teórico)\")\np2 &lt;- plot_f_function(pp_aleatorio, \"(b) Aleatório\", \"Obs = Teórico (AEC)\")\np3 &lt;- plot_f_function(pp_agrupado, \"(c) Agrupado\", \"Obs &lt; Teórico\")\n\n\np1 + p2 + p3\n\n\n\n\n\n\n\n\nFigura 5.4: Comportamento da Função F(r): (a) Regular (Acima da teórica), (b) Aleatório (Sobreposta) e (c) Agrupado (Abaixo da teórica).\n\n\n\n\n\nConsiderando a região de estudo \\(S\\), uma sub-região de uma área infinita, o estimador \\(\\hat{F}(r)\\), representado pela expressão Eq. 5.11, apresentará viés. Esse viés ocorre porque a distância \\(d(z_{k}, s_{i})\\) de um ponto \\(z_{k}\\) localizado na borda (limite da área de estudo) até o evento mais próximo \\(s_{i}\\) pode parecer maior do que a distância \\(r\\), porque na sua determinação, não são considerados os eventos fora da sub-região em estudo (Scalon 2024).\nNo entanto, se todos os eventos existentes na sub-região fora da área de estudo fossem levados em consideração, essa distância seria menor ou igual a \\(r\\) (Peter John Diggle 2003; Leininger 2014). Esse viés é conhecido como o “efeito da borda”. A correção para esse viés é denominada “correção da borda” (\\(e_{k}\\)), que corresponde à distância de um ponto \\(z_{k}\\) até o limite da área de estudo \\(B\\) (borda). Portanto, um estimador que não apresenta viés para essa situação é,\n\\[\n\\hat{F}_{bord}(r)=\\frac{\\sum_{k} \\mathbb{I}\\left\\{d\\left(z_{k}, s_{i}\\right) \\leq r\\right\\} \\mathbb{I}\\left\\{e_{k}&gt;r\\right\\}}{\\sum_{k} \\mathbb{I}\\left\\{e_{k}&gt;r\\right\\}}, \\text{ onde } e_{k}=d(z_{k}, S \\cap B), \\; r \\geq 0 ,\n\\tag{5.12}\\]\nonde \\(d\\left(z_{k}, S \\cap B\\right)\\) representa a distância entre quaisquer pontos amostrais (não eventos) \\(z_{k}\\) para o evento \\(s_{i}\\in s\\) mais próximo, considerando a existência dos efeitos borda na região \\(B\\) de estudo.\nFunção G (r)\nDiferentemente da função \\(F(r)\\), que mede a distância entre um ponto e um evento, a função \\(G(r)\\) mede a distância entre eventos ( Figura 5.6 (a)). Seja \\(s_{i}\\) um evento de um processo pontual \\(s\\), a distância até o vizinho mais próximo \\(d_{\\min}(s_{i}, s_{i^{'}}) = \\min_{i^{'}\\neq i}||s_{i^{'}} - s_{i}||\\) pode ser expressa por \\(d_{\\min}(s_{i}, s_{i^{'}}) = d(s_{i}, s_{i^{'}} \\setminus s_{i})\\), que corresponde à distância mínima de um evento \\(s_{i}\\) até os outros eventos \\(s_{i^{'}}\\) diferentes de \\(s_{i}\\) (Adrian Baddeley, Rubak, e Turner 2015; Scalon 2024).\nConforme mencionado por Peter John Diggle (2003), Leininger (2014) e Scalon (2024), considerando \\(n(s)\\) como o número de eventos \\(s\\) em uma região de estudo \\(B\\) e \\(d_{\\min}(s_{i}, s_{i^{'}}) = d(s_{i}, s_{i^{'}} \\setminus s_{i})\\) como a distância do \\(i\\)-ésimo evento \\(s_{i}\\) até o evento mais próximo \\(s_{i} \\setminus s_{i^{'}}\\),\\(d_{\\min}(s_{i}, s_{i^{'}})\\) é designada como a distância do vizinho mais próximo. Esta medida inclui distâncias repetidas em pares de vizinhos mais próximos recíprocos {\\(d(s_{i}, s_{i^{'}} \\setminus s_{i})\\) e \\(d(s_{i^{'}} \\setminus s_{i}, s_{i})\\)}, ou seja,\n\\[\nG(r)=P\\{d(s_{i}, s_{i^{'}} \\setminus s_{i})\\leq r \\} \\quad e \\quad \\hat{G} (r)=\\frac{1}{n(s)} \\sum_{i} \\mathbb{I}\\left\\{d\\left(s_{i},s_{i^{'}}\\setminus s_{i}\\right)\\leq r\\right\\}, \\quad r \\geq 0,\n\\tag{5.13}\\]\nonde \\(\\hat{G}(r)\\) é respectivo estimador.\nSe a curva \\(\\hat{G}(r)\\) estiver abaixo da curva \\(G_{pois}(r)\\), ou seja, \\(\\hat{G}(r)&lt;G_{pois}(r)\\), indica que a distância entre quaisquer eventos \\(s_{i}\\) até os eventos \\(s_{i^{'}}\\) mais próximo é maior do que seria sob aleatoriedade espacial completa. Nesse caso, há maior distância de separação entre os eventos e, consequentemente, o padrão espacial dos eventos é regular ( Figura 5.5 (a)). Se \\(\\hat{G}(r)\\) é equivalente a \\(G_{pois}(r)\\)\\((\\hat{G}(r) \\equiv G_{pois}(r))\\), o padrão é considerado aleatório, também designado aleatoriedade espacial completa (AEC) ( Figura 5.5 (b)). Se a curva \\(\\hat{G}(r)\\) estiver acima da curva \\(G_{pois}(r)\\)\\((\\hat{G}(r) &gt; G_{pois}(r))\\), significa que a distância entre quaisquer eventos \\(s_{i}\\) até os eventos \\(s_{i^{'}}\\) mais próximo é menor do que seria sob aleatoriedade espacial completa. Isto sugere que existe menor distância de separação entre os eventos e, consequentemente, o padrão espacial dos eventos é agrupado ( Figura 5.5 (c)).\n\\[\n\\hat{G}_{bord}(r)=\\frac{\\sum_{i} \\mathbb{I}\\left\\{e_{i}\\geq r \\, \\cap \\, d(s_{i}, s_{i^{'}} \\setminus s_{i})\\leq r \\right\\}}{\\sum_{i} \\mathbb{I}\\left\\{e_{i}\\geq r\\right\\}}, \\, \\text{ onde }\\; e_{i}= d(s_{i},S \\cap B)\\quad r \\geq 0.\n\\tag{5.14}\\]\n\n\nCódigo\nset.seed(42)\njanela &lt;- owin(c(0, 1), c(0, 1))\n\n#(b) Aleatório (Referência)\npp_aleatorio &lt;- rpoispp(lambda = 60, win = janela)\n\n# (a) Regular (Inibição forte - Vizinhos afastados)\npp_regular &lt;- rSSI(r = 0.08, n = 60, win = janela)\n\n# (c) Agrupado (Atração forte - Vizinhos muito próximos)\npp_agrupado &lt;- rThomas(kappa = 5, scale = 0.03, mu = 12, win = janela)\n\n# Função auxiliar para calcular G(r) e plotar\nplot_g_function &lt;- function(pp, titulo, anotacao) {\n  G_calc &lt;- Gest(pp, correction = \"km\")\n  df_G &lt;- as.data.frame(G_calc)\n  \n  # Limitar o eixo X para focar onde a ação acontece\n  df_G &lt;- df_G[df_G$r &lt;= 0.20, ]\n  \n  ggplot(df_G, aes(x = r)) +\n    geom_line(aes(y = theo, linetype = \"Teórico (Poisson)\"), color = \"red\", size = 0.8) +\n    geom_line(aes(y = km, linetype = \"Observado\"), color = \"black\", size = 1) +\n    scale_linetype_manual(name = \"\", values = c(\"Observado\" = \"solid\", \"Teórico (Poisson)\" = \"dashed\")) +\n    labs(title = titulo, \n         subtitle = anotacao,\n         x = \"Distância (r)\", y = \"G(r)\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\",\n          plot.title = element_text(size = 11))\n}\n\np1 &lt;- plot_g_function(pp_regular, \"(a) Regular\", \"Obs &lt; Teórico\")\n\np2 &lt;- plot_g_function(pp_aleatorio, \"(b) Aleatório\", \"Obs = Teórico (AEC)\")\n\np3 &lt;- plot_g_function(pp_agrupado, \"(c) Agrupado\", \"Obs &gt; Teórico\")\n\n\np1 + p2 + p3\n\n\n\n\n\n\n\n\nFigura 5.5: Comportamento da Função G(r): (a) Regular (Abaixo da teórica), (b) Aleatório (Sobreposta) e (c) Agrupado (Acima da teórica).\n\n\n\n\n\nConsiderando a região de estudo \\(s\\), uma sub-área de uma infinita área \\(B\\), o estimador \\(\\hat{G}(r)\\) representado pela expressão Eq. 5.13, apresentará viés, pois a distância \\(d(s_{i},s_{i^{'}}\\setminus s_{i})\\) de um evento \\(s_{i}\\) que esteja na borda para o evento \\(s_{i^{'}}\\) mais próximo pode aparentemente ser maior que a distância \\(r\\), enquanto na realidade, se fossem considerados os eventos que estão fora da área de estudo, esta distância seria menor ou igual a \\(r\\) ( Figura 5.7 (c)). Este viés ocorre porque a distância \\(d(s_{i},s_{i^{'}}\\setminus s_{i})\\) de qualquer evento em \\(B\\), para o evento mais próximo que esteja fora da área de estudo não é considerada, resultando em um viés. Portanto, um estimador que não apresentara viés para esta situação em concreto é,\n\\[\n\\hat{G}_{bord}(r)=\\frac{\\sum_{i} \\mathbb{I}\\left\\{e_{i}\\geq r \\, \\cap \\, d(s_{i}, s_{i^{'}} \\setminus s_{i})\\leq r \\right\\}}{\\sum_{i} \\mathbb{I}\\left\\{e_{i}\\geq r\\right\\}}, \\, \\text{ onde }\\; e_{i}= d(s_{i},S \\cap B)\\quad r \\geq 0.\n\\tag{5.15}\\]\n\n\nCódigo\n# Função auxiliar para desenhar círculos\nget_circle &lt;- function(center_x, center_y, radius) {\n  angle &lt;- seq(0, 2 * pi, length.out = 100)\n  data.frame(x = center_x + radius * cos(angle), y = center_y + radius * sin(angle))\n}\n\n# Função G(r)\ndf_pts_g &lt;- data.frame(x = c(0.4, 0.7), y = c(0.4, 0.7), type = c(\"si\", \"sj\"))\nradius_g &lt;- sqrt((0.7-0.4)^2 + (0.7-0.4)^2) # Distância euclidiana\ncircle_g &lt;- get_circle(0.4, 0.4, radius_g)\n\np1 &lt;- ggplot() +\n  # Círculo de busca\n  geom_path(data = circle_g, aes(x, y), linetype = \"dashed\", color = \"gray60\") +\n  \n  # Eventos (Pontos sólidos)\n  geom_point(data = df_pts_g, aes(x, y), size = 4) +\n  \n  # Seta de distância\n  geom_segment(aes(x = 0.4, y = 0.4, xend = 0.7, yend = 0.7), \n               arrow = arrow(length = unit(0.3, \"cm\")), color = \"blue\") +\n  \n  # Labels\n  geom_text(aes(x = 0.4, y = 0.4, label = \"s[i]\"), parse = TRUE, vjust = 2, size = 6) +\n  geom_text(aes(x = 0.7, y = 0.7, label = \"s[j]\"), parse = TRUE, vjust = -1, size = 6) +\n  annotate(\"text\", x = 0.5, y = 0.6, label = \"d(s[i], s[j])\", parse = TRUE, color = \"blue\") +\n  \n  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +\n  theme_void() +\n  geom_rect(aes(xmin=0, xmax=1, ymin=0, ymax=1), fill=NA, color=\"black\") +\n  labs(title = \"(a) Função G(r)\\n(Evento para Evento)\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Função F(r)\ndf_evt_f &lt;- data.frame(x = 0.7, y = 0.6) # Evento\ndf_pt_z &lt;- data.frame(x = 0.3, y = 0.4)  # Ponto Z\nradius_f &lt;- sqrt((0.7-0.3)^2 + (0.6-0.4)^2)\ncircle_f &lt;- get_circle(0.3, 0.4, radius_f)\n\np2 &lt;- ggplot() +\n  # Círculo de busca\n  geom_path(data = circle_f, aes(x, y), linetype = \"dashed\", color = \"gray60\") +\n  \n  # Evento (Ponto sólido)\n  geom_point(data = df_evt_f, aes(x, y), size = 4) +\n  \n  # Ponto Arbitrário Z (Cruz)\n  geom_point(data = df_pt_z, aes(x, y), shape = 4, size = 5, stroke = 2) +\n  \n  # Seta de distância\n  geom_segment(aes(x = 0.3, y = 0.4, xend = 0.7, yend = 0.6), \n               arrow = arrow(length = unit(0.3, \"cm\")), color = \"red\") +\n  \n  # Labels\n  geom_text(aes(x = 0.7, y = 0.6, label = \"s[i]\"), parse = TRUE, vjust = -1, size = 6) +\n  geom_text(aes(x = 0.3, y = 0.4, label = \"z\"), vjust = 2, size = 6, fontface = \"italic\") +\n  annotate(\"text\", x = 0.5, y = 0.55, label = \"d(z, s[i])\", parse = TRUE, color = \"red\") +\n  \n  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +\n  theme_void() +\n  geom_rect(aes(xmin=0, xmax=1, ymin=0, ymax=1), fill=NA, color=\"black\") +\n  labs(title = \"(b) Função F(r)\\n(Ponto Vazio para Evento)\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\np1 + p2\n\n\n\n\n\n\n\n\nFigura 5.6: Esquema Conceitual: (a) Função G mede a distância entre eventos; (b) Função F mede a distância de um ponto vazio (z) até um evento.\n\n\n\n\n\n\n\nCódigo\nget_circle &lt;- function(center_x, center_y, radius, n_points = 100) {\n  angle &lt;- seq(0, 2 * pi, length.out = n_points)\n  data.frame(\n    x = center_x + radius * cos(angle),\n    y = center_y + radius * sin(angle)\n  )\n}\n\n#\ndf_g_pts &lt;- data.frame(x = c(0.3, 0.7), y = c(0.5, 0.7), label = c(\"si\", \"sj\"))\ncircle_g &lt;- get_circle(0.3, 0.5, radius = sqrt((0.7-0.3)^2 + (0.7-0.5)^2))\n\np1 &lt;- ggplot() +\n  geom_path(data = circle_g, aes(x, y), linetype = \"dashed\", color = \"grey50\") +\n  geom_point(data = df_g_pts, aes(x, y), size = 3) +\n  geom_segment(aes(x = 0.3, y = 0.5, xend = 0.7, yend = 0.7), arrow = arrow(length = unit(0.2, \"cm\"))) +\n  geom_text(data = df_g_pts, aes(x, y, label = label), vjust = -1.5, size = 5) +\n  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +\n  theme_void() +\n  geom_rect(aes(xmin=0, xmax=1, ymin=0, ymax=1), fill=NA, color=\"black\") +\n  labs(title = \"(a) Função G(r)\\n(Evento para Evento)\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# --- Plot B: Função F (Ponto Vazio para Evento) ---\ndf_f_evt &lt;- data.frame(x = 0.6, y = 0.6, label = \"si\")\ndf_f_pt &lt;- data.frame(x = 0.4, y = 0.4, label = \"z\")\ncircle_f &lt;- get_circle(0.4, 0.4, radius = sqrt((0.6-0.4)^2 + (0.6-0.4)^2))\n\np2 &lt;- ggplot() +\n  geom_path(data = circle_f, aes(x, y), linetype = \"dashed\", color = \"grey50\") +\n  geom_point(data = df_f_evt, aes(x, y), size = 3) + # Evento (bolinha)\n  geom_point(data = df_f_pt, aes(x, y), shape = 4, size = 4, stroke = 2) + # Ponto Z (X)\n  geom_segment(aes(x = 0.4, y = 0.4, xend = 0.6, yend = 0.6), arrow = arrow(length = unit(0.2, \"cm\"))) +\n  geom_text(aes(x = 0.6, y = 0.6, label = \"si\"), vjust = -1.5, size = 5) +\n  geom_text(aes(x = 0.4, y = 0.4, label = \"z\"), vjust = 1.5, size = 5) +\n  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +\n  theme_void() +\n  geom_rect(aes(xmin=0, xmax=1, ymin=0, ymax=1), fill=NA, color=\"black\") +\n  labs(title = \"(b) Função F(r)\\n(Ponto para Evento)\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n#\ncenter_x &lt;- 0.8; center_y &lt;- 0.5\nobs_neigh_x &lt;- 0.4; obs_neigh_y &lt;- 0.5 # Distância 0.4\nradius &lt;- 0.4\ncircle_edge &lt;- get_circle(center_x, center_y, radius)\n\np3 &lt;- ggplot() +\n  # Área \"Fora\" do estudo\n  geom_rect(aes(xmin=1, xmax=1.3, ymin=0, ymax=1), fill=\"gray90\", alpha=0.5) +\n  geom_rect(aes(xmin=0, xmax=1, ymin=0, ymax=1), fill=NA, color=\"black\", size=1) +\n  \n  # Círculo de busca que \"vaza\"\n  geom_path(data = circle_edge, aes(x, y), linetype = \"dotted\", color = \"red\", size=1) +\n  \n  # Evento Focal\n  geom_point(aes(x=center_x, y=center_y), size=3) +\n  geom_text(aes(x=center_x, y=center_y, label=\"si\"), vjust=-1.5, size=5) +\n  \n  # Vizinho Observado (Longe)\n  geom_point(aes(x=obs_neigh_x, y=obs_neigh_y), size=3) +\n  geom_text(aes(x=obs_neigh_x, y=obs_neigh_y, label=\"sj\\n(Observado)\"), vjust=2, size=3) +\n  \n  geom_point(aes(x=1.1, y=0.55), shape=1, color=\"red\", size=3) +\n  geom_text(aes(x=1.1, y=0.55, label=\"?\\n(Real?)\"), vjust=1.5, color=\"red\", size=3) +\n  \n  # Setas indicando o problema\n  geom_segment(aes(x=center_x, y=center_y, xend=obs_neigh_x, yend=obs_neigh_y), arrow=arrow(length=unit(0.2,\"cm\"))) +\n  geom_segment(aes(x=center_x, y=center_y, xend=1, yend=0.5), linetype=\"solid\", color=\"blue\", size=1) +\n  \n  annotate(\"text\", x=0.9, y=0.45, label=\"Dist. Borda\", color=\"blue\", size=3) +\n  \n  coord_fixed(xlim = c(0, 1.3), ylim = c(0, 1)) +\n  theme_void() +\n  labs(title = \"(c) Efeito de Borda\\n(Círculo vaza a fronteira)\") +\n  theme(plot.title = element_text(hjust = 0.5, color=\"red\"))\n\np1 + p2 + p3\n\n\n\n\n\n\n\n\nFigura 5.7: Esquema Conceitual: (a) Função G (Evento-Evento), (b) Função F (Ponto-Evento) e (c) O Problema do Efeito de Borda (Censura).\n\n\n\n\n\nFunção J(r)\nA função \\(J(r)\\) é uma função que não apresenta viés e resulta da razão entre as funções \\(G(r)\\) e \\(F(r)\\) acima descritas, ou seja,\n\\[\nJ(r) = \\frac{1-G(r)}{1-F(r)} \\text{ e } \\hat{J}(r) =\\frac{1-\\hat{G}(r)}{1-\\hat{F}(r)}= \\frac{1-\\frac{1}{n(s)} \\sum_{i}  \\mathbb{I}\\left\\{d\\left(s_{i},s_{i^{'}}\\setminus s_{i}\\right)\\right\\}\\leq r)}{1-\\frac{1}{J} \\sum_{j=1}^{J} \\mathbb{I}\\left\\{d(z_{j}, s_{i^{'}})\\leq r\\right\\}},\\quad \\, F(r), \\hat{F} (r)&lt; 1,\n\\tag{5.16}\\]\nonde \\(\\hat{J}(r)\\) é o respectivo estimador (Van Lieshout e Baddeley 1996; Scalon 2024).\nComo \\(F_{pois}(r)= G_{pois}(r) = 1- e^{-\\pi \\lambda r^{2}}\\),\\(J_{pois} (r) = \\frac{1-G_{pois}(r)}{1-F_{pois}(r)}= \\frac{1-(1- e^{-\\pi \\lambda r^{2}})}{1-(1- e^{-\\pi \\lambda r^{2}})}=1\\), consequentemente, se \\(\\hat{J}(r) &gt; 1\\), o processo pontual apresenta um padrão regular ( Figura 5.8 (a)). Se \\(\\hat{J}(r)\\equiv 1\\) o processo pontual apresenta um padrão aleatório ( Figura 5.8 (b)). Se \\(\\hat{J}(r)&lt;1\\) o processo pontual apresenta um padrão agrupado ( Figura 5.8 (c)).\n\n\nCódigo\nset.seed(888) \njanela &lt;- owin(c(0, 1), c(0, 1))\n\n#  (b) Aleatório (Referência - Poisson)\npp_aleatorio &lt;- rpoispp(lambda = 50, win = janela)\n\n# (a) Regular (Inibição - Hard Core)\npp_regular &lt;- rSSI(r = 0.09, n = 50, win = janela)\n\n# (c) Agrupado (Atração - Thomas)\npp_agrupado &lt;- rThomas(kappa = 5, scale = 0.04, mu = 10, win = janela)\n\n# Função auxiliar para calcular J(r) e plotar\nplot_j_function &lt;- function(pp, titulo, anotacao) {\n  # Calcular J(r)\n  J_calc &lt;- Jest(pp, correction = \"km\")\n  df_J &lt;- as.data.frame(J_calc)\n  \n  # Limitar o eixo X para onde a interação é relevante\n  df_J &lt;- df_J[df_J$r &lt;= 0.22 & is.finite(df_J$km), ]\n  \n  ggplot(df_J, aes(x = r)) +\n    geom_hline(aes(yintercept = 1, linetype = \"Teórico (Poisson)\"), \n               color = \"red\", size = 0.8) +\n    # Linha Observada\n    geom_line(aes(y = km, linetype = \"Observado\"), color = \"black\", size = 1) +\n    \n    scale_linetype_manual(name = \"\", values = c(\"Observado\" = \"solid\", \"Teórico (Poisson)\" = \"dashed\")) +\n    coord_cartesian(ylim = c(0, 2.5)) +\n    \n    labs(title = titulo, \n         subtitle = anotacao,\n         x = \"Distância (r)\", y = \"J(r)\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\",\n          plot.title = element_text(size = 11))\n}\n\n\np1 &lt;- plot_j_function(pp_regular, \"(a) Regular\", \"J(r) &gt; 1 (Inibição)\")\np2 &lt;- plot_j_function(pp_aleatorio, \"(b) Aleatório\", \"J(r) = 1 (Independência)\")\np3 &lt;- plot_j_function(pp_agrupado, \"(c) Agrupado\", \"J(r) &lt; 1 (Atração)\")\n\n\np1 + p2 + p3\n\n\n\n\n\n\n\n\nFigura 5.8: Comportamento da Função J(r): (a) Regular (J &gt; 1), (b) Aleatório (J = 1) e (c) Agrupado (J &lt; 1).\n\n\n\n\n\nFunção \\(K (r)\\) ou função \\(\\kappa\\) de Ripley\nA função de \\(K(r)\\) ou função \\(\\kappa\\) de Ripley estima o número esperado de \\(r\\)-ésimos vizinhos de um evento \\(s\\), dividido pela intensidade \\(\\lambda (s)\\). Em outras palavras, esta função estima o número médio de eventos \\(( \\mathbb{E}[d(s_{i^{'}},r,s_{i})\\vert s_{i^{'}} \\in S])\\) que estão contidos em um círculo de raio \\(r\\), centrado em um evento \\(s_{i}\\) de referência, sem contar o próprio evento \\(s_{i}\\) (\\(r\\) é a distância entre o evento de referência \\(s_{i}\\) até outro evento \\(s_{i^{'}}\\)). Em seguida, número médio de eventos estimados é dividido pela intensidade \\(\\lambda (s)\\) ( Figura 5.9), ou seja,\n\\[\n\\begin{aligned}\nK(r)=&\\frac{ \\mathbb{E}[d(s_{i},r,s_{i^{'}})\\leq r\\vert s_{i^{'}} \\in S]}{\\lambda (s_{i}) \\lambda (s_{i^{'}})} = \\frac{ \\mathbb{E}[\\sum_{i^{'}\\neq i}  \\mathbb{I} \\{0 &lt; ||s_{i}-s_{i^{'}}|| \\leq r \\} \\vert s_{i^{'}} \\in S]}{\\lambda (s_{i}) \\lambda (s_{i^{'}})}, \\lambda (s) &gt; 0, r \\geq 0, \\\\\n\\hat{K}(r)&=\\frac{1}{|B|}\\sum_{i=1} \\sum_{i^{'}\\neq i} \\frac{ \\mathbb{I} \\left[||s_{i}-s_{i^{'}}||\\leq r \\right]}{\\hat{\\lambda} (s_{i}) \\hat{\\lambda} (s_{i^{'}})} \\text{ ou } \\hat{K}_{bord}(r)=\\frac{1}{|B|}\\sum_{i=1} \\sum_{i^{'}\\neq i} \\frac{ \\mathbb{I} \\left[||s_{i}-s_{i^{'}}||\\leq r \\right]}{\\hat{\\lambda} (s_{i}) \\hat{\\lambda} (s_{i^{'}})e_{ii^{'}}},\n\\end{aligned}\n\\tag{5.17}\\]\nonde \\(\\hat{K}(r)\\) e \\(\\hat{K}_{bord}(r)\\) são respectivos estimadores na ausência e na presença dos efeitos da borda respectivamente.\\(e_{ii^{'}}\\) é correção da borda, \\(\\mathbb{I}[\\cdot]\\) função indicadora, \\(|B|\\) área de região em estudo e \\(\\hat{\\lambda} (s)=n/|B|\\) é intensidade (Cressie 1993; Gatrell et al. 1996; Peter J. Diggle 2013; Leininger 2014; A González e Moraga 2023; Scalon 2024).\n\n\nCódigo\nlibrary(ggplot2)\n\n# 1. Configuração do Cenário\nset.seed(123)\ncentro_x &lt;- 0.5\ncentro_y &lt;- 0.5\nraio_r &lt;- 0.25\n\n# 2. Gerar Eventos Aleatórios\ndf_pontos &lt;- data.frame(\n  x = runif(20, 0, 1),\n  y = runif(20, 0, 1)\n)\n\n# Forçar o evento de referência (si) no centro\ndf_pontos &lt;- rbind(df_pontos, data.frame(x = centro_x, y = centro_y))\n\n# 3. Calcular distâncias e classificar\ndf_pontos$dist &lt;- sqrt((df_pontos$x - centro_x)^2 + (df_pontos$y - centro_y)^2)\n\ndf_pontos$tipo &lt;- dplyr::case_when(\n  df_pontos$dist == 0 ~ \"Referencia (si)\",\n  df_pontos$dist &lt;= raio_r ~ \"Vizinho (sj)\",\n  TRUE ~ \"Outros\"\n)\n\n# 4. Criar o Círculo para o plot\nangulo &lt;- seq(0, 2 * pi, length.out = 100)\ncirculo &lt;- data.frame(\n  x = centro_x + raio_r * cos(angulo),\n  y = centro_y + raio_r * sin(angulo)\n)\n\n# 5. Plotagem\nggplot() +\n  # Área de busca (Círculo)\n  geom_polygon(data = circulo, aes(x, y), fill = \"blue\", alpha = 0.1) +\n  geom_path(data = circulo, aes(x, y), linetype = \"dashed\", color = \"blue\") +\n  \n  # Raio r (Seta)\n  geom_segment(aes(x = centro_x, y = centro_y, \n                   xend = centro_x + raio_r * cos(pi/4), \n                   yend = centro_y + raio_r * sin(pi/4)), \n               arrow = arrow(length = unit(0.2, \"cm\")), color = \"black\") +\n  annotate(\"text\", x = centro_x + 0.1, y = centro_y + 0.12, label = \"r\", size = 6) +\n  \n  # Pontos (Eventos)\n  geom_point(data = df_pontos, aes(x, y, color = tipo, size = tipo, shape = tipo)) +\n  \n  # Rótulos\n  annotate(\"text\", x = centro_x, y = centro_y - 0.04, label = \"s[i]\", parse = TRUE, color = \"red\", size = 5) +\n  \n  # Estilo\n  scale_color_manual(values = c(\"Referencia (si)\" = \"red\", \"Vizinho (sj)\" = \"blue\", \"Outros\" = \"gray70\")) +\n  scale_size_manual(values = c(\"Referencia (si)\" = 5, \"Vizinho (sj)\" = 3, \"Outros\" = 2)) +\n  scale_shape_manual(values = c(\"Referencia (si)\" = 19, \"Vizinho (sj)\" = 19, \"Outros\" = 1)) +\n  \n  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +\n  theme_void() +\n  theme(legend.position = \"bottom\", \n        legend.title = element_blank(),\n        plot.margin = margin(10, 10, 10, 10)) +\n  labs(title = \"Contagem K(r): Quantos pontos azuis existem?\")\n\n\n\n\n\n\n\n\nFigura 5.9: Intuição da Função K de Ripley: Contagem de eventos vizinhos (azuis) dentro de um círculo de raio r centrado no evento de referência si (vermelho).\n\n\n\n\n\nA interpretação da função K(r) é idêntica à função G(r). Se \\(K(r)&lt;K_{pois}(r)\\) (abaixo de), significa que na distância considerada, há poucos eventos contidos no círculo do que seria esperado se o padrão fosse aleatório. Consequentemente, o padrão é regular ( Figura 5.10 (a)). Se \\(K(r)\\equiv K_{pois}\\), o padrão é aleatório ( Figura 5.10 (b)). Se \\(K(r)&gt;K_{pois}\\) (acima de), significa que na distância (r) considerada, existem muitos eventos contidos no círculo do que seria esperado se o padrão fosse aleatório. Consequentemente, o padrão é agrupado ( Figura 5.10 (c)).\n\n\nCódigo\nset.seed(500)\njanela &lt;- owin(c(0, 1), c(0, 1))\n\n# (b) Aleatório (Referência)\npp_aleatorio &lt;- rpoispp(lambda = 50, win = janela)\n\n# (a) Regular (Inibição - Vizinhos distantes reduzem o valor acumulado de K)\npp_regular &lt;- rSSI(r = 0.1, n = 50, win = janela)\n\n# (c) Agrupado (Atração - Vizinhos próximos aumentam o valor acumulado de K)\npp_agrupado &lt;- rThomas(kappa = 5, scale = 0.04, mu = 10, win = janela)\n\nplot_k_function &lt;- function(pp, titulo, anotacao) {\n  K_calc &lt;- Kest(pp, correction = \"iso\")\n  df_K &lt;- as.data.frame(K_calc)\n  df_K &lt;- df_K[df_K$r &lt;= 0.25, ]\n  \n  ggplot(df_K, aes(x = r)) +\n    # Linha Teórica (Poisson = pi * r^2)\n    geom_line(aes(y = theo, linetype = \"Teórico (Poisson)\"), \n              color = \"red\", size = 0.8) +\n    geom_line(aes(y = iso, linetype = \"Observado\"), \n              color = \"black\", size = 1) +\n    \n    scale_linetype_manual(name = \"\", values = c(\"Observado\" = \"solid\", \"Teórico (Poisson)\" = \"dashed\")) +\n    \n    labs(title = titulo, \n         subtitle = anotacao,\n         x = \"Distância (r)\", y = \"K(r)\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\",\n          plot.title = element_text(size = 11))\n}\n\n\np1 &lt;- plot_k_function(pp_regular, \"(a) Regular\", \"Obs &lt; Teórico (Crescimento Lento)\")\np2 &lt;- plot_k_function(pp_aleatorio, \"(b) Aleatório\", \"Obs = Teórico (Parábola)\")\np3 &lt;- plot_k_function(pp_agrupado, \"(c) Agrupado\", \"Obs &gt; Teórico (Crescimento Rápido)\")\n\n\np1 + p2 + p3\n\n\n\n\n\n\n\n\nFigura 5.10: Comportamento da Função K(r): (a) Regular (Abaixo da teórica), (b) Aleatório (Sobreposta) e (c) Agrupado (Acima da teórica).\n\n\n\n\n\nFunção L (r)\nConforme descrito em Scalon (2024), a função L (r) é uma transformação da função \\(K(r)\\) sugerida por Besag (1977), visando transformar o modelo Poisson teórico (representada por “- - -”) em uma linha reta, bem como estabilizar a variância nos dados sob completa aleatoriedade espacial ( Figura 5.11), ou seja,\n\\[L(r)=\\sqrt{\\frac{K(r)}{\\pi}} \\text{ e } \\hat{L}(r)=\\sqrt{\\frac{\\hat{K}(r)}{\\pi}} \\tag{5.18}\\]\n\n\nCódigo\nset.seed(300)\njanela &lt;- owin(c(0, 1), c(0, 1))\n\n# 1. Simulação dos Processos\n# (b) Aleatório (Referência)\npp_aleatorio &lt;- rpoispp(lambda = 60, win = janela)\n\n# (a) Regular (Inibição forte)\npp_regular &lt;- rSSI(r = 0.09, n = 60, win = janela)\n\n# (c) Agrupado (Atração forte)\npp_agrupado &lt;- rThomas(kappa = 5, scale = 0.04, mu = 12, win = janela)\n\nplot_l_function &lt;- function(pp, titulo, anotacao) {\n  # L(r) = sqrt(K(r)/pi)\n  L_calc &lt;- Lest(pp, correction = \"iso\")\n  df_L &lt;- as.data.frame(L_calc)\n  \n  df_L &lt;- df_L[df_L$r &lt;= 0.25, ]\n  \n  ggplot(df_L, aes(x = r)) +\n    geom_line(aes(y = theo, linetype = \"Teórico (Poisson)\"), \n              color = \"red\", size = 0.8) +\n    geom_line(aes(y = iso, linetype = \"Observado\"), \n              color = \"black\", size = 1) +\n    \n    scale_linetype_manual(name = \"\", values = c(\"Observado\" = \"solid\", \"Teórico (Poisson)\" = \"dashed\")) +\n    \n    labs(title = titulo, \n         subtitle = anotacao,\n         x = \"Distância (r)\", y = \"L(r)\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\",\n          plot.title = element_text(size = 11))\n}\n\np1 &lt;- plot_l_function(pp_regular, \"(a) Regular\", \"L(r) &lt; r (Abaixo da reta)\")\np2 &lt;- plot_l_function(pp_aleatorio, \"(b) Aleatório\", \"L(r) = r (Linha Reta)\")\np3 &lt;- plot_l_function(pp_agrupado, \"(c) Agrupado\", \"L(r) &gt; r (Acima da reta)\")\n\n\np1 + p2 + p3\n\n\n\n\n\n\n\n\nFigura 5.11: Comportamento da Função L(r): (a) Regular (Abaixo da diagonal), (b) Aleatório (Sobre a diagonal) e (c) Agrupado (Acima da diagonal).\n\n\n\n\n\nComo \\(L_{pois}(r) = r\\), que corresponde a uma linha reta de inclinação 1, é comum subtrair \\(r\\) de \\(L(r)\\), resultando em \\(L_{pois}(r) - r = 0\\), o que permite estudar o padrão espacial em torno de zero, conforme será detalhado na seção dos resultados.\nEm qualquer uma das possibilidades aqui apresentadas, se \\(\\hat{L}(r) &lt; L_{pois}(r)\\), o processo pontual apresenta um padrão regular ( Figura 5.11 (a)). Se \\(\\hat{L}(r)\\equiv L_{pois}(r)\\) o processo pontual apresenta um padrão aleatório ( Figura 5.11 (b)). Se \\(\\hat{L}(r) &gt; L_{pois}(r)\\) o processo pontual apresenta um padrão agrupado ( Figura 5.11 (c)).\nFunção \\(g(r)\\)\nA função g(r), também conhecida como função correlação par, é uma função de densidade de probabilidade padronizada que descreve a ocorrência conjunta dos eventos \\(s_{i}\\) e \\(s_{i^{'}}\\) em uma determinada região de estudo e, diferentemente de todas as funções já apresentadas, não é cumulativa ( Figura 5.12), ou seja,\n\\(g(r)=\\frac{\\lambda(s_{i},s_{i^{'}})}{\\lambda(s_{i})\\lambda(s_{i^{'}})} \\text{ e } \\hat{g}_{bord}(r)=\\frac{1}{2\\pi r |B|} \\sum_{i=1}^{n}\\sum_{j} \\frac{K_{h}\\left(||s_{i}-s_{i^{'}}||-r\\right)}{\\hat{\\lambda}(s_{i})\\hat{\\lambda}(s_{i^{'}})e_{ii^{'}}}\\), onde \\(g(r)\\neq G(r)\\),\\(\\hat{g}_{bord}(r)\\) é seu estimador na presença dos efeitos da borda,\\(K_{h}\\) é função Kernel,\\(h\\) é largura da banda e \\(e_{ii^{'}}\\) correção da borda. Na ausência destes,\\(\\hat{g}(r)\\) é obtido omitindo \\(e_{ii^{'}}\\) em \\(\\hat{g}_{bord}(r)\\) (Peter J. Diggle 2010; A González e Moraga 2023; Scalon 2024).\nSegundo Waagepetersen e Guan (2009) e Scalon (2024), se o processo pontual for isotrópico, a função \\(g(r)\\) é dada pela razão entre a derivada da função \\(K(r)\\) e \\(2\\pi r\\), ou seja,\n\\(g(r) = \\frac{K^{'}(r)}{2\\pi r} \\text{ e } \\hat{g}(r)= \\frac{\\hat{K}^{'}(r)}{2\\pi r}\\).\n\n\nCódigo\nset.seed(42)\njanela &lt;- owin(c(0, 1), c(0, 1))\nn_pts &lt;- 100 # Aumentei para 100 para suavizar as curvas\n\n\n# (b) Aleatório: Poisson Homogêneo\npp_aleatorio &lt;- rpoispp(lambda = n_pts, win = janela)\n\n# (a) Regular: Inibição (Hard Core)\npp_regular &lt;- rSSI(r = 0.07, n = n_pts, win = janela)\n\n# (c) Agrupado: Processo de Thomas\n# kappa=10 (pais), scale=0.02 (filhos bem pertinho -&gt; pico alto no g(r))\npp_agrupado &lt;- rThomas(kappa = 10, scale = 0.02, mu = 10, win = janela)\n\nplot_pcf_custom &lt;- function(pp, titulo, anotacao, y_max = NULL) {\n  g_calc &lt;- pcf(pp, correction = \"best\", divisor = \"d\") \n  df_g &lt;- as.data.frame(g_calc)\n  df_g &lt;- df_g[df_g$r &lt;= 0.20, ]\n  \n  p &lt;- ggplot(df_g, aes(x = r)) +\n    geom_hline(yintercept = 1, linetype = \"dashed\", color = \"red\", size = 0.8) +\n    geom_line(aes(y = iso), color = \"black\", size = 1.2) +\n    \n    labs(title = titulo, \n         subtitle = anotacao,\n         x = \"Distância (r)\", y = \"g(r)\") +\n    theme_light() +\n    theme(plot.title = element_text( size = 12),\n          plot.subtitle = element_text(size = 10))\n    if (!is.null(y_max)) {\n    p &lt;- p + coord_cartesian(ylim = c(0, y_max))\n  }\n  \n  return(p)\n}\n\np1 &lt;- plot_pcf_custom(pp_regular, \n                      \"(a) Regular\", \n                      \"Inicia em ZERO (Inibição) Sobe para 1\", \n                      y_max = 2.5)\n\np2 &lt;- plot_pcf_custom(pp_aleatorio, \n                      \"(b) Aleatório\", \n                      \"Oscila em torno de 1 (Linha Vermelha)\", \n                      y_max = 2.5)\n\np3 &lt;- plot_pcf_custom(pp_agrupado, \n                      \"(c) Agrupado\", \n                      \"Pico muito alto no início (&gt;1) Decai rapidamente\", \n                      y_max = NULL) # Deixa o ggplot decidir a altura do pico\n\n\np1 + p2 + p3\n\n\n\n\n\n\n\n\nFigura 5.12: Comportamento da Função g(r): (a) Regular (g &lt; 1), (b) Aleatório (g ≈ 1) e (c) Agrupado (g &gt; 1).\n\n\n\n\n\nSe \\(\\hat{g}(r) &lt; 1\\), o processo pontual apresenta um padrão regular ( Figura 5.12 (a)). Se \\(\\hat{g}(r)\\equiv 1\\) o processo pontual apresenta um padrão aleatório ( Figura 5.12 (b)). Se \\(\\hat{g}(r) &gt; 1\\) o processo pontual apresenta um padrão agrupado ( Figura 5.12 (c)).",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#sec-2.2.5.2",
    "href": "point_process.html#sec-2.2.5.2",
    "title": "5  Processos Pontuais",
    "section": "5.8 Funções \\(F_{inhom} (r), G_{inhom} (r), J_{inhom} (r), K_{inhom} (r), L_{inhom} (r)\\, e \\,g_{inhom} (r)\\)",
    "text": "5.8 Funções \\(F_{inhom} (r), G_{inhom} (r), J_{inhom} (r), K_{inhom} (r), L_{inhom} (r)\\, e \\,g_{inhom} (r)\\)\nAs funções \\(F(r), G(r), J(r), K(r), L(r) \\, e \\, g(r)\\) apresentadas na seção Seção 5.7 são úteis quando a condição de estacionariedade ou homogeneidade é satisfeita. Caso contrário, elas têm sua extensão para processos pontuais não homogêneos, sendo,\n\\[\nF_{inhom} (r) = 1- \\mathbb{E}\\left[\\prod_{s_{i}} \\left(1-\\frac{\\lambda_{\\min}}{\\lambda (s_{i})}\\right)\\right], \\\\\n\\]\n\\[\n\\hat{F}_{inhom} (r) = 1-\\frac{\\sum_{i}\\left( \\mathbb{I}\\left(w_{i}&gt;r\\right)\\left[\\prod_{i}  \\mathbb{I}\\left(||s_{i}-z_{i}||\\leq r\\right)\\left(1-\\frac{\\hat{\\lambda}_{\\min}}{\\hat{\\lambda} (s_{i})}\\right)\\right]\\right)}{\\sum_{i} \\mathbb{I}\\left(w_{i}&gt;r\\right)} ,\n\\text{para a função F(r) não homogênea.}\n\\]\n\\[\nG_{inhom} (r) = 1- \\mathbb{E}\\left[\\prod_{s_{i}} \\left(1-\\frac{\\lambda_{\\min}}{\\lambda (s_{i})}\\right) \\middle\\lvert z\\in S \\right],\n\\]\n\\[\n\\hat{G}_{inhom} (r) = 1-\\frac{\\sum_{i}\\left( \\mathbb{I}\\left(e_{i}&gt;r\\right)\\left[\\prod  \\mathbb{I}\\left(||s_{i}-s_{i^{'}}||\\leq r\\right)\\left(1-\\frac{\\hat{\\lambda}_{\\min}}{\\hat{\\lambda} (s_{i})}\\right)\\right]\\right)}{\\sum_{i} \\mathbb{I}\\left(e_{i}&gt;r\\right)},\n\\]\n\\[\n\\text{para a função G(r) não homogênea} (G_{inhom} (r)).\n\\]\n\\[\nJ_{inhom}(r)= \\frac{1-G_{inhom}(r)}{1-F_{inhom} (r)}, r \\geq 0 , F_{inhom} (r) &gt; 1,\n\\]\n\\[\n\\text{para a função J (r) não homogênea } (J_{inhom} (r)).\n\\]\n\\[\nK_{inhom} (r) = \\frac{1}{|B|} \\mathbb{E} \\left[\\sum_{s_{i}}^{n}\\sum_{s_{i^{'}} \\setminus \\{s_{i}\\}} \\frac{ \\mathbb{I}\\left(||s_{i}-s_{i^{'}}||\\leq r\\right)}{\\lambda (s_{i}) \\lambda (s_{i^{'}})} \\right] \\text{ e }\n\\]\n\\[\n\\hat{K}_{inhom} (r) = \\frac{1}{|B|}\\sum_{i}\\sum_{i^{'}} \\frac{ \\mathbb{I}\\left(||s_{i}-s_{i^{'}}||\\leq r\\right)}{\\hat{\\lambda} (s_{i})\\hat{\\lambda} (s_{i^{'}})e_{ii^{'}}},\n\\]\n\\[\n\\text{para a função K(r) não homogênea } (K_{inhom} (r)).\n\\] \\[\nL_{inhom}(r)=\\sqrt{\\frac{K_{inhom}(r)}{\\pi}},\\,\\hat{L}_{inhom}(r)=\\sqrt{\\frac{\\hat{K}_{inhom}(r)}{\\pi}}, \\text{para a função L(r) não homogênea }  (L_{inhom} (r)) e,\n\\]\n\\[\ng_{inhom}(r) = \\frac{K_{inhom}^{'}(r)}{2\\pi r} \\text{ e } \\hat{g}_{inhom}(r) = \\frac{\\hat{K}_{inhom}^{'}(r)}{2\\pi r}, \\text{ para a função g(r) não homogênea} (g_{inhom } (r)).\n\\]\nÉ comum encontrar na literatura, como em A. J. Baddeley, Møller, e Waagepetersen (2000) e Adrian Baddeley, Rubak, e Turner (2015), a correção de borda \\(e_{ii'}\\) no numerador, pois apenas eventos dentro de uma certa distância da borda são considerados. Por outro lado, em algumas literaturas, como as de Peter J. Diggle (2010), Leininger (2014) e A González e Moraga (2023), a correção de borda é colocada no denominador, pois o ajuste é feito normalizando a estatística de teste, ou seja, \\(e_{ii'}\\) é igual a 1 quando o círculo centrado em \\(s_{i'}\\) ou \\(z_{k}\\), passando pelo evento \\(s_{i}\\), está completamente contido na área de estudo. Se parte do círculo estiver fora da área de estudo, então \\(e_{ii'}\\) é calculada como a proporção do círculo contida dentro da área de estudo. Assim, a escolha da correção de borda dependerá da pesquisa e da preferência do pesquisador.\nApesar das funções homogêneas e não homogêneas apresentarem expressões matemáticas diferentes, a sua interpretação é igual (Quadro Tabela 5.1).\n\n\n\nTabela 5.1: Resumo da interpretação gráfica das funções que capturam o padrão de distribuição espacial.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\downarrow\\) Padrão\n\\(F(r)\\)\n\\(G(r)\\)\n\\(J(r)\\)\n\\(K(r)\\)\n\\(L(r)\\)\n\\(g(r)\\)\n\n\n\n\nAEC\n\\(F(r)\\equiv F_{pois}(r)\\)\n\\(G(r) \\equiv G_{pois}(r)\\)\n\\(J(r) \\equiv 1\\)\n\\(K(r) \\equiv K_{pois}(r)\\)\n\\(L(r) \\equiv L_{pois}(r)\\)\n\\(g(r) \\equiv 1\\)\n\n\nAgrupado\n\\(F(r)&lt;F_{pois}(r)\\)\n\\(G(r)&gt;G_{pois}(r)\\)\n\\(J(r)&lt;1\\)\n\\(K(r)&gt;K_{pois}(r)\\)\n\\(L(r)&gt;L_{pois}(r)\\)\n\\(g(r)&gt;1\\)\n\n\nRegular\n\\(F(r)&gt;F_{pois}(r)\\)\n\\(G(r)&lt;G_{pois}(r)\\)\n\\(J(r)&gt;1\\)\n\\(K(r)&lt;K_{pois}(r)\\)\n\\(L(r)&lt;L_{pois}(r)\\)\n\\(g(r)&lt;1\\)\n\n\nPadrão \\(\\uparrow\\)\n\\(F_{inhom}(r)\\)\n\\(G_{inhom}(r)\\)\n\\(J_{inhom}(r)\\)\n\\(K_{inhom}(r)\\)\n\\(L_{inhom}(r)\\)\n\\(g_{inhom}(r)\\)\n\n\n\n\n\n\nEm um processo pontual, a presença de tendência pode levar as funções apresentadas na seção Seção 5.7 a identificar dependência espacial entre eventos (padrão regular ou agrupado), mesmo que essa dependência não seja devido à afinidade entre os eventos ou à influência de um evento \\(s_{i}\\) sobre a presença de outro evento \\(s_{j}\\). A dependência espacial identificada pode resultar da competição por recursos (como nutrientes no solo), e na ausência desses recursos, os eventos não apresentariam dependência espacial. Uma forma eficiente de investigar o padrão de distribuição espacial é a inclusão de envelopes de simulação nas funções \\(F(r)\\), \\(G(r)\\), \\(J(r)\\), \\(K(r)\\), \\(L(r)\\) e \\(g(r)\\) ou suas extensões não homogêneas. Os envelopes representam um intervalo mínimo e máximo dentro do qual é aceitável assumir a existência ou não de dependência espacial. Se as funções \\(F(r)\\), \\(G(r)\\), \\(J(r)\\), \\(K(r)\\), \\(L(r)\\) e \\(g(r)\\) ou suas extensões não homogêneas estiverem totalmente dentro do envelope, mas acima ou abaixo das funções \\(F_{pois}(r)\\), \\(G_{pois}(r)\\), \\(J_{pois}(r)\\), \\(K_{pois}(r)\\), \\(L_{pois}(r)\\) e \\(g_{pois}(r)\\), isso indicará a presença de tendência ( Figura 5.13 (a)). No entanto, se pelo menos uma parte dessas funções estiver fora do envelope ( Figura 5.13 (b)), mantêm-se todas as regras de interpretação apresentadas no Quadro Tabela 5.1.\n\n\nCódigo\nset.seed(123)\njanela &lt;- owin(c(0, 1), c(0, 1))\n\n\npp_trend &lt;- rpoispp(lambda = function(x,y) { 100 * x }, win = janela)\n\n# (b) Dependência Espacial (Padrão Agrupado)\npp_cluster &lt;- rThomas(kappa = 10, scale = 0.05, mu = 5, win = janela)\n\n#Cálculo dos Envelopes (Monte Carlo)\n\ncalc_envelope &lt;- function(pp) {\n  env &lt;- envelope(pp, Lest, nsim = 39, rank = 1, \n                  correction = \"iso\", global = FALSE, \n                  verbose = FALSE, savefuns = FALSE)\n  as.data.frame(env)\n}\n\ndf_env_trend &lt;- calc_envelope(pp_trend)\ndf_env_cluster &lt;- calc_envelope(pp_cluster)\n\nplot_env_custom &lt;- function(df, titulo, subtitulo) {\n  df &lt;- df[df$r &lt;= 0.25, ]\n  \n  ggplot(df, aes(x = r)) +\n    geom_ribbon(aes(ymin = lo, ymax = hi), fill = \"grey70\", alpha = 0.5) +\n    geom_line(aes(y = theo, linetype = \"Teórico (CSR)\"), \n              color = \"red\", size = 0.8) +\n    geom_line(aes(y = obs, linetype = \"Observado\"), \n              color = \"black\", size = 1) +\n    \n    scale_linetype_manual(name = \"\", \n                          values = c(\"Observado\" = \"solid\", \"Teórico (CSR)\" = \"dashed\")) +\n    \n    labs(title = titulo, \n         subtitle = subtitulo,\n         x = \"Distância (r)\", y = \"L(r)\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\",\n          plot.title = element_text(size = 11, hjust = 0.5))\n}\n\n\np1 &lt;- plot_env_custom(df_env_trend, \n                      \"(a) Sem Dependência (Tendência)\", \n                      \"Padrão Poisson Não-Homogêneo\\n(Mantém-se no envelope ou desvio suave)\")\n\np2 &lt;- plot_env_custom(df_env_cluster, \n                      \"(b) Com Dependência (Agrupado)\", \n                      \"Padrão Cluster (Thomas)\\n(Rompe o envelope significativamente)\")\n\np1 + p2\n\n\n\n\n\n\n\n\nFigura 5.13: Diagnóstico com Envelopes de Simulação (L-function): (a) Tendência (Poisson Não-Homogêneo) - curva contida ou limítrofe; (b) Dependência (Cluster) - curva rompe o envelope.\n\n\n\n\n\nComo pode ser observado na Figura 5.13 (a), a função \\(\\hat{J}(r)\\) está abaixo de \\(J_{pois}(r)\\) e, se não houvesse envelope (área cinza), dir-se-ia que existe dependência espacial e o padrão seria agrupado, conforme descrito na seção Seção 5.6 e no Quadro Tabela 5.1. No entanto, apesar de \\(\\hat{J}(r)\\) estar abaixo de \\(J_{pois}(r)\\), como está totalmente dentro do envelope, isso significa que os eventos não apresentam dependência espacial (não possuem afinidade natural), mas sim tendência, ou seja, o padrão observado sem considerar o envelope é resultado da tendência, provavelmente porque os eventos (por exemplo, espécies florestais) estão competindo por um recurso que está restrito a uma sub-região.\nNa Figura 5.13 (b), se não houvesse envelope (área cinza), haveria dependência espacial e o padrão seria agrupado. Com a incorporação do envelope, a dependência espacial permanece e o padrão continua agrupado, indicando que os eventos possuem uma afinidade natural que não é influenciada por fatores externos (como recursos).",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#sec-2.7",
    "href": "point_process.html#sec-2.7",
    "title": "5  Processos Pontuais",
    "section": "5.9 Modelos de processos pontuais",
    "text": "5.9 Modelos de processos pontuais\nIdentificar o padrão de distribuição (estrutura de dependência) ou tendência espacial dos eventos em um processo pontual é importante, mas por si só não fornece uma análise completa dos possíveis fatores (variáveis) que influenciam essa estrutura observada. É necessário modelar essa distribuição dos eventos, tomando (ou não) como covariáveis os possíveis fatores que podem ajudar a explicar cada padrão observado. Assim, em processos pontuais, existem três classes de modelos mais utilizadas, que são: modelos Poisson, modelos Cox e modelos Gibbs.\n\n5.9.1 Modelos Poisson\nOs modelos Poisson são utilizados quando não há dependência espacial entre os eventos, ou seja, o padrão não é regular nem agrupado, como ocorre na presença de tendência ou padrão aleatório. Os modelos Poisson podem ser divididos em duas categorias principais, que são: modelo Poisson homogêneo e modelo Poisson não homogêneo (Isham 2010; Scalon 2024).\nModelo Poisson homogêneo\nO modelo Poisson homogêneo é utilizado quando não existem locais com mais eventos em relação aos outros na região de estudo (intensidade constante), ou seja, é um modelo utilizado quando o padrão é aleatório, sinônimo da não existência da tendência.\nNo modelo Poisson homogêneo, os eventos são independentes e identicamente distribuídos, com função intensidade denotada por \\(\\lambda (s) = \\beta\\) e função densidade de probabilidade por\n\\[\nf(s)= \\beta^{n(s)}\\exp\\{\\left(1-\\beta\\right)|B|\\},\n\\tag{5.19}\\]\nonde,\\(\\beta\\) é uma constante que representa a intensidade (\\(\\lambda\\)), \\(n(s)\\) )é o número de eventos existentes, B é a região de estudo e \\(|B|\\) é a respectiva área (Adrian Baddeley, Rubak, e Turner 2015; Scalon 2024).\nModelo Poisson não homogêneo\nO modelo Poisson não homogêneo é uma extensão do modelo Poisson homogêneo e é utilizado quando existe tendência, ou seja, quando existem locais com uma maior concentração de eventos em relação a outros na região de estudo (Adrian Baddeley, Rubak, e Turner 2015; J. B. Illian 2019; Scalon 2024). Neste modelo, a intensidade não é constante, mas é uma função determinística (fixa) e pode ser modelada utilizando diversas covariáveis que podem explicar o padrão observado, ou seja,\n\\[\n\\lambda(s) = \\exp \\left[\\alpha + \\theta^{\\top} Z(s)\\right] = \\exp \\left[\\alpha + \\theta_{1} Z_{1}(s) + \\theta_{2} Z_{2}(s) + \\ldots + \\theta_{n} Z_{n}(s)\\right],\n\\tag{5.20}\\]\nonde \\(\\alpha\\) é intercepto, \\(Z(s)\\) é um vetor de covariáveis e \\(\\theta^{\\top}\\) são parâmetros (coeficientes) \\(\\theta_{1}, \\theta_{2} , \\ldots  , \\theta_{n}\\), associados às covariáveis. Exemplos de covariáveis incluem características do solo, altitude, precipitação, temperatura, diâmetro à altura do peito de árvores, entre outras.\nNa ausência de covariáveis, é possível utilizar as coordenadas geográficas (latitude e longitude) ou transformá-las em coordenadas cartesianas (x, y), conforme descrito por Adrian Baddeley, Rubak, e Turner (2015) e Scalon (2024). Conforme descrito por Scalon (2024), o uso das coordenadas cartesianas faz com que a função intensidade assuma várias formas, que são, Linear: \\(\\lambda_{\\theta} (x, y) = \\exp \\left(\\theta_{0} + \\theta_{1} x + \\theta_{2} y \\right)\\), onde, \\(\\lambda_{\\theta} (x, y)\\) corresponde a função intensidade, variando em função das coordenadas \\((x, y)\\) (possíveis covariáveis não conhecidas) e \\(\\theta_{i=\\{0,1,2\\}}\\) parâmetros a serem estimados.\nQuadrática: \\(\\lambda_{\\theta} (x, y) = \\exp\\left(\\theta_{0} + \\theta_{1} x + \\theta_{2} y + \\theta_{3} xy + \\theta_{4} x^{2} + \\theta_{5} y^{2}\\right)\\).\nCúbica: \\(\\lambda_{\\theta} (x, y) = \\exp\\left(\\theta_{0} + \\theta_{1} x + \\theta_{2} y + \\theta_{3} xy + \\theta_{4} x^{2} + \\theta_{5} y^{2} + \\theta_{6} x^{2}y +\\theta_{7} xy^{2} + \\theta_{8} x^{3} + \\theta_{9} y^{3}\\right )\\), ou n-ésima forma de maneira geral dada por\n\\[\n\\lambda_{\\theta} (x, y) = \\exp\\left(\\sum_{i=0}^{n} \\sum_{i^{'}=0}^{n} \\theta_{ii^{'}} x^{i}y^{i^{'}} \\right ).\n\\tag{5.21}\\]\nEsta classe de modelo Poisson apresenta a seguinte função densidade de probabilidade,\n\\[\nf(s)=\\beta(s_{1})\\ldots \\beta(s_{n})\\exp{\\left[\\int_{B} \\left(1-\\beta(s)\\right)dz\\right]} = \\prod_{i=1}^{n}\\beta(s_{i})\\exp{\\left[\\int_{B} \\left(1-\\beta(s)\\right)dz\\right]},\n\\tag{5.22}\\]\nonde \\(\\beta=\\lambda\\), outrora designada intensidade dos eventos \\(s_{i}\\) e \\(B\\) é a região de estudo.\n\n\n5.9.2 Modelos Cox\nConforme descrito na seção Seção 5.9.1, a disposição dos eventos na área de estudo pode ser influenciada por diversos fatores (covariáveis), como os fatores ambientais. A escassez desses fatores em certas partes da região de estudo pode resultar em uma menor propensão à ocorrência dos eventos, criando uma tendência na distribuição espacial. Como essa tendência é atribuída a fatores ambientais, é possível, por meio do modelo Poisson não homogêneo, modelar a distribuição dos eventos por área (intensidade) levando em consideração essas variáveis ambientais. O modelo resultante é capaz de capturar (explicar) a variabilidade na distribuição dos eventos observados.\nNo entanto, é comum que as covariáveis disponíveis não consigam explicar completamente o padrão observado, porque, além da influência das covariáveis, os eventos apresentam uma afinidade natural (dependência espacial). Essa afinidade natural faz com que, ao ajustar um modelo Poisson não homogêneo, este, não seja capaz de explicar completamente variabilidade na distribuição dos eventos observados.\nConsiderando essa limitação, é possível, em um processo pontual, modelar a função de intensidade como uma função aleatória \\(\\Lambda(s)\\), que além de considerar a influência das covariáveis, pressupõe a existência de dependência espacial entre os eventos. Esse tipo de modelagem resulta em um processo pontual duplamente estocástico (aleatoriedade na intensidade e nos eventos), conhecido como modelo Cox.\nOs modelos Cox são uma extensão dos modelos Poisson, nos quais a intensidade originalmente denotada por \\(\\lambda(s)\\) é expandida para incorporar um campo aleatório \\(\\Psi(s)\\), conforme apresentado na Figura 5.14. Este campo aleatório torna a função de intensidade aleatória e denotada por \\(\\Lambda(s)\\), onde \\(\\{\\Lambda = \\Lambda(s) : s \\in  \\mathbb{R}^2\\}\\).\nEstes modelos são frequentemente utilizados para modelar padrões pontuais que apresentam dependência espacial do tipo padrão agrupado (Cressie 1993; Scalon 2024).\n\n\n\n\n\n\nFigura 5.14: Ilustração do campo aleatório (superfície)\\(\\Psi (s)\\), que controla a abundância e distribuição dos eventos em um modelo Cox. Fonte: Jalilian, Safari, e Sohrabi (2020)\n\n\n\nNa Figura 5.14, o plano representa a região de estudo onde os eventos estão distribuídos. Os pontos no plano indicam a localização dos eventos. Abaixo do plano, há um campo aleatório que influencia o padrão de distribuição dos eventos no plano. As variações neste campo aleatório refletem áreas com maiores ou menores quantidades da variável aleatória. %É importante observar que, nos modelos Cox, a dependência espacial (agrupamento) pressupõe-se que ocorra exclusivamente entre os descendentes do mesmo progenitor A. J. Baddeley, Van Lieshout, e Møller (1996). Dentre as subclasses dos modelos Cox, incluem-se o modelo Log-Cox Gaussiano e os modelos de Neyman-Scott, que podem ser Matérn, Thomas, Cauchy e Variância gama.\nModelo Log Cox Gaussiano\nO papel do campo aleatório apresentado na Figura 5.14 é de explicar a configuração espacial dos eventos, que não é explicada pelas covariáveis (J. B. Illian 2019). Seja, \\(\\Psi (s)\\) um campo aleatório Gaussiano de média \\(\\mu\\) e função covariância \\(\\gamma(r)\\), ou seja, uma função cujo valor em qualquer ponto é uma variável aleatória, que explica a variabilidade espacial não explicada pelas covariáveis. Em outras palavras, um campo aleatório \\(\\Psi (s)\\) é gaussiano se, para qualquer coleção finita de eventos \\(s_{1}, s_{2}, \\ldots , s_{n}\\), qualquer combinação linear \\(\\beta_{1} \\Psi_{s_{1}} +\\beta_{2} \\Psi_{s_{2}} + \\ldots +\\beta_{n} \\Psi_{s_{n}}\\), com \\(\\beta_{i} \\in  \\mathbb{R}\\), tem em uma distribuição normal unidimensional (J. Illian et al. 2008).\nAssim, sob essa condição, pode-se definir uma função intensidade aleatória \\(\\Lambda(s)\\), que além das covariáveis, incorpora um campo aleatório Gaussiano, como,\n\\[\n\\Lambda(s)= Z(s)\\beta^{T} + \\Psi (s) .\n\\tag{5.23}\\]\nNo entanto, a expressão Eq. 5.23 não pode ser diretamente utilizada como função de intensidade em um modelo de Cox, pois o campo aleatório \\(\\Psi(s)\\) sendo Gaussiano pode assumir valores negativos, o que não é admissível em processos pontuais (a intensidade deve ser não-negativa) (Møller, Syversveen, e Waagepetersen 1998; Møller e Waagepetersen 2007). Para resolver essa questão, uma transformação apropriada é aplicar o logaritmo neperiano (ln), resultando no modelo conhecido como Log-Cox Gaussiano, dado por,\n\\[\nln \\left[\\Lambda(s)\\right] = Z(s)\\beta^{T} + \\Psi (s) \\Leftrightarrow \\Lambda(s)= \\exp \\{Z(s)\\beta^{T} + \\Psi (s)\\}.\n\\tag{5.24}\\]\nEm síntese, um modelo Cox será Log Cox Gaussiano se o campo aleatório \\(\\Psi(s)\\) for Gaussiano (possuir distribuição normal).\n\n5.9.2.1 Modelos de Neyman-Scott: Modelo Matérn, Thomas, Variância-Gama e Cauchy\nDesenvolvidos por Neyman e Scott (1958), os modelos Neyman-Scott constituem uma classe de modelos Cox utilizados para descrever padrões de agrupamento quando se presume que os eventos observados são descendentes de um ou mais eventos progenitores (Moller e Waagepetersen 2003; Adrian Baddeley, Rubak, e Turner 2015; Scalon 2024).\nOs modelos de Neyman-Scott são compostos por duas etapas distintas. Na primeira etapa, os eventos \\(s_{i}\\) (progenitores) são gerados, como por exemplo árvores. Na segunda etapa, cada evento \\(s_{i}\\) (progenitor) gera eventos \\(s_{i^{'}}\\) descendentes, como por exemplo mudas. O padrão de eventos que consiste exclusivamente nos eventos descendentes (mudas), independentemente de seus progenitores, forma realização do processo de agrupamento Neyman-Scott. Dependendo da distribuição espacial dos eventos descendentes em relação aos seus progenitores, os modelos podem ser do tipo Thomas, Matérn, variância-gama ou Cauchy.\nModelo Matérn\nUm modelo Matérn é caracterizado pela distribuição de probabilidade \\(h(s_{i^{'}})\\) dos descendentes em relação aos seus progenitores, onde os descendentes, em média por progenitor, estão distribuídos uniformemente em um círculo de raio r centrado no progenitor, ou seja, \\(h(s_{i^{'}}) = \\frac{ \\mathbb{I}[||s_{i}-s_{i^{'}}|| \\leq r]}{\\omega^{2} r^{2}}\\), onde \\(\\omega\\) é um parâmetro (escala) de agrupamento dos descendentes em relação aos progenitores \\(s_{i}\\) (Scalon 2024).\nPara J. Illian et al. (2008), no modelo Matérn, a função intensidade \\(\\Lambda (s_{i^{'}})\\) pode ser denotada por\n\\(\\Lambda (s_{i^{'}}) = \\lambda (s_{i^{'}}) \\sum_{s_{i^{'}}\\in S}  \\mathbb{I}_{b(s_{i^{'}}, \\:r)} (s)\\), onde \\(\\lambda (s_{i^{'}})\\) é a intensidade do \\(i\\)-ésimo aglomerado formado pelos descendentes,\\(\\sum_{\\in s_{i^{'}}}  \\mathbb{I}_{b(s_{i^{'}}, \\:r)}(s)\\) é a soma sobre todos os eventos \\(s_{i^{'}}\\) no processo Poisson homogêneo \\(s\\), \\(\\mathbb{I}_{b(s_{i^{'}}, \\:r)} (s)\\) é uma função indicadora que é igual a 1 se o evento \\(s_{i^{'}}\\) estiver no círculo de raio \\(r\\) centrado no progenitor, e 0 caso contrário.\nAdrian Baddeley, Rubak, e Turner (2015) para a mesma função intensidade propuseram a seguinte expressão, \\(\\Lambda (s_{i^{'}}) = \\sum_{i} h(s_{i^{'}}), \\, h(s_{i^{'}}) = \\left\\{\\begin{array}{rcl} \\frac{s_{i^{'}}}{\\pi r^{2}} & \\text{ se } & ||s_{i^{'}}|| \\leq r \\\\ 0 & \\text{ se } & ||s_{i^{'}}|| &gt; r \\end{array} \\right.\\), onde \\(||\\cdot||\\) representa a norma ou distância entre os eventos \\(s_{{i}^{'}}\\) e \\(r\\) distância de referência.\nModelo Thomas\nNeste modelo, os eventos descendentes \\(s_{{i}^{'}}\\) estão distribuídos ao redor dos eventos principais (progenitores) seguindo uma distribuição normal simétrica, ou seja,\n\\[\nh(s_{{i}^{'}}) = \\frac{1}{\\sqrt{2\\pi \\sigma^{2}}} e^{\\frac{-||s_{{i}^{'}} ||^{2}}{2\\sigma^{2}}}, \\,s_{{i}^{'}} \\sim N (0, \\sigma^{2}),\n\\tag{5.25}\\]\nonde \\(\\sigma\\) é um parâmetro mede o padrão de agrupamento dos descendentes em relação aos progenitores e \\(||\\cdot||\\) representa a norma ou distância entre os eventos \\(s_{{i}^{'}}\\) (Adrian Baddeley, Rubak, e Turner 2015; Scalon 2024).\nModelo Cauchy\nNeste modelo, a densidade de probabilidade \\(h(s_{i^{'}})\\) dos descendentes é uma distribuição bivariada de Cauchy, dada por \\(h(s_{i^{'}})= \\frac{1}{2\\pi \\omega^{2}} \\left[1 + \\frac{||s_{i^{'}}||^{2}}{\\omega^{2}}\\right]^{\\frac{-3}{2}}\\), onde \\(\\omega\\) é um parâmetro que mede o agrupamento (Jalilian, Guan, e Waagepetersen 2013; Adrian Baddeley, Rubak, e Turner 2015; J. B. Illian 2019).\nModelo Variância Gama\nSegundo Jalilian, Guan, e Waagepetersen (2013) e Adrian Baddeley, Rubak, e Turner (2015), no modelo variância-gama, a densidade de probabilidade dos descendentes é variância gama, dada por\n\\[\nh(s_{{i}^{'}}) = \\frac{1}{2^{\\nu+1} \\pi \\eta^{2} \\Gamma(\\nu+1)} \\frac{||s_{{i}^{'}}||^{\\nu}}{\\eta^{\\nu}} Y_{\\nu} \\left(\\frac{||s_{{i}^{'}}||}{\\nu}\\right),\n\\tag{5.26}\\]\nonde \\(\\eta\\) é o parâmetro de escala e \\(\\nu\\) é um parâmetro adicional que controla a forma da densidade e deve satisfazer \\(\\nu&gt;\\frac{1}{2}\\).\\(\\Gamma\\) é função gama e \\(Y_{\\nu}\\) é função Bessel modificada de segunda espécie e ordem \\(\\nu\\) que pode ser encontrada no livro de Bell (2004).\nPara o modelo variância gama, J. Illian et al. (2008), propõe uma possível representação da função intensidade \\(\\Lambda (s_{{i}^{'}})\\), dada por \\(\\Lambda (s_{{i}^{'}}) = \\sum_{s_{{i}^{'}}\\in S} \\omega_{i} k_{z} (z-s_{{i}^{'}})\\), onde \\(s\\) representa o processo Poisson que gera uma coleção de eventos \\(s_{{i}^{'}}\\) no espaço \\(d\\), \\(\\omega_{i}\\) representa uma sequência de variáveis aleatórias independentes e identicamente distribuídas (iid), com distribuição gama. \\(k_{z}\\) representa a função de densidade de probabilidade (Kernel) que modela a influência ou interação dos eventos.\n\n\n\n5.9.3 Modelos Gibbs (Markov)\nOs modelos apresentados nas seções Seção 5.9.1 e Seção 5.9.2 são úteis para eventos que não apresentam dependência espacial e para eventos que apresentam dependência espacial do tipo padrão agrupado, respectivamente. Para eventos que apresentam dependência espacial do tipo regular, é necessária uma classe diferente de modelos denominada Gibbs.\nOs modelos Gibbs podem ser subdivididos em modelos de interação par-a-par e modelos de interação por área.\nModelos de interação par a par\nOs modelos de interação par-a-par são representados pela densidade de probabilidade dada por\n\\[\nf(s)= \\alpha \\left[\\prod_{i=1}^{n(s)} b(s_{i})\\right]\\left[\\prod_{i&lt;i^{'}} c(s_{i}, s_{i^{'}})\\right] \\quad \\text{ou} \\quad g(s) \\propto \\prod_{s_{i} \\in s} \\phi \\left(s_{i}\\right) \\prod_{\\{s_{i}, \\,\\eta\\} \\subseteq s} \\phi \\left(\\{s_{i}, \\eta\\}\\right),\n\\tag{5.27}\\]\nonde \\(\\propto\\) indica proporcionalidade e \\(\\alpha\\) constante de normalização (J. Illian et al. 2008; Peter J. Diggle 2013; Adrian Baddeley, Rubak, e Turner 2015; Scalon et al. 2022; Scalon 2024). As funções \\(b(s_{i})\\) e \\(\\phi (s_{i})\\) desempenham o mesmo papel, representando estatísticas de primeira ordem (\\(\\lambda\\)) enquanto \\(c(s_{i}, s_{i^{'}})\\) e \\(\\phi\\left({s_{i}, \\eta}\\right)\\), que também poderiam ser expressos como \\(\\phi\\left(||s_{i} -s_{i^{'}}||\\right)\\), representam funções de interação par-a-par (Moller e Waagepetersen 2003; Adrian Baddeley, Rubak, e Turner 2015; Scalon 2024).\nConforme J. Illian et al. (2008), a função de interação \\(c(s_{i}, s_{i^{'}})\\), também conhecida como potencial par, um termo originário da física, mede a “energia potencial” gerada pela interação entre pares de eventos \\((s_{i}, s_{i^{'}})\\) em função de sua distância \\(||s_{i} - s_{i^{'}}||\\).\nSegundo Scalon (2024), os modelos Poisson descritos nas seções Seção 5.9.1 e Seção 5.9.2, apesar de não apresentarem dependência espacial (interação), fazem parte dos modelos Gibbs, pois, se \\(b(s_{i})\\equiv \\lambda\\) e \\(c(s_{i}, s_{i^{'}})=1\\) na equação Eq. 5.27, obtém-se a função densidade de probabilidade do modelo Poisson homogêneo, onde \\(b(s_{i})=\\beta\\) e \\(\\alpha=\\exp\\{\\left(1-\\beta\\right)|B|\\}\\). De forma análoga, se \\(b(s_{i})\\equiv \\lambda (s_{i})\\) e \\(c(s_{i}, s_{i^{'}})=1\\) na equação Eq. 5.27, obtém-se a função densidade de probabilidade do modelo Poisson não homogêneo.\nNa prática, os modelos da classe Gibbs são frequentemente apresentados com base na intensidade condicional \\(\\lambda(s_{i}|s_{i^{'}})\\), que pode ser interpretada como o número esperado de eventos \\(s_{i}\\) por unidade de área (infinitesimalmente pequena) em uma localização, dada a existência do evento \\(s_{i^{'}}\\).\nNo entanto, Scalon (2024) mostra que as funções densidade de probabilidade \\(f(s)\\) e intensidade condicional \\(\\lambda(s_{i}|s_{i^{'}})\\) são relacionadas,\n\\[\n\\lambda(s_{i}|s_{i^{'}})=\\frac{f\\left(s \\cup s_{i^{'}}\\right)}{f(s_{i^{'}})}=b(s_{i})\\left[\\prod_{i\\neq i^{'}}^{n} c(s_{i}, s_{i^{'}})\\right]\n\\tag{5.28}\\]\ne esta intensidade apresenta relação com a intensidade dos modelos Poisson, se \\(\\prod_{i}^{n}c(s_{i}, s_{i^{'}})=1\\) e \\(b(s_{i})=\\beta\\), obtém-se a intensidade do modelo Poisson homogêneo.\nEmbora modelos Gibbs sejam adequados para padrões regulares, podem identificar padrões de atração (agrupamento) com baixa eficácia e sem aplicação prática significativa (Adrian Baddeley, Rubak, e Turner 2015).\nEm processos pontuais, ao utilizar modelos Gibbs de interação par-a-par, diferentes modelos Gibbs podem ser obtidos dependendo da forma que a função de interação \\(c(s_i, s_{i'})\\) assumir e alguns desses modelos incluem modelos Hard Core (núcleo duro), modelo Strauss, Strauss Hard Core, etc.\nModelo Hard core\nNeste tipo de modelo da classe Gibbs considera-se que não existem eventos que estejam mais próximos do que a distância mínima \\(h\\) e o modelo só é útil na situação em que os eventos são centros de partículas não elásticas esféricas ou circulares do mesmo tamanho, e \\(h\\) é o diâmetro dessas partículas, que deve ser o mesmo para todos os eventos (J. Illian et al. 2008; Adrian Baddeley, Rubak, e Turner 2015; Scalon 2024). Este modelo, é obtido considerando \\(b(s) \\equiv \\beta\\) e\n\\[c(s_{i}, s_{i^{'}})= \\left\\{\\begin{array}{rcl} 1 & \\text{se} & ||s_{i}-s_{i^{'}}||&gt; h \\\\ 0 & \\text{se} & ||s_{i}-s_{i^{'}}|| \\leq h \\end{array}\\right.\\], na equação Eq. 5.28., o que resulta na função densidade de probabilidade\n\\[f(x)= \\left\\{\\begin{array}{rcl} \\alpha \\beta^{n(x)} & \\text{se} & ||s_{i}-s_{i^{'}}||&gt; h \\quad \\forall i\\neq i^{'}\\\\ 0 \\quad &\\text{se} & \\text{caso contrário} \\end{array}\\right.\\] e intensidade condicional\n\\[\\lambda (s_{i}|s_{i^{'}}\\setminus s_{i}) = \\left \\{\\begin{array}{rcl} \\beta & \\text{se} & ||s_{i}- s_{i^{'}}||&gt; h \\quad \\forall i\\neq i^{'}\\\\ 0 &\\text{se} & \\text{caso contrário} \\end{array}\\right.\\]\nModelo de nascimento e morte\nÉ um modelo mais adequado para a sucessão florestal que envolve a morte de árvores existentes em momentos aleatórios e a germinação de novas mudas em lugares e tempos aleatórios (Cressie 1993; Adrian Baddeley, Rubak, e Turner 2015).\nSupondo que em cada intervalo de tempo \\(\\Delta t\\), cada árvore existente tenha probabilidade \\(m \\Delta t\\) de morrer, independentemente das outras árvores, onde \\(m\\) é taxa de mortalidade por árvore e por unidade de tempo. Durante o mesmo intervalo, em uma região \\(\\Delta a\\), nova árvore germina com probabilidade \\(g \\Delta a \\Delta t\\), se estiver a mais de \\(d\\) unidades de distância da árvore mais próxima e considerando \\(g\\) a taxa de germinação por unidade de tempo. Independentemente do estado inicial da floresta, ao longo de tempo, este processo de nascimento e morte espacial alcançará um equilíbrio no qual qualquer evento da floresta é uma realização do processo de Hard Core com parâmetro \\(\\beta = \\frac{g}{m}\\) e diâmetro de Hard Core \\(h\\).\nEste modelo, é obtido considerando \\(b(s) \\equiv \\beta=\\frac{g}{m}\\) e a função densidade de probabilidade \\(f(x)= \\left\\{\\begin{array}{rcl} \\alpha (\\frac{g}{m})^{n(s)} & \\text{se} & ||s_{i}-s_{i^{'}}||&gt; h\\\\ 0 \\, &\\text{se} & \\text{caso contrário} \\end{array}\\right.\\) e \\(\\lambda (s_{i}|s_{i^{'}}) = \\left \\{\\begin{array}{rcl} (\\frac{g}{m}) & \\text{se} & ||s_{i}-s_{i^{'}}||&gt; h\\\\ 0 &\\text{se} & \\text{caso contrário} \\end{array}\\right.\\).\nModelo Strauss\nSegundo Adrian Baddeley, Rubak, e Turner (2015) e Scalon (2024), o modelo Hard Core é apropriado quando é fisicamente impossível que dois eventos estejam a uma distância menor que \\(d\\). Se não é impossível que dois ou mais eventos estejam próximos, mas é improvável, um modelo apropriado é o modelo Strauss, obtido considerando \\(c(s_{i}, s_{i^{'}})= \\left\\{\\begin{array}{rcl} 1 & \\text{se} & ||s_{i}-s_{i^{'}}||&gt; d \\\\ \\gamma & \\text{se} & ||s_{i}-s_{i^{'}}|| \\leq d \\end{array}\\right.\\) e \\(f(s)= \\alpha \\beta^{n(s)} \\gamma^{n(s, d)} \\, \\text{e} \\, \\lambda (s_{i}|s_{i^{'}})=\\beta \\gamma^{n(s_{i}, d,s_{i^{'}})}\\), onde \\(n(s,d)\\) é o número de pares não ordenados de eventos em \\(s\\) que estão mais próximos do que a distância de interação \\(d\\); \\(n(s_{i}, d,s_{i^{'}})=n(s,d)-n(s_{i^{'}},d)\\) é o número de eventos vizinhos de \\(s_{i}\\) excluindo de \\(s_{i^{'}}\\) e \\(\\gamma\\) um parâmetro de interação.\nSe \\(0&lt;\\gamma&lt;1\\) o modelo caracteriza inibição; se \\(\\gamma =0\\) e não existirem eventos próximos do que a distância \\(d\\), o modelo passa a ser de Hard Core; se \\(\\gamma=1\\), o modelo é Poisson homogêneo e o modelo não está definido se \\(\\gamma&gt;1\\) pois, a função densidade \\(f(s)\\) não é integrável.\nModelo Strauss Hard Core\nNeste modelo, é impossível que dois ou mais eventos estejam a uma distância menor que a distância de Hard Core (\\(h\\)), mas eles podem apresentar interação a uma distância \\(d &gt; h\\), onde \\(d\\) é a distância de interação de Strauss. A função de interação é expressa por:\n\\(c(s_{i}, s_{i^{'}})= \\left\\{\\begin{array}{rcl} 0 & \\text{se} & ||s_{i}-s_{i^{'}}||\\leq h \\\\ \\gamma & \\text{se} & h&lt;||s_{i}-s_{i^{'}}|| \\leq d \\\\ 1 & \\text{se} & ||s_{i}-s_{i^{'}}||&gt;d \\end{array}\\right.\\).\nModelo de interação soft-core\nSegundo Okabe e Tanemura (2006) observamos frequentemente padrões regulares de eventos no mundo natural e nesse padrão, ocorre um espaçamento entre os \\(i\\)-ésimos eventos, que pode ser devido à competição entre os eventos por territórios, nutrientes e assim por diante. Para representar o alcance e a suavidade das interações, os chamados potenciais Soft-Core são os mais convenientes. Neste modelo, a interação entre os eventos é suave e diminui gradualmente à medida que a distância \\(d\\) entre os eventos aumenta. Em contraste com o modelo de Hard core, onde a interação abruptamente se torna zero após uma certa distância, e no modelo de Strauss, onde a interação é constante em uma certa distância e zero fora dela. A função interação é dado por: \\(c(s_{i}, s_{i^{'}})= \\left(\\frac{\\sigma}{||s_{i}-s_{i^{'}}||}\\right)^{\\frac{2}{\\kappa}}, \\;\\sigma&gt;0 \\; e\\;0&lt;\\kappa&lt;1\\).\nModelo de interação Diggle-Gates-Stibbard\nO modelo proposto por Peter J. Diggle, Gates, e Stibbard (1987) é um modelo de processo pontual com interação par a par, útil para modelar padrões onde a interação entre os pontos é influenciada pela distância entre eles. Este modelo é especialmente adequado quando há evidências de que a intensidade da interação entre os pontos diminui à medida que a distância entre eles aumenta. A função interação é dado por: \\(c(s_{i}, s_{i^{'}})= \\left\\{\\begin{array}{rcl} \\sin\\left(\\frac{\\pi||s_{i}-s_{i^{'}}||}{2d}\\right)^{2} & \\text{se} & ||s_{i}-s_{i^{'}}||\\leq d \\\\ 1 \\quad \\quad \\quad \\quad & \\text{se} & ||s_{i}-s_{i^{'}}||&gt;d \\end{array}\\right.\\).\nModelo de interação Diggle-Gratton\nProposto por Peter J. Diggle e Gratton (1984), o modelo Diggle-Gratton é um tipo de modelo estatístico utilizado para descrever a distribuição espacial de eventos pontuais. A função de potencial de par nesse modelo é definida em termos da distância entre pares de pontos, e quantifica como a presença de um evento afeta a probabilidade de encontrar outro ponto em sua vizinhança. Os parâmetros do modelo incluem a distância mínima de interação \\(h\\) (também conhecida como “hard core”), a distância máxima de interação \\(d\\), e um parâmetro \\(\\kappa\\) que controla a força da interação. A função interação é dado por:\n\\(c(s_{i}, s_{i^{'}})= \\left\\{\\begin{array}{rcl} 0 \\quad \\quad \\quad & \\text{se} & ||s_{i}-s_{i^{'}}||\\leq h \\\\ \\left(\\frac{||s_{i}-s_{i^{'}}||- h}{d-h}\\right)^{\\kappa} & \\text{se} & h&lt;||s_{i}-s_{i^{'}}|| \\leq d \\\\ 1 \\quad \\quad \\quad & \\text{se} & ||s_{i}-s_{i^{'}}||&gt;d \\end{array}\\right.\\).\n\n\n5.9.4 Modelos de interação por área\nAté agora, os modelos Gibbs (Markov) que foram discutidos são conhecidos como modelos de interação par-a-par. Eles descrevem interações intraespecíficas (eventos do mesmo tipo) no caso univariado e interespecíficos espécies (eventos diferentes) no caso multivariado. No entanto, além desses modelos, existem aqueles que consideram a interação por área, os quais são mais adequados quando se considera que os eventos podem competir por recursos, como nutrientes ou alimentos, em uma determinada região.\nConforme descrito em Peter J. Diggle (2013), Adrian Baddeley, Rubak, e Turner (2015) e Nightingale et al. (2019), os modelos de interação por área, também conhecidos como modelos de esfera penetrável de equilíbrio líquido-vapor de Widom-Rowlinson, foram propostos por A. J. Baddeley e Van Lieshout (1995), visando identificar eventos que possam exibir tanto inibição quanto agregação.\nSegundo A. J. Baddeley e Van Lieshout (1995), a distinção fundamental entre os modelos de interação par-a-par e de área reside na especificação da função de interação para cada tipo de modelo.\nNos modelos de interação par-a-par, a função de interação é definida como uma função da distância euclidiana \\(\\left(||\\cdot||\\right)\\) entre cada par de eventos no padrão em estudo. Em contrapartida, a função de interação em um modelo de interação de área é descrita como a área da união de círculos associados a cada evento em um processo pontual.\nA função densidade de probabilidade para este modelo é expressa, por\n\\[\nf(x)=\\alpha\\beta^{n(s)}\\gamma^{-A(s, r)}, \\; \\text{onde} \\; A(s, r)=\\left| B \\cap \\bigcup_{i=1}^{n(s)} b(s_{i},r) \\right|,\n\\]\nonde \\(\\alpha\\) é uma constante,\\(\\beta &gt;0\\) é um parâmetro de intensidade,\\(\\gamma&gt;0\\) é um parâmetro de interação, e \\(A(s, r)\\) denota a área da região obtida ao desenhar um círculo de raio \\(r\\) centrado em cada evento \\(s_{i}\\) e unir esses círculos.\nÉ importante observar que a função de densidade para o modelo de interação por área é integrável para todos os valores de \\(\\gamma\\), ao passo que, para modelos de interação par-a-par, como o modelo Strauss, para \\(\\gamma&gt;1\\) a função não é integrável. Porém, para ambos modelos, quando \\(\\gamma =1\\), temos um modelo Poisson. Se \\(\\gamma&lt;1\\), os eventos apresentam inibição ou regularidade, enquanto para \\(\\gamma&gt;1\\), os eventos exibem aglomeração ou atração nos modelos de interação por área.\nAlém das subclasses dos modelos Gibbs mencionadas anteriormente, existem outros que surgem da combinação desses modelos em uma única classe, denominada modelos híbridos. Além disso, existem modelos conhecidos como modelos de interação tripla, nos quais três eventos interagem simultaneamente. Outra possibilidade é o modelo de saturação de Geyer, uma variação do modelo Strauss, porém com uma restrição \\(\\gamma&lt;1\\). No entanto, esses modelos não estão no escopo destas notas de aula, podendo consultar Adrian Baddeley, Rubak, e Turner (2015) para mais detalhes.",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#diagnóstico-e-validação-de-modelos",
    "href": "point_process.html#diagnóstico-e-validação-de-modelos",
    "title": "5  Processos Pontuais",
    "section": "5.10 Diagnóstico e Validação de Modelos",
    "text": "5.10 Diagnóstico e Validação de Modelos\n\n5.10.1 Diagnóstico\nO diagnóstico dos modelos pode ser feito utilizando diversos tipos de testes estatísticos, a saber:\n\nTeste da razão de verossimilhança - este teste, é útil na presença de covariáveis e avalia se há ou não influência das covariáveis no padrão de distribuição dos eventos em estudo. Suas hipóteses nulas e alternativas são, respectivamente, \\(H_{0}: \\varphi = 0\\) e \\(H_{a}: \\varphi \\neq 0\\), onde \\(\\varphi\\) é o vetor de covariáveis. A estatística do teste é dada por\n\n\\[\\Gamma = 2 \\log \\frac{L_{1}}{L_{0}} = 2 \\left( \\log L_{1} - \\log L_{0} \\right), \\tag{5.29}\\]\nonde \\(L_{0}\\) e \\(L_{1}\\) são os valores máximos da verossimilhança sob as hipóteses nula e alternativa, respectivamente. Para decidir sobre a aceitação ou rejeição da hipótese nula de não influência das covariáveis no padrão observado, utiliza-se um teste \\(\\chi^{2}\\) com \\(n\\) graus de liberdade, onde \\(n\\) é a dimensão do vetor das covariáveis (\\(\\varphi\\)).\n\nTeste Escore - também designado teste de multiplicação de Lagrange, requer e apenas um modelo considerado de referência, podendo ser Poisson homogêneo \\(k(r)=\\pi r^{2}\\), representando aleatoriedade espacial completa, com parâmetros \\(\\theta\\) associados a cada coordenada x, y. O teste de hipóteses para os parâmetros é: \\(H_{0}: \\theta = \\theta_{0}\\) e \\(H_{a}: \\theta \\neq \\theta_{0}\\), cuja estatística de teste é:\n\n\\[S=U(\\theta_{0}) I_{\\theta}^{-1}U(\\theta_{0}), \\: \\text{ se } \\theta = \\psi \\text{ ou } S=U( \\theta)_{\\varphi}^{\\top} \\left( I_{\\theta}^{-1}\\right)_{\\varphi \\varphi}U( \\theta_{0})_{\\varphi}, \\text{ se } \\theta=(\\phi, \\varphi)\\],\nonde \\(U(\\theta_{0})\\) é vetor escore e no caso uniparamétrico é designada função escore, \\(I_{\\theta}\\) é informação de Fisher1 e os subscritos \\(\\varphi\\) e \\(\\varphi \\varphi\\), são componentes do vetor score e matriz inversa de informação de Fisher respectivamente.\n\nTeste \\(\\chi^{2}\\) - o teste \\(\\chi^{2}\\) pode ser utilizado tanto para testar a hipótese nula de aleatoriedade espacial completa quanto para avaliar a qualidade de ajuste de um modelo. No contexto de ajuste de modelos, o teste avalia a hipótese nula de que os eventos seguem um modelo espacial específico ajustado em relação à hipótese de que não o seguem. Aqui, ao utilizar o teste qui-quadrado \\(\\chi^{2}\\),\\(m-p\\) serão os graus de liberdade, onde \\(p\\) é número de parâmetros do modelo ajustado e \\(m\\) número de quadrats e a estatística de teste é,\n\n\\[\n\\chi^{2}=\\sum_{j} \\frac{\\left(n_{j}-\\hat{\\mu}_{j}\\right)^{2}}{\\hat{\\mu}_{j}}, \\, \\hat{\\mu}_{j} = \\int_{B_{j}} \\hat{\\lambda} (s) ds ,\n\\tag{5.30}\\]\nonde \\(n_{j}\\) e \\(\\hat{\\mu}_{j}\\) representam o número observado e esperado de eventos no local \\(j\\) da área de estudo \\(B_{j}\\).\n\nSimulações de Monte Carlo - realizadas de forma análoga ao teste contra a hipótese de completa aleatoriedade espacial, entretanto os eventos em estudo são simulados múltiplas vezes usando o modelo ajustado ou um modelo de referência. Se for difícil distinguir visualmente a diferença entre os eventos observados e simulados, o modelo é considerado ajustado ao padrão dos eventos observados (Scalon 2024).\n\nPara evitar equívocos entre tendência e dependência espacial, é importante incorporar envelopes de simulação. Segundo Adrian Baddeley, Rubak, e Turner (2015), para a incorporação dos envelopes de simulação, primeiro calcula-se a estimativa \\(s\\) para os dados observados \\(\\hat{S}_{obs}\\), onde \\(s\\) representa qualquer modelo tomado como referência. Em seguida, usando a função \\(s\\) como referência, gera-se \\(m\\) padrões de eventos por simulação de Monte Carlo, e obtêm-se as suas estimativas \\(\\{\\hat{S}_{1} (r), \\hat{S}_{2} (r), \\ldots, \\hat{S}_{m} (r)\\}\\), na distância \\(r\\) tomada como referência. Considerando \\(L_{inf}\\) e \\(L_{sup}\\) como os limites inferior e superior das estatísticas associadas aos padrões simulados nas \\(r\\)-ésimas distâncias, rejeita-se a hipótese nula de que os eventos observados seguem o padrão do modelo tomado como referência se as estimativas de \\(\\hat{S}_{obs}\\) para as mesmas distâncias estiverem totalmente ou parcialmente acima do limite superior, ou abaixo do limite inferior, ou seja,\n\\[\n\\begin{aligned}\nL_{\\text{inf}}(r) &= {\\underset{j}{\\max}}\\, S_{j}(r) = \\max\\{S_{1}(r), \\ldots, S_{m}(r)\\} \\\\\nL_{\\text{sup}}(r) &= {\\underset{j}{\\min}}\\, S_{j}(r) =\\min\\{S_{1}(r), \\ldots, S_{m}(r)\\} .\n\\end{aligned}\n\\]\n\n\n5.10.2 Validação\nMuitos dos métodos de validação de modelos usados na estatística clássica podem ser utilizados na validação de modelos para configurações pontuais, incluindo a análise de resíduos e a avaliação da independência por meio do gráfico dos quantis dos resíduos (Q-Q plot). Estes métodos são aplicáveis a todos os modelos apresentados, exceto aos modelos de Cox, que não permitem a análise dos resíduos.\nPara modelos Poisson, os resíduos são dados pela seguinte expressão:\n\\[\nR(B) = n(S\\cap B) - \\int_{B}\\hat{\\lambda}(s)ds,\n\\]\nonde \\(n(S\\cap B)\\) representa o número de eventos existentes em toda área de estudo e \\(\\int_{B}\\hat{\\lambda}(s)ds\\) representa o número esperado de eventos na região \\(B\\), sendo \\(\\hat{\\lambda}(s)\\) a respectiva intensidade estimada pelo modelo ajustado ou função intensidade (modelo ajustado). Assim, se \\(R(B)\\) for negativo, o modelo superestima a intensidade. Se \\(R(B)\\) for positivo, o modelo subestima e se for zero, ou aproximadamente, o modelo se ajusta ao padrão dos eventos em estudo. Os resíduos podem ser estimados considerando subáreas \\(i\\) (quadrats \\(i\\)) da região de estudo, resultando nos conhecidos resíduos de Pearson, dados por:\n\\[\nR_{i}^{P} (B)=\\frac{n(S\\cap B)-\\int_{B}\\hat{\\lambda}(s)ds}{\\sqrt{\\int_{B}\\hat{\\lambda}(s)ds}},\n\\]\nconforme descrito em Adrian Baddeley et al. (2005), Adrian Baddeley (2007), Adrian Baddeley et al. (2008), Adrian Baddeley (2010), Adrian Baddeley, Chang, et al. (2013), AJ Baddeley et al. (2019) e Adrian Baddeley, Rubak, e Turner (2015), podendo ser suavizados ou cumulativos.\nPara modelos de Cox, como descrito por Adrian Baddeley (2010), uma vez que não é possível testar os resíduos, uma alternativa é recorrer ao método dos momentos e a funções que capturam a estrutura de dependência (F, G, J, K, L, g), juntamente com envelopes de simulação.\nSegundo Adrian Baddeley et al. (2022), os métodos existentes para ajustar modelos de processos de agrupamento (modelos Cox) a dados de processos pontuais frequentemente enfrentam dificuldades em convergir, convergem para valores implausíveis dos parâmetros ou apresentam instabilidade numérica. Como resultado, vários métodos foram desenvolvidos para lidar com esses problemas, mas as dificuldades persistem. Alguns autores, como Waagepetersen e Guan (2009) e Adrian Baddeley, Rubak, e Turner (2015), sugerem que o problema está relacionado à fraca estrutura de agrupamento dos dados analisados. No entanto, este problema pode ocorrer tanto em situações de forte agrupamento quanto de agrupamento fraco (Adrian Baddeley et al. 2022).\nA validação dos modelos Gibbs segue a mesma lógica dos Poisson, porém substituindo a intensidade pela intensidade condicional e mantendo todas regras, ou seja,\n\\[\nR(B) = n(S\\cap B) - \\int_{B}\\hat{\\lambda}(s_{i}|s_{j})ds.\n\\]\nFora os resíduos, envelopes de simulação e funções que ajudam a identificar a estrutura de dependência espacial, pode-se utilizar o Critério de Informação de Akaike, dado por:\n\\[\nAIC=-2 log L_{max}+2p ,\n\\]\nonde \\(L_{max}=L(\\hat{\\theta})\\) é função de máxima verossimilhança para o modelo em questão, e \\(p\\) é o número de parâmetros para este modelo. Para AIC, o melhor modelo é aquele que tiver valor mais baixo de AIC.",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#pacote-spatstat",
    "href": "point_process.html#pacote-spatstat",
    "title": "5  Processos Pontuais",
    "section": "5.11 Pacote spatstat",
    "text": "5.11 Pacote spatstat\nO pacote spatstat (Adrian Baddeley e Turner 2005; Adrian Baddeley, Turner, et al. 2013; Adrian Baddeley, Rubak, e Turner 2015) é o maior se não o principal pacote utilizado para análise estatística de padrões pontuais espaciais. De autoria do professor Adrian Baddeley, seu desenvolvimento é acompanhado pelo livro de referência Adrian Baddeley, Rubak, e Turner (2015), que detalha os fundamentos teóricos da área com aplicações práticas em R.\nAtualmente, o spatstat é estruturado como um ecossistema de subpacotes especializados:\n\nspatstat.data: Contém os conjuntos de dados.\nspatstat.geom: Define as estruturas fundamentais de geometria espacial, como as janelas de observação (owin), padrões de pontos (ppp) e padrões de segmentos de linha (psp). É a base sobre a qual todos os outros subpacotes operam.\nspatstat.explore: Focado na Análise Exploratória de Dados Espaciais (ESDA). Inclui funções para estimativa de densidade de kernel (intensidade de primeira ordem) e funções sumárias de segunda ordem, como as funções \\(F, G, J, K, L\\) e a função de correlação par (\\(g\\)).\nspatstat.model: Contém funções para ajuste de modelos. Permite a modelagem da intensidade via Processos de Poisson Não-Homogêneos e a modelagem de interações via Processos de Gibbs (como os modelos de Strauss e Hard Core). Inclui também diagnósticos de resíduos e validação de modelos.\nspatstat.random: Para simulação estocástica de processos pontuais. É essencial para a geração de envelopes de simulação (Testes de Monte Carlo) para validar se um padrão observado se desvia da aleatoriedade espacial completa (AEC).\nspatstat.linnet: Dedicado à análise de padrões de pontos que ocorrem sobre redes lineares, como acidentes em rodovias ou crimes em redes de ruas urbanas.\nspatstat.univar: Fornece ferramentas auxiliares para distribuições de probabilidade univariadas e técnicas de estimativa de densidade.\nspatstat.sparse: Lida com computação matricial esparsa, otimizando o desempenho interno do pacote em cálculos complexos.\nspatstat.utils: Conjunto de funções utilitárias internas para manipulação de dados, strings e diagnósticos de sistema.\n\nInstalação do pacote e importação\n\n\nCódigo\npacman::p_load(spatstat, spatstat.model,spatstat.data )",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#criação-manipulação-e-geometria-spatstat.geom",
    "href": "point_process.html#criação-manipulação-e-geometria-spatstat.geom",
    "title": "5  Processos Pontuais",
    "section": "5.12 Criação, Manipulação e Geometria (spatstat.geom)",
    "text": "5.12 Criação, Manipulação e Geometria (spatstat.geom)\nEsta seção cobre a construção dos objetos fundamentais. Sem a definição correta da geometria, a estatística é impossível.\nPadrões de Pontos (ppp) e Janelas (`owin```)\nO objeto ppp requer coordenadas e uma janela owin.\n\nCriação do objeto ppp:\n\n\nppp(x, y, window, marks): Construtor principal.\nas.ppp(X): Converte data.frames/matrizes.\nclickppp(n): (Interativo) Permite clicar no gráfico para adicionar pontos.\n\n\nCriação Janelas (owin):\n\n\nowin(xrange, yrange, mask): Cria retângulos (polígonos) ou máscaras binárias.\nsquare(s), disc(r, c), ellipse(a, b): para a geração rápida de primitivas geométricas elementares (quadrados, discos e elipses).\nripras(x, y): Estimador de Ripley-Rasson. Realiza a inferência estatística da fronteira do domínio a partir da distribuição dos eventos, assumindo que a região original é convexa.\nconvexhull(x, y): Calcula o fecho convexo matemático, determinando o menor polígono convexo possível que engloba todos os eventos observados.\n\n\nOperações de Conjunto (Janelas)\n\n\nintersect.owin(A, B), union.owin(A, B), setminus.owin(A, B): úteis para, respetivamente, realizar a interseção (extrair a área comum), a união (combinar as áreas totais) e a diferença setorial (excluir de uma janela a área sobreposta por outra) de domínios espaciais.\ninside.owin(x, y, w): Teste de pertinência de ponto.\nerosion(w, r), dilation(w, r): Encolhe ou expande a janela por raio \\(r\\).\nopening(w, r), closing(w, r): Suaviza contornos (abertura/fechamento).\nmarks(X) e marks(X) &lt;- value: Funções para a manipulação de marcas (covariáveis ou metadados). Permitem, respetivamente, extrair e atribuir informações adicionais a cada ponto do padrão espacial (ex: altura da árvore, tipo de espécie, diâmetro), transformando um processo puramente geométrico em um Processo Pontual Marcado.\nunmark(X): Remove marcas.\nsubset(X, subset): Filtra pontos por lógica booleana.\nrotate(X, angle), shift(X, vec): Rotação e Translação.\nflipxy(X), reflect(X): Espelhamento.\naffine(X, mat): Realiza uma transformação afim arbitrária sobre o objeto espacial X. Ao contrário das funções simples de rotação ou translação, esta função utiliza uma matriz linear mat para aplicar distorções geométricas complexas, como cisalhamento (shearing), escala anisotrópica (esticamento desigual entre eixos) e reflexão, preservando a colinearidade e as proporções relativas das distâncias ao longo de linhas paralelas.\n\n\n\nCódigo\n# Construção de Janelas (Geometria Construtiva)\n\nW_ret &lt;- owin(xrange = c(0, 10), yrange = c(0, 10))\n\nW_circ &lt;- disc(radius = 3, centre = c(5, 5))\n\nW_complexa &lt;- setminus.owin(W_ret, W_circ) # Retângulo menos o furo circular\n\n#Janelas inferidas dos dados\n\nx_raw &lt;- runif(20, 0, 10); y_raw &lt;- runif(20, 0, 10)\n\nW_ripley &lt;- ripras(x_raw, y_raw)     # Estimador estatístico da janela\n\nW_hull &lt;- convexhull.xy(x = x_raw, y = y_raw) # Fecho convexo estrito\n\n#Criação do objeto PPP\n\nmeu_ppp &lt;- ppp(x = x_raw, y = y_raw, window = W_ret)\n\n#Manipulação de Marcas\n\nmarks(meu_ppp) &lt;- sample(c(\"A\", \"B\"), 20, replace = TRUE) # Categórica\n\nmarks(meu_ppp) &lt;- runif(20) # Numérica (sobrescreve)\n\nsem_marcas &lt;- unmark(meu_ppp)\n\n#Transformações\n\nppp_rot &lt;- rotate(meu_ppp, angle = pi/4)\n\nppp_shift &lt;- shift(meu_ppp, vec = c(2, 0))\n\nppp_flip &lt;- flipxy(meu_ppp)\n\n#Morfologia\n\nW_erodida &lt;- erosion(W_complexa, r = 0.5)\n\n\n\npar(mfrow=c(2,2), mar=c(1,1,1,1))\nplot(meu_ppp, main=\"PPP Original\")\nplot(W_ripley, main=\"Estimador Ripley-Rasson\")\nplot(ppp_rot, main=\"Rotacionado\")\nplot(W_erodida, main=\"Erosão da Janela\")",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#imagens-segmentos-tesselações-e-3d",
    "href": "point_process.html#imagens-segmentos-tesselações-e-3d",
    "title": "5  Processos Pontuais",
    "section": "5.13 Imagens, Segmentos, Tesselações e 3D",
    "text": "5.13 Imagens, Segmentos, Tesselações e 3D\n\nImagens (im): Dados raster/grid.\nim(mat), as.im(X): Criação/Conversão.\nblur(X, sigma): Suavização Gaussiana de imagem.\ncontour.im(X): Extrai linhas de contorno (vetorial).\nSegmentos (psp): Linhas no espaço (falhas geológicas, agulhas).\npsp(x0, y0, x1, y1, window): Cria segmentos.\nlengths_psp(X), angles.psp(X): Geometria dos segmentos.\ncrossing.psp(A, B): Detecta onde segmentos se cruzam.\nTesselações (tess): Divisão do espaço em mosaicos.\ntess(images), quadrats(X): Cria tesselações.\ndirichlet(X): Polígonos de Voronoi.\ndelaunay(X): Triangulação de Delaunay.\npp3(x, y, z, box3()): Padrão de pontos em 3D.\n\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatstat)\n\n\npar(mfrow = c(2, 2), mar = c(1, 1, 1, 1))\n\n# Imagens (Pixel)\nW_ret &lt;- owin(c(0, 1), c(0, 1))\nZ &lt;- as.im(function(x,y) { x^2 + y^2 }, W_ret)\n\nplot(Z, main = \"Imagem (Pixel)\")\n\n# Segmentos de Linha (psp) e Cruzamentos\nset.seed(42)\n# Criar segmentos aleatórios\nseg &lt;- psp(x0=runif(10), y0=runif(10), x1=runif(10), y1=runif(10), window=owin())\n\n#auto-intersecção\ncruzamentos &lt;- selfcrossing.psp(seg)\n\nplot(seg, main = \"Segmentos e Cruzamentos\", col = \"blue\")\nplot(cruzamentos, add = TRUE, col = \"red\", pch = 19, cex = 1.5) \n\n#Tesselação de Voronoi\n\ndata(cells)\n\n# Calcular Voronoi (Dirichlet)\nvoro &lt;- dirichlet(cells)\n\n# Plotagem\nplot(voro, main = \"Voronoi\", border = \"blue\", col = gray.colors(ntiles(voro), alpha=0.3))\nplot(cells, add = TRUE, pch = 16, col = \"red\", cex = 0.8)\n\n# riangulação de Delaunay\ndela &lt;- delaunay(cells)\n\nplot(dela, main = \"Delaunay\", border = \"darkgreen\")\nplot(cells, add = TRUE, pch = 16, col = \"red\", cex = 0.8)\n\n\n\n\n\n\n\n\n\nCódigo\npar(mfrow = c(1, 1))",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#análise-exploratória-de-dados-spatstat.explore",
    "href": "point_process.html#análise-exploratória-de-dados-spatstat.explore",
    "title": "5  Processos Pontuais",
    "section": "5.14 Análise Exploratória de Dados (spatstat.explore)",
    "text": "5.14 Análise Exploratória de Dados (spatstat.explore)\nFocada em estatísticas descritivas, intensidade e testes de hipótese.\nIntensidade (\\(\\lambda\\)) e Covariáveis\n\nsummary(X): Visão geral.\nclarkevans(X): Índice R (agrupamento vs regularidade).\n\nEstimativa de Densidade (Kernel)\n\ndensity(X): Kernel fixo.\ndensityAdaptiveKernel(X): Largura de banda variável (melhor para intensidade heterogênea).\ndensityVoronoi(X): Estimador constante por polígono de Voronoi.\n\nSeleção de Largura de Banda (Bandwidth)\n\nbw.diggle(X): Minimização do erro quadrático médio (para Cox).\nbw.ppl(X): Verossimilhança (para Poisson).\nbw.scott(X): Regra de Scott (normal).\n\nDependência de Covariáveis\n\nrhohat(X, Z): Estima \\(\\rho(z)\\) não parametricamente.\nrho2hat(X, Z1, Z2): Intensidade conjunta de duas covariáveis.\nroc(X, Z), auc(X, Z): Curva ROC e Área sob a curva para capacidade preditiva da covariável.\n\n\n\nCódigo\npar(mfrow=c(2,2), mar=c(1,1,1,1))\ndata(bei); data(bei.extra) # Árvores e gradiente/elevação\n\n# Seleção de Banda e Densidade\nsig &lt;- bw.ppl(bei)\n\ndens_fixa &lt;- density(bei, sigma = sig)\n\ndens_adap &lt;- densityAdaptiveKernel(bei) # Mais robusto\n\n# Dependência de Covariável\nrho_elev &lt;- rhohat(bei, bei.extra$elev) # Intensidade vs Elevação\n\nplot(rho_elev, main=\"Rhohat: Efeito da Elevação\")\n\n# Capacidade Preditiva (ROC/AUC)\nroc_curve &lt;- roc(bei, bei.extra$grad)\n\narea_roc  &lt;- auc(bei, bei.extra$grad)\n\nplot(roc_curve, main=paste(\"ROC Gradiente (AUC =\", round(area_roc,2), \")\"))\n\n\n\n\n\n\n\n\n\nFunções de Distância e Interação\n\nFest(X): Espaço vazio (“Empty Space”).\nGest(X): Vizinho mais próximo (“Nearest Neighbor”).\nKest(X): Ripley’s K (cumulativa).\nLest(X): Besag’s L (transformação de K para linearizar Poisson).\npcf(X): Correlação de Pares (\\(g(r)\\)).\nJest(X): Função J (combinação de F e G).\n\nVersões Inhomogêneas (para \\(\\lambda(u)\\) variável):\n\nKinhom, Linhom, pcfinhom, Finhom, Ginhom.\n\nEnvelopes de simulação\nenvelope(X, fun, nsim): Gera bandas de confiança via Monte Carlo para rejeitar CSR.\n\n\nCódigo\n#Funções de Distância (F, G, J)\n\npar(mfrow = c(2, 4), mar = c(1, 1, 3, 1))\n\n# Função F (Espaço Vazio)\nplot(Fest(bei), main = \"Função F (Vazio)\", legend = FALSE)\n\n# Função G (Vizinho Mais Próximo)\nplot(Gest(bei), main = \"Função G (Vizinho)\", legend = FALSE)\n\n# Função J (Interação)\nplot(Jest(bei), main = \"Função J\", legend = FALSE)\n\n\n# Funções de Segunda Ordem (K, L, pcf)\n# Função K de Ripley\nplot(Kest(bei), main = \"Função K\", legend = FALSE)\n\n# Função L (Linearizada), Plotando \"L(r) - r\" para ficar horizontal\nplot(Lest(bei), . - r ~ r, main = \"Função L\", legend = FALSE)\n\n# Função g (Correlação de Pares) - pcf\n# divisor=\"d\" corrige viés em distâncias curtas\nplot(pcf(bei, divisor = \"d\"), main = \"Função g (PCF)\", legend = FALSE)\n\n\n# Análise Inhomogênea e Envelopes\n\n# Estimar a intensidade (tendência)\nlambda_bei &lt;- density(bei, sigma = bw.scott(bei))\n\n#Calcular L Inhomogêneo\nL_inhom &lt;- Linhom(bei, lambda = lambda_bei)\n\nplot(L_inhom, . - r ~ r, \n     main = \"L Inhomogêneo\", \n     legend = FALSE)\n\n#Envelopes de Simulação (Teste de Monte Carlo)\nenv_L &lt;- envelope(bei, Linhom, \n                  sigma = bw.scott(bei), \n                  nsim = 19, rank = 1, global = TRUE, \n                  simulate = expression(rpoispp(lambda_bei)))\n\n\nGenerating 38 simulations by evaluating expression (19 to estimate the mean and \n19 to calculate envelopes) ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, \n38.\n\nDone.\n\n\nCódigo\nplot(env_L, . - r ~ r, main = \"Envelopes\", legend = FALSE)\n\n\n\n\n\n\n\n\n\nCódigo\npar(mfrow = c(1, 1))",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#análise-multitype-e-gráficos-de-diagnóstico",
    "href": "point_process.html#análise-multitype-e-gráficos-de-diagnóstico",
    "title": "5  Processos Pontuais",
    "section": "5.15 Análise Multitype e Gráficos de Diagnóstico",
    "text": "5.15 Análise Multitype e Gráficos de Diagnóstico\nPara dados com marcas ou diagnósticos visuais de estrutura.\n\nKcross(X, i, j), Lcross: Interação Tipo \\(i\\) vs Tipo \\(j\\).\nKdot(X, i), Ldot: Tipo \\(i\\) vs Qualquer outro.\nmarkcorr(X): Correlação para marcas contínuas.\nmarkvario(X): Variograma de marcas.\nrelrisk(X): Probabilidade espacial relativa de tipos.\n\nDiagnóstico Visual\n\nfryplot(X): Vetores entre todos os pares (detecta anisotropia/reticulado).\nmiplot(X): Índice de Morisita.\n\nEstatísticas Locais (LISA)\n\nlocalK(X), localL(X): Contribuição individual de cada ponto.\nclusterset(X): Método Allard-Fraley para isolar clusters densos.\nnnclean(X): Remove ruído (Byers-Raftery).\nsharpen(X): Intensifica pontos para destacar estruturas.\n\n\n\nCódigo\npar(mfrow=c(1,1), mar=c(2,3,3,4))\n\ndata(amacrine) # Células On/Off# Risco Relativo (Probabilidade de ser \"On\" no espaço)\n\nrr &lt;- relrisk(amacrine)\nplot(rr, main=\"Risco Relativo\")# Correlação Cruzada\n\n\n\n\n\n\n\n\n\nCódigo\nL_cr &lt;- Lcross(amacrine, \"on\", \"off\")\nplot(L_cr, main=\"L Cross\")# Diagnósticos Visuais\n\n\n\n\n\n\n\n\n\nCódigo\nfryplot(amacrine, main=\"Fry Plot (Anisotropia)\", cex=0.1)\n\n\n\n\n\n\n\n\n\nCódigo\nlimpos &lt;- nnclean(amacrine, k=5) # Detectar outliers\n\n\nIteration 1     logLik = 8240.77670246102   p = 0.5031 \nIteration 2     logLik = 8235.95198819664   p = 0.5027 \nEstimated parameters:\np [cluster] = 0.50269 \nlambda [cluster] = 148.74 \nlambda [noise]   = 136.08 \n\n\nCódigo\npar(mfrow=c(1,1), mar=c(1,1,1,1))\nplot(limpos, main=\"Classificação Ruído vs Feature\", size=0.2)",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#ajuste-de-modelos-spatstat.model",
    "href": "point_process.html#ajuste-de-modelos-spatstat.model",
    "title": "5  Processos Pontuais",
    "section": "5.16 Ajuste de Modelos (spatstat.model)",
    "text": "5.16 Ajuste de Modelos (spatstat.model)\nAjuste de modelos paramétricos (Poisson, Gibbs, Cox, Cluster).\nProcessos de Poisson e Gibbs (ppm)\nAjuste via Máxima Pseudoverossimilhança e a função principal é:\nppm(formula, interaction, ...)\n\nEspecificação de Tendência (Fórmula):\n\n\n~1: Homogêneo.\n~x + y: Log-linear.\n~polynom(x, y, 2): Quadrática.\n~Z: Covariável imagem \\(Z\\).\n\nInterações (Família Gibbs):\n\nPoisson(): Sem interação.\nStrauss(r): Repulsão hard/soft até raio \\(r\\).\nHardcore(h): Exclusão absoluta até \\(h\\).\nGeyer(r, sat): Processo de saturação (cluster ou repulsão).\nAreaInter(r): Interação de área (mais suave que Strauss).\nPairwise(), SatPiece(), Saturated(): Interações personalizadas ou por partes.",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#seleção-e-validação",
    "href": "point_process.html#seleção-e-validação",
    "title": "5  Processos Pontuais",
    "section": "5.17 Seleção e Validação",
    "text": "5.17 Seleção e Validação\n\nAIC(modelo_ajustado): Critério de Akaike.\nanova.ppm(fit1, fit2): Teste de Razão de Verossimilhança (para aninhados).\n\nProcessos de Cluster e Cox (kppm)\nModelos onde pontos se agrupam em torno de “pais” não observados. Método de Contraste Mínimo ou Palm Likelihood.\n\nFunção Principal: kppm(formula, clusters)\nTipos de Cluster\n\n“Thomas”: Dispersão Gaussiana (Thomas Process).\n“MatClust”: Dispersão Uniforme em disco (Matérn Cluster).\n“LGCP”: Log-Gaussian Cox Process.\n“Cauchy”, “VarGamma”: Clusters com caudas longas.\n\n\nMétodos Auxiliares\n\nmincontrast(): Função de baixo nível para ajuste via K ou pcf.\nthomas.estK(), matclust.estK(): Ajustes diretos específicos.\n\nOutros Modelos (SLRM, DPPM)\n\nSLRM (slrm): Spatial Logistic Regression. Aproximação discreta (pixelada). Rápido para grandes dados.\nslrm(formula): Ajuste.\nDPPM (dppm): Determinantal Point Process. Para modelar regularidade forte (repulsão) de forma tratável.\ndppm(formula, family): Ex: família Gaussian ou Bessel.\n\n\n\nCódigo\nif(!require(pacman)) install.packages(\"pacman\")\npacman::p_load(spatstat)\n\n# Dados\ndata(cells)   # Para modelos de repulsão (Gibbs/DPPM)\ndata(redwood) # Para modelos de cluster (Cox/kppm)\n\nZ_im &lt;- as.im(function(x,y) x + y, Window(cells)) # Covariável sintética\n\n#AJUSTE DE MODELOS PPM (Poisson e Gibbs)\n\n#Especificação de Tendência\nfit_homo &lt;- ppm(cells ~ 1)                    # Homogêneo\nfit_lin  &lt;- ppm(cells ~ x + y)                # Log-linear\nfit_poly &lt;- ppm(cells ~ polynom(x, y, 2))     # Quadrática\nfit_cov  &lt;- ppm(cells ~ Z, data=list(Z=Z_im)) # Covariável Imagem\n\n# Interações (Família Gibbs)\n# Nota: O raio 'r' deve ser escolhido com base na análise exploratória (ex: pcf)\nr_int &lt;- 0.08 \n\nfit_pois    &lt;- ppm(cells ~ 1, Poisson())\nfit_strauss &lt;- ppm(cells ~ 1, Strauss(r=r_int))\nfit_hard    &lt;- ppm(cells ~ 1, Hardcore(h=0.05))\nfit_geyer   &lt;- ppm(cells ~ 1, Geyer(r=r_int, sat=2))\nfit_area    &lt;- ppm(cells ~ 1, AreaInter(r=r_int))\n# fit_pair  &lt;- ppm(cells ~ 1, Pairwise(pot=function(d) { ... })) # Personalizado\n\n#SELEÇÃO E VALIDAÇÃO (PPM)\n\n# Critério de Akaike (menor é melhor)\nAIC(fit_homo)\n\n\n[1] -227.9642\n\n\nCódigo\nAIC(fit_strauss)\n\n\n[1] -295.2359\n\n\nCódigo\n# Teste de Razão de Verossimilhança (Modelos aninhados)\nanova(fit_homo, fit_lin, test=\"LRT\")\n\n\n\n  \n\n\n\nCódigo\n# Diagnóstico de Resíduos (Gráfico)\npar(mfrow=c(1,2))\ndiagnose.ppm(fit_homo, which=\"smooth\", main=\"Resíduos Poisson\")\n\n\nModel diagnostics (raw residuals)\nDiagnostics available:\n    smoothed residual field\nrange of smoothed field =  [-28.15, 15.33]\n\n\nCódigo\ndiagnose.ppm(fit_strauss, which=\"smooth\", main=\"Resíduos Strauss\")\n\n\n\n\n\n\n\n\n\nModel diagnostics (raw residuals)\nDiagnostics available:\n    smoothed residual field\nrange of smoothed field =  [-22.6, 36]\n\n\nCódigo\n#PROCESSOS DE CLUSTER E COX (KPPM)\n\n# Ajuste via Min Contrast ou Palm Likelihood\nfit_thomas   &lt;- kppm(redwood ~ 1, clusters=\"Thomas\")\nfit_matclust &lt;- kppm(redwood ~ 1, clusters=\"MatClust\")\nfit_lgcp     &lt;- kppm(redwood ~ 1, clusters=\"LGCP\")\nfit_cauchy   &lt;- kppm(redwood ~ 1, clusters=\"Cauchy\")\nfit_vargam   &lt;- kppm(redwood ~ 1, clusters=\"VarGamma\")\n\n# Métodos Auxiliares (Baixo nível / Ajuste direto no K)\nK_obs &lt;- Kest(redwood)\nfit_mincon_t &lt;- thomas.estK(K_obs)\nfit_mincon_m &lt;- matclust.estK(K_obs)\n\n# Diagnóstico KPPM (Envelopes de Simulação)\npar(mfrow=c(1,1))\nplot(envelope(fit_thomas, Lest, nsim=19, global=TRUE), \n     main=\"Diagnóstico Thomas (Envelopes)\")\n\n\nGenerating 38 simulated realisations of fitted cluster model (19 to estimate \nthe mean and 19 to calculate envelopes) ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, \n38.\n\nDone.\n\n\n\n\n\n\n\n\n\nCódigo\n# OUTROS MODELOS (SLRM e DPPM)\n\n# SLRM - Spatial Logistic Regression (Aproximação pixelada)\nfit_slrm &lt;- slrm(redwood ~ x + y)\nplot(fit_slrm, main=\"Predição SLRM\")\n\n\n\n\n\n\n\n\n\nCódigo\n# DPPM - Determinantal Point Process (Repulsão forte)\nfit_dppm &lt;- dppm(cells ~ 1, dppGauss())\n\n# Visualização do modelo DPPM ajustado\nplot(fit_dppm, main=\"DPPM Gaussiano\")",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#simulação-spatstat.random",
    "href": "point_process.html#simulação-spatstat.random",
    "title": "5  Processos Pontuais",
    "section": "5.18 Simulação (spatstat.random)",
    "text": "5.18 Simulação (spatstat.random)\nGerar realizações estocásticas dos processos.\n\nPadrões Aleatórios Básicos:\n\n\nrunifpoint(n): Uniforme (CSR) fixo \\(n\\).\nrpoispp(lambda): Poisson (CSR) com intensidade \\(\\lambda\\) (número aleatório de pontos).\nrpoint(n, f): \\(n\\) pontos com densidade de probabilidade \\(f\\).\n\nProcessos de Gibbs (MCMC)\n\nrmh(model): Metropolis-Hastings. Simula de um modelo ppm ou especificação manual.\nrStrauss(beta, gamma, r), rHardcore(): Wrappers diretos para simulação.\nrThomas(), rMatClust(), rVarGamma(), rCauchy():Simulação direta dos processos Neyman-Scott.",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#reamostragem-e-perturbação",
    "href": "point_process.html#reamostragem-e-perturbação",
    "title": "5  Processos Pontuais",
    "section": "5.19 Reamostragem e Perturbação",
    "text": "5.19 Reamostragem e Perturbação\n\nrshift(X): Deslocamento toroidal (preserva estrutura interna, quebra relação com geofísica).\nrjitter(X): Adiciona ruído às coordenadas.\nsimulate(fit): Simula de qualquer objeto ajustado (ppm, kppm, slrm).\n\n\n\nCódigo\nif(!require(pacman)) install.packages(\"pacman\")\npacman::p_load(spatstat)\n\n# Janela\nW &lt;- owin(c(0, 10), c(0, 10))\n\n#PADRÕES BÁSICOS (Leves)\n\n# Uniforme (CSR)\nX_unif &lt;- runifpoint(n = 50, win = W)\n\n# Poisson (CSR)\nX_pois &lt;- rpoispp(lambda = 2, win = W)\n\n# Não homogêneo\nf_dens &lt;- function(x,y) { x + y }\n\nX_inhom &lt;- rpoint(n = 50, f = f_dens, win = W, nsim=1) \n\npar(mfrow=c(1,3), mar=c(1,1,1,1))\nplot(X_unif, main=\"Uniforme (n=50)\")\nplot(X_pois, main=\"Poisson (lambda=2)\")\nplot(X_inhom, main=\"Não homogêneo\")\n\n\n\n\n\n\n\n\n\nCódigo\n# PROCESSOS DE GIBBS\nctrl_leve &lt;- rmhcontrol(expand=1, nrep=1e4) \n\n# Strauss (Repulsão suave)\nmod_strauss &lt;- list(cif=\"strauss\", par=list(beta=1, gamma=0.5, r=0.7), w=W)\nX_strauss &lt;- rmh(model=mod_strauss, start=list(n.start=40), control=ctrl_leve)\n\n\nChecking arguments..determining simulation windows...Starting simulation.\nInitial state...Ready to simulate. Generating proposal points...Running Metropolis-Hastings.\n\n\nCódigo\n# Hardcore (Inibição estrita)\nmod_hard &lt;- list(cif=\"hardcore\", par=list(beta=2, hc=0.3), w=W)\nX_hard &lt;- rmh(model=mod_hard, start=list(n.start=30), control=ctrl_leve)\n\n\nChecking arguments..determining simulation windows...Starting simulation.\nInitial state...Ready to simulate. Generating proposal points...Running Metropolis-Hastings.\n\n\nCódigo\n# Simulação\nmod_manual &lt;- rmhmodel(cif=\"strauss\", par=list(beta=1, gamma=0.2, r=0.7), w=W)\nX_rmh &lt;- rmh(model = mod_manual, start = list(n.start=40), control = ctrl_leve)\n\n\nChecking arguments..determining simulation windows...Starting simulation.\nInitial state...Ready to simulate. Generating proposal points...Running Metropolis-Hastings.\n\n\nCódigo\npar(mfrow=c(1,3), mar=c(1,1,1,1))\nplot(X_strauss, main=\"Strauss\")\nplot(X_hard, main=\"Hardcore\")\nplot(X_rmh, main=\"rmh()\")\n\n\n\n\n\n\n\n\n\nCódigo\n#PROCESSOS DE CLUSTER (Neyman-Scott)\n\n# Thomas (Dispersão Gaussiana)\nX_thomas &lt;- rThomas(kappa = 0.5, scale = 0.2, mu = 10, win = W)\n\n# Matérn Cluster (Dispersão Uniforme)\nX_mat &lt;- rMatClust(kappa = 0.5, scale = 0.5, mu = 10, win = W)\n\n# Variância-Gama\nX_vargam &lt;- rVarGamma(kappa = 0.5, scale = 0.2, mu = 10, win = W)\n\npar(mfrow=c(1,3), mar=c(1,1,1,1))\nplot(X_thomas, main=\"Thomas\")\nplot(X_mat, main=\"Matérn Cluster\")\nplot(X_vargam, main=\"Var-Gamma\")\n\n\n\n\n\n\n\n\n\nCódigo\n#REAMOSTRAGEM E PERTURBAÇÃO\n\ndata(cells)\n\n# Deslocamento\nX_shift &lt;- rshift(cells, radius = 0.1)\n\n# Jittering\nX_jitter &lt;- rjitter(cells, radius = 0.05)\n\npar(mfrow=c(1,3), mar=c(1,1,1,1))\nplot(cells, main=\"Original\", pch=16)\nplot(X_shift, main=\"Shift\", pch=16, col=\"blue\")\nplot(X_jitter, main=\"Jitter\", pch=16, col=\"red\")\n\n\n\n\n\n\n\n\n\nCódigo\n# SIMULAÇÃO DE MODELOS AJUSTADOS\nfit_strauss &lt;- ppm(cells ~ 1, Strauss(r=0.07))\n\nsim_fit &lt;- simulate(fit_strauss, nsim = 2, control=ctrl_leve, retry=0)\n\n\nGenerating 2 simulated patterns ...1, \n2.\n\n\nCódigo\npar(mfrow=c(1,2), mar=c(1,1,1,1))\nplot(sim_fit[[1]], main=\"Simulação 1\")\nplot(sim_fit[[2]], main=\"Simulação 2\")",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#testes-e-diagnósticos",
    "href": "point_process.html#testes-e-diagnósticos",
    "title": "5  Processos Pontuais",
    "section": "5.20 Testes e Diagnósticos",
    "text": "5.20 Testes e Diagnósticos\nValidação estatística rigorosa dos modelos.\n\n5.20.1 Testes de Bondade de Ajuste (Goodness-of-Fit)\n*quadrat.test(fit): \\(\\chi^2\\) para modelos ajustados.\n\ncdf.test(fit, covariate): Kolmogorov-Smirnov espacial. Compara distribuição da covariável nos pontos vs. no mapa.\ndclf.test(X, model): Diggle-Cressie-Loosmore-Ford. Teste de Monte Carlo comparando envelopes globais (L ou K).\nmad.test(X, model): Desvio Absoluto Médio. Similar ao DCLF, mas usa norma \\(L_1\\).\ndg.test(X): Teste de Dao-Genton.\n\n\n\n5.20.2 Diagnósticos de Resíduos (PPM)\n\nresiduals.ppm(fit): Resíduos brutos, Pearson, e “Inverse-Lambda”.\ndiagnose.ppm(fit): Plota a tendência cumulativa vs resíduos. Mostra onde o modelo falha.\n\n\n\n5.20.3 Diagnósticos de Influência e Sensibilidade\n\nleverage.ppm(fit): Onde a adição de um ponto altera mais o modelo?\ninfluence.ppm(fit): Influência global.\ndfbetas.ppm(fit): Mudança nos coeficientes dos parâmetros.\nparres(fit, covariate): Partial Residuals. Sugere transformação funcional para covariável.\naddvar(fit, covariate): Added Variable Plot. A covariável adiciona informação nova?\n\n\n\nCódigo\nif(!require(pacman)) install.packages(\"pacman\")\npacman::p_load(spatstat)\npar(mfrow = c(1, 1), mar=c(1,1,1,1))\n# Dados: Árvores (bei) e Covariáveis (bei.extra)\ndata(bei)\ngrad &lt;- bei.extra$grad # Gradiente do terreno\nelev &lt;- bei.extra$elev # Elevação\n\n# Ajuste de um Modelo de Poisson ñ nhomogêneo (dependente do gradiente)\nfit &lt;- ppm(bei ~ grad)\n\n#TESTES DE BOM AJUSTE (Goodness-of-Fit)\n\n\n# Teste de Quadrats (Chi-quadrado espacial)\n# H0: O modelo ajustado é adequado\nqt &lt;- quadrat.test(fit, nx = 4, ny = 2)\nplot(qt, main = \"Quadrat Test (Resíduos Pearson)\")\n\n\n\n\n\n\n\n\n\nCódigo\n# Teste de Kolmogorov-Smirnov Espacial (CDF)\n# Verifica se a distribuição da covariável nos pontos difere do mapa geral\ncdf_t &lt;- cdf.test(fit, covariate=grad)\nplot(cdf_t, main = \"Teste CDF (Covariável: Grad)\")\n\n\n\n\n\n\n\n\n\nCódigo\n# Testes de Monte Carlo (Comparação de Envelopes Globais)\n# H0: O padrão observado é gerado pelo modelo ajustado\n\ndclf_t &lt;- dclf.test(fit, Lest, nsim = 19)\n\n\nGenerating 19 simulated realisations of fitted Poisson model  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n19.\n\nDone.\n\n\nCódigo\nprint(dclf_t)\n\n\n\n    Diggle-Cressie-Loosmore-Ford test of fitted Poisson model\n    Monte Carlo test based on 19 simulations\n    Summary function: L(r)\n    Reference function: sample mean\n    Alternative: two.sided\n    Interval of distance values: [0, 125] metres\n    Test statistic: Integral of squared absolute deviation\n    Deviation = leave-one-out\n\ndata:  fit\nu = 48139, rank = 1, p-value = 0.05\n\n\nCódigo\n# MAD (Desvio Absoluto Médio)\nmad_t &lt;- mad.test(fit, Lest, nsim = 19)\n\n\nGenerating 19 simulated realisations of fitted Poisson model  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n19.\n\nDone.\n\n\nCódigo\nprint(mad_t)\n\n\n\n    Maximum absolute deviation test of fitted Poisson model\n    Monte Carlo test based on 19 simulations\n    Summary function: L(r)\n    Reference function: sample mean\n    Alternative: two.sided\n    Interval of distance values: [0, 125] metres\n    Test statistic: Maximum absolute deviation\n    Deviation = leave-one-out\n\ndata:  fit\nmad = 23.396, rank = 1, p-value = 0.05\n\n\nCódigo\n# Teste de Dao-Genton\ndg_t &lt;- dg.test(fit, Lest, nsim = 19)\n\n\nApplying first-stage test to original data... Done.\nRunning second-stage tests on 19 simulated patterns... 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n19.\n\n\nCódigo\nprint(dg_t)\n\n\n\n    Dao-Genton adjusted goodness-of-fit test\n    based on Diggle-Cressie-Loosmore-Ford test of fitted Poisson model\n    First stage: Monte Carlo test based on 19 simulations\n    Summary function: L(r)\n    Reference function: sample mean\n    Alternative: two.sided\n    Interval of distance values: [0, 125] metres\n    Test statistic: Integral of squared absolute deviation\n    Deviation = leave-one-out\n    Second stage: nested, 18 simulations for each first-stage simulation\n\ndata:  X\np0 = 0.05, p-value = 0.05\n\n\nCódigo\n# DIAGNÓSTICOS DE RESÍDUOS (PPM)\n\nres &lt;- residuals(fit, type = \"pearson\")\n\n\ndiagnose.ppm(fit, which = \"all\", main = \"Diagnóstico Geral\")\n\n\n\n\n\n\n\n\n\nModel diagnostics (raw residuals)\nDiagnostics available:\n    four-panel plot\n    mark plot \n    smoothed residual field\n    x cumulative residuals\n    y cumulative residuals\n    sum of all residuals\nsum of raw residuals in entire window = -6.924e-09\narea of entire window = 5e+05\nquadrature area = 5e+05\nrange of smoothed field =  [-0.005587, 0.008917]\n\n\nCódigo\n#DIAGNÓSTICOS DE INFLUÊNCIA E SENSIBILIDADE\n\n# Leverage: Onde a presença de um ponto afeta mais o ajuste?\nlev &lt;- leverage(fit)\n\n# Influence: Medida global de influência (DFBetas espacializado)\ninfl &lt;- influence(fit)\n\n# DFBetas: Mudança nos coeficientes se removermos um ponto\ndfb &lt;- dfbetas(fit)\n\n\n\nplot(lev, main = \"Leverage\")\n\n\n\n\n\n\n\n\n\nCódigo\nplot(infl, main = \"Influence\")\npoints(bei, pch = \".\", col = \"white\") # Sobrepor pontos\n\n\n\n\n\n\n\n\n\nCódigo\nplot(dfb, main = \"DFBetas\")\n\n\n\n\n\n\n\n\n\nCódigo\n# Partial Residuals (Sugere transformação funcional)\n# Se a linha verde (alisada) desvia da reta tracejada, a relação não é linear\nparres(fit, \"grad\", main = \"Resíduos Parciais (Grad)\")\n\n\n\n  \n\n\n\nCódigo\n# Added Variable Plot (A covariável 'elev' adiciona informação nova?)\n# Testa se devemos adicionar 'elev' ao modelo que já tem 'grad'\naddvar(fit, covariate=elev, data = list(elev = elev), main = \"Added Variable (Elev)\")",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#redes-lineares-spatstat.linnet",
    "href": "point_process.html#redes-lineares-spatstat.linnet",
    "title": "5  Processos Pontuais",
    "section": "5.21 Redes Lineares (spatstat.linnet)",
    "text": "5.21 Redes Lineares (spatstat.linnet)\nPara dados em ruas, rios ou grafos (geometria não-Euclidiana).\nInfraestrutura (linnet, lpp)\n\nRede (linnet):\n\nlinnet(vertices, m, edges): Criação manual.\nas.linnet(psp): Converte segmentos em rede.\nthinNetwork(L, retain): Remove arestas.\ninsertVertices(L): Adiciona nós.\nconnected.linnet(L): Acha componentes conexos.\nvertices(L): Extrai nós.\n\nPontos na Rede (lpp):\n\nlpp(coords, L): Cria pontos na rede.\nas.lpp(x, y, L): Projeta pontos 2D na rede mais próxima.\nclicklpp(L): Adição interativa.\ndistmap(X): Mapa de distância geodésica (shortest-path).\n\n\n\n5.21.1 Análise Exploratória na Rede\nIntensidade\n\ndensity.lpp(X, distance=\"path\"): Kernel usando “calor” na rede (respeita topologia).\nbw.lppl(X): Validação cruzada para largura de banda na rede.\n\nEstatísticas K e PCF:\n\nlinearK(X), linearKinhom(X): K de Ripley com distância de caminho.\nlinearpcf(X), linearpcfinhom(X): Correlação de pares na rede.\n\nTesselações e Imagens na Rede\n\nlinfun(f, L): Define função matemática na rede.\nlinim(X): Imagem pixelada restrita à rede.\nlintess(L): Tesselação da rede.\nlineardirichlet(X): Voronoi métrico na rede.\ndivide.linnet(L): Particionamento.\n\nModelagem e Validação (lppm)\n\nModelo:\n\n\nlppm(formula): Ajusta Poisson/Gibbs na rede.\n\nValidação e Diagnóstico\n\ncdf.test.lpp(fit): KS na rede.\nberman.test.lpp(X): Teste Z1/Z2 de Berman.\nquadrat.test.lpp(X): Qui-quadrado na rede.\nresiduals.lppm(fit): Resíduos.\nlurking(fit): Gráfico de variável oculta (lurking variable plot).\nroc.lppm(fit), auc.lppm(fit): ROC para modelos de rede.\n\nSimulação\n\nrpoislpp(lambda, L): Poisson na rede.\nrThomaslpp(L): Cluster Thomas na rede.\nrjitter.lpp(X): Perturbação sobre as linhas.\nsimulate.lppm(fit): Simulação do modelo.\n\n\n\nCódigo\nif(!require(pacman)) install.packages(\"pacman\")\npacman::p_load(spatstat)\n\n#INFRAESTRUTURA (Redes e Pontos)\n\ndata(simplenet)\nL &lt;- simplenet\n\n# Criação de pontos na rede\nset.seed(42)\nX &lt;- rpoislpp(lambda = 50, L = L)\n\n# Projetar pontos 2D para a rede\npts_2d &lt;- runifpoint(20, win = Window(L))\nX_proj &lt;- as.lpp(pts_2d, L = L)\n\n# Mapa de Distância Geodésica\ndist_img &lt;- distmap(X)\n\npar(mfrow=c(1,3))\nplot(L, main=\"Rede (simplenet)\")\nplot(X, main=\"Pontos (lpp)\")\nplot(dist_img, main=\"Mapa de Distância\")\n\n\n\n\n\n\n\n\n\nCódigo\n# ANÁLISE EXPLORATÓRIA\n\nden &lt;- density(X, sigma=0.1, distance=\"path\")\nK_lin &lt;- linearK(X)\ng_lin &lt;- linearpcf(X)\n\npar(mfrow=c(1,3))\nplot(den, main=\"Densidade Kernel\")\nplot(K_lin, main=\"K Linear\")\nplot(g_lin, main=\"Pair Correlation (PCF)\")\n\n\n\n\n\n\n\n\n\nCódigo\n#  TESSELAÇÕES E IMAGENS\n\nvoro_net &lt;- lineardirichlet(X)\nf_net &lt;- linfun(function(x,y,seg,tp) { x + y }, L)\nim_net &lt;- as.linim(f_net) \n\npar(mfrow=c(1,2))\nplot(voro_net, main=\"Voronoi Linear\")\nplot(im_net, main=\"Imagem Linear\")\n\n\n\n\n\n\n\n\n\nCódigo\n#  MODELAGEM (lppm)\n\n\ndata(chicago)\nchicago_clean &lt;- unmark(chicago)\n\n# Ajuste de Modelo Poisson\nfit_lppm &lt;- lppm(chicago_clean ~ x + y)\n\nprint(AIC(fit_lppm))\n\n\n[1] 1498.881\n\n\nCódigo\n# VALIDAÇÃO E DIAGNÓSTICO\n\n#Teste de Qui-quadrado\n# Apenas imprimimos o resultado estatístico. Plotar gera erro em redes lineares.\nqt_net &lt;- quadrat.test(fit_lppm, nx=3, ny=3)\nprint(qt_net) \n\n\n\n    Chi-squared test of fitted Poisson model 'fit_lppm' on network using\n    quadrat counts\n\ndata:  data from fit_lppm\nX2 = 11.875, df = 6, p-value = 0.1296\nalternative hypothesis: two.sided\n\nQuadrats: Tessellation on a linear network\n9 tiles\n\n\nCódigo\n#Visualização da Predição\npred &lt;- predict(fit_lppm)\npar(mfrow=c(1,2))\nplot(pred, main=\"Intensidade Ajustada (Modelo)\")\nplot(chicago_clean, add=TRUE, pch=\".\", cols=\"white\") # Sobrepor pontos\n\n# 3. Teste CDF\ncdf_net &lt;- cdf.test(fit_lppm, \"x\") \nplot(cdf_net, main=\"CDF Test (Coord X)\")\n\n\n\n\n\n\n\n\n\nCódigo\n# 4. Curva ROC\npar(mfrow=c(1,1))\nroc_curve &lt;- roc(fit_lppm)\nplot(roc_curve, main=\"Curva ROC\")\n\n\n\n\n\n\n\n\n\nCódigo\nprint(auc(fit_lppm))\n\n\n      obs      theo \n0.6598881 0.6598997 \n\n\nCódigo\n# SIMULAÇÃO \n\n# Simular do modelo ajustado (lppm)\nX_sim_fit &lt;- simulate(fit_lppm, nsim=1)[[1]]\n\n# Simular Cluster de Thomas na Rede (Método de Projeção)\nNet_Chi &lt;- domain(chicago_clean) \n\n#Simulamos o processo Thomas em 2D (usando a janela da rede)\n# kappa: intensidade dos pais, scale: dispersão, mu: filhos\nX_thomas_2d &lt;- rThomas(kappa=0.01, scale=100, mu=5, win=Window(Net_Chi))\n\n# Projetamos os pontos 2D para a rede linear\nX_thomas &lt;- as.lpp(X_thomas_2d, L=Net_Chi)\n\n# Jittering (Perturbação)\n# Usamos rjitter direto no objeto lpp\nX_jit &lt;- rjitter(chicago_clean, radius=30)\n\n# Visualização Comparativa\npar(mfrow=c(1,3))\nplot(X_sim_fit, main=\"Simulação Modelo Poisson\", cols=\"red\", cex=0.5, pch=16)\nplot(X_thomas, main=\"Cluster Thomas (Projetado)\", cols=\"blue\", cex=0.5, pch=16)\nplot(X_jit, main=\"Chicago com Jitter\", cols=\"green\", cex=0.5, pch=\".\")",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#integração-da-geoestatística-gams-e-processos-pontuais",
    "href": "point_process.html#integração-da-geoestatística-gams-e-processos-pontuais",
    "title": "5  Processos Pontuais",
    "section": "5.22 Integração da Geoestatística, GAMs e processos pontuais",
    "text": "5.22 Integração da Geoestatística, GAMs e processos pontuais\n\nGeoestatística (automap, gstat, geoR): Frequentemente, as covariáveis (ex: pH do solo, temperatura) são medidas apenas em locais amostrais. Utilizamos a krigagem para gerar mapas contínuos (imagens raster) dessas variáveis em toda a janela de observação, conforme discutido no Capítulo 3.\nProcessos Pontuais (spatstat): Os mapas krigados são convertidos para objetos de imagem (im) e utilizados como preditores espaciais para o padrão de pontos (ppp).\nModelagem (mgcv): Em vez de assumir relações lineares simples, ajustamos a intensidade \\(\\lambda(u)\\) usando splines de suavização (termos s()) para capturar respostas ecológicas complexas e não-lineares. O spatstat permite ajustar esses modelos através da função ppm com a opção `use.gam=TRUE```.\nVisualização (gratia): Extraímos o objeto GAM subjacente e utilizamos o pacote gratia para visualizar as funções parciais suaves e seus intervalos de confiança, substituindo os gráficos padrão do R base.\n\n\nCódigo\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(spatstat, gstat, automap, sf, stars, mgcv, gratia, ggplot2, sp)\n\n#CARREGAMENTO DOS DADOS (Amostragem de Solo)\ndata(meuse)      # Locais de coleta de solo (Rikken & Van Rijn, 1993)\ndata(meuse.grid) # Grade de referência da planície de inundação\n\n# Pontos de Coleta para SF\npontos_sf &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\ngrid_sf   &lt;- st_as_sf(meuse.grid, coords = c(\"x\", \"y\"), crs = 28992)\n\n# Converter para Spatial (Necessário para o automap)\ninput_sp &lt;- as(pontos_sf, \"Spatial\")\ngrid_sp  &lt;- as(grid_sf, \"Spatial\")\ngridded(grid_sp) &lt;- TRUE \n\n#PREPARAÇÃO DA COVARIÁVEL (KRIGAGEM)\n# Interpolação da concentração de Zinco para toda a área\n\nkrigagem &lt;- autoKrige(log(zinc) ~ 1, input_data = input_sp, new_data = grid_sp, debug.level = 0)\n\n# Conversão para Imagem (Covariável preditora)\n\nmapa_zinco_im &lt;- as.im(st_as_stars(krigagem$krige_output)[\"var1.pred\"])\n\n# CONFIGURAÇÃO DO PADRÃO DE PONTOS (PPM)\njanela &lt;- Window(mapa_zinco_im)\ncoords &lt;- st_coordinates(pontos_sf)\n\n# Criação do objeto PPP representando os LOCAIS DE AMOSTRAGEM\npontos_observados &lt;- ppp(x = coords[,1], \n                         y = coords[,2], \n                         window = janela, \n                         checkdup = TRUE)\n\npar(mfrow = c(1, 2), mar=c(1,1,3,1))\nplot(mapa_zinco_im, main = \"Log(Zinco) no Solo\", box = FALSE)\n\nplot(pontos_observados, main = \"Locais de\\nColeta (Meuse)\", pch = 16, cex = 0.5, cols = \"black\", col = c(NA, NA), \n     box = FALSE)\npar(mfrow = c(1, 1))\n\n#MODELAGEM (Investigação do Viés de Coleta)\n# ?Pergunta: A escolha dos locais de coleta foi influenciada pelo nível de poluição?\n# Modelo: Intensidade_de_Amostragem ~ s(Zinco)\n\nfit_real &lt;- ppm(pontos_observados ~ s(zinco), \n                covariates = list(zinco = mapa_zinco_im), \n                use.gam = TRUE) \n\n#RESULTADOS E INTERPRETAÇÃO ---\n\nmodelo_gam &lt;- fit_real$internal$glmfit\n\ndraw(modelo_gam) +\n  ggplot2::labs(\n    title = \"Resposta da densidade de amostras ao gradiente de poluição\",\n    y = \"Efeito na Densidade de Coleta (s(zinco))\",\n    x = \"Log(Concentração de Zinco) [ppm]\"\n  ) +\n  ggplot2::theme_bw() +\n  ggplot2::theme(plot.title = element_text(face = \"bold\"))\n#Intensidade Predita (Onde o modelo \"espera\" que haja amostras)\nmapa_predito &lt;- predict(fit_real, type = \"trend\")\n\nplot(mapa_predito, main = \"Densidade de Amostragem Predita (PPM)\")\nplot(pontos_observados, add = TRUE, pch = \".\", cols = \"white\")\n#Diagnóstico de Resíduos\nresiduos &lt;- residuals(fit_real, type = \"pearson\")\nplot(Smooth(residuos, sigma = 50), \n     main = \"Resíduos Espaciais \\n(Vermelho = Amostragem \\nmais densa que o previsto)\")\n\n\n\n\n\n\n\nIntegração da Geoestatística, GAMs e processos pontuais\n\n\n\n\n\n\n\nIntegração da Geoestatística, GAMs e processos pontuais\n\n\n\n\n\n\n\n\n\nIntegração da Geoestatística, GAMs e processos pontuais\n\n\n\n\n\n\n\nIntegração da Geoestatística, GAMs e processos pontuais\n\n\n\n\n\nO gráfico ilustra a relação direta e não linear entre o esforço de amostragem realizado pelos pesquisadores e o gradiente de poluição por zinco, onde o eixo horizontal apresenta o logaritmo da concentração do metal em ppm e o eixo vertical indica o efeito parcial dessa variável na densidade de coleta. A linha preta sólida central descreve o comportamento médio dessa relação, revelando uma curva sigmoide que permanece relativamente estável e próxima de zero em áreas de baixa a média contaminação, mas que apresenta uma inclinação ascendente acentuada a partir do valor 6.0, evidenciando que a estratégia de campo privilegiou uma coleta muito mais intensiva nas zonas mais poluídas.\nEnvolvendo a curva de tendência, a faixa sombreada em cinza representa o intervalo de confiança de 95%, cuja variação de largura serve como um indicador visual da incerteza estatística do modelo; nota-se que essa faixa é estreita na região central do gráfico, denotando alta precisão, mas se expande significativamente na extremidade direita (valores acima de 7.0), refletindo a escassez de dados nessas condições extremas. Essa distribuição dos dados é confirmada pelas pequenas barras verticais pretas situadas na base da figura (rug plot), que aparecem densamente agrupadas nos valores intermediários de zinco, mas tornam-se esparsas nas concentrações mais elevadas, o que explica matematicamente o aumento da incerteza e confirma visualmente o viés intencional do desenho experimental voltado para as áreas de maior interesse ambiental.\n\nas.im(): Função crucial de “ponte”. Ela converte os resultados espaciais de outros pacotes (como o output do automap ou sf) para o formato de imagem de pixel (im) que o spatstat exige para covariáveis.\nppm(..., use.gam=TRUE): Esta é a função de ajuste de modelos de processos pontuais (ppm). Ao ativar use.gam=TRUE, instruímos o spatstat a não usar o algoritmo padrão de regressão linear (glm), mas sim invocar o mgcv::gam para ajustar os coeficientes da intensidade. Isso permite o uso de termos s() (splines penalizados) na fórmula, capturando tendências complexas que uma função linear ou quadrática simples perderia.\ngratia::draw(): Uma função moderna projetada especificamente para objetos gam. Ela produz diagnósticos visuais e gráficos de efeitos parciais (mostrando como a intensidade muda em função da covariável, mantendo o resto constante) prontos para publicação. Como o spatstat armazena o objeto do modelo ajustado internamente, a extração (fit\\(internal\\)glmfit) permite conectar esses dois mundos.\n\n\n\n\n\nA González, Jonatan, e Paula Moraga. 2023. “Non-Parametric Analysis of Spatial and Spatio-Temporal Point Patterns”.\n\n\nBaddeley, Adrian. 2007. “Validation of statistical models for spatial point patterns”. Em Statistical Challenges in Modern Astronomy IV, 371:22.\n\n\nBaddeley, Adrian et al. 2008. “Analysing spatial point patterns in R”. Technical report, CSIRO, 2010. Version 4. Available at www. csiro. au ….\n\n\nBaddeley, Adrian. 2010. “Multivariate and marked point processes”. Handbook of spatial statistics, 371–402.\n\n\nBaddeley, Adrian J, Jesper Møller, e Rasmus Waagepetersen. 2000. “Non-and semi-parametric estimation of interaction in inhomogeneous point patterns”. Statistica Neerlandica 54 (3): 329–50.\n\n\nBaddeley, Adrian J, e MNM Van Lieshout. 1995. “Area-interaction point processes”. Annals of the Institute of Statistical Mathematics 47: 601–19.\n\n\nBaddeley, Adrian J, MNM Van Lieshout, e Jesper Møller. 1996. “Markov properties of cluster processes”. Advances in Applied Probability 28 (2): 346–55.\n\n\nBaddeley, Adrian, Ya-Mei Chang, Yong Song, e Rolf Turner. 2013. “Residual diagnostics for covariate effects in spatial point process models”. Journal of Computational and Graphical Statistics 22 (4): 886–905.\n\n\nBaddeley, Adrian, Tilman M Davies, Martin L Hazelton, Suman Rakshit, e Rolf Turner. 2022. “Fundamental problems in fitting spatial cluster process models”. Spatial Statistics 52: 100709.\n\n\nBaddeley, Adrian, Ege Rubak, e Rolf Turner. 2015. Spatial Point Patterns: Methodology and Applications with R. London: Chapman; Hall/CRC Press. https://www.routledge.com/Spatial-Point-Patterns-Methodology-and-Applications-with-R/Baddeley-Rubak-Turner/p/book/9781482210200/.\n\n\nBaddeley, Adrian, e Rolf Turner. 2005. “spatstat: An R Package for Analyzing Spatial Point Patterns”. Journal of Statistical Software 12 (6): 1–42. https://doi.org/10.18637/jss.v012.i06.\n\n\nBaddeley, Adrian, Rolf Turner, Jorge Mateu, e Andrew Bevan. 2013. “Hybrids of Gibbs Point Process Models and Their Implementation”. Journal of Statistical Software 55 (11): 1–43. https://doi.org/10.18637/jss.v055.i11.\n\n\nBaddeley, Adrian, Rolf Turner, Jesper Møller, e Martin Hazelton. 2005. “Residual analysis for spatial point processes (with discussion)”. Journal of the Royal Statistical Society Series B: Statistical Methodology 67 (5): 617–66.\n\n\nBaddeley, AJ, M Hazelton, J Møller, e R Turner. 2019. “Residuals and diagnostics for spatial point processes”. Em. Citeseer.\n\n\nBell, William Wallace. 2004. Special functions for scientists and engineers. Courier Corporation.\n\n\nBesag, J. 1977. “Discussion of Dr Ripley’s paper”. Journal of the Royal Statistical Society, Series B 39 (2): 193–95.\n\n\nCasella, George, e Roger L Berger. 2001. Statistical inference. Duxbury.\n\n\nClark, Philip J, e Francis C Evans. 1954. “Distance to nearest neighbor as a measure of spatial relationships in populations”. Ecology 35 (4): 445–53.\n\n\nCressie, N. 1993. Statistics for patial ata: iley eries in robability and tatistics. Wiley-Interscience.\n\n\nCronie, O, e MNM Van Lieshout. 2016. “Bandwidth selection for kernel estimators of the spatial intensity function”. arXiv preprint arXiv:1611.10221.\n\n\nDiggle, Peter J. 2010. Nonparametric methods. Chapman & Hall/CRC Handb. Mod. Stat. Methods.\n\n\n———. 2013. Statistical analysis of spatial and spatio-temporal point patterns. CRC press.\n\n\nDiggle, Peter J, David J Gates, e Alyson Stibbard. 1987. “A nonparametric estimator for pairwise-interaction point processes”. Biometrika 74 (4): 763–70.\n\n\nDiggle, Peter J, e Richard J Gratton. 1984. “Monte Carlo methods of inference for implicit statistical models”. Journal of the Royal Statistical Society Series B: Statistical Methodology 46 (2): 193–212.\n\n\nDiggle, Peter John. 2003. “Statistical analysis of spatial point patterns”. Em, 2nd ed.\n\n\nFerreira, Daniel Furtado. 2020. Fundamentos de probabilidade. Lavras: UFLA.\n\n\nGarcía-Portugués, E. 2024. Notes for Nonparametric Statistics. https://bookdown.org/egarpor/NP-UC3M/.\n\n\nGatrell, Anthony C, Trevor C Bailey, Peter J Diggle, e Barry S Rowlingson. 1996. “Spatial point pattern analysis and its application in geographical epidemiology”. Transactions of the Institute of British geographers, 256–74.\n\n\nGelfand, Alan E, Peter Diggle, Peter Guttorp, e Montserrat Fuentes. 2010. Handbook of spatial statistics. CRC press.\n\n\nHopkins, Brian, e John Gordon Skellam. 1954. “A new method for determining the type of distribution of plant individuals”. Annals of Botany 18 (2): 213–27.\n\n\nIllian, Janine B. 2019. “Spatial and spatio-temporal point processes in ecological applications”. Em Handbook of environmental and ecological statistics, 97–131. Chapman; Hall/CRC.\n\n\nIllian, Janine, Antti Penttinen, Helga Stoyan, e Dietrich Stoyan. 2008. Statistical analysis and modelling of spatial point patterns. John Wiley & Sons.\n\n\nIsham, Valerie. 2010. “Spatial point process models”. Handbook of spatial statistics, 283–98.\n\n\nJalilian, Abdollah, Yongtao Guan, e Rasmus Waagepetersen. 2013. “Decomposition of variance for spatial Cox processes”. Scandinavian Journal of Statistics 40 (1): 119–37.\n\n\nJalilian, Abdollah, Amir Safari, e Hormoz Sohrabi. 2020. “Modeling spatial patterns and species associations in a Hyrcanian forest using a multivariate log-Gaussian Cox process”. Journal of Statistical Modelling: Theory and Applications 1 (2): 59–76.\n\n\nLeininger, Thomas J. 2014. “Bayesian analysis of spatial point patterns”. Tese de doutorado, Duke University.\n\n\nLima, Renato Ribeiro de. 2005. “Modelagem espaço temporal para dados de incidência de doenças em plantas”. Tese de doutorado, Tese piracicaba-SP.\n\n\nMateus, Ana Lúcia Souza Silva. 2013. “Proposição de novas metodologias para análise de aleatoriedade em processos pontuais no espaço-tempo”. Tese (Doutorado em Estatística e Experimentação Agropecuária), Lavras: Universidade Federal de Lavras.\n\n\nMoller, Jesper, e Rasmus Plenge Waagepetersen. 2003. Statistical inference and simulation for spatial point processes. CRC press.\n\n\nMøller, Jesper, Anne Randi Syversveen, e Rasmus Plenge Waagepetersen. 1998. “Log gaussian cox processes”. Scandinavian journal of statistics 25 (3): 451–82.\n\n\nMøller, Jesper, e Rasmus P Waagepetersen. 2007. “Modern statistics for spatial point processes”. Scandinavian Journal of Statistics 34 (4): 643–84.\n\n\nMoraga, Paula. 2023. Spatial Statistics for Data Science: Theory and Practice with R. CRC Press.\n\n\nMorisita, Masaaki. 1959. “Measuring of the dispersion of individuals and analysis of the distributional patterns”. Memoirs of the Faculty of Science, E 2, Kyushu Univ. Ser.\n\n\nNightingale, Glenna F, Janine B Illian, Ruth King, e Peter Nightingale. 2019. “Area interaction point processes for bivariate point patterns in a Bayesian context”. Journal of Environmental Statistics 9 (2).\n\n\nOkabe, Masahilo, e Masaharu Tanemura. 2006. “Bayesian Estimation of Soft-Core Potential Models for Spatial Point Patterns”. Journal of the Japan Statistical Society 36 (2): 121–47.\n\n\nPebesma, Edzer, e Roger Bivand. 2023. Spatial data science: With applications in R. Chapman; Hall/CRC.\n\n\nScalon, João Domingos. 2024. Análise de Dados Espaciais com Aplicações em R. Lavras: Ed. UFLA.\n\n\nScalon, João Domingos, Victor Ferreira da Silva, Wélson Antônio de Oliveira, e Mateus Santos Peixoto. 2022. “Statistical characterization of spatial and size distributions of particles in composite materials used in the manufacturing of biomedical instruments”. Brazilian Journal of Biometrics 40 (4): 428–41.\n\n\nVan Lieshout, MNM, e AJ1422574 Baddeley. 1996. “A nonparametric measure of spatial interaction in point patterns”. Statistica Neerlandica 50 (3): 344–61.\n\n\nWaagepetersen, Rasmus, e Yongtao Guan. 2009. “Two-step estimation for inhomogeneous spatial point processes”. Journal of the Royal Statistical Society Series B: Statistical Methodology 71 (3): 685–702.\n\n\nWiegand, Thorsten, e Kirk A Moloney. 2014. Handbook of spatial point-pattern analysis in ecology. CRC press.",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "point_process.html#footnotes",
    "href": "point_process.html#footnotes",
    "title": "5  Processos Pontuais",
    "section": "",
    "text": "A informação de Fisher é uma medida estatística que quantifica a quantidade de informação que uma variável aleatória contém sobre um parâmetro desconhecido de uma distribuição, sendo definida como a variância do escore, ou seja, a derivada parcial do logaritmo natural da função de verossimilhança em relação ao parâmetro e indica o quão bem podemos estimar um parâmetro com base nos dados observados (Casella e Berger 2001).↩︎",
    "crumbs": [
      "Processos Pontuais",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Processos Pontuais</span>"
    ]
  },
  {
    "objectID": "spatiotemporal.html",
    "href": "spatiotemporal.html",
    "title": "6  Modelagem Espaço-Temporal",
    "section": "",
    "text": "6.1 Referências Gerais\nAté aqui, focamos em dados que variam apenas no espaço \\(\\{Y(s), s \\subset D\\}\\). No entanto, é comum que os fenômenos variem tanto no espaço quanto no tempo. Ou seja, observamos o mesmo processo espacial em múltiplos instantes temporais, e esse efeito temporal é importante para a análise.\nA única mudança fundamental é a adição de uma segunda dimensão: em vez de considerarmos apenas \\(\\{Y(s), s \\subset D\\}\\), agora trabalhamos com \\(\\{\\mathbf{Y(s, t)}, \\mathbf{s} \\subset D; \\mathbf{t} \\subset T\\}\\).\nTrata-se de uma extensão natural dos modelos puramente espaciais. Portanto, se você compreendeu bem os conteúdos anteriores, geoestatística (Capítulo 3), dados de área (Capítulo 4) e processos pontuais (Capítulo 5) será capaz de entender facilmente a extensão espaço-temporal. Para auxiliar nesse passo, apresentamos abaixo uma seleção de referências e pacotes do R para aplicação prática. A lista não é exaustiva, mas oferece um ponto de partida sólido.",
    "crumbs": [
      "Espaço-Temporal",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelagem Espaço-Temporal</span>"
    ]
  },
  {
    "objectID": "spatiotemporal.html#referências-gerais",
    "href": "spatiotemporal.html#referências-gerais",
    "title": "6  Modelagem Espaço-Temporal",
    "section": "",
    "text": "Pebesma, E.spacetime: Spatio-temporal data in R. Journal of Statistical Software, 51, 2012, pp. 1–30.\n\nBakar, K. S. & Sahu, S. K.spTimer: Spatio-temporal Bayesian modeling using R. Journal of Statistical Software, 63, 2015, pp. 1–32.\nWikle, C. K., Zammit-Mangion, A. & Cressie, N. Spatio-temporal statistics with R. Chapman and Hall/CRC, 2019. Disponível gratuitamente em: spacetimewithr.org.\nCressie, N. & Wikle, C. K.Statistics for spatio-temporal data. John Wiley & Sons, 2011.",
    "crumbs": [
      "Espaço-Temporal",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelagem Espaço-Temporal</span>"
    ]
  },
  {
    "objectID": "spatiotemporal.html#modelagem-espaço-temporal-em-dados-geoestatísticos",
    "href": "spatiotemporal.html#modelagem-espaço-temporal-em-dados-geoestatísticos",
    "title": "6  Modelagem Espaço-Temporal",
    "section": "6.2 Modelagem Espaço-Temporal em Dados Geoestatísticos",
    "text": "6.2 Modelagem Espaço-Temporal em Dados Geoestatísticos\n\nGräler, B., Pebesma, E. & Heuvelink, G. Spatio-temporal interpolation using gstat, 2016.\nSahu, S. Bayesian modeling of spatio-temporal data with R. Chapman and Hall/CRC, 2022.",
    "crumbs": [
      "Espaço-Temporal",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelagem Espaço-Temporal</span>"
    ]
  },
  {
    "objectID": "spatiotemporal.html#modelagem-espaço-temporal-em-processos-pontuais",
    "href": "spatiotemporal.html#modelagem-espaço-temporal-em-processos-pontuais",
    "title": "6  Modelagem Espaço-Temporal",
    "section": "6.3 Modelagem Espaço-Temporal em Processos Pontuais",
    "text": "6.3 Modelagem Espaço-Temporal em Processos Pontuais\n\nGonzález, J. A. et al. Spatio-temporal point process statistics: a review. Spatial Statistics, 18, 2016, pp. 505–544.\nDiggle, P. J.Statistical analysis of spatial and spatio-temporal point patterns. CRC Press, 2013.\nD’Angelo, N. & Adelfio, G. stopp: An R package for spatio-temporal point pattern analysis. Journal of Statistical Software, 113, 2025, pp. 1–35.\nGabriel, E. et al.stpp: an R package for plotting, simulating and analyzing Spatio-Temporal Point Patterns. Journal of Statistical Software, 53, 2013, pp. 1–29.",
    "crumbs": [
      "Espaço-Temporal",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelagem Espaço-Temporal</span>"
    ]
  },
  {
    "objectID": "spatiotemporal.html#modelagem-espaço-temporal-de-dados-de-área",
    "href": "spatiotemporal.html#modelagem-espaço-temporal-de-dados-de-área",
    "title": "6  Modelagem Espaço-Temporal",
    "section": "6.4 Modelagem Espaço-Temporal de Dados de Área",
    "text": "6.4 Modelagem Espaço-Temporal de Dados de Área\n\nBlangiardo, M. & Cameletti, M. Spatial and spatio-temporal Bayesian models with R-INLA. John Wiley & Sons, 2015.",
    "crumbs": [
      "Espaço-Temporal",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelagem Espaço-Temporal</span>"
    ]
  },
  {
    "objectID": "spatiotemporal.html#modelagem-local",
    "href": "spatiotemporal.html#modelagem-local",
    "title": "6  Modelagem Espaço-Temporal",
    "section": "6.5 Modelagem local",
    "text": "6.5 Modelagem local\n\nWu, C., Ren, F., Hu, W. and Du, Q.Multiscale geographically and temporally weighted regression: Exploring the spatiotemporal determinants of housing prices. International Journal of Geographical Information Science, 33(3), 2019, pp.489-511.",
    "crumbs": [
      "Espaço-Temporal",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelagem Espaço-Temporal</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referências Bibliográficas",
    "section": "",
    "text": "A González, Jonatan, and Paula Moraga. 2023. “Non-Parametric\nAnalysis of Spatial and Spatio-Temporal Point Patterns.”\n\n\nAbdulah, Sameh, Yuxiao Li, Jian Cao, Hatem Ltaief, David E Keyes, Marc G\nGenton, and Ying Sun. 2023. “Large-Scale Environmental Data\nScience with ExaGeoStatR.” Environmetrics 34 (1): e2770.\n\n\nAnselin, Luc. 1988. “Spatial Econometrics: Methods and\nModels.” Kluwer Academic Publishers Google Schola 2:\n283–91.\n\n\n———. 1995. “Local Indicators of Spatial Association—LISA.”\nGeographical Analysis 27 (2): 93–115.\n\n\n———. 2001. “Spatial Econometrics.” In A Companion to\nTheoretical Econometrics, edited by Badi H. Baltagi, 310–30.\nOxford: Blackwell Publishing.\n\n\n———. 2002. “Under the Hood Issues in the Specification and\nInterpretation of Spatial Regression Models.” Agricultural\nEconomics 27 (3): 247–67.\n\n\n———. 2010. “Thirty Years of Spatial Econometrics.”\nPapers in Regional Science 89 (1): 3–26.\n\n\nBaddeley, Adrian. 2007. “Validation of Statistical Models for\nSpatial Point Patterns.” In Statistical Challenges in Modern\nAstronomy IV, 371:22.\n\n\nBaddeley, Adrian et al. 2008. “Analysing Spatial Point Patterns in\nr.” Technical report, CSIRO, 2010. Version 4. Available at www.\ncsiro. au ….\n\n\nBaddeley, Adrian. 2010. “Multivariate and Marked Point\nProcesses.” Handbook of Spatial Statistics, 371–402.\n\n\nBaddeley, Adrian J, Jesper Møller, and Rasmus Waagepetersen. 2000.\n“Non-and Semi-Parametric Estimation of Interaction in\nInhomogeneous Point Patterns.” Statistica Neerlandica 54\n(3): 329–50.\n\n\nBaddeley, Adrian J, and MNM Van Lieshout. 1995. “Area-Interaction\nPoint Processes.” Annals of the Institute of Statistical\nMathematics 47: 601–19.\n\n\nBaddeley, Adrian J, MNM Van Lieshout, and Jesper Møller. 1996.\n“Markov Properties of Cluster Processes.” Advances in\nApplied Probability 28 (2): 346–55.\n\n\nBaddeley, Adrian, Ya-Mei Chang, Yong Song, and Rolf Turner. 2013.\n“Residual Diagnostics for Covariate Effects in Spatial Point\nProcess Models.” Journal of Computational and Graphical\nStatistics 22 (4): 886–905.\n\n\nBaddeley, Adrian, Tilman M Davies, Martin L Hazelton, Suman Rakshit, and\nRolf Turner. 2022. “Fundamental Problems in Fitting Spatial\nCluster Process Models.” Spatial Statistics 52: 100709.\n\n\nBaddeley, Adrian, Ege Rubak, and Rolf Turner. 2015. Spatial Point\nPatterns: Methodology and Applications with R. London:\nChapman; Hall/CRC Press. https://www.routledge.com/Spatial-Point-Patterns-Methodology-and-Applications-with-R/Baddeley-Rubak-Turner/p/book/9781482210200/.\n\n\nBaddeley, Adrian, and Rolf Turner. 2005. “spatstat: An R Package for Analyzing\nSpatial Point Patterns.” Journal of Statistical Software\n12 (6): 1–42. https://doi.org/10.18637/jss.v012.i06.\n\n\nBaddeley, Adrian, Rolf Turner, Jorge Mateu, and Andrew Bevan. 2013.\n“Hybrids of Gibbs Point Process Models and Their\nImplementation.” Journal of Statistical Software 55\n(11): 1–43. https://doi.org/10.18637/jss.v055.i11.\n\n\nBaddeley, Adrian, Rolf Turner, Jesper Møller, and Martin Hazelton. 2005.\n“Residual Analysis for Spatial Point Processes (with\nDiscussion).” Journal of the Royal Statistical Society Series\nB: Statistical Methodology 67 (5): 617–66.\n\n\nBaddeley, AJ, M Hazelton, J Møller, and R Turner. 2019. “Residuals\nand Diagnostics for Spatial Point Processes.” In. Citeseer.\n\n\nBakka, Haakon, Håvard Rue, Geir-Arne Fuglstad, Andrea Riebler, David\nBolin, Janine Illian, Elias Krainski, Daniel Simpson, and Finn Lindgren.\n2018. “Spatial Modeling with r-INLA: A Review.” Wiley\nInterdisciplinary Reviews: Computational Statistics 10 (6): e1443.\n\n\nBandyopadhyay, Soutir, and Suhasini Subba Rao. 2017. “A Test for\nStationarity for Irregularly Spaced Spatial Data.” Journal of\nthe Royal Statistical Society Series B: Statistical Methodology 79\n(1): 95–123.\n\n\nBanerjee, Sudipto. 2016. “Spatial Data Analysis.”\nAnnual Review of Public Health 37 (1): 47–60.\n\n\nBanerjee, Sudipto, Bradley P Carlin, and Alan E Gelfand. 2003.\nHierarchical Modeling and Analysis for Spatial Data. Chapman;\nHall/CRC.\n\n\nBartholomew, David J. 1995. “What Is Statistics?”\nJournal of the Royal Statistical Society Series A: Statistics in\nSociety 158 (1): 1–20.\n\n\nBell, William Wallace. 2004. Special Functions for Scientists and\nEngineers. Courier Corporation.\n\n\nBeron, Kurt J, and Wim PM Vijverberg. 2004. “Probit in a Spatial\nContext: A Monte Carlo Analysis.” In Advances in Spatial\nEconometrics: Methodology, Tools and Applications, 169–95.\nSpringer.\n\n\nBesag, J. 1977. “Discussion of Dr Ripley’s Paper.”\nJournal of the Royal Statistical Society, Series B 39 (2):\n193–95.\n\n\nBesag, Julian. 1974. “Spatial Interaction and the Statistical\nAnalysis of Lattice Systems.” Journal of the Royal\nStatistical Society: Series B (Methodological) 36 (2): 192–225.\n\n\n———. 1975. “Statistical Analysis of Non-Lattice Data.”\nJournal of the Royal Statistical Society Series D: The\nStatistician 24 (3): 179–95.\n\n\nBesag, Julian, and Peter J Green. 1993. “Spatial Statistics and\nBayesian Computation.” Journal of the Royal Statistical\nSociety Series B: Statistical Methodology 55 (1): 25–37.\n\n\nBesag, Julian, and Charles Kooperberg. 1995. “On Conditional and Intrinsic\nAutoregression.” Biometrika 82 (4): 733–46. https://doi.org/10.2307/2337341.\n\n\nBesag, Julian, Jeremy York, and Annie Mollié. 1991. “Bayesian\nImage Restoration, with Two Applications in Spatial Statistics.”\nAnnals of the Institute of Statistical Mathematics 43 (1):\n1–20.\n\n\nBillé, Anna Gloria, and Giuseppe Arbia. 2019. “Spatial Limited\nDependent Variable Models: A Review Focused on Specification,\nEstimation, and Health Economics Applications.” Journal of\nEconomic Surveys 33 (5): 1531–54.\n\n\nBillé, Anna Gloria, and Samantha Leorato. 2020. “Partial ML\nEstimation for Spatial Autoregressive Nonlinear Probit Models with\nAutoregressive Disturbances.” Econometric Reviews 39\n(5): 437–75.\n\n\nBoschma, Ron. 2005. “Proximity and Innovation: A Critical\nAssessment.” Regional Studies 39 (1): 61–74.\n\n\nBrunsdon, Chris, A Stewart Fotheringham, and Martin E Charlton. 1996.\n“Geographically Weighted Regression: A Method for Exploring\nSpatial Nonstationarity.” Geographical Analysis 28 (4):\n281–98.\n\n\nBurbano-Moreno, Alvaro Alexander, and Vinı́cius Diniz Mayrink. 2024.\n“Spatial Functional Data Analysis: Irregular Spacing and Bernstein\nPolynomials.” Spatial Statistics 60: 100832.\n\n\nBurridge, Peter. 1980. “On the Cliff-Ord Test for Spatial\nCorrelation.” Journal of the Royal Statistical Society:\nSeries B (Methodological) 42 (1): 107–8.\n\n\n———. 1981. “Testing for a Common Factor in a Spatial\nAutoregression Model.” Environment and Planning A 13\n(7): 795–800.\n\n\nCalabrese, Raffaella, and Johan A Elkink. 2014. “Estimators of\nBinary Spatial Autoregressive Models: A Monte Carlo Study.”\nJournal of Regional Science 54 (4): 664–87.\n\n\nCarvalho, Dhaniel, and CV Deutsch. 2017. “An Overview of Multiple\nIndicator Kriging.” Geostatistics Lessons 7.\n\n\nCasella, George, and Roger L Berger. 2001. Statistical\nInference. Duxbury.\n\n\nChen, Wanfang, Marc G Genton, and Ying Sun. 2021. “Space-Time\nCovariance Structures and Models.” Annual Review of\nStatistics and Its Application 8 (1): 191–215.\n\n\nCheng, Joe, Barret Schloerke, Bhaskar Karambelkar, Yihui Xie, and\nGarrick Aden-Buie. 2025. Leaflet: Create Interactive Web Maps with\nthe JavaScript ’Leaflet’ Library. https://doi.org/10.32614/CRAN.package.leaflet.\n\n\nChiles, Jean-Paul, and Pierre Delfiner. 2012. Geostatistics:\nModeling Spatial Uncertainty. John Wiley & Sons.\n\n\nChun, Yongwan, and Daniel A. Griffith. 2017. “Measuring Spatial\nDependence.” International Encyclopedia of Geography,\nMarch, 1–14. https://doi.org/10.1002/9781118786352.wbieg0850.\n\n\nClark, Philip J, and Francis C Evans. 1954. “Distance to Nearest\nNeighbor as a Measure of Spatial Relationships in Populations.”\nEcology 35 (4): 445–53.\n\n\nCliff, Andrew David, and J Keith Ord. 1981. “Spatial Processes:\nModels & Applications.” (No Title).\nhttps://doi.org/https://doi.org/10.2307/2530324.\n\n\nConte, Bruno. 2023. “Lecture 02: Spatial Data Theory and Tools\n(a.k.a. GIS Tools Lab.).” Alma Mater Studiorum Università di\nBologna; Lecture slides.\n\n\nCox, Simon Jonathan David. 2011. ISO 19156:2011 - Geographic\nInformation – Observations and Measurements. International\nOrganization for Standardization. https://doi.org/10.13140/2.1.1142.3042.\n\n\nCrawford, T. W. 2009. “Scale Analytical.” In, 29–36.\nElsevier. https://doi.org/10.1016/b978-008044910-4.00399-0.\n\n\nCressie, N. 1993. Statistics for Patial Ata: Iley Eries in\nRobability and Tatistics. Wiley-Interscience.\n\n\nCressie, Noel. 1985. “Fitting Variogram Models by Weighted Least\nSquares.” Journal of the International Association for\nMathematical Geology 17 (5): 563–86.\n\n\n———. 1989. “Geostatistics.” The American\nStatistician 43 (4): 197–202.\n\n\n———. 1990. “The Origins of Kriging.” Mathematical\nGeology 22 (3): 239–52.\n\n\n———. 1991. “Geostatistical Analysis of Spatial Data.”\nSpatial Statistics and Digital Image Analysis 1991: 87–108.\n\n\n———. 1993. Statistics for Spatial Data. John Wiley & Sons.\n\n\nCressie, Noel, and Ngai H Chan. 1989. “Spatial Modeling of\nRegional Variables.” Journal of the American Statistical\nAssociation 84 (406): 393–401.\n\n\nCressie, Noel, and Douglas M Hawkins. 1980. “Robust Estimation of\nthe Variogram: i.” Journal of the International Association\nfor Mathematical Geology 12 (2): 115–25.\n\n\nCressie, Noel, and Matthew T Moores. 2022. “Spatial\nStatistics.” In Encyclopedia of Mathematical\nGeosciences, 1–11. Springer.\n\n\nCressie, Noel, Matthew Sainsbury-Dale, and Andrew Zammit-Mangion. 2022.\n“Basis-Function Models in Spatial Statistics.” Annual\nReview of Statistics and Its Application 9 (1): 373–400.\n\n\nCressie, Noel, and Andrew Zammit-Mangion. 2016. “Multivariate\nSpatial Covariance Models: A Conditional Approach.”\nBiometrika 103 (4): 915–35.\n\n\nCronie, O, and MNM Van Lieshout. 2016. “Bandwidth Selection for\nKernel Estimators of the Spatial Intensity Function.” arXiv\nPreprint arXiv:1611.10221.\n\n\nda Silva, Anderson R., and A. Stewart Fotheringham. 2016. “The\nMultiple Testing Issue in Geographically Weighted Regression.”\nGeographical Analysis 48 (3): 233–47.\n\n\nda Silva, Anderson R., and Flávio F. Mendes. 2018. “On Comparing\nSome Algorithms for Finding the Optimal Bandwidth in Geographically\nWeighted Regression.” Applied Soft Computing 73: 943–57.\n\n\nDelicado, Pedro, Ramón Giraldo, Carlos Comas, and Jorge Mateu. 2010.\n“Statistics for Spatial Functional Data: Some Recent\nContributions.” Environmetrics: The Official Journal of the\nInternational Environmetrics Society 21 (3-4): 224–39.\n\n\nDeutsch, Clayton V., and André G. Journel. 1997. GSLIB:\nGeostatistical Software Library and User’s Guide. 2nd ed. New York:\nOxford University Press.\n\n\nDiggle, Peter J. 2010. Nonparametric Methods. Chapman &\nHall/CRC Handb. Mod. Stat. Methods.\n\n\n———. 2013. Statistical Analysis of Spatial and Spatio-Temporal Point\nPatterns. CRC press.\n\n\nDiggle, Peter J, David J Gates, and Alyson Stibbard. 1987. “A\nNonparametric Estimator for Pairwise-Interaction Point\nProcesses.” Biometrika 74 (4): 763–70.\n\n\nDiggle, Peter J, and Richard J Gratton. 1984. “Monte Carlo Methods\nof Inference for Implicit Statistical Models.” Journal of the\nRoyal Statistical Society Series B: Statistical Methodology 46 (2):\n193–212.\n\n\nDiggle, Peter John. 2003. “Statistical Analysis of Spatial Point\nPatterns.” In, 2nd ed.\n\n\nDiggle, Peter J, Jonathan A Tawn, and Rana A Moyeed. 1998.\n“Model-Based Geostatistics.” Journal of the Royal\nStatistical Society Series C: Applied Statistics 47 (3): 299–350.\n\n\nDreesman, Johannes M, and Gerhard Tutz. 2001. “Non-Stationary\nConditional Models for Spatial Data Based on Varying\nCoefficients.” Journal of the Royal Statistical Society:\nSeries D (The Statistician) 50 (1): 1–15.\n\n\nDunnington, Dewey. 2025. Ggspatial: Spatial Data Framework for\nGgplot2. https://doi.org/10.32614/CRAN.package.ggspatial.\n\n\nEcker, Mark D. 2003. “Geostatistics: Past, Present and\nFuture.” Encyclopedia of Life Support Systems (EOLSS),\n50614–506.\n\n\nElhorst, J Paul et al. 2014. Spatial Econometrics: From\nCross-Sectional Data to Spatial Panels. Vol. 479. Springer.\n\n\nElhorst, J Paul. 2022. “The Dynamic General Nesting Spatial\nEconometric Model for Spatial Panels with Common Factors: Further\nRaising the Bar.” Review of Regional Research 42 (3):\n249–67.\n\n\nFávero, Luiz Paulo Lopes. 2003. “Modelos de Preços\nHedônicos Aplicados a Imóveis Residenciais Em\nLançamento No Municı́pio de são\nPaulo.” PhD thesis, Universidade de São Paulo.\n\n\nFerreira, Daniel Furtado. 2020. Fundamentos de Probabilidade.\nLavras: UFLA.\n\n\nFienberg, Stephen E. 2014. “What Is Statistics?” Annual\nReview of Statistics and Its Application 1 (1): 1–9.\n\n\nFleming, Mark M. 2004. “Techniques for Estimating Spatially\nDependent Discrete Choice Models.” In Advances in Spatial\nEconometrics: Methodology, Tools and Applications, 145–68.\nSpringer.\n\n\nFotheringham, A. Stewart, Wenbai Yang, and Wei Kang. 2017.\n“Multiscale Geographically Weighted Regression (MGWR).”\nAnnals of the American Association of Geographers 107 (6):\n1247–65. https://doi.org/10.1080/24694452.2017.1352480.\n\n\nFotheringham, A. Stewart, Hanchen Yu, Levi John Wolf, Taylor M. Oshan,\nand Ziqi Li. 2022. “On the Notion of ‘Bandwidth’ in\nGeographically Weighted Regression Models of Spatially Varying\nProcesses.” International Journal of Geographical Information\nScience 36 (8): 1485–1502. https://doi.org/10.1080/13658816.2022.2034829.\n\n\nGarcía-Portugués, E. 2024. Notes for Nonparametric Statistics.\nhttps://bookdown.org/egarpor/NP-UC3M/.\n\n\nGatrell, Anthony C, Trevor C Bailey, Peter J Diggle, and Barry S\nRowlingson. 1996. “Spatial Point Pattern Analysis and Its\nApplication in Geographical Epidemiology.” Transactions of\nthe Institute of British Geographers, 256–74.\n\n\nGelfand, Alan E, Peter Diggle, Peter Guttorp, and Montserrat Fuentes.\n2010. Handbook of Spatial Statistics. CRC press.\n\n\nGelman, Andrew. 2006. “Prior Distributions for Variance Parameters\nin Hierarchical Models.” Bayesian Analysis 1 (3):\n515–34.\n\n\nGenton, Marc G. 1998. “Variogram Fitting by Generalized Least\nSquares Using an Explicit Formula for the Covariance Structure.”\nMathematical Geology 30 (4): 323–45.\n\n\nGetis, Arthur. 1995. “Cliff, Ad and Ord, Jk 1973: Spatial\nAutocorrelation. London: Pion.” Progress in Human\nGeography 19 (2): 245–49.\n\n\n———. 1999. “Spatial Statistics.” Geographical\nInformation Systems 1: 239–51.\n\n\n———. 2010. “Spatial Autocorrelation.” In Handbook of\nApplied Spatial Analysis: Software Tools, Methods and Applications,\nedited by Manfred M. Fischer and Arthur Getis, 255–78. Berlin,\nHeidelberg: Springer.\n\n\nGetis, Arthur, and J Keith Ord. 1992. “The Analysis of Spatial\nAssociation by Use of Distance Statistics.” Geographical\nAnalysis 24 (3): 189–206.\n\n\nGiraud, Timothée. 2025. “Mapsf: Thematic Cartography.”\n\n\nGollini, Isabella, Binbin Lu, Martin Charlton, Christopher Brunsdon, and\nPaul Harris. 2015. “GWmodel: An r Package for Exploring Spatial\nHeterogeneity Using Geographically Weighted Models.” Journal\nof Statistical Software 63: 1–50.\n\n\nGoovaerts, Pierre. 1997. Geostatistics for Natural Resources\nEvaluation. Oxford university press.\n\n\nGorsich, David J, and Marc G Genton. 2000. “Variogram Model\nSelection via Nonparametric Derivative Estimation.”\nMathematical Geology 32 (3): 249–70.\n\n\nGräler, Benedikt, Edzer Pebesma, and Gerard Heuvelink. 2016.\n“Spatio-Temporal Interpolation Using Gstat.”\n\n\nGreene, William H. 2003. Econometric Analysis. 5th ed. Prentice\nHall.\n\n\nGuimaraes, Ricardo JPS, Corina C Freitas, Luciano V Dutra, Carlos A\nFelgueiras, Sandra C Drummond, Sandra HC Tibiriçá, Guilherme Oliveira,\nand Omar S Carvalho. 2012. “Use of Indicator Kriging to\nInvestigate Schistosomiasis in Minas Gerais State, Brazil.”\nJournal of Tropical Medicine 2012 (1): 837428.\n\n\nGuo, L., Z. Ma, and L. Zhang. 2008. “Comparison of Bandwidth\nSelection in Application of Geographically Weighted Regression: A Case\nStudy.” Canadian Journal of Forest Research 38 (9):\n2526–34.\n\n\nGuttorp, Peter, and Tilmann Gneiting. 2006. “Studies in the\nHistory of Probability and Statistics XLIX on the Matérn\nCorrelation Family.” Biometrika 93 (4): 989–95.\n\n\nHaining, Robert Haining, Stephen Wise, and Jingsheng Ma. 1998.\n“Exploratory Spatial Data Analysis in a Geographic Information\nSystem Environment.” Journal of the Royal Statistical Society\nSeries D: The Statistician 47 (3): 457–69.\n\n\nHaining, Robert P. 2003. Spatial Data Analysis: Theory and\nPractice. Cambridge university press.\n\n\nHalleck Vega, Solmaria, and J Paul Elhorst. 2015. “The SLX\nModel.” Journal of Regional Science 55 (3): 339–63.\n\n\nHarris, Richard, John Moffat, and Victoria Kravtsova. 2011. “In\nSearch of ‘w’.” Spatial Economic Analysis 6\n(3): 249–70.\n\n\nHarville, David A. 1977. “Maximum Likelihood Approaches to\nVariance Component Estimation and to Related Problems.”\nJournal of the American Statistical Association 72 (358):\n320–38.\n\n\nHastie, Trevor, and Robert Tibshirani. 1990. Generalized Additive\nModels. Monographs on Statistics and Applied Probability. London:\nChapman; Hall.\n\n\nHe, Zhanjun, Zhe Wang, Zhiqiang Xie, Lin Wu, and Zhen Chen. 2022.\n“Multiscale Analysis of the Influence of Street Built Environment\non Crime Occurrence Using Street-View Images.” Computers,\nEnvironment and Urban Systems 97: 101865. https://doi.org/10.1016/j.compenvurbsys.2022.101865.\n\n\nHeld, Leonhard, and Havard Rue. 2010. “Conditional and Intrinsic\nAutoregressions.” Handbook of Spatial Statistics,\n201–16.\n\n\nHepple, Leslie W. 1979. “Bayesian Analysis of the Linear Model\nwith Spatial Dependence.” In Exploratory and Explanatory\nStatistical Analysis of Spatial Data, 179–99. Springer.\n\n\nHiemstra, P. H., E. J. Pebesma, C. J. W. Twenh\"ofel, and G. B. M.\nHeuvelink. 2008. “Real-Time Automatic Interpolation of Ambient\nGamma Dose Rates from the Dutch Radioactivity Monitoring\nNetwork.” Computers & Geosciences.\n\n\nHijmans, Robert J. 2025. Geodata: Access Geographic Data. https://doi.org/10.32614/CRAN.package.geodata.\n\n\nHill, Donna. 1998. “Comparison of Median Indicator Kriging with\nFull Indicator Kriging in the Analysis of Spatial Data.”\n\n\nHodges, James S, and Brian J Reich. 2010. “Adding\nSpatially-Correlated Errors Can Mess up the Fixed Effect You\nLove.” The American Statistician 64 (4): 325–34.\n\n\nHopkins, Brian, and John Gordon Skellam. 1954. “A New Method for\nDetermining the Type of Distribution of Plant Individuals.”\nAnnals of Botany 18 (2): 213–27.\n\n\nHurvich, Clifford M., Jeffrey S. Simonoff, and Chih-Ling Tsai. 1998.\n“Smoothing Parameter Selection in Nonparametric Regression Using\nan Improved Akaike Information Criterion.” Journal of the\nRoyal Statistical Society: Series B (Statistical Methodology) 60\n(2): 271–93.\n\n\nIliffe, Jonathan. 2000. Datums and Map Projections for Remote\nSensing, GIS, and Surveying. CRC Press.\n\n\nIllian, Janine B. 2019. “Spatial and Spatio-Temporal Point\nProcesses in Ecological Applications.” In Handbook of\nEnvironmental and Ecological Statistics, 97–131. Chapman; Hall/CRC.\n\n\nIllian, Janine, Antti Penttinen, Helga Stoyan, and Dietrich Stoyan.\n2008. Statistical Analysis and Modelling of Spatial Point\nPatterns. John Wiley & Sons.\n\n\nIsaaks, Edward H, R Mohan Srivastava, et al. 1989. “Applied\nGeostatistics.”\n\n\nIsham, Valerie. 2010. “Spatial Point Process Models.”\nHandbook of Spatial Statistics, 283–98.\n\n\nJalilian, Abdollah, Yongtao Guan, and Rasmus Waagepetersen. 2013.\n“Decomposition of Variance for Spatial Cox Processes.”\nScandinavian Journal of Statistics 40 (1): 119–37.\n\n\nJalilian, Abdollah, Amir Safari, and Hormoz Sohrabi. 2020.\n“Modeling Spatial Patterns and Species Associations in a Hyrcanian\nForest Using a Multivariate Log-Gaussian Cox Process.”\nJournal of Statistical Modelling: Theory and Applications 1\n(2): 59–76.\n\n\nJanssen, Volker. 2009. “Understanding Coordinate Reference\nSystems, Datums and Transformations.”\n\n\nJi, Guangjun, Zizhao Cai, Keyan Xiao, Yan Lu, and Qian Wang. 2025.\n“Ordered Indicator Kriging Interpolation Method with Field\nVariogram Parameters for Discrete Variables in the Aquifers of\nQuaternary Loose Sediments.” Water 17 (21): 3116.\n\n\nJong, Peter de, C Sprenger, and Frans van Veen. 1984. “On Extreme\nValues of Moran’s i and Geary’s c.” Geographical\nAnalysis 16 (1): 17–24.\n\n\nJournel, Andre G. 1986. “Constrained Interpolation and Qualitative\nInformation—the Soft Kriging Approach.” Mathematical\nGeology 18 (3): 269–86.\n\n\nJournel, Andre G, and Charles J Huijbregts. 1976. “Mining\nGeostatistics.”\n\n\nJournel, André G. 1983. “Nonparametric Estimation of Spatial\nDistributions.” Journal of the International Association for\nMathematical Geology 15 (3): 445–68.\n\n\nJuang, Kai-Wei, and Dar-Yuan Lee. 1998. “Simple Indicator Kriging\nfor Estimating the Probability of Incorrectly Delineating Hazardous\nAreas in a Contaminated Site.” Environmental Science &\nTechnology 32 (17): 2487–93.\n\n\nKaplan, Elliott D, and Christopher Hegarty. 2017. Understanding\nGPS/GNSS: Principles and Applications. Artech house.\n\n\nKeefe, Matthew J., Marcelo A. Ferreira, and Christopher T. Franck.\n2018a. “On the Formal Specification of Sum-Zero Constrained\nIntrinsic Conditional Autoregressive Models.” Spatial\nStatistics 24: 54–65. https://doi.org/10.1016/j.spasta.2018.02.005.\n\n\nKeefe, Matthew J., Marco A. R. Ferreira, and Christopher T. Franck.\n2019. “Objective Bayesian Analysis for Gaussian Hierarchical\nModels with Intrinsic Conditional Autoregressive Priors.”\nBayesian Analysis 14 (1): 181–209. https://doi.org/10.1214/18-BA1107.\n\n\nKeefe, Matthew J, Marco AR Ferreira, and Christopher T Franck. 2018b.\n“On the Formal Specification of Sum-Zero Constrained Intrinsic\nConditional Autoregressive Models.” Spatial Statistics\n24: 54–65.\n\n\nKeefe, Michael J., Marco A. R. Ferreira, and Christopher T. Franck.\n2019. “Objective Bayesian Analysis for Gaussian Hierarchical\nModels with Intrinsic Conditional Autoregressive Priors.”\nBayesian Analysis 14 (1): 181–209. https://doi.org/10.1214/18-BA1107.\n\n\nKelejian, Harry H, and Ingmar R Prucha. 1998. “A Generalized\nSpatial Two-Stage Least Squares Procedure for Estimating a Spatial\nAutoregressive Model with Autoregressive Disturbances.” The\nJournal of Real Estate Finance and Economics 17 (1): 99–121.\n\n\n———. 1999. “A Generalized Moments Estimator for the Autoregressive\nParameter in a Spatial Model.” International Economic\nReview 40 (2): 509–33.\n\n\n———. 2010. “Specification and Estimation of Spatial Autoregressive\nModels with Autoregressive and Heteroskedastic Disturbances.”\nJournal of Econometrics 157 (1): 53–67.\n\n\nKelejian, Harry, and Gianfranco Piras. 2017. Spatial\nEconometrics. Academic Press.\n\n\nKlier, Thomas, and Daniel P McMillen. 2008. “Clustering of Auto\nSupplier Plants in the United States: Generalized Method of Moments\nSpatial Logit for Large Samples.” Journal of Business &\nEconomic Statistics 26 (4): 460–71.\n\n\nKoç, Tufan. 2022. “Bandwidth Selection in Geographically Weighted\nRegression Models via Information Complexity Criteria.”\nJournal of Mathematics.\n\n\nKrige, Danie, and Wynand Kleingeld. 2005. “The Genesis of\nGeostatistics in Gold and Diamond Industries.” In Space,\nStructure and Randomness: Contributions in Honor of Georges Matheron in\nthe Field of Geostatistics, Random Sets and Mathematical\nMorphology, 5–16. Springer.\n\n\nLangley, Richard B et al. 1999. “Dilution of Precision.”\nGPS World 10 (5): 52–59.\n\n\nLark, RM. 2000. “Estimating Variograms of Soil Properties by the\nMethod-of-Moments and Maximum Likelihood.” European Journal\nof Soil Science 51 (4): 717–28.\n\n\nLaslett, Geoffrey M. 1994. “Kriging and Splines: An Empirical\nComparison of Their Predictive Performance in Some Applications.”\nJournal of the American Statistical Association 89 (426):\n391–400.\n\n\nLeick, Alfred, Lev Rapoport, and Dmitry Tatarnikov. 2015. GPS\nSatellite Surveying. John Wiley & Sons.\n\n\nLeininger, Thomas J. 2014. “Bayesian Analysis of Spatial Point\nPatterns.” PhD thesis, Duke University.\n\n\nLeSage, James P. 1997. “Bayesian Estimation of Spatial\nAutoregressive Models.” International Regional Science\nReview 20 (1-2): 113–29.\n\n\n———. 2000. “Bayesian Estimation of Limited Dependent Variable\nSpatial Autoregressive Models.” Geographical Analysis 32\n(1): 19–35.\n\n\nLeSage, James, and Robert Kelley Pace. 2009. Introduction to Spatial\nEconometrics. Chapman; Hall/CRC.\n\n\nLi, Habin, and James F Reynolds. 1994. “A Simulation Experiment to\nQuantify Spatial Heterogeneity in Categorical Maps.”\nEcology 75 (8): 2446–55.\n\n\nLiesenfeld, Roman, Jean-François Richard, and Jan Vogler. 2013.\n“Analysis of Discrete Dependent Variable Models with Spatial\nCorrelation.” Economics Working Paper.\n\n\nLima, Renato Ribeiro de. 2005. “Modelagem Espaço\nTemporal Para Dados de Incidência de Doenças\nEm Plantas.” PhD thesis, Tese piracicaba-SP.\n\n\nLindgren, Finn, David Bolin, and Håvard Rue. 2022. “The SPDE\nApproach for Gaussian and Non-Gaussian Fields: 10 Years and Still\nRunning.” Spatial Statistics 50: 100599.\n\n\nLindgren, Finn, and Håvard Rue. 2015. “Bayesian Spatial Modelling\nwith r-INLA.” Journal of Statistical Software 63: 1–25.\n\n\nLindgren, Finn, Håvard Rue, and Johan Lindström. 2011. “An\nExplicit Link Between Gaussian Fields and Gaussian Markov Random Fields:\nThe Stochastic Partial Differential Equation Approach.”\nJournal of the Royal Statistical Society Series B: Statistical\nMethodology 73 (4): 423–98.\n\n\nLiu, Jun, K. W. Chau, and Zhen Bao. 2023. “Multiscale Spatial\nAnalysis of Metro Usage and Its Determinants for Sustainable Urban\nDevelopment in Shenzhen, China.” Tunnelling and Underground\nSpace Technology. https://doi.org/10.1016/j.tust.2022.104912.\n\n\nLoonis, Vincent, and Marie-Pierre de Bellefon. 2018. “Handbook of\nSpatial Analysis: Theory and Application with r.” Paris:\nEurostat, INSEE, 394.\n\n\nLu, Binbin, Martin Charlton, Paul Harris, and A Stewart Fotheringham.\n2014. “Geographically Weighted Regression with a Non-Euclidean\nDistance Metric: A Case Study Using Hedonic House Price Data.”\nInternational Journal of Geographical Information Science 28\n(4): 660–81.\n\n\nLu, Bo, Yong Ge, Yeqiao Shi, Jing Zheng, and Paul Harris. 2023.\n“Uncovering Drivers of Community-Level House Price Dynamics\nThrough Multiscale Geographically Weighted Regression: A Case Study of\nWuhan, China.” Spatial Statistics 53: 100723. https://doi.org/10.1016/j.spasta.2022.100723.\n\n\nMallows, Cohn L. 1995. “More Comments on Cp.”\nTechnometrics 37 (4): 362–72.\n\n\nMallows, Colin L. 1973. “Some Comments on Cp.”\nTechnometrics 15 (4): 661–75. https://doi.org/10.1080/00401706.1973.10489103.\n\n\nMarchant, BP, and RM Lark. 2007. “Robust Estimation of the\nVariogram by Residual Maximum Likelihood.” Geoderma 140\n(1-2): 62–72.\n\n\nMartinetti, Davide, and Ghislain Geniaux. 2017. “Approximate\nLikelihood Estimation of Spatial Probit Models.” Regional\nScience and Urban Economics 64: 30–45.\n\n\nMateu, Jorge, and Ramón Giraldo. 2022. “Introduction to\nGeostatistical Functional Data Analysis.” Geostatistical\nFunctional Data Analysis, 1–25.\n\n\nMateus, Ana Lúcia Souza Silva. 2013. “Proposição de Novas\nMetodologias Para Análise de Aleatoriedade Em Processos Pontuais No\nEspaço-Tempo.” Tese (Doutorado em Estatística e Experimentação\nAgropecuária), Lavras: Universidade Federal de Lavras.\n\n\nMatheron, G. 1963. “\" Principles of Geostatistics\", Economic\nGeology, 58, Pp 1246-1266.”\n\n\nMatheron, George. 1971. “The Theory of Regionalised Variables and\nIts Applications.” Les Cahiers Du Centre de Morphologie\nMathématique 5: 212.\n\n\nMcBratney, AB, and R Webster. 1986. “Choosing Functions for\nSemi-Variograms of Soil Properties and Fitting Them to Sampling\nEstimates.” Journal of Soil Science 37 (4): 617–39.\n\n\nMcMillen, Daniel P. 1992. “Probit with Spatial\nAutocorrelation.” Journal of Regional Science 32 (3):\n335–48.\n\n\nMiao, Xin, Fang Fang, Xuening Zhu, and Hansheng Wang. 2025.\n“Spatial Weights Matrix Selection and Model Averaging for\nMultivariate Spatial Autoregressive Models.” Econometric\nReviews, 1–31.\n\n\nMocnik, Franz-Benjamin. 2023. “Why We Can Read Maps.”\nCartography and Geographic Information Science 50 (1): 1–19.\n\n\nMohammadpour, Mahyadin, Abbas Bahroudi, Maysam Abedi, Gholamreza\nRahimipour, Golnaz Jozanikohan, and Farzaneh Mami Khalifani. 2019.\n“Geochemical Distribution Mapping by Combining Number-Size\nMultifractal Model and Multiple Indicator Kriging.” Journal\nof Geochemical Exploration 200: 13–26.\n\n\nMoller, Jesper, and Rasmus Plenge Waagepetersen. 2003. Statistical\nInference and Simulation for Spatial Point Processes. CRC press.\n\n\nMøller, Jesper, Anne Randi Syversveen, and Rasmus Plenge Waagepetersen.\n1998. “Log Gaussian Cox Processes.” Scandinavian\nJournal of Statistics 25 (3): 451–82.\n\n\nMøller, Jesper, and Rasmus P Waagepetersen. 2007. “Modern\nStatistics for Spatial Point Processes.” Scandinavian Journal\nof Statistics 34 (4): 643–84.\n\n\nMonmonier, Mark. 2005. “Lying with Maps.” Statistical\nScience, 215–22.\n\n\nMoraga, Paula. 2023. Spatial Statistics for Data Science: Theory and\nPractice with r. CRC Press.\n\n\nMoran, Patrick AP. 1950. “Notes on Continuous Stochastic\nPhenomena.” Biometrika 37 (1/2): 17–23.\n\n\nMoreno, Alvaro Alexander Burbano et al. 2023. “Functional Data\nAnalysis: Spatial Association of Curves and Irregular Spacing.”\n\n\nMorisita, Masaaki. 1959. “Measuring of the Dispersion of\nIndividuals and Analysis of the Distributional Patterns.”\nMemoirs of the Faculty of Science, E 2, Kyushu Univ. Ser.\n\n\nMyers, Donald E. 1994. “Spatial Interpolation: An\nOverview.” Geoderma 62 (1-3): 17–28.\n\n\nNhancololo, A. M. 2024b. “Processos Pontuais Espaciais Univariados\nAplicados à Distribuição de Espécies Arbóreas Em Florestas\nNaturais.” Dissertação (Mestrado em Estatística e Experimentação\nAgropecuária), Lavras: Universidade Federal de Lavras.\n\n\n———. 2024a. “Processos Pontuais Espaciais Univariados Aplicados à\nDistribuição de Espécies Arbóreas Em Florestas Naturais.”\nDissertação (Mestrado em Estatística e Experimentação Agropecuária),\nLavras: Universidade Federal de Lavras.\n\n\nNhancololo, A. M., Wélson A. Oliveira, Fernandes A. C. Pereira, Bruno\nMontoani Silva, and João Domingos Scalon. 2024. “Comparison\nBetween the Laboratory Method and the Stolf Penetrometer in Soil Density\nAnalysis: A Study Using Geostatistical Approaches.”\nSigmae 13 (1): 63–78. https://doi.org/10.29327/2520355.13.1-7.\n\n\nNightingale, Glenna F, Janine B Illian, Ruth King, and Peter\nNightingale. 2019. “Area Interaction Point Processes for Bivariate\nPoint Patterns in a Bayesian Context.” Journal of\nEnvironmental Statistics 9 (2).\n\n\nOkabe, Masahilo, and Masaharu Tanemura. 2006. “Bayesian Estimation\nof Soft-Core Potential Models for Spatial Point Patterns.”\nJournal of the Japan Statistical Society 36 (2): 121–47.\n\n\nOliver, MA, and R Webster. 2014. “A Tutorial Guide to\nGeostatistics: Computing and Modelling Variograms and Kriging.”\nCatena 113: 56–69.\n\n\nOpenshaw, Stan. 1984. “The Modifiable Areal Unit Problem.”\nConcepts and Techniques in Modern Geography.\n\n\nOshan, Taylor M., James P. Smith, and A. Stewart Fotheringham. 2020.\n“Targeting the Spatial Context of Obesity Determinants via\nMultiscale Geographically Weighted Regression.” International\nJournal of Health Geographics 19 (1): 11. https://doi.org/10.1186/s12942-020-00204-8.\n\n\nParadis, Emmanuel. 2005. R for Beginners. https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf.\n\n\nPaula, Gilberto A. 2013. “On Diagnostics in Double Generalized\nLinear Models.” Computational Statistics & Data\nAnalysis 68: 44–51. https://doi.org/10.1016/j.csda.2013.07.003.\n\n\n———. 2025. Modelos de Regressão e\nAplicações. São Paulo:\nInstituto de Matemática e Estatı́stica,\nUniversidade de São Paulo.\n\n\nPebesma, Edzer. 2018. “Simple Features for r: Standardized Support\nfor Spatial Vector Data.”\n\n\nPebesma, Edzer J. 2004. “Multivariable Geostatistics in s: The\nGstat Package.” Computers & Geosciences 30 (7):\n683–91.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With\nApplications in r. Chapman; Hall/CRC.\n\n\nPeng, Zhan, and Ryo Inoue. 2024. “Multiscale Continuous and\nDiscrete Spatial Heterogeneity Analysis: The Development of a Local\nModel Combining Eigenvector Spatial Filters and Generalized Lasso\nPenalties.” Geographical Analysis 56 (2): 303–27.\n\n\nPereira, Rafael HM, and Caio Nogueira Goncalves. 2024. “Geobr:\nDownload Official Spatial Data Sets of Brazil.” R Package\nVersion 1 (0): 18.\n\n\nPinkse, Joris, and Margaret E Slade. 1998. “Contracting in Space:\nAn Application of Spatial Statistics to Discrete-Choice Models.”\nJournal of Econometrics 85 (1): 125–54.\n\n\nPrestby, Timothy J. 2025. “Trust in Maps: What We Know and What We\nNeed to Know.” Cartography and Geographic Information\nScience 52 (1): 1–18.\n\n\nRamsay, James O, and Bernard W Silverman. 2005. Functional Data\nAnalysis. Springer.\n\n\nReich, Brian J, James S Hodges, and Vesna Zadnik. 2006. “Effects\nof Residual Smoothing on the Posterior of the Fixed Effects in\nDisease-Mapping Models.” Biometrics 62 (4): 1197–1206.\n\n\nRibeiro Jr, Paulo J, and Peter J Diggle. 2006. “geoR: Package for\nGeostatistical Data Analysis an Illustrative Session.”\nArtificial Intelligence 1: 1–24.\n\n\nRibeiro Jr, Paulo Justiniano, and Peter Diggle. 2025. geoR: Analysis\nof Geostatistical Data. https://doi.org/10.32614/CRAN.package.geoR.\n\n\nRichardson, Alice M, and Alan H Welsh. 1995. “Robust Restricted\nMaximum Likelihood in Mixed Linear Models.” Biometrics,\n1429–39.\n\n\nRiebler, Andrea, Sigrunn H Sørbye, Daniel Simpson, and Håvard Rue. 2016.\n“An Intuitive Bayesian Spatial Model for Disease Mapping That\nAccounts for Scaling.” Statistical Methods in Medical\nResearch 25 (4): 1145–65.\n\n\nRigby, Robert A., Mikis D. Stasinopoulos, Gillian Z. Heller, and Frosso\nDe Bastiani. 2019. Distributions for Modeling Location, Scale, and\nShape: Using GAMLSS in R. Boca Raton:\nChapman; Hall/CRC.\n\n\nRue, Havard, and Leonhard Held. 2005. Gaussian Markov Random Fields:\nTheory and Applications. Chapman; Hall/CRC.\n\n\nRue, Håvard, and Leonhard Held. 2005. Gaussian Markov Random Fields:\nTheory and Applications. Chapman & Hall/CRC Monographs on\nStatistics & Applied Probability. Boca Raton: Chapman; Hall/CRC.\n\n\nRue, Håvard, Sara Martino, and Nicolas Chopin. 2009. “Approximate\nBayesian Inference for Latent Gaussian Models by Using Integrated Nested\nLaplace Approximations.” Journal of the Royal Statistical\nSociety Series B: Statistical Methodology 71 (2): 319–92.\n\n\nSachdeva, Manish, A. Stewart Fotheringham, and Zhen Li. 2022. “Do\nPlaces Have Value? Quantifying the Intrinsic Value of\nHousing Neighborhoods Using MGWR.” Journal of\nHousing Research 31 (1): 24–52.\n\n\nSahu, Sujit. 2022. Bayesian Modeling of Spatio-Temporal Data with\nr. Chapman; Hall/CRC.\n\n\nScalon, João Domingos. 2024. Análise de Dados Espaciais Com\nAplicações Em r. Lavras: Ed. UFLA.\n\n\nScalon, João Domingos, Victor Ferreira da Silva, Wélson Antônio de\nOliveira, and Mateus Santos Peixoto. 2022. “Statistical\nCharacterization of Spatial and Size Distributions of Particles in\nComposite Materials Used in the Manufacturing of Biomedical\nInstruments.” Brazilian Journal of Biometrics 40 (4):\n428–41.\n\n\nSchmidt, Alexandra M, and Anthony O’Hagan. 2003. “Bayesian\nInference for Non-Stationary Spatial Covariance Structure via Spatial\nDeformations.” Journal of the Royal Statistical Society\nSeries B: Statistical Methodology 65 (3): 743–58.\n\n\nShareef, Hitham, A. A. Ibrahim, and A. H. Mutlag. 2015. “Lightning\nSearch Algorithm.” Applied Soft Computing 36: 315–33.\n\n\nSimpson, Daniel, Håvard Rue, Andrea Riebler, Thiago G Martins, and\nSigrunn H Sørbye. 2017. “Penalising Model Component Complexity: A\nPrincipled, Practical Approach to Constructing Priors.”\n\n\nSlowikowski, Kamil. 2024. Ggrepel: Automatically Position\nNon-Overlapping Text Labels with ’Ggplot2’. https://doi.org/10.32614/CRAN.package.ggrepel.\n\n\nSnyder, John Parr. 1987. Map Projections–a Working Manual. Vol.\n1395. US Government Printing Office.\n\n\nSørbye, Sigrunn Holbek, and Håvard Rue. 2017. “Penalised\nComplexity Priors for Stationary Autoregressive Processes.”\nJournal of Time Series Analysis 38 (6): 923–35.\n\n\nSørbye, Sigrunn H., and Håvard Rue. 2014. “Scaling Intrinsic\nGaussian Markov Random Field Priors in Spatial Modelling.”\nSpatial Statistics 8: 39–51. https://doi.org/10.1016/j.spasta.2013.06.004.\n\n\nStasinopoulos, Mikis D., Thomas Kneib, Nadja Klein, Andreas Mayr, and\nGillian Z. Heller. 2024. Generalized Additive Models for Location,\nScale and Shape: A Distributional Regression Approach, with\nApplications. Vol. 56. Cambridge: Cambridge University Press.\n\n\nStasinopoulos, Mikis D., Robert A. Rigby, Gillian Z. Heller, Vlasios\nVoudouris, and Frosso De Bastiani. 2017. Flexible Regression and\nSmoothing: Using GAMLSS in R. Chapman\n& Hall/CRC Texts in Statistical Science. Boca Raton: CRC Press.\n\n\nSuster, Carl. 2024. Ggmapinset: Add Inset Panels to Maps. https://doi.org/10.32614/CRAN.package.ggmapinset.\n\n\nTiefelsdorf, Michael. 2000. Modelling Spatial Processes: The\nIdentification and Analysis of Spatial Relationships in Regression\nResiduals by Means of Moran’s i. Springer.\n\n\nTobias, Justin L. 2024. “A Note on the Cowles-EM Algorithm for\nBayesian Ordinal Probit Models.” Research in Statistics\n3 (1): 2476409.\n\n\nTobler, Waldo R. 1970. “A Computer Movie Simulating Urban Growth\nin the Detroit Region.” Economic Geography 46 (sup1):\n234–40.\n\n\nUngaro, F, F Ragazzi, R Cappellin, and P Giandon. 2008. “Arsenic\nConcentration in the Soils of the Brenta Plain (Northern Italy): Mapping\nthe Probability of Exceeding Contamination Thresholds.”\nJournal of Geochemical Exploration 96 (2-3): 117–31.\n\n\nUnwin, David J, and Leslie W Hepple. 1974. “The Statistical\nAnalysis of Spatial Series.” Journal of the Royal Statistical\nSociety: Series D (The Statistician) 23 (3-4): 211–27.\n\n\nVan Lieshout, MNM, and AJ1422574 Baddeley. 1996. “A Nonparametric\nMeasure of Spatial Interaction in Point Patterns.” Statistica\nNeerlandica 50 (3): 344–61.\n\n\nVanicek, Petr, and Edward J Krakiwsky. 2015. Geodesy: The\nConcepts. Elsevier.\n\n\nVenables, W. N., D. M. Smith, and R Core Team. 2015. An Introduction\nto r. Vienna, Austria: R Foundation for Statistical Computing. https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf.\n\n\nVer Hoef, Jay M, and Ronald Paul Barry. 1998. “Constructing and\nFitting Models for Cokriging and Multivariable Spatial\nPrediction.” Journal of Statistical Planning and\nInference 69 (2): 275–94.\n\n\nVer Hoef, Jay M, Ephraim M Hanks, and Mevin B Hooten. 2018. “On\nthe Relationship Between Conditional (CAR) and Simultaneous (SAR)\nAutoregressive Models.” Spatial Statistics 25: 68–85.\n\n\nWaagepetersen, Rasmus, and Yongtao Guan. 2009. “Two-Step\nEstimation for Inhomogeneous Spatial Point Processes.”\nJournal of the Royal Statistical Society Series B: Statistical\nMethodology 71 (3): 685–702.\n\n\nWackernagel, Hans. 2003. Multivariate Geostatistics: An Introduction\nwith Applications. Springer Science & Business Media.\n\n\nWagner, Helene H, and Marie-Josée Fortin. 2005. “Spatial Analysis\nof Landscapes: Concepts and Statistics.” Ecology 86 (8):\n1975–87.\n\n\nWall, Melanie M. 2004. “A Close Look at the Spatial Structure\nImplied by the CAR and SAR Models.” Journal of Statistical\nPlanning and Inference 121 (2): 311–24.\n\n\nWaller, Lance A. 2024. “Maps: A Statistical View.”\nAnnual Review of Statistics and Its Application 11.\n\n\nWang, Jane-Ling, Jeng-Min Chiou, and Hans-Georg Müller. 2016.\n“Functional Data Analysis.” Annual Review of Statistics\nand Its Application 3 (1): 257–95.\n\n\nWang, Xiaokun, and Kara M Kockelman. 2009. “Baysian Inference for\nOrdered Response Data with a Dynamic Spatial-Ordered Probit\nModel.” Journal of Regional Science 49 (5): 877–913.\n\n\nWheeler, David, and Michael Tiefelsdorf. 2005. “Multicollinearity\nand Correlation Among Local Regression Coefficients in Geographically\nWeighted Regression.” Journal of Geographical Systems 7\n(2): 161–87. https://doi.org/10.1007/s10109-005-0155-6.\n\n\nWhittle, Peter. 1954. “On Stationary Processes in the\nPlane.” Biometrika, 434–49.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. 2nd ed. Springer-Verlag New York.\n\n\n———. 2019. Advanced r. 2nd ed. CRC Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualize, and Model\nData. 2nd ed. O’Reilly Media.\n\n\nWiegand, Thorsten, and Kirk A Moloney. 2014. Handbook of Spatial\nPoint-Pattern Analysis in Ecology. CRC press.\n\n\nWild, Christopher J, Jessica M Utts, and Nicholas J Horton. 2017.\n“What Is Statistics?” In International Handbook of\nResearch in Statistics Education, 5–36. Springer.\n\n\nWilhelm, Stefan, and Miguel Godinho de Matos. 2013. “Estimating\nSpatial Probit Models in r.”\n\n\nWu, Chengdong, Feng Ren, Wei Hu, and Qingyun Du. 2019. “Multiscale\nGeographically and Temporally Weighted Regression: Exploring the\nSpatiotemporal Determinants of Housing Prices.” International\nJournal of Geographical Information Science 33 (3): 489–511. https://doi.org/10.1080/13658816.2018.1528246.\n\n\nYamamoto, Jorge Kazuo, and Paulo M. Barbosa Landim. 2013.\nGeoestatística: Conceitos e Aplicações. São Paulo: Oficina de\nTextos.\n\n\nYang, Wenbai, A. Stewart Fotheringham, and Paul Harris. 2011. “An\nExtension of Geographically Weighted Regression with Flexible\nBandwidths.” In Proceedings of an International Conference on\nSpatial Analysis and Modelling. St Andrews, Scotland, UK.\n\n\nYang, Wenbo. 2014. “An Extension of Geographically Weighted\nRegression with Flexible Bandwidths.” PhD thesis, University of\nSt Andrews.\n\n\nYu, Hanchen, A. Stewart Fotheringham, Zhen Li, Taylor M. Oshan, Wei\nKang, and Levi J. Wolf. 2020. “Inference in Multiscale\nGeographically Weighted Regression.” Geographical\nAnalysis 52 (1): 87–106.\n\n\nYu, Huili, A. Stewart Fotheringham, Zhenlong Li, Taylor M. Oshan, and\nLevi J. Wolf. 2020. “On the Measurement of Bias in Geographically\nWeighted Regression Models.” Spatial Statistics 38:\n100453. https://doi.org/10.1016/j.spasta.2020.100453.\n\n\nZhang, Xinyu, and Jihai Yu. 2018. “Spatial Weights Matrix\nSelection and Model Averaging for Spatial Autoregressive Models.”\nJournal of Econometrics 203 (1): 1–18.",
    "crumbs": [
      "Referências Bibliográficas"
    ]
  }
]